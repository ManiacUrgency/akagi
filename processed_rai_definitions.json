{
    "papers": [
        {
            "paper_title": "Responsible-AI-by-design: A pattern collection for designing responsible AI systems",
            "rai_definition_1": "The research paper conceptualizes Responsible AI by focusing on ethical principles and system-level design patterns that can be embedded into AI systems. The paper does not provide an explicit definition of Responsible AI but instead uses a conceptual framework based on ethical principles to guide the design and development of AI systems.\n\t\n\tThe ethical principles used in the paper are derived from Harvard University’s mapping study and include Privacy, Accountability, Safety & Security, Transparency & Explainability, Fairness and Non-discrimination, Human Control of Technology, and Promotion of Human Values . These principles serve as the foundation for identifying reusable design patterns that architects and developers can use to ensure that AI systems behave and make decisions responsibly.\n\t\n\tThe primary sub-pillar emphasized in the paper is the system-level guidance for designing the architecture of responsible AI systems. The authors argue that responsible AI issues often go beyond data and algorithms and are crosscutting many system components and the entire software engineering lifecycle . This focus on system-level design patterns is intended to provide practical, actionable guidance that can be embedded into AI systems as product features, thereby operationalizing the high-level ethical principles.\n\t\n\tSecondary sub-pillars discussed include the various ethical principles themselves, such as Privacy, Accountability, and Transparency & Explainability. These principles are given equal emphasis as they collectively contribute to a comprehensive understanding of Responsible AI .\n\t\n\tThe paper's focus on specific sub-pillars, particularly the system-level design patterns, shapes its overall approach to Responsible AI by providing concrete solutions that can be implemented during the development process. This approach aims to bridge the gap between high-level ethical guidelines and practical implementation, ensuring that AI systems are designed to be responsible from the ground up.\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a framework of ethical principles and emphasizes system-level design patterns to operationalize these principles in AI systems.",
            "reference": "Qin Lu, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. https://arxiv.org/pdf/2203.00905"
        },
        {
            "paper_title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
            "rai_definition_1": "The research paper conceptualizes Responsible AI as a multifaceted paradigm that necessitates the systematic adoption of several AI principles to ensure the practical and ethical deployment of AI models. These principles include fairness, transparency, accountability, and privacy . The paper emphasizes that Responsible AI is not solely about explainability but also involves a broader set of ethical considerations that must be integrated into AI systems.\n\t\n\tThe paper does not provide a single explicit definition of Responsible AI but rather discusses it within a broader conceptual framework that includes various sub-pillars. The primary sub-pillar emphasized is explainability, which is considered crucial for understanding and managing AI systems effectively. Explainability is defined as the ability to provide a direct understanding of the mechanisms by which a model works, which is essential for ensuring trustworthiness, fairness, and robustness in AI systems .\n\t\n\tSecondary sub-pillars discussed include fairness, accountability, and privacy. Fairness involves ensuring that AI systems do not lead to discrimination against individuals or groups based on race, religion, gender, or other personal conditions. Accountability refers to the responsibility of AI developers and users to ensure that AI systems are used ethically and transparently. Privacy involves safeguarding personal data and ensuring that AI systems comply with data protection standards .\n\t\n\tThe paper's focus on these specific sub-pillars shapes its overall approach to Responsible AI by advocating for a comprehensive framework that addresses multiple ethical dimensions. This approach ensures that AI systems are not only technically proficient but also socially and ethically responsible, thereby fostering greater trust and acceptance among users and stakeholders .\n\t\n\tIn summary, Responsible AI in this research paper is conceptualized through a combination of explainability, fairness, accountability, and privacy, with a particular emphasis on explainability as a key aspect of ethical AI deployment.",
            "reference": "Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
        },
        {
            "paper_title": "Principles to practices for responsible AI: closing the gap",
            "rai_definition_1": "The research paper conceptualizes Responsible AI through a comprehensive framework that aims to bridge the gap between high-level principles and practical implementation. The framework is designed to be broad, operationalizable, flexible, iterative, guided, and participatory . \n\t\n\tThe paper does not provide a single explicit definition of Responsible AI but instead discusses it in terms of broader concepts and sub-pillars. The primary sub-pillar emphasized is the impact assessment framework, particularly the IEEE 7010 standard, which assesses the well-being implications of AI systems . This framework is seen as a promising approach to ensure that AI systems are designed and deployed responsibly, considering their impacts on human well-being comprehensively.\n\t\n\tSecondary sub-pillars include various ethical considerations such as fairness, accountability, transparency, and privacy. These are often mentioned in the context of the broader framework but are not given equal emphasis as the impact assessment approach . \n\t\n\tThe focus on the impact assessment framework shapes the overall approach to Responsible AI by ensuring that the ethical and social implications of AI systems are systematically evaluated and addressed throughout their lifecycle. This approach helps to operationalize high-level principles into specific, actionable strategies that can be implemented in real-world systems .\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a multi-faceted framework that integrates various ethical considerations and emphasizes the importance of impact assessments to ensure comprehensive and practical implementation of responsible AI practices.",
            "reference": "David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
        },
        {
            "paper_title": "Responsible AI by design in practice",
            "rai_definition_1": "The research paper conceptualizes Responsible AI through a methodology called \"Responsible AI by Design,\" which is intended for organizations planning to use AI on a large scale and aims to prevent undesired side effects such as unfair discrimination and lack of interpretability of AI conclusions . This methodology is built on several key components:\n\t\n\t1. **AI Principles**: These principles set the values and boundaries for AI development and use within the organization. They are aligned with broader ethical guidelines but tailored to the specific needs and context of the organization .\n\t\n\t2. **Awareness and Training**: The methodology emphasizes the importance of awareness campaigns and training programs to educate employees about AI, its potential impacts, and the principles of Responsible AI. This includes both technical and non-technical training tailored to different roles within the organization .\n\t\n\t3. **Development Process Questions**: A set of questions and checkpoints ensures that all AI principles are considered during the AI development process. These questions are designed to be answered by the responsible persons within the organization, often requiring specific tools and training to address them effectively .\n\t\n\t4. **Specific Tools**: Tools are provided to help answer the development process questions and mitigate any identified problems. These tools are essential for making justifiable statements about aspects like fairness and explainability .\n\t\n\t5. **Governance Process**: A governance model defines responsibilities and accountability within the organization. This model is designed to be agile, delegating responsibility to those directly involved with AI products and services while providing support and oversight .\n\t\n\tThe paper also discusses the broader ethical considerations and societal impacts of AI, such as bias, discrimination, interpretability, transparency, and accountability. These considerations are integrated into the Responsible AI by Design methodology to ensure that AI systems are developed and used in a manner that is ethically sound and socially responsible .\n\t\n\tThe primary sub-pillar emphasized in the paper is **explainability**, which is considered crucial across various domains, including telecommunications. The paper highlights the need for explanations to be tailored to different profiles and transparency levels required by the domain and regulations, such as GDPR . Explainability is seen as a key component of Responsible AI, as it helps build trust and understanding among users and stakeholders.\n\t\n\tSecondary sub-pillars discussed include **fairness** and **accountability**, which are also integral to the methodology. Fairness is addressed through tools and training that help identify and mitigate bias, while accountability is ensured through the governance model that assigns clear responsibilities and supports a culture of self-control and awareness .\n\t\n\tOverall, the paper's focus on these specific sub-pillars and broader conceptual frameworks shapes its approach to Responsible AI by providing a structured and comprehensive methodology that organizations can adopt to ensure their AI systems are ethical and responsible.",
            "reference": "Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
        },
        {
            "paper_title": "Socially responsible ai algorithms: Issues, purposes, and challenges",
            "rai_definition_1": "The research paper conceptualizes and defines Responsible AI through the lens of \"Social Responsibility of AI\" (SRA), which encompasses a broad range of responsibilities and principles. The paper provides an inclusive definition and introduces a hierarchical framework to outline these responsibilities.\n\t\n\t### Definition and Conceptual Framework\n\t\n\tThe paper defines Social Responsibility of AI as a comprehensive concept that includes principles such as fairness and inclusiveness, means like socially responsible AI algorithms (SRAs), and objectives aimed at improving humanity . The authors emphasize that the functional and societal aspects are integral parts of AI algorithms, aiming to make just and trustworthy decisions while meeting business objectives .\n\t\n\t### Pyramid of Social Responsibility of AI\n\t\n\tThe paper adapts Carroll’s Pyramid of Corporate Social Responsibility (CSR) to the AI context, proposing a pyramid with four levels of responsibilities:\n\t\n\t1. **Functional Responsibilities**: These are the foundational responsibilities requiring AI systems to perform efficiently and maximize profits while meeting key performance indicators .\n\t2. **Legal Responsibilities**: AI systems must comply with laws and regulations, ensuring their actions are consistent with societal expectations codified in law .\n\t3. **Ethical Responsibilities**: AI systems have an obligation to do what is right, just, and fair, and to prevent or mitigate negative impacts on stakeholders .\n\t4. **Philanthropic Responsibilities**: AI systems are expected to contribute positively to society, addressing challenges such as healthcare and climate change, although these are not ethically mandatory .\n\t\n\t### Emphasis on Ethical and Philanthropic Responsibilities\n\t\n\tThe paper particularly focuses on the ethical and philanthropic responsibilities of AI. Ethical responsibilities are highlighted as essential for preventing harm and ensuring fairness and justice in AI systems . Philanthropic responsibilities, while not ethically required, are seen as important for enhancing the quality of life and addressing societal challenges .\n\t\n\t### Influence on the Overall Approach\n\t\n\tThe emphasis on ethical and philanthropic responsibilities shapes the paper's approach to Responsible AI by advocating for a holistic understanding that goes beyond technical performance and legal compliance. This broader perspective aims to ensure that AI systems are developed and deployed in ways that are socially beneficial and ethically sound .\n\t\n\t### Conclusion\n\t\n\tIn summary, the paper defines Responsible AI through a multi-dimensional framework that includes functional, legal, ethical, and philanthropic responsibilities. The primary emphasis on ethical responsibilities underscores the importance of fairness, justice, and harm prevention, while philanthropic responsibilities highlight the role of AI in contributing to societal well-being.",
            "reference": "Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
        },
        {
            "paper_title": "The role and challenges of education for responsible AI",
            "rai_definition_1": "The research paper conceptualizes Responsible AI as a socio-technical system that integrates both technical and social components, emphasizing the importance of human values and ethical principles in the development, deployment, and use of AI systems. It explicitly states that AI systems are more than just their software components; they are socio-technical systems that include the social context in which they are developed and used, involving various stakeholders, institutions, cultures, norms, and spaces .\n\t\n\tThe paper does not provide a single explicit definition of Responsible AI but instead frames it within the ART (Accountability, Responsibility, Transparency) principles. These principles are meant to guide the socio-technical AI system rather than focusing solely on the software .\n\t\n\t1. **Accountability**: This principle emphasizes the need for AI systems to explain and justify their decisions to users and other relevant actors. It requires that the decision-making mechanisms be transparent and that the moral values and societal norms informing the system's purpose be openly elicited from all stakeholders .\n\t\n\t2. **Responsibility**: This principle focuses on the role of people and organizations in relation to AI systems. It highlights the need to link an AI system’s decisions to its input data and the actions of stakeholders involved in the system’s decision-making process. Responsibility is not just about governing intelligent machines but also about the role of people and institutions in the effects of developing and using the system .\n\t\n\t3. **Transparency**: This principle involves the capability to describe, inspect, and reproduce the mechanisms through which AI systems make decisions and learn to adapt to their environment. Transparency also includes being explicit and open about choices and decisions concerning data sources, development processes, and stakeholder involvement .\n\t\n\tThe primary sub-pillar emphasized in the paper is **Responsibility**, which underscores the human and institutional roles in AI systems. This focus on responsibility contributes to a comprehensive understanding of Responsible AI by ensuring that ethical considerations are not just about the technology itself but about the broader socio-technical system and the people who interact with and are affected by it .\n\t\n\tSecondary sub-pillars such as **Accountability** and **Transparency** are also discussed, and they are given significant emphasis. These principles collectively shape the paper's overall approach to Responsible AI by advocating for a socio-technical perspective that integrates ethical governance with technical development .\n\t\n\tIn summary, the paper's focus on the ART principles and the socio-technical framework influences its approach to Responsible AI by ensuring that ethical considerations are deeply embedded in both the technical and social aspects of AI systems.",
            "reference": "Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
        },
        {
            "paper_title": "How different groups prioritize ethical values for responsible AI",
            "rai_definition_1": "The research paper by Jakesch et al.  does not provide an explicit definition of Responsible AI. Instead, it conceptualizes Responsible AI through a framework of ethical values derived from various AI ethics guidelines. The paper identifies and examines twelve responsible AI values, which are transparency, fairness, safety, accountability, privacy, autonomy, performance, justice, non-maleficence, beneficence, freedom, and dignity .\n\t\n\tThe study emphasizes the importance of these values by exploring how different groups prioritize them. For instance, the paper highlights that AI practitioners, on average, rated responsible AI values less important than other groups, with a particular emphasis on fairness. In contrast, participants from a US-census representative sample prioritized safety, privacy, and performance more highly .\n\t\n\tThe primary sub-pillar emphasized in the paper is fairness, especially among AI practitioners. This focus on fairness contributes to a comprehensive understanding of Responsible AI by highlighting the need to ensure that AI systems do not perpetuate biases and are equitable in their decision-making processes. The secondary sub-pillars discussed include safety, privacy, and performance, which are also given significant attention, particularly by the general public and crowdworker samples .\n\t\n\tThe paper's approach to Responsible AI is shaped by its empirical investigation into how different demographic groups and professional backgrounds prioritize these values. This empirical approach aims to contextualize and probe the ethical intuitions and assumptions of AI practitioners, thereby increasing the context sensitivity of the responsible AI development process .\n\t\n\tIn summary, while the paper does not provide a single, explicit definition of Responsible AI, it offers a detailed conceptual framework based on a set of ethical values and explores how these values are prioritized by different groups. This approach underscores the importance of considering diverse perspectives in the development and implementation of AI systems to ensure they are responsible and aligned with broader societal values.",
            "reference": "Julian Jakesch, Zana Buçinca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
        },
        {
            "paper_title": "Measurement as governance in and for responsible AI",
            "rai_definition_1": "The research paper conceptualizes Responsible AI through a multifaceted lens, emphasizing the importance of governance, measurement, and the inherent feedback loops within sociotechnical systems. It does not provide a single explicit definition of Responsible AI but rather frames it within broader conceptual frameworks and sub-pillars.\n\t\n\t### Conceptual Frameworks and Broader Categories\n\t\n\t1. **Governance**: The paper discusses governance in and for Responsible AI extensively. Governance decisions are crucial in shaping what an AI system can and cannot do, including decisions about data usage, prediction targets, and accessibility . Governance is also about ensuring that systems are fair, robust, and responsible, which involves making explicit the assumptions and decisions that underpin these systems .\n\t\n\t2. **Measurement as Governance**: The paper introduces the concept of measurement as a form of governance. This perspective integrates critical and social science views into the development of Responsible AI, highlighting that measurement processes are where governance happens implicitly . By understanding measurement as governance, researchers and practitioners can better develop Responsible AI systems .\n\t\n\t### Sub-pillars of Responsible AI\n\t\n\t1. **Fairness**: Fairness is described as an essentially contested construct with diverse and conflicting understandings. The paper highlights the challenges in operationalizing fairness and the importance of content and consequential validity in ensuring that fairness-related harms are mitigated .\n\t\n\t2. **Robustness**: Robustness is concerned with how well a system performs across different contexts and whether it does what it claims to do. The paper discusses the political dimensions of technical decisions about model performance and the importance of considering the broader impacts of these decisions .\n\t\n\t3. **Responsibility**: Responsibility is framed around the concept of harms, emphasizing the need to mitigate fairness-related harms and the importance of accountability through better auditing benchmarks . The paper also discusses the role of human rights perspectives in fundamentally addressing fairness-related harms .\n\t\n\t### Primary and Secondary Sub-pillars\n\t\n\t- **Primary Sub-pillar**: Governance is the most prominently featured sub-pillar. The paper's focus on governance shapes its overall approach to Responsible AI by emphasizing the need for explicit decision-making processes and the importance of understanding the feedback loops inherent in measurement processes .\n\t\n\t- **Secondary Sub-pillars**: Fairness, robustness, and responsibility are also discussed, but they are framed within the broader context of governance and measurement. These sub-pillars are given significant attention, but governance remains the central theme that ties them together .\n\t\n\t### Influence on Overall Approach\n\t\n\tThe paper's focus on governance and measurement as central themes influences its approach to Responsible AI by advocating for a more explicit and transparent decision-making process. This approach aims to uncover hidden governance decisions and ensure that AI systems are developed and evaluated with a comprehensive understanding of their social, cultural, and political implications .\n\t\n\tIn summary, the paper conceptualizes Responsible AI through the lenses of governance and measurement, emphasizing the importance of explicit decision-making processes and the need to consider the broader impacts of AI systems. Governance is the primary sub-pillar, with fairness, robustness, and responsibility also playing crucial roles in shaping a comprehensive understanding of Responsible AI.",
            "reference": "AZ Jacobs. 2021. Measurement as governance in and for responsible AI. arXiv:2109.05658. Retrieved from https://arxiv.org/pdf/2109.05658"
        },
        {
            "paper_title": "Toward an understanding of responsible artificial intelligence practices",
            "rai_definition_1": "Responsible AI is conceptualized in the research paper as a governance framework designed to harness, deploy, evaluate, and monitor AI systems to create new opportunities for better service provision. It emphasizes the importance of designing and implementing ethical, transparent, and accountable AI solutions that help maintain individual trust and minimize privacy invasion . The framework places humans, particularly end-users, at the center and aims to meet stakeholder expectations and comply with applicable regulations and laws .\n\t\n\tThe paper identifies four primary practices of responsible AI: (1) Data governance, (2) Ethically designed solutions, (3) Human-centric surveillance/risk control, and (4) Training and education . These practices are illustrated through real-world cases and are essential for driving ethics and trust in AI use .\n\t\n\tThe primary sub-pillar emphasized in the paper is \"ethically designed solutions,\" which involves creating AI systems that are transparent, accountable, and aligned with ethical standards . This sub-pillar contributes to a comprehensive understanding of Responsible AI by ensuring that AI systems operate in a manner that is fair, unbiased, and respects user privacy and autonomy.\n\t\n\tSecondary sub-pillars include data governance, which ensures the integrity and security of data used by AI systems; human-centric surveillance/risk control, which focuses on monitoring AI systems to mitigate risks; and training and education, which aims to equip stakeholders with the knowledge and skills needed to implement and manage responsible AI . These sub-pillars are given equal emphasis and collectively contribute to the overall approach to Responsible AI by addressing various aspects of AI system development and deployment.\n\t\n\tThe paper's focus on these specific sub-pillars shapes its overall approach to Responsible AI by providing a structured framework that organizations can follow to ensure their AI systems are ethical, transparent, and accountable. This approach not only addresses technical and functional aspects of AI but also incorporates broader ethical considerations, thereby promoting trust and acceptance among users and stakeholders .",
            "reference": "Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
        },
        {
            "paper_title": "The role of cooperation in responsible AI development",
            "rai_definition_1": "The research paper conceptualizes Responsible AI primarily through the lens of safety, security, and social impact. It defines Responsible AI development as the process of ensuring that AI systems have an acceptably low risk of causing harm to users or society and ideally increasing their likelihood of being socially beneficial. This involves rigorous testing for safety and security during development, evaluating potential social impacts before release, and being willing to abandon or delay projects that do not meet high safety standards .\n\t\n\tThe paper emphasizes three main sub-pillars of Responsible AI: safety, security, and structural risks. \n\t\n\t1. **Safety**: This sub-pillar focuses on mitigating accident risks and ensuring that AI systems function as intended and behave in ways that are desirable to people . The importance of safety is highlighted by the need to prevent accidents and ensure reliable performance.\n\t\n\t2. **Security**: This sub-pillar aims to prevent AI systems from being attacked, co-opted, or misused by malicious actors . Security is crucial to protect AI systems from external threats and ensure their integrity.\n\t\n\t3. **Structural Risks**: This sub-pillar addresses the broader societal impacts of AI, such as job displacement, military conflict, and threats to political and social institutions . It involves identifying and mitigating long-term risks that do not fit neatly into the categories of accidents or misuse.\n\t\n\tThe paper's primary focus on these sub-pillars shapes its overall approach to Responsible AI by emphasizing the need for comprehensive risk management and cooperation among AI companies to address collective action problems. The authors argue that competitive pressures could incentivize companies to underinvest in safety, security, and social impact, making cooperation essential for responsible development .\n\t\n\tSecondary sub-pillars, such as the economic and social incentives for responsible development, are also discussed. The paper notes that responsible development may come at a cost to companies, including the need for additional resources for data collection, system testing, and social impact research . These costs highlight the importance of creating incentives and strategies to encourage companies to invest in responsible AI development despite competitive pressures.\n\t\n\tIn summary, the paper defines Responsible AI through a framework that includes safety, security, and structural risks, with a strong emphasis on cooperation and collective action to address these challenges. This approach underscores the need for a multi-faceted strategy to ensure that AI systems are developed in a manner that is safe, secure, and beneficial to society.",
            "reference": "Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
        },
        {
            "paper_title": "Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices",
            "rai_definition_1": "The research paper conceptualizes Responsible AI through a multi-faceted thematic analysis that encompasses prevalent, emerging, and aspirational practices within organizations. It does not provide a single explicit definition of Responsible AI but rather frames it within broader categories and sub-pillars that include both competence/functionality and ethical considerations.\n\t\n\t### Conceptual Frameworks and Broader Categories\n\t\n\tThe paper identifies four key questions that organizations must address to support Responsible AI:\n\t1. **When and how do we act?**\n\t2. **How do we measure success?**\n\t3. **What are the internal structures we rely on?**\n\t4. **How do we resolve tensions?**\n\t\n\tThese questions are explored through three clusters of data:\n\t- **Prevalent Practices:** Reactive actions, performance trade-offs, lack of accountability, and fragmented structures.\n\t- **Emerging Practices:** Proactive actions, provenance metrics, structural support, and rigid organizational incentives.\n\t- **Aspirational Future:** Anticipatory frameworks, redefined results to include societal impact, integrated processes, and aligned ethical tensions with organizational values .\n\t\n\t### Sub-Pillars of Responsible AI\n\t\n\tThe paper emphasizes several sub-pillars, with accountability being the most prominently featured. The prevalent practices show a lack of accountability, which is a significant barrier to Responsible AI. Emerging practices begin to address this by implementing organization-level frameworks and metrics, while the aspirational future envisions integrated accountability throughout all business processes .\n\t\n\t#### Primary Sub-Pillar: Accountability\n\t- **Prevalent Practices:** Accountability is often neglected due to role uncertainty and reliance on individual incentives rather than systemic structures.\n\t- **Emerging Practices:** Distributed accountability on top of existing structures.\n\t- **Aspirational Future:** Integrated accountability across all organizational functions, with leadership taking responsibility for broader implications .\n\t\n\t#### Secondary Sub-Pillars\n\t- **Ethical Governance:** Legal practitioners discuss Responsible AI in terms of comprehensive pillars and ethical guidelines.\n\t- **Performance and Usability:** Project managers frame their work in terms of product life-cycles and industry trends.\n\t- **Explainability and Fairness:** Fairness evaluations, algorithmic audits, and explainability are common themes among practitioners .\n\t\n\t### Influence on Overall Approach\n\t\n\tThe focus on accountability as a primary sub-pillar shapes the paper's approach to Responsible AI by highlighting the need for systemic organizational changes rather than relying on individual efforts. This approach underscores the importance of integrating Responsible AI practices into all levels of an organization, ensuring that ethical considerations are part of the core business processes and decision-making structures.\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a combination of competence/functionality and ethical considerations, with a strong emphasis on accountability. This comprehensive approach aims to transition organizations from reactive to proactive and anticipatory practices, ultimately integrating Responsible AI into the fabric of organizational operations.",
            "reference": "Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
        },
        {
            "paper_title": "Designing responsible ai: Adaptations of ux practice to meet responsible ai challenges",
            "rai_definition_1": "The research paper conceptualizes Responsible AI (RAI) through the lens of user-centered practices and the roles of UX practitioners and RAI experts in the design and development of AI applications. It does not provide a single explicit definition of Responsible AI but rather frames it within a set of emerging practices and challenges faced by practitioners.\n\t\n\tThe paper identifies three key emerging practices that UX practitioners are developing to meet evolving RAI needs: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications . These practices are not linear but are embedded throughout work practices in iterative ways .\n\t\n\t1. **Building and Reinforcing an RAI Lens**: This involves UX practitioners self-educating and communicating RAI issues with their teams, sensitizing people to RAI concerns, and considering potential consequences of users’ mental models of AI systems .\n\t\n\t2. **Responsible Prototyping**: This practice includes strategies to surface and mitigate potential RAI issues through techniques such as prompt programming and ensuring that model capabilities and limitations are communicated effectively to users .\n\t\n\t3. **Responsible Evaluation of AI Applications**: This involves preparing both the models and the users for potentially harmful model outputs and conducting adversarial testing of potential model harms .\n\t\n\tThe paper emphasizes the importance of these practices in adapting existing UX practices to meet RAI challenges, highlighting the hidden work of RAI carried out by UX practitioners and the need for further research to support these evolving practices .\n\t\n\tThe primary sub-pillar emphasized in the paper is the **user-centered approach** to RAI, which involves incorporating and examining RAI considerations during the early stages and refinement of AI application design . This approach is crucial for understanding how UX practitioners adapt to meet RAI challenges and for identifying strategies to support their work.\n\t\n\tSecondary sub-pillars include the involvement of potentially impacted user groups and domain experts in the design and evaluation of RAI applications, and the need to protect participants from potential harms during these processes . The paper also discusses the challenges of involving users in RAI work and the need for robust methods and frameworks to support UX practitioners in these efforts .\n\t\n\tOverall, the paper's focus on specific sub-pillars such as user-centered design and responsible prototyping shapes its approach to Responsible AI by emphasizing the practical, day-to-day challenges and strategies of UX practitioners in the industry. This focus highlights the importance of adapting UX practices to address RAI concerns and suggests areas for further research to support these evolving practices.",
            "reference": "Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278"
        },
        {
            "paper_title": "Tools and practices for responsible AI engineering",
            "rai_definition_1": "Responsible AI is conceptualized in the research paper as the practice of creating and maintaining AI systems that are not only accurate but also exhibit several critical qualities. These qualities include robustness, safety, security, privacy, fairness, inclusiveness, transparency, interpretability, explainability, accountability, governability, and the promotion of societal and environmental well-being . \n\t\n\tThe paper emphasizes the need for more attention to these principles, as evidenced by the publication of over 400 policy documents and guidance from leading industry and government organizations . The authors present two software libraries, hydra-zen and the rAI-toolbox, which address critical needs for responsible AI engineering by making AI applications more configurable, reproducible, robust, and explainable .\n\t\n\tThe primary sub-pillars of Responsible AI discussed in the paper are robustness and explainability. The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI models, which is a key facet of responsible AI . The toolbox supports scalable workflows and is compatible with existing tools that enable distributed computation, ensuring that robustness analyses can be easily scaled . \n\t\n\tExplainability is also a significant focus, with the paper highlighting the need for AI models to be engineered to obey responsible AI principles, including transparency and interpretability . The hydra-zen library extends the Hydra framework to standardize the process of making complex AI applications configurable and their behaviors reproducible, thereby increasing the traceability and scalability of machine learning workflows .\n\t\n\tSecondary sub-pillars such as safety, security, privacy, fairness, and inclusiveness are mentioned as important qualities of responsible AI, but the paper primarily focuses on robustness and explainability . The emphasis on these sub-pillars shapes the overall approach to Responsible AI by providing concrete tools and methods to enhance these specific aspects, thereby contributing to a comprehensive understanding of Responsible AI.\n\t\n\tIn summary, the paper defines Responsible AI through a combination of broader concepts and specific sub-pillars, with a particular emphasis on robustness and explainability. This focus influences the development of tools and practices that aim to make AI systems more reliable, transparent, and accountable.",
            "reference": "Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
        },
        {
            "paper_title": "Responsible ai pattern catalogue: A collection of best practices for ai governance and engineering",
            "rai_definition_1": "Responsible Artificial Intelligence (RAI) in this research paper is conceptualized as the ethical development of AI systems to benefit humans, society, and the environment . The paper emphasizes that RAI has garnered significant attention from various stakeholders, including governments, organizations, companies, and societies, due to its potential to address real-world challenges while also posing ethical risks due to the dynamic, autonomous, and opaque nature of AI systems .\n\t\n\tThe paper does not provide a single explicit definition of Responsible AI but rather frames it within a broader conceptual framework that includes both competence/functionality and ethical considerations. The authors classify RAI into three main categories: governance, process, and product patterns . These categories are designed to ensure that AI systems are responsible throughout their entire lifecycle, from development to deployment and beyond.\n\t\n\t1. **Governance Patterns**: These patterns focus on establishing multi-level governance structures to ensure compliance with AI standards and laws. They are essential for creating an overarching framework that guides the ethical development and deployment of AI systems .\n\t\n\t2. **Process Patterns**: These patterns are aimed at setting up trustworthy development processes. They ensure that ethical considerations are integrated into every stage of the AI development lifecycle, from design to implementation and monitoring .\n\t\n\t3. **Product Patterns**: These patterns involve building RAI principles directly into the AI products. They focus on the technical aspects of AI systems, ensuring that the products themselves adhere to ethical guidelines and can be verified and validated for responsible behavior .\n\t\n\tThe primary sub-pillar emphasized in the paper is the **governance** aspect, which is crucial for establishing a comprehensive framework for RAI. Governance patterns provide the necessary oversight and regulatory compliance mechanisms that ensure AI systems are developed and used responsibly . This focus on governance helps shape the overall approach to RAI by emphasizing the importance of multi-level oversight and the need for a structured framework to guide ethical AI development.\n\t\n\tSecondary sub-pillars include process and product patterns, which are also given significant attention. These sub-pillars complement the governance framework by providing concrete, actionable guidelines for developers and other stakeholders to operationalize RAI principles in practice .\n\t\n\tIn summary, the paper's approach to Responsible AI is multi-faceted, incorporating governance, process, and product patterns to create a comprehensive framework for ethical AI development. This structured approach ensures that RAI principles are not only theoretical but are also practically implemented throughout the AI system's lifecycle.",
            "reference": "Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
        },
        {
            "paper_title": "Responsible-AI-by-design: A pattern collection for designing responsible AI systems",
            "rai_definition_1": "The research paper conceptualizes Responsible AI by focusing on ethical principles and system-level design patterns that can be embedded into AI systems. The paper does not provide an explicit definition of Responsible AI but instead uses a conceptual framework based on ethical principles to guide the design and development of AI systems.\n\t\n\tThe ethical principles used in the paper are derived from Harvard University’s mapping study and include Privacy, Accountability, Safety & Security, Transparency & Explainability, Fairness and Non-discrimination, Human Control of Technology, and Promotion of Human Values . These principles serve as the foundation for identifying reusable design patterns that architects and developers can use to ensure responsible AI during the development process.\n\t\n\tThe primary sub-pillar emphasized in the paper is the system-level guidance for designing the architecture of responsible AI systems. The authors argue that responsible AI issues often go beyond data and algorithms and are crosscutting many system components and the entire software engineering lifecycle . This focus on system-level design patterns is intended to provide practical, actionable guidance that can be embedded into AI systems as product features, thereby operationalizing the high-level ethical principles into concrete design solutions.\n\t\n\tSecondary sub-pillars discussed include the various ethical principles themselves, such as Privacy, Accountability, and Transparency & Explainability, among others. These principles are given equal emphasis as they collectively contribute to a comprehensive understanding of Responsible AI .\n\t\n\tThe paper's focus on specific sub-pillars, particularly the system-level design patterns, shapes its overall approach to Responsible AI by providing a structured methodology for embedding ethical considerations into the architecture of AI systems. This approach aims to bridge the gap between high-level ethical guidelines and practical implementation, ensuring that AI systems can behave and make decisions responsibly throughout their lifecycle.",
            "reference": "Qin Lu, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. https://arxiv.org/pdf/2203.00905"
        },
        {
            "paper_title": "Towards a roadmap on software engineering for responsible AI",
            "rai_definition_1": "The research paper conceptualizes Responsible AI as a multifaceted challenge that encompasses both legal and ethical aspects. It emphasizes that achieving Responsible AI requires adherence to a broad set of requirements, which include but are not limited to ethical principles and legal standards. The paper uses the terms \"responsible AI,\" \"ethical AI,\" and \"ethics\" interchangeably to cover this broader set of requirements .\n\t\n\tThe paper does not provide an explicit, singular definition of Responsible AI. Instead, it frames Responsible AI within a broader conceptual framework that includes multiple perspectives: governance, process, and system-level architectural styles. These perspectives are aimed at operationalizing Responsible AI through software engineering methods rather than staying at the high-level ethical principle stage or focusing solely on algorithmic solutions .\n\t\n\tThe primary sub-pillars emphasized in the paper are:\n\t\n\t1. **Governance Perspective**: This involves establishing multi-level governance structures to ensure that AI systems adhere to responsible AI principles throughout their lifecycle. This perspective is crucial for setting up the regulatory and organizational frameworks needed to support responsible AI practices .\n\t\n\t2. **Process Perspective**: This focuses on incorporating process-oriented best practices into the development lifecycle of AI systems. It aims to integrate responsible AI principles into the software development processes, including requirement engineering, system design, and operations .\n\t\n\t3. **System Perspective**: This involves building responsible-AI-by-design into AI systems through system-level architectural styles, patterns, and techniques. It emphasizes the need to consider ethical principles at the system level, including both AI and non-AI components and their interactions .\n\t\n\tThe paper also discusses secondary sub-pillars such as:\n\t\n\t- **Trustworthiness and Trust**: Differentiating between the inherent trustworthiness of an AI system and the user's subjective trust in the system. The paper highlights the need for design mechanisms that improve both aspects .\n\t- **Continuous Monitoring and Validation**: Emphasizing the importance of continuously monitoring and validating AI systems against responsible AI requirements to ensure they behave appropriately in real-world scenarios .\n\t\n\tBy focusing on these sub-pillars, the paper aims to provide a comprehensive approach to operationalizing Responsible AI. This approach influences the overall methodology by ensuring that responsible AI principles are embedded throughout the entire software engineering lifecycle, from governance and process management to system design and continuous validation.\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a multi-perspective framework that includes governance, process, and system-level considerations, with a strong emphasis on integrating ethical principles into the software engineering lifecycle .",
            "reference": "Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
        },
        {
            "paper_title": "Behavioral use licensing for responsible ai",
            "rai_definition_1": "The research paper conceptualizes Responsible AI by emphasizing the unique challenges AI systems create compared to traditional software systems. It highlights that AI systems do not offer the same degree of control and run-time guarantees of system behavior, making it difficult to predict how an AI model will respond to various scenarios . \n\t\n\tThe paper does not provide an explicit definition of Responsible AI but discusses it through broader conceptual frameworks and ethical considerations. It mentions that principles and guidelines for Responsible AI often include aspects such as Transparency, Trust, Accountability, and Justice & Fairness . These principles are related to the acquisition, use, and transformation of data, model design, and end-use application, suggesting that AI systems should be interpretable and explainable to a trained human worker .\n\t\n\tThe primary sub-pillar emphasized in the paper is \"Transparency,\" which includes providing details about the data used for development and testing, the nature of testing, error characteristics, and risks of bias. This is seen in the discussion of AI Factsheets and Model cards, which advocate for detailed documentation and quantitative evaluation to ensure fairness and robustness . \n\t\n\tSecondary sub-pillars include \"Accountability\" and \"Fairness,\" where the paper suggests that AI systems should explicitly identify themselves and that decisions taken by an AI system should have a human being accountable . The paper also discusses the importance of interpretability and explainability, mentioning various methods and models that provide explanations for AI decisions, such as Decision Trees, Lasso Linear Regression, Bayesian formulations, LIME, and SHAP .\n\t\n\tThe focus on these sub-pillars shapes the paper's overall approach to Responsible AI by advocating for better documentation, transparency, and interpretability to ensure that AI systems are used responsibly and ethically. The emphasis on these aspects suggests a comprehensive approach to addressing the ethical challenges posed by AI systems.",
            "reference": "Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
        },
        {
            "paper_title": "Software engineering for responsible AI: An empirical study and operationalised patterns",
            "rai_definition_1": "The research paper conceptualizes Responsible AI by integrating both ethical and legal aspects, recognizing that law sets the minimum standards of behavior while ethics establishes the maximum standards. Throughout the paper, the terms responsible AI, ethical AI, and ethics are used interchangeably to cover a broader set of requirements .\n\t\n\tThe paper does not provide a single explicit definition of Responsible AI but rather discusses it through various conceptual frameworks and sub-pillars. It emphasizes the need for AI systems to embody responsible AI principles and requirements, which are often high-level and lack concrete guidance for developers . The paper identifies several key sub-pillars and broader categories that contribute to a comprehensive understanding of Responsible AI:\n\t\n\t1. **Ethical Considerations**: The paper highlights several ethical principles such as privacy & security, reliability & safety, fairness, human, societal, and environmental well-being, and human-centered values. These principles are often treated as software quality attributes or functional requirements that need to be operationalized into concrete practices .\n\t\n\t2. **Trustworthiness and Trust**: The paper differentiates between trustworthiness (the ability of an AI system to meet ethical principles) and trust (users' subjective estimates of the system's trustworthiness). It emphasizes the importance of gaining and maintaining human trust in AI systems, which is crucial for their acceptance and widespread use .\n\t\n\t3. **Risk Management**: The paper discusses the importance of understanding and managing risks associated with AI systems, particularly those that are highly uncertain and involve continual learning. It critiques the current practice of \"done-once-and-forget\" risk assessments and advocates for continuous monitoring and validation of AI systems post-deployment .\n\t\n\t4. **System-Level Considerations**: The paper stresses the need for system-level thinking in AI projects, where ethical considerations should encompass both AI and non-AI components and their interactions. It points out the lack of end-to-end development tools to support continuous assurance of AI ethics .\n\t\n\t5. **Operationalization of Ethical Principles**: The paper proposes a pattern template to operationalize AI ethics principles into concrete patterns that can be used throughout the lifecycle of an AI system. This approach aims to provide practical, actionable guidance for developers to implement responsible AI .\n\t\n\tThe primary sub-pillar emphasized in the paper is the operationalization of ethical principles into concrete patterns. This focus is crucial as it addresses the gap between high-level ethical guidelines and practical implementation, providing developers with the tools and methods needed to build responsible AI systems . Secondary sub-pillars such as trustworthiness, risk management, and system-level considerations are also discussed, highlighting the multifaceted nature of Responsible AI and the need for a holistic approach.\n\t\n\tOverall, the paper's focus on specific sub-pillars and broader conceptual frameworks shapes its approach to Responsible AI by emphasizing the need for practical, actionable guidance that can be integrated into the AI development lifecycle. This approach aims to bridge the gap between high-level ethical principles and their implementation in real-world AI systems .",
            "reference": "Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
        },
        {
            "paper_title": "Data cards: Purposeful and transparent dataset documentation for responsible ai",
            "rai_definition_1": "The research paper conceptualizes Responsible AI primarily through the lens of transparency in dataset documentation. It does not provide an explicit definition of Responsible AI but instead focuses on frameworks and practices that contribute to responsible AI development. The primary conceptual framework discussed is the creation and use of Data Cards, which are structured summaries of essential facts about various aspects of machine learning (ML) datasets needed by stakeholders across a dataset’s lifecycle for responsible AI development .\n\t\n\tThe paper emphasizes the importance of transparency as a critical sub-pillar of Responsible AI. Transparency is defined as “a clear, easily understandable, and plain language explanation of what something is, what it does and why it does that” . This definition underscores the need for documentation that is accessible and comprehensible to a diverse set of stakeholders, including non-technical users. The paper identifies eight characteristics of transparency that are vital for robust discussions about the benefits, values, ethics, and limitations of AI datasets .\n\t\n\tThe focus on transparency shapes the overall approach to Responsible AI by highlighting the need for clear, consistent, and comprehensive documentation. This approach is intended to foster informed decision-making and accountability in the use of datasets for AI models. The paper also discusses the role of Data Cards in providing explanations of processes and rationales that shape the data and consequently the models, such as upstream sources, data collection and annotation methods, training and evaluation methods, and intended use .\n\t\n\tSecondary sub-pillars discussed include usability and human-centricity, as the paper emphasizes the need for documentation to be treated as a user-centric product. This involves creating documentation that is not only transparent but also practical and useful for stakeholders across different domains and organizational contexts .\n\t\n\tIn summary, the paper's focus on transparency and human-centric documentation practices significantly influences its approach to Responsible AI, aiming to create value at scale and ensure that AI systems are developed and deployed responsibly.",
            "reference": "Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
        },
        {
            "paper_title": "AI and ethics—Operationalizing responsible AI",
            "rai_definition_1": "The research paper conceptualizes Responsible AI by focusing on how to design, develop, and validate AI technologies and systems responsibly to address ethical and legal concerns, particularly those related to human values . The paper does not provide a single explicit definition of Responsible AI but instead discusses it through various conceptual frameworks and broader categories.\n\t\n\tThe paper identifies three main categories of existing works on ethical AI:\n\t\n\t1. **High-level ethical principle frameworks**: These frameworks identify important ethical and legal principles that Responsible AI technologies and systems should adhere to. However, these principles are often high-level and challenging to operationalize .\n\t\n\t2. **Ethical algorithms**: This category includes research on algorithms that incorporate ethical and legal properties, such as privacy and fairness. These algorithms are mathematically defined and provide theoretical guarantees but are limited to a small subset of ethical principles and human values .\n\t\n\t3. **Human values in software engineering and their operationalization**: This includes extending value-based design methods, human factor research, and software engineering methods to embed human values and ethical considerations throughout the software development lifecycle (SDLC). It also covers architecture and design patterns that can improve or assure ethical or human-value-related quality attributes .\n\t\n\tThe paper emphasizes the importance of integrating these categories into a comprehensive framework that considers the unique characteristics of AI, such as its autonomy, adaptability, and interactivity. This integrated view aims to improve both trust and trustworthiness of AI systems for a wider set of stakeholders .\n\t\n\tThe primary sub-pillar emphasized in the paper is the operationalization of high-level ethical principles into practical mechanisms that can be embedded in AI systems. This includes ensuring privacy, security, reliability, safety, transparency, explainability, contestability, and accountability throughout the AI system lifecycle . The paper also discusses the importance of continuous validation and monitoring of AI systems after deployment to address the inherent under-specification challenges of AI .\n\t\n\tSecondary sub-pillars include the development of ethical algorithms and the incorporation of human values in software engineering processes. These sub-pillars are given significant attention but are considered part of the broader effort to operationalize ethical principles in AI systems .\n\t\n\tOverall, the paper's focus on specific sub-pillars and broader conceptual frameworks shapes its approach to Responsible AI by emphasizing the need for practical, operational mechanisms that can be integrated into AI systems to ensure ethical and legal compliance while maintaining public trust .",
            "reference": "Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics—Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
        },
        {
            "paper_title": "Towards responsible AI for financial transactions",
            "rai_definition_1": "The research paper conceptualizes Responsible AI primarily through the principles of explainability and robustness. These principles are emphasized as essential for the implementation of AI systems, particularly in the context of financial transactions.\n\t\n\tThe paper does not provide an explicit definition of Responsible AI but discusses it within the framework of these two key principles:\n\t\n\t1. **Explainability**: The paper highlights the importance of explainability in AI models, particularly through the use of decision trees and random forests as surrogate models to explain the predictions of a deep neural network (DNN) model. However, it notes that the complexity of these models, with more than 50,000 nodes, limits their interpretability. To address this, the authors propose using smaller decision trees fitted to clusters of data, which offer improved interpretability and are easier for humans to understand .\n\t\n\t2. **Robustness**: The paper also emphasizes the need for robustness in AI models. It discusses testing the robustness of the model by subjecting it to targeted evasion attacks. The results indicate that the model is vulnerable to changes in transaction text, which is identified as the most important feature for classification. However, the paper notes that this vulnerability is considered low risk for certain types of transactions where the text is mostly immutable .\n\t\n\tThe focus on these sub-pillars shapes the overall approach to Responsible AI in the paper. By concentrating on explainability and robustness, the authors aim to ensure that AI systems are not only accurate but also transparent and resilient to adversarial attacks. This approach underscores the importance of making AI systems understandable and reliable, particularly in high-stakes domains like financial transactions.\n\t\n\tIn summary, the paper conceptualizes Responsible AI through the principles of explainability and robustness, with a particular emphasis on making AI models interpretable and resilient to attacks.",
            "reference": "Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
        },
        {
            "paper_title": "A human rights-based approach to responsible AI",
            "rai_definition_1": "The research paper conceptualizes and defines Responsible AI through a human rights-based approach. This approach is anchored in the idea that AI systems should be developed and deployed in ways that respect and promote human rights. The paper argues that human rights can serve as a grounding framework for value alignment in AI, given their cross-cultural validity and moral significance .\n\t\n\tThe paper identifies three main functions of human rights in the context of AI:\n\t\n\t1. **Value Alignment**: Human rights can help align AI systems with values that have cross-cultural affirmation. This is crucial for ensuring that AI systems are ethical and just across different national and social contexts .\n\t\n\t2. **Responsibility Allocation**: A human rights framework can clarify the responsibilities of various actors within the AI ecosystem, including states, organizations, and individuals. This helps in understanding how ethical principles translate into different responsibilities for these actors .\n\t\n\t3. **Shared Vocabulary and Collaboration**: Human rights provide a shared language that can facilitate deeper collaboration between AI researchers, civil society groups, and the people impacted by these technologies. This can help bridge the gap between technical research and the claims of those who interact with AI systems .\n\t\n\tThe paper also delves into specific rights enshrined in the Universal Declaration of Human Rights (UDHR) to illustrate how human rights might shape responsible AI development and deployments. It emphasizes that while a human rights perspective is not a panacea for all issues in AI, it provides a robust set of principles centered on human vulnerabilities and needs .\n\t\n\tIn summary, the primary sub-pillar emphasized in the paper is the human rights framework, which contributes to a comprehensive understanding of Responsible AI by ensuring value alignment, clarifying responsibilities, and fostering collaboration. This focus on human rights shapes the overall approach to Responsible AI by prioritizing ethical considerations that are universally recognized and morally significant.",
            "reference": "Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
        },
        {
            "paper_title": "Responsible Artificial Intelligence---From Principles to Practice: A Keynote at TheWebConf 2022",
            "rai_definition_1": "The research paper conceptualizes Responsible AI as a socio-technical ecosystem that emphasizes the interaction between technology and society, rather than attributing responsibility to the AI systems themselves. According to Dignum , Responsible AI is not about giving machines responsibility for their actions but rather about increasing the accountability of the people and organizations involved in the development and use of AI systems. This perspective underscores that the ethical and responsible use of AI must be directed towards the socio-technical ecosystem in which AI operates, ensuring that the social components of this ecosystem take responsibility and act within an ethical framework.\n\t\n\tThe paper identifies several broader categories and sub-pillars that define Responsible AI, including:\n\t\n\t1. **Ethical Considerations**: The paper highlights the importance of ethical principles such as transparency, justice and fairness, non-maleficence, responsibility, and privacy . These principles are essential for ensuring that AI systems are developed and used in ways that are trustworthy and contribute to societal well-being.\n\t\n\t2. **Socio-Technical Ecosystem**: AI is described as a socio-technical ecosystem, recognizing the interaction between people and technology and how complex infrastructures affect and are affected by society and human behavior . This approach emphasizes the need for a multidisciplinary stance to understand governance, power, trust, and accountability.\n\t\n\t3. **Inclusion and Diversity**: The paper stresses the importance of inclusion and diversity in AI development, advocating for a broad group of people to have a basic knowledge of AI and for AI professionals to have diverse backgrounds, including philosophy, social science, law, and economy .\n\t\n\t4. **Impact Assessment Tools**: These tools provide a step-by-step evaluation of the impact of AI systems on aspects such as privacy, transparency, explanation, bias, and liability . This practical approach helps developers and users understand the relevant legal and ethical standards when making decisions about AI applications.\n\t\n\t5. **Design for Values Methodologies**: The paper discusses methodologies like Design for Values, which provide a structured way to translate abstract values into concrete norms and functionalities in AI systems . This approach ensures that societal values are identified, deliberated upon, and linked to formal system requirements.\n\t\n\tThe primary sub-pillar emphasized in the paper is the socio-technical ecosystem, which provides a comprehensive understanding of Responsible AI by integrating ethical considerations, governance, and the interaction between technology and society. This focus shapes the overall approach to Responsible AI by advocating for a holistic view that includes technical, social, legal, and economic dimensions.\n\t\n\tSecondary sub-pillars such as inclusion and diversity, impact assessment tools, and design for values methodologies are also discussed, highlighting the multifaceted nature of Responsible AI. These sub-pillars contribute to a broader understanding by addressing specific aspects of ethical AI development and use.\n\t\n\tIn summary, the paper's focus on the socio-technical ecosystem and its integration of various sub-pillars influence its overall approach to Responsible AI, advocating for a comprehensive, multidisciplinary, and value-based perspective.",
            "reference": "Virginia Dignum. 2023. Responsible Artificial Intelligence---From Principles to Practice: A Keynote at TheWebConf 2022. ACM SIGIR Forum. Retrieved from https://arxiv.org/pdf/2205.10785"
        },
        {
            "paper_title": "A pathway towards responsible ai generated content",
            "rai_definition_1": "The research paper by Chen et al.  conceptualizes Responsible AI Generated Content (AIGC) by identifying and addressing various risks associated with AIGC technologies. The paper does not provide a single explicit definition of Responsible AI but rather discusses it through a framework of concerns and mitigation strategies.\n\t\n\tThe paper categorizes Responsible AI in terms of both competence/functionality and ethical considerations. The primary sub-pillars emphasized include:\n\t\n\t1. **Privacy**: The paper highlights the risks of data replication and memorization in AIGC models, which can lead to privacy violations. Measures such as deduplication and differentially private diffusion models are discussed as potential solutions .\n\t\n\t2. **Bias, Toxicity, and Misinformation**: The authors note that AIGC models can inherit and amplify social biases, produce toxic content, and spread misinformation. They emphasize the need for transparency and responsible open-sourcing to mitigate these issues .\n\t\n\t3. **Intellectual Property (IP)**: Concerns about the use of uncurated web-scale data for training AIGC models, which can lead to copyright infringement and ownership disputes, are discussed. The paper suggests proactive steps like the creation of websites to identify memorized images .\n\t\n\t4. **Robustness**: The robustness of AIGC models is crucial to ensure they perform reliably across different scenarios and do not produce harmful outputs .\n\t\n\t5. **Open Source and Explanation**: The lack of transparency in AIGC models is a significant concern. The paper advocates for open-sourcing to facilitate better understanding and explanation of model behaviors .\n\t\n\t6. **Technology Abuse**: The potential misuse of AIGC technologies for malicious purposes is another critical area of concern. The paper calls for responsible practices to prevent such abuse .\n\t\n\t7. **Consent, Credit, and Compensation**: Ensuring that creators whose works are used for training AIGC models are properly credited and compensated is essential for ethical AI development .\n\t\n\t8. **Environment**: The environmental impact of training large AIGC models is also considered, with a call for more sustainable practices .\n\t\n\tThe paper's focus on these sub-pillars shapes its overall approach to Responsible AI by providing a comprehensive understanding of the risks and proposing actionable steps to mitigate them. This multi-faceted approach ensures that Responsible AI is not just about technical performance but also about ethical and societal impacts.\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a detailed examination of various risks and ethical considerations, emphasizing the need for transparency, fairness, and proactive mitigation strategies to ensure the responsible development and deployment of AIGC technologies.",
            "reference": "Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
        },
        {
            "paper_title": "Responsible AI (RAI) Games and Ensembles",
            "rai_definition_1": "The research paper conceptualizes Responsible AI (RAI) through a general framework referred to as \"RAI games.\" This framework is designed to address various desiderata such as ethics, fairness, robustness, and safety, which are often collated under the umbrella of Responsible AI . The paper does not provide a single explicit definition of Responsible AI but rather introduces a conceptual framework that encompasses multiple aspects of responsible AI.\n\t\n\tThe primary sub-pillar emphasized in the paper is robustness, particularly in the context of distribution shifts and adversarial robustness. The framework is built around the idea of minimizing worst-case loss over a set of predefined distributions, which is a common approach in robustness studies . This is evident from the focus on subpopulation shift problems and the use of Distributionally Robust Optimization (DRO) techniques to ensure that AI models are robust to deviations in data distribution .\n\t\n\tSecondary sub-pillars include fairness and safety. Fairness is addressed through the minimax group fairness approach, which aims to ensure that the model performs well across different sub-groups in the data . Safety considerations are also implicitly included through the robustness measures, as robust models are generally more reliable and safer in high-stakes decision-making contexts .\n\t\n\tThe paper's focus on robustness as the primary sub-pillar shapes its overall approach to Responsible AI by emphasizing the need for models that can handle worst-case scenarios and distributional shifts. This is achieved through the introduction of RAI games, which are computational frameworks designed to optimize for multiple responsible AI aspects simultaneously . The use of boosting-based algorithms and game-theoretic approaches further supports this comprehensive understanding of Responsible AI by providing practical methods to achieve these goals .\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a framework that integrates multiple aspects such as robustness, fairness, and safety, with a primary focus on robustness. This approach allows for a comprehensive understanding of Responsible AI, ensuring that models are not only fair and ethical but also robust and safe in various scenarios.",
            "reference": "Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
        },
        {
            "paper_title": "Progressing towards responsible AI",
            "rai_definition_1": "The research paper conceptualizes Responsible AI through a comprehensive framework that includes various responsible practices, ethical guidelines, and operational resources. It does not provide a single explicit definition of Responsible AI but rather discusses it in terms of broader concepts and sub-pillars.\n\t\n\tThe paper classifies responsible practices into five main groups:\n\t\n\t1. **Assessments, Questionnaires, and Checklists**: These tools raise questions, encourage reflection, and inspire potential action. Examples include the HLEG-AI TAIL and Consequence Scanning .\n\t\n\t2. **End-to-End Frameworks**: These frameworks address each stage of the entire process with appropriate activities and involve multiple audiences. Examples include the End-to-End Framework for Internal Algorithmic Auditing and the People + AI Guidebook .\n\t\n\t3. **Strategy Guides and Canvases**: These are thinking frameworks and diagnostic tools that help break down and work through complex challenges. Examples include the Data Ethics Canvas and the Ethical Operating System .\n\t\n\t4. **Design Guides**: These are sets of recommendations towards responsible good practice in design. Examples include the Guidelines for Human-AI Interaction and AI Ethics Cards .\n\t\n\t5. **Software Toolkits**: These provide metrics and algorithms to support the ethical development of AI-powered software. Examples include the AI Fairness 360 Toolkit and the Aequitas bias audit toolkit .\n\t\n\tThe paper emphasizes the need for a curated set of responsible practices throughout the entire process, ensured by governance procedures such as audit services, certifications, and AI labels . It also highlights the importance of collaboration, diverse perspectives, and guidance derived from values, principles, and policies .\n\t\n\tThe primary sub-pillar emphasized in the paper is the need for **multidisciplinary and multi-stakeholder collaboration**. This is seen as essential for progressing towards responsible AI outcomes. The paper discusses the role of new professional roles, such as \"ethics owners,\" who are responsible for transforming principles, values, and ethical stances into concrete practices within organizations .\n\t\n\tSecondary sub-pillars include the promotion of public engagement and the integration of ethical and legal requirements into the assessment of AI systems. The paper suggests forming small teams to collaborate with companies to experiment with Trustworthy AI requirements and generate good practices .\n\t\n\tThe focus on these sub-pillars shapes the paper's overall approach to Responsible AI by emphasizing the need for practical, operational resources and the importance of a collaborative, interdisciplinary approach to ethical AI development.\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a framework of responsible practices, ethical guidelines, and operational resources, with a strong emphasis on multidisciplinary collaboration and public engagement.",
            "reference": "Antonella Scantamburlo, Amanda Cortés, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
        },
        {
            "paper_title": "Responsible ai pattern catalogue: a multivocal literature review",
            "rai_definition_1": "The research paper conceptualizes Responsible AI as the ethical development of AI systems to benefit humans, society, and the environment. This concept has garnered significant attention from various stakeholders, including governments, organizations, companies, and societies . The paper emphasizes that Responsible AI is one of the greatest scientific challenges of our time and is crucial for increasing the adoption of AI technologies .\n\t\n\tThe paper does not provide a single explicit definition of Responsible AI but instead discusses it through a comprehensive framework that includes multiple sub-pillars. These sub-pillars are classified into three main categories: governance patterns, process patterns, and product patterns . \n\t\n\t1. **Governance Patterns**: These patterns focus on establishing multi-level governance for responsible AI. This includes compliance with AI standards and laws, ensuring that AI systems are developed and operated within a framework that promotes ethical behavior and accountability .\n\t\n\t2. **Process Patterns**: These patterns are aimed at setting up trustworthy development processes. They ensure that ethical considerations are integrated throughout the entire software development lifecycle, addressing issues that may arise at any step of the development process .\n\t\n\t3. **Product Patterns**: These patterns are designed to build responsible-AI-by-design paradigms into AI systems. They focus on embedding ethical principles directly into the AI products and verifying and validating these products to ensure they meet ethical standards .\n\t\n\tThe primary sub-pillar emphasized in the paper is the **product patterns** category, which includes solutions like federated learning to address data privacy issues and the \"Bill of materials registry\" for continuous learning and transparency . This focus on product patterns contributes to a comprehensive understanding of Responsible AI by ensuring that ethical principles are not only theoretical but are practically implemented in AI systems.\n\t\n\tSecondary sub-pillars include governance and process patterns, which are also given significant attention. These patterns complement the product patterns by providing a holistic approach to Responsible AI, ensuring that ethical considerations are maintained at both the organizational and procedural levels .\n\t\n\tThe paper's focus on these specific sub-pillars shapes its overall approach to Responsible AI by providing systematic and actionable guidance for stakeholders. This approach ensures that AI systems are responsible throughout their entire lifecycle, from governance and development to deployment and operation .\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a multi-faceted framework that includes governance, process, and product patterns, with a particular emphasis on product patterns to ensure practical implementation of ethical principles.",
            "reference": "Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
        },
        {
            "paper_title": "Towards implementing responsible AI",
            "rai_definition_1": "The research paper conceptualizes Responsible AI through various interpretations and frameworks rather than providing a single explicit definition. The paper identifies three primary interpretations of responsible AI: normative, possessive, and descriptive. Normative responsibility refers to behaving in positive and socially acceptable forms, possessive responsibility involves duty and obligation, and descriptive responsibility means being worthy of response or answerable .\n\t\n\tThe paper also discusses several sub-pillars and broader categories that contribute to the understanding of Responsible AI. These include:\n\t\n\t1. **Ethics Requirements**: Privacy and security are the most discussed requirements, with other principles like HSE Wellbeing often expressed as indirect objectives rather than verifiable requirements .\n\t\n\t2. **Reliability & Safety**: The approach of fail-safe by design is recommended to address reliability and safety, although it is acknowledged that not all failure modes can be anticipated .\n\t\n\t3. **Trust**: Human trust in AI is emphasized, with the need for evidence to support the reliability and explainability of AI systems .\n\t\n\t4. **Explainability**: Practical aspects of explainability and interpretability are considered important for gaining user trust. Explainability is seen as a temporary measure until the system's reliability is established .\n\t\n\t5. **Continuous Validation**: Continuous monitoring and validation of AI systems post-deployment are necessary to ensure adherence to ethics requirements and to address potential mismatches between training data and operational data .\n\t\n\t6. **Traceability of Artefacts**: Tracking the use of AI systems and keeping track of information related to model provenance are seen as useful for improving transparency and accountability .\n\t\n\tThe primary sub-pillar emphasized in the paper is **Trust**, which is considered crucial for the acceptance and reliability of AI systems. Trust is built through evidence of reliable operation and explainability of results, which in turn supports the broader goal of responsible AI .\n\t\n\tSecondary sub-pillars such as **Privacy and Security**, **Reliability & Safety**, and **Explainability** are also discussed, with each contributing to a comprehensive understanding of Responsible AI. The focus on these sub-pillars shapes the paper's overall approach by highlighting the need for a multi-faceted strategy that addresses various ethical and functional aspects of AI systems .\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a combination of ethical requirements, trust, explainability, continuous validation, and traceability, with a particular emphasis on building and maintaining trust in AI systems.",
            "reference": "Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big … - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
        },
        {
            "paper_title": "Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations",
            "rai_definition_1": "The research paper by Madaio et al.  does not provide an explicit definition of Responsible AI (RAI). Instead, it conceptualizes RAI through a combination of broader categories and sub-pillars, focusing on both competence/functionality and ethical considerations.\n\t\n\tThe paper identifies several key themes and orientations towards RAI:\n\t\n\t1. **Competence/Functionality**:\n\t   - **Procedural Orientation**: This includes teaching practitioners how to use RAI toolkits and comply with their companies' RAI policies. The focus is on practical implementation, such as using fairness toolkits like Fairlearn and AI Fairness 360, and transparency tools like Datasheets and Model Cards .\n\t   - **Computational Orientation**: Many RAI learning resources are oriented towards computational approaches, emphasizing technical solutions to RAI issues .\n\t\n\t2. **Ethical Considerations**:\n\t   - **Sociotechnical Approaches**: The paper highlights aspirations for RAI learning that draw on sociotechnical ways of understanding potential harms of AI systems. This includes considering the broader social and political contexts in which AI systems operate .\n\t   - **Critical Reflection**: There is a call for pedagogical approaches that encourage critical reflection among AI practitioners, pushing them to consider the socio-political forces shaping AI development and deployment .\n\t\n\tThe primary sub-pillar emphasized in the paper is the **sociotechnical approach**. This approach is seen as essential for a comprehensive understanding of RAI, as it encourages AI practitioners to consider the broader impacts of their work beyond technical metrics. The paper argues that focusing solely on computational and procedural aspects may limit the understanding of RAI to what is deemed relevant by the creators of RAI tools and policies, potentially neglecting broader ethical and social considerations .\n\t\n\tSecondary sub-pillars include the **procedural and computational orientations**, which are also discussed but are critiqued for their limitations in addressing the full scope of RAI issues .\n\t\n\tThe paper's focus on sociotechnical approaches and critical reflection shapes its overall approach to RAI by advocating for a more holistic understanding of AI's impacts. This includes integrating ethical considerations into the technical and procedural aspects of AI development, thus fostering a more responsible and reflective practice among AI practitioners .",
            "reference": "Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
        },
        {
            "paper_title": "Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness",
            "rai_definition_1": "The research paper conceptualizes Responsible AI primarily through the lens of fairness, which is a significant concern in automated decision-making systems. The paper does not provide an explicit definition of Responsible AI but instead focuses on a human-centered artificial intelligence (HCAI) design approach to address fairness in AI systems.\n\t\n\tThe paper emphasizes the importance of integrating user control into tasks, involving stakeholders in the development process, and designing user interfaces (UIs) that allow users to interact with AI systems. This approach is part of a broader effort to ensure that AI systems are reliable, safe, and trustworthy, and that they consider human values and contexts that may change depending on the situation .\n\t\n\tThe primary sub-pillar of Responsible AI discussed in the paper is fairness. The authors explore various dimensions of fairness, including individual fairness, group fairness, and subgroup fairness. They also discuss different fairness measures such as statistical parity, equalized odds, and counterfactual fairness, and the challenges associated with these measures, such as trade-offs between fairness and accuracy .\n\t\n\tSecondary sub-pillars include transparency and explainability. The paper highlights the necessity of making fairness measures transparent and providing explanations of machine learning (ML) and fairness concepts to both technical and non-technical stakeholders. This is crucial for ensuring that stakeholders understand how fairness measures are calculated and applied, and for addressing any confusion about when decision-making bias becomes unfairness .\n\t\n\tThe focus on fairness and the integration of human-centered design principles shape the paper's overall approach to Responsible AI. By concentrating on fairness, the authors aim to develop AI systems that are not only fair but also transparent and understandable to a wide range of stakeholders, including domain experts and data scientists. This approach underscores the importance of involving diverse stakeholder groups in the AI development process to ensure that AI systems align with societal values and ethical principles .\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a human-centered approach that prioritizes fairness, transparency, and stakeholder involvement. This focus influences the design and evaluation of AI systems, aiming to create more equitable and trustworthy AI technologies.",
            "reference": "Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
        },
        {
            "paper_title": "Responsible Artificial Intelligence: A Structured Literature Review",
            "rai_definition_1": "The research paper provides a comprehensive and unified definition of Responsible AI, emphasizing a human-centric approach that ensures users' trust through ethical decision-making. According to the paper, Responsible AI is defined as follows:\n\t\n\t\"Responsible AI is human-centered and ensures users’ trust through ethical ways of decision making. The decision-making must be fair, accountable, not biased, with good intentions, non-discriminating, and consistent with societal laws and norms. Responsible AI ensures, that automated decisions are explainable to users while always preserving users' privacy through a secure implementation\" .\n\t\n\tThe paper categorizes Responsible AI into several key sub-pillars: Ethics, Trustworthiness, Security, Privacy, and Explainability. These sub-pillars are derived from an analysis of terms frequently associated with Responsible AI in the literature, such as fairness, accountability, transparency, and safety. The authors argue that terms like fairness and accountability can be grouped under Ethics, while transparency is often synonymous with explainability .\n\t\n\tThe primary sub-pillar emphasized in the paper is Ethics, which encompasses fairness, accountability, non-bias, good intentions, and non-discrimination. This focus on ethical decision-making is crucial for ensuring that AI systems align with societal laws and norms, thereby fostering trust among users .\n\t\n\tSecondary sub-pillars include Trustworthiness, Security, Privacy, and Explainability. Trustworthiness is considered an outcome of a responsible AI system, achieved through ethical decision-making and human-centered design. Security and Privacy are essential for protecting users' data and ensuring that AI systems operate safely. Explainability is critical for making automated decisions understandable to users, thereby enhancing transparency and trust .\n\t\n\tThe paper's focus on these specific sub-pillars shapes its overall approach to Responsible AI by advocating for a holistic framework that integrates ethical considerations, user trust, and technical safeguards. This comprehensive approach aims to guide both policymakers and practitioners in developing and regulating AI systems that are not only effective but also aligned with societal values and norms .",
            "reference": "Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
        },
        {
            "paper_title": "Resolving ethics trade-offs in implementing responsible AI",
            "rai_definition_1": "The research paper conceptualizes Responsible AI by focusing on the need to design and implement AI technologies in a manner that aligns with high-level AI ethics principles. These principles include accuracy/performance, robustness/safety, fairness, privacy, explainability/interpretability, transparency, and accountability . The paper emphasizes that these principles interact and often lead to various tensions and trade-offs, such as the trade-off between accuracy and explainability .\n\t\n\tThe paper does not provide a single explicit definition of Responsible AI but rather discusses it through a conceptual framework that includes societal, organizational, and practitioner levels. At the societal level, governments and industry bodies determine high-level principles and regulations. At the organizational level, policies and frameworks are created to manage trade-offs. At the practitioner level, developers implement these frameworks and make trade-off decisions .\n\t\n\tThe primary sub-pillar emphasized in the paper is the management of trade-offs between different ethical aspects. The paper proposes a multi-step framework for AI/ML system developers that includes proactive identification of potential tensions, prioritization and weighting of ethics aspects, and justification and documentation of trade-off decisions . This framework aims to ensure that AI systems are designed and implemented in a manner that is transparent, accountable, and aligned with ethical principles.\n\t\n\tSecondary sub-pillars discussed include the need for a proactive and structured assessment at the beginning of the AI/ML design pipeline, and the importance of explicability and transparency in the design process . The paper also highlights the use of principlism, an approach from bioethics, to guide ethical decisions by balancing and specifying principles such as respect for autonomy, nonmaleficence, beneficence, and justice .\n\t\n\tThe focus on managing trade-offs and the use of principlism shapes the paper's overall approach to Responsible AI by providing a structured method for addressing ethical tensions and ensuring that ethical considerations are integrated into the design and implementation of AI systems from the outset .",
            "reference": "Elizabeth Sanderson, Elise Schleiger, and Daniel Douglas. 2024. Resolving ethics trade-offs in implementing responsible AI. arXiv:2401.08103. Retrieved from https://arxiv.org/pdf/2401.08103"
        },
        {
            "paper_title": "Establishing data provenance for responsible artificial intelligence systems",
            "rai_definition_1": "The research paper conceptualizes Responsible AI primarily through the lens of four key characteristics: Fairness, Accountability, Transparency, and Explainability (FATE) . These characteristics are considered essential for developing and implementing AI systems that are responsible and trustworthy.\n\t\n\t1. **Fairness**: The paper discusses how AI systems can introduce discrimination due to imbalanced data, reflecting societal biases and leading to algorithmic bias. For instance, training an AI system using only medical records from male patients can result in discrimination against female patients .\n\t\n\t2. **Accountability**: The complexity of AI systems often makes it difficult to determine who is accountable for the results. This is exacerbated when individual AI services are integrated into larger systems, creating opaqueness and ambiguity about ownership and responsibility .\n\t\n\t3. **Transparency**: AI systems are often criticized for their \"black box\" nature, which limits understanding of how recommendations are made. Transparency is necessary to assess the quality and adequacy of the data used in AI systems .\n\t\n\t4. **Explainability**: The lack of explainability in AI predictions can lead to negligence of inaccuracies and biases in data. Explainability is crucial for the acceptance of AI systems, especially in high-stakes domains like healthcare, where understanding the underlying mechanisms of disease and treatment is essential .\n\t\n\tThe paper emphasizes that data provenance—a record that describes the origins and processing of data—can significantly enhance these FATE characteristics. Data provenance helps in assessing and improving the fairness, accountability, transparency, and explainability of AI systems by providing a detailed account of the data's lifecycle, from its origins to its processing and use .\n\t\n\tThe primary sub-pillar emphasized in the paper is **data provenance**, which is seen as a critical component for achieving Responsible AI. Data provenance helps mitigate biases stemming from the data's origins and pre-processing, thus enhancing the FATE characteristics of AI systems. For example, it can uncover data quality concerns related to labor-intensive data labeling, which is often performed by unqualified workers, and identify the causes of poor AI performance .\n\t\n\tSecondary sub-pillars discussed include the need for organizational data governance, data traceability, and leveraging technological advances like explainable AI (XAI) to support data provenance. These elements collectively contribute to a comprehensive understanding of Responsible AI by ensuring that AI systems are developed and implemented in a manner that is fair, accountable, transparent, and explainable .\n\t\n\tThe paper's focus on these specific sub-pillars and broader conceptual frameworks shapes its overall approach to Responsible AI by highlighting the importance of data quality and provenance in mitigating biases and enhancing trust in AI systems. This approach underscores the need for continuous monitoring and validation of AI systems to ensure they meet ethical and performance standards .",
            "reference": "Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
        },
        {
            "paper_title": "Fair and Responsible AI: A focus on the ability to contest",
            "rai_definition_1": "The research paper conceptualizes Responsible AI primarily through the lens of contestability, emphasizing the importance of designing AI systems that allow individuals to challenge and seek redress for decisions made by these systems. This focus is rooted in the broader ethical considerations of fairness, accountability, and transparency.\n\t\n\tThe paper does not provide an explicit definition of Responsible AI but frames it within the context of ensuring that AI systems are designed to be fair, accountable, and transparent, especially in high-stakes decision-making scenarios such as sentencing, hiring, and loan applications . The primary sub-pillar emphasized in the paper is contestability, which is seen as a crucial safeguard for ensuring procedural fairness and enhancing perceptions of fairness and satisfaction among decision subjects.\n\t\n\tThe paper highlights several key aspects of contestability:\n\t1. **Transparency and Explainability**: The need for AI systems to be transparent and provide explanations for their decisions to enable meaningful contestation .\n\t2. **Human-Centered Design**: The importance of understanding the needs and expectations of decision subjects and the community to design effective contestation processes .\n\t3. **Procedural Fairness**: Drawing from procedural justice literature, the paper underscores that the design of the contestation process itself impacts perceptions of fairness. For instance, having a new decision-maker assess an appeal is perceived as fairer than having the same decision-maker .\n\t\n\tSecondary sub-pillars discussed include:\n\t- **Accountability**: Ensuring that AI systems are accountable for their decisions and actions .\n\t- **Fairness**: Both substantive and procedural dimensions of fairness are considered, with a focus on the ability to contest and seek redress .\n\t\n\tThe paper's focus on contestability shapes its overall approach to Responsible AI by advocating for design processes that prioritize user needs and perceptions of fairness. This approach is intended to mitigate the negative impacts of AI systems deployed without appropriate safeguards and to ensure that AI systems are not only technically robust but also ethically sound and socially acceptable .\n\t\n\tIn summary, while the paper does not provide a single, explicit definition of Responsible AI, it conceptualizes it through the framework of contestability, emphasizing transparency, human-centered design, and procedural fairness as key components.",
            "reference": "Helena Liang Lyons, Ehsan Velloso, and Tim Miller. 2021. Fair and Responsible AI: A focus on the ability to contest. arXiv:2102.10787. Retrieved from https://arxiv.org/pdf/2102.10787."
        },
        {
            "paper_title": "The use of responsible artificial intelligence techniques in the context of loan approval processes",
            "rai_definition_1": "The research paper conceptualizes Responsible AI primarily through the lenses of explainability and fairness. These two ethical principles are emphasized as fundamental components of Responsible and Trustworthy AI systems.\n\t\n\tExplainability is defined as the core of any AI system that aims to be considered trustworthy. It is described as a dynamic characteristic of a model that involves providing explicit knowledge about why a specific prediction was made in terms that are easy to understand. This property is crucial for building trust in the decisions taken by a model and is one of the critical factors influencing the adoption of ML techniques in high-risk applications .\n\t\n\tFairness, on the other hand, is conceptualized as the absence of any prejudice or favoritism toward an individual or a group based on their inherent or acquired characteristics. The paper discusses the importance of detecting and mitigating biases within the model’s behavior to ensure fairness. The reweighing algorithm, which follows the independence criterion, is used for bias mitigation in the presented system .\n\t\n\tThe paper's focus on these specific sub-pillars—explainability and fairness—shapes its overall approach to Responsible AI by emphasizing the need for transparency and equity in AI systems. This approach is operationalized through the development of tools and frameworks that support these principles, such as a standardized explainability tool and a fairness tool, which are integrated into a proprietary framework for loan approval processes .\n\t\n\tIn summary, the paper defines Responsible AI through the ethical principles of explainability and fairness, with a particular emphasis on building trust and ensuring non-discrimination in AI systems.",
            "reference": "Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
        },
        {
            "paper_title": "Causal learning for socially responsible AI",
            "rai_definition_1": "The research paper conceptualizes and defines Responsible AI (SRAI) primarily through the lens of Causal Learning (CL). It does not provide a single explicit definition of Responsible AI but rather frames it within a broader conceptual framework that includes both competence/functionality and ethical considerations.\n\t\n\tThe paper emphasizes that AI systems, while capable of high efficiency and precision, also pose significant risks due to their potential for socially indifferent behaviors and the lack of understanding of their underlying data-generating processes (DGP) . To address these challenges, the paper proposes the use of CL as a means to enhance the social responsibility of AI systems.\n\t\n\tThe primary sub-pillar emphasized in the paper is **Causal Learning (CL)**, which is presented as a comprehensive approach to achieving SRAI. CL is described as essential for uncovering real-world DGPs, which is crucial for understanding and mitigating biases, ensuring fairness, and improving transparency and generalizability of AI systems . The paper outlines seven CL tools that are inherently related to SRAI, including causal assumptions, do-calculus, counterfactual analysis, mediation analysis, adaptability, handling missing data, and causal discovery .\n\t\n\t**Fairness** is another significant sub-pillar discussed in the paper. The authors review how CL can be used to train fair AI systems by addressing biases and ensuring that models produce results independent of sensitive variables such as gender . This is crucial for mitigating societal issues that arise from biased AI systems.\n\t\n\t**Transparency** and **explainability** are also highlighted as critical components of SRAI. The paper discusses how causal interpretability can help generate human-friendly explanations for AI decisions, thereby increasing user trust and understanding of AI systems .\n\t\n\t**Privacy** is mentioned as a crucial tenet of SRAI, although it is noted that the use of CL to enhance privacy has been barely studied in the literature . The paper suggests that CL can be used to create privacy-preserving data representations by removing sensitive information.\n\t\n\tThe paper's focus on these specific sub-pillars—particularly CL, fairness, transparency, and privacy—shapes its overall approach to Responsible AI by advocating for a more scientifically grounded and ethically aware development of AI systems. By leveraging CL, the paper aims to address both the functional and ethical challenges of AI, ensuring that AI systems are not only effective but also socially responsible.\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a multi-faceted framework that integrates causal learning with ethical considerations such as fairness, transparency, and privacy. This approach aims to create AI systems that are both functionally robust and ethically sound.",
            "reference": "Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
        },
        {
            "paper_title": "RAISE: leveraging responsible AI for service excellence",
            "rai_definition_1": "The research paper conceptualizes and defines Responsible AI through the RAISE framework, which stands for Responsible AI for Service Excellence. This framework is described as \"a strategic framework for responsibly integrating artificial intelligence into service industries\" and emphasizes collaborative AI design and deployment that aligns with evolving global standards and societal well-being while promoting business success and sustainable development .\n\t\n\tThe RAISE framework is built upon three key components:\n\t\n\t1. **Ethical Foundations**: This component includes principles such as Justice and Fairness, Transparency, Accountability, Privacy, and Non-maleficence, which are foundational for ethical AI applications. These principles ensure that AI technologies are developed and utilized in ways that are fair, open, and respectful of individual rights and societal norms .\n\t\n\t2. **Service Entities**: This component identifies the various stakeholders integral to the AI service landscape, including service organizations, policymakers, AI developers, researchers, and customers. These entities play a crucial role in pushing the boundaries of AI capabilities while shaping the ethical, legal, and societal implications of AI adoption and transformation .\n\t\n\t3. **RAISE Principles**: These principles represent the actionable aspect of the framework and include:\n\t   - **Embrace AI to Serve the Greater Good**: Leveraging AI technology to benefit society and contribute positively to the greater good.\n\t   - **Design and Deploy Responsible AI**: Ensuring AI systems adhere to ethical standards and are beneficial and fair.\n\t   - **Practice Transformative Collaboration**: Encouraging collaborative efforts among different stakeholders to create significant and positive changes through the use of AI .\n\t\n\tThe paper emphasizes that these components are interdependent and form a continuous, iterative process, reinforcing each other to ensure a dynamic and ongoing commitment to ethical principles in AI-driven service excellence .\n\t\n\tThe primary sub-pillar emphasized in the paper is the **Ethical Foundations**, which form the bedrock of the RAISE framework. This sub-pillar contributes to a comprehensive understanding of Responsible AI by ensuring that AI technologies are developed and utilized in ways that are fair, open, and respectful of individual rights and societal norms . Secondary sub-pillars include the **Service Entities** and **RAISE Principles**, which are also given significant emphasis as they outline the roles of various stakeholders and the actionable steps for implementing responsible AI .\n\t\n\tThe paper's focus on these specific sub-pillars and broader conceptual frameworks shapes its overall approach to Responsible AI by providing a structured and holistic model that integrates ethical considerations, stakeholder roles, and practical principles for responsible AI deployment. This approach ensures that AI technologies contribute to both business success and societal well-being, aligning with the evolving global standards and ethical guidelines .",
            "reference": "Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
        },
        {
            "paper_title": "A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI",
            "rai_definition_1": "The research paper conceptualizes Responsible AI (RAI) by focusing on a subset of principles that characterize it, specifically those identified by Jobin et al. . The paper acknowledges the complexities and nuances around defining RAI and its principles, sometimes referred to as Trustworthy or Ethical AI. To address the issue of principle proliferation, the authors decided to concentrate on four main principles: Transparency, Diversity & Non-discrimination & Fairness, Technical Robustness & Safety, and Privacy & Data Governance .\n\t\n\tThe paper does not provide a single explicit definition of Responsible AI but instead uses a conceptual framework based on these principles. Each principle is mapped to system requirements to provide a clearer understanding. For instance, Transparency is linked with traceability and explainability, while Diversity & Non-discrimination & Fairness is associated with avoiding unfair bias and ensuring accessibility and universal design . Technical Robustness & Safety includes requirements such as resilience to attack, security, and reliability, and Privacy & Data Governance covers respect for privacy and data integrity .\n\t\n\tThe primary sub-pillar emphasized in the paper is the need for comprehensive frameworks that cover all phases of the Software Development Life Cycle (SDLC) and provide practical tools for implementation and auditing. The paper highlights a significant gap in existing frameworks, noting that most focus only on the Requirements Elicitation phase and lack tools for practical validation and implementation across the entire SDLC . This focus on practical support and comprehensive coverage shapes the paper's approach to Responsible AI, emphasizing the need for frameworks that are both theoretically sound and practically useful.\n\t\n\tSecondary sub-pillars discussed include the diversity of perspectives and the lack of standardization in RAI frameworks, as well as the need for tools that can translate ethical principles into concrete actions . The paper suggests that the lack of comprehensive and standardized frameworks hinders the effective adoption of RAI practices in real-world projects.\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a framework of key principles and emphasizes the need for comprehensive, practical tools that support all phases of the SDLC. This approach aims to bridge the gap between high-level ethical guidelines and their practical implementation, ensuring that RAI practices can be effectively adopted by both technical and non-technical stakeholders.",
            "reference": "Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
        },
        {
            "paper_title": "Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions",
            "rai_definition_1": "The research paper by Woodgate and Ajmeri  does not provide an explicit definition of Responsible AI. Instead, it conceptualizes Responsible AI through a framework of ethical principles derived from normative ethics, which are operationalized within AI and computer science literature. The paper categorizes these principles into three main strands of normative ethical theory: deontology, virtue ethics, and consequentialism.\n\t\n\tDeontological theories focus on rules, rights, and duties, emphasizing the importance of adhering to ethical norms and obligations . Virtue ethics, on the other hand, posits that ethicality arises from the inherent character of an individual rather than the rightness or wrongness of specific actions . Consequentialist theories assert that the ethicality of an action is determined entirely by its outcomes .\n\t\n\tThe paper further categorizes ethical principles into a taxonomy of 21 principles discussed in AI and computer science literature, providing a detailed mapping of how each principle has been operationalized. This includes both abstract definitions and technical implementations, highlighting the challenges and applications associated with each principle .\n\t\n\tThe primary sub-pillar emphasized in the paper is the operationalization of ethical principles, which involves selecting technical implementations, clarifying system architectures, specifying ethical principles, and using rules, consequences, or virtues to guide ethical decision-making . This focus on operationalization is crucial for enabling AI systems to methodically reason about ethics and make ethical decisions in various contexts.\n\t\n\tSecondary sub-pillars include the identification of gaps in current research, such as the need to expand the taxonomy of ethical principles, resolve ethical dilemmas where principles conflict, and incorporate ethical principles in socio-technical systems (STS) while considering broad social dynamics . These secondary sub-pillars are given significant attention, suggesting that the paper aims to provide a comprehensive framework for improving the ethical evaluation capacities of Responsible AI.\n\t\n\tOverall, the paper's approach to Responsible AI is shaped by its emphasis on the operationalization of ethical principles and the identification of research gaps, which collectively contribute to a more nuanced and practical understanding of how to implement ethical considerations in AI systems.",
            "reference": "Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
        },
        {
            "paper_title": "Strategies for Increasing Corporate Responsible AI Prioritization",
            "rai_definition_1": "The research paper conceptualizes Responsible AI (RAI) through a multifaceted lens, emphasizing both ethical considerations and practical implementation challenges. While it does not provide a single explicit definition of Responsible AI, it discusses various conceptual frameworks and sub-pillars that collectively shape its understanding of RAI.\n\t\n\t### Conceptual Frameworks and Broader Categories\n\t\n\t1. **Ethical Considerations**:\n\t   - The paper highlights the importance of ethical considerations in the treatment of participants, ensuring their comfort with anonymity, and obtaining consent before recording interviews or taking notes .\n\t   - Ethical considerations also extend to the broader impact of AI on society, emphasizing the need for substantive ethics work over performative ethics-washing .\n\t\n\t2. **Practical Implementation Challenges**:\n\t   - The paper identifies several obstacles to implementing RAI, such as organizational barriers, vague principles, and a lack of resources invested by companies .\n\t   - It also discusses the need for better alignment between research outputs and practitioner needs, suggesting that current research often does not match what practitioners require .\n\t\n\t### Sub-Pillars of Responsible AI\n\t\n\t1. **Accountability**:\n\t   - The paper emphasizes the need for accountability to prevent ethics-washing and ensure that companies are held responsible for their ethical claims .\n\t   - Collaboration between RAI experts and journalists is suggested as a way to hold companies accountable and make sense of ethics statements .\n\t\n\t2. **Education and Expertise**:\n\t   - The integration of ethics modules in education is proposed to instill both intention and expertise in future AI practitioners .\n\t   - The paper stresses that RAI work requires collaboration with experts and that being a \"good person\" is not sufficient for effective RAI .\n\t\n\t3. **Structural Incentives**:\n\t   - The paper discusses the potential of alternative corporate structures, such as non-profits and public benefit corporations (PBCs), to prioritize RAI .\n\t   - It also mentions the role of ESG ratings and other indicators in creating an incentive structure that values RAI .\n\t\n\t4. **Research and Legal Interventions**:\n\t   - The need for research that fits well into the typical software engineering stack and is informed by the needs of practitioners is highlighted .\n\t   - The paper also discusses the importance of interdisciplinary work between technology and law to clarify what RAI interventions are legally permissible .\n\t\n\t### Primary and Secondary Sub-Pillars\n\t\n\t- **Primary Sub-Pillar**: **Accountability** is the most prominently featured sub-pillar. The paper argues that accountability mechanisms are crucial for shifting incentives from performative ethics to substantive ethical actions .\n\t- **Secondary Sub-Pillars**: **Education and Expertise**, **Structural Incentives**, and **Research and Legal Interventions** are also discussed but with slightly less emphasis compared to accountability .\n\t\n\t### Influence on Overall Approach\n\t\n\tThe paper's focus on these specific sub-pillars shapes its overall approach to Responsible AI by advocating for a comprehensive strategy that includes ethical accountability, educational reforms, structural changes, and research alignment. This multifaceted approach aims to address both the ethical and practical challenges of implementing RAI in corporate settings.\n\t\n\tIn summary, while the paper does not provide a single explicit definition of Responsible AI, it conceptualizes it through a combination of ethical considerations and practical implementation challenges, with a strong emphasis on accountability as the primary sub-pillar.",
            "reference": "Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855"
        },
        {
            "paper_title": "Responsible AI challenges in end-to-end machine learning",
            "rai_definition_1": "The research paper conceptualizes Responsible AI as a multifaceted approach that encompasses several key objectives, including fairness, robustness, explainability, transparency, and accountability. These objectives are not limited to model training but extend to all steps of end-to-end machine learning, such as data collection, data cleaning and validation, model evaluation, and model management and serving .\n\t\n\tThe paper does not provide a single explicit definition of Responsible AI but rather frames it within a broader conceptual framework that includes both competence/functionality and ethical considerations. The primary sub-pillars emphasized are fairness and robustness, which are closely related to the training data. Fairness involves ensuring that AI models do not discriminate against users, while robustness pertains to the resilience of models against noisy or poisoned data .\n\t\n\tThe paper proposes three key research directions to support Responsible AI: depth, breadth, and usability. Depth involves addressing multiple objectives like fairness and robustness together. Breadth focuses on supporting Responsible AI across all steps of the machine learning lifecycle. Usability aims to make these techniques easy to deploy and actionable for machine learning users .\n\t\n\tThe primary sub-pillar of fairness is prominently featured, with various systems proposed to address it, such as FR-Train for fair and robust model training, Slice Tuner for selective data acquisition, and FairBatch for easy-to-use batch selection. Robustness is also a significant focus, with systems like MLClean for data cleaning and FR-Train for robust model training .\n\t\n\tSecondary sub-pillars like explainability, transparency, and accountability are acknowledged but are currently outside the primary scope of the paper. The emphasis on fairness and robustness shapes the overall approach to Responsible AI by focusing on mitigating biases and defending against adversarial attacks, which are critical for ensuring the ethical deployment of AI systems .\n\t\n\tIn summary, the paper's approach to Responsible AI is comprehensive, addressing multiple ethical and functional objectives across the entire machine learning lifecycle, with a particular focus on fairness and robustness.",
            "reference": "Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
        },
        {
            "paper_title": "Human-Centered Responsible Artificial Intelligence: Current & Future Trends",
            "rai_definition_1": "The research paper conceptualizes Responsible AI through the lens of Human-Centered Responsible Artificial Intelligence (HCR-AI), emphasizing the integration of human values into the design and development of AI systems. This approach aims to create AI systems that benefit people and societies while mitigating potential harms .\n\t\n\tThe paper does not provide a single explicit definition of Responsible AI but instead discusses it within a broader conceptual framework that includes both competence/functionality and ethical considerations. The competence/functionality aspects are addressed through the emphasis on performance, usability, and impact, while the ethical considerations encompass values such as transparency, fairness, explainability, accountability, autonomy, sustainability, and trust .\n\t\n\tThe primary sub-pillar emphasized in the paper is the human-centered approach, which focuses on the socio-technical integration of AI systems to preserve human autonomy and control. This approach also considers the impacts of AI deployment on society, organizations, and individuals . By prioritizing human-centered design, the paper underscores the importance of aligning AI systems with human rights and ethics to reduce potential harms .\n\t\n\tSecondary sub-pillars discussed include the socio-cultural and technical factors that influence the ethical values of Responsible AI. These factors highlight the varying perceptions and priorities of different stakeholders, such as the general population valuing safety, privacy, and performance, while practitioners prioritize fairness, dignity, and inclusiveness . Additionally, the paper touches on the role of AI labor, emphasizing the importance of the workforce behind data annotation and the challenges they face .\n\t\n\tThe paper's focus on these specific sub-pillars and broader conceptual frameworks shapes its overall approach to Responsible AI by advocating for a comprehensive, human-centered methodology that integrates ethical considerations throughout the AI lifecycle. This approach aims to foster collaboration among researchers from diverse backgrounds to advance the field of Responsible AI .",
            "reference": "Marzieh Tahaei, Myrsini Constantinides, Daniele Quercia, P. Alex Dow, and Dave Murray-Rust. 2023. Human-Centered Responsible Artificial Intelligence: Current & Future Trends. arXiv:2302.08157. Retrieved from https://arxiv.org/pdf/2302.08157."
        },
        {
            "paper_title": "Five Ps: Leverage zones towards responsible AI",
            "rai_definition_1": "The research paper conceptualizes Responsible AI through a holistic, systems-oriented approach, emphasizing the need to address root causes and unintended consequences of AI systems rather than merely focusing on technical fixes. The paper does not provide a single explicit definition of Responsible AI but instead frames it within broader conceptual frameworks and sub-pillars.\n\t\n\tThe primary framework used in the paper is the \"Five Ps\" framework, which categorizes interventions into five leverage zones: Parameter, Process, Pathway, and Purpose. These zones are used to understand and plan interventions in AI systems to achieve Responsible AI .\n\t\n\t1. **Parameter Zone**: This zone deals with tractable characteristics of an AI system, such as tweaking algorithms and parameters to address visible flaws. Changes in this zone are typically incremental and do not address the root causes of problems .\n\t\n\t2. **Process Zone**: This zone considers the interactions between feedback elements of an AI system, including social and technical processes associated with its design, development, and deployment. Changes here are likely to resolve emerging issues or amplify the effects of assumptions .\n\t\n\t3. **Pathway Zone**: This zone focuses on the ways information flows, rules are set, and power is organized within the system. Interventions here aim to improve transparency, governance, and the structural aspects that allow the AI to operate .\n\t\n\t4. **Purpose Zone**: This zone has the most potential for transformative change, addressing the norms, values, goals, and worldviews of AI developers. It includes the underlying paradigms and the ability to imagine new paradigms for the system .\n\t\n\tThe paper emphasizes the importance of considering these zones in a holistic manner to avoid fragmented and siloed approaches. It argues that focusing solely on technical solutions (Parameter and Process zones) often leads to short-term fixes and overlooks deeper structural and normative issues (Pathway and Purpose zones) .\n\t\n\tThe primary sub-pillar emphasized in the paper is the Purpose zone, which is seen as crucial for achieving transformative change. This zone encourages developers to question the fundamental purpose of the AI system and align it with broader societal values and goals . Secondary sub-pillars include the Pathway and Process zones, which are also important but are seen as less transformative compared to the Purpose zone .\n\t\n\tBy focusing on these sub-pillars, the paper advocates for a comprehensive approach to Responsible AI that goes beyond technical fixes and addresses the broader socio-technical challenges. This approach aims to create a more responsible, transparent, and trustworthy AI by engaging with the deeper questions about the governing rules, structure, business model, and purpose of AI systems .\n\t\n\tIn summary, the paper's conceptualization of Responsible AI is deeply rooted in the Five Ps framework, with a strong emphasis on the Purpose zone for achieving long-term, transformative change. This holistic approach aims to address the root causes of issues in AI systems and promote a more responsible and ethical development and deployment of AI technologies.",
            "reference": "Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
        },
        {
            "paper_title": "Collect, measure, repeat: Reliability factors for responsible AI data collection",
            "rai_definition_1": "The research paper conceptualizes Responsible AI (RAI) primarily through the lens of data quality and reliability in crowdsourced data collection. It does not provide an explicit definition of Responsible AI but instead focuses on a methodology designed to guide data collection with a set of metrics for an in-depth, iterative analysis of human factors influencing data quality and reliability .\n\t\n\tThe paper emphasizes several sub-pillars of Responsible AI, particularly transparency, fairness, and accountability. These sub-pillars are integrated into the proposed methodology, which aims to bring transparency to the data collection process by systematically analyzing and documenting the factors that influence data quality. This includes understanding human disagreement, validating rater quality, and measuring the reliability and stability of crowdsourced data .\n\t\n\tThe primary sub-pillar emphasized is transparency. The methodology aims to make the data collection process transparent through a set of metrics that allow for a systematic analysis of data quality. This transparency is achieved by providing a step-wise guide for practitioners to explore factors that influence or impact the reliability and quality of their collected data. The methodology includes reliability and reproducibility scorecards that offer a holistic picture of data quality, enabling thorough and systematic evaluation and comparison of different data collection experiments .\n\t\n\tSecondary sub-pillars include fairness and accountability. The paper discusses how the proposed methodology can help address fairness and accountability aspects in data collection by making the analysis process transparent and by systematically measuring and mitigating cognitive biases that may affect data quality. The methodology also emphasizes the importance of documenting the provenance of data collection to facilitate responsible reuse of datasets .\n\t\n\tThe focus on these specific sub-pillars shapes the overall approach to Responsible AI in the paper by ensuring that data collection practices are not only transparent but also fair and accountable. This comprehensive approach aims to improve the reliability and quality of AI systems by addressing the human factors that influence data quality, thereby contributing to the broader goals of Responsible AI .\n\t\n\tIn summary, while the paper does not provide an explicit definition of Responsible AI, it conceptualizes it through a framework that emphasizes transparency, fairness, and accountability in data collection practices. The primary sub-pillar of transparency is most prominently featured, with secondary emphasis on fairness and accountability, shaping a comprehensive approach to Responsible AI.",
            "reference": "Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
        },
        {
            "paper_title": "“It is currently hodgepodge”: Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values",
            "rai_definition_1": "The research paper conceptualizes Responsible AI (RAI) as an overarching field that integrates various human values, principles, and actions to ensure the ethical and responsible development of AI/ML systems. The paper does not provide a single explicit definition of Responsible AI but rather describes it through a broad conceptual framework that includes multiple dimensions and values.\n\t\n\tRAI is described as having its roots in Ethical AI, which emphasizes critical engagement with ethical values within the AI/ML field . This includes translating ethical values into implementation scenarios and guidelines, and addressing critical ethical problems . Additionally, RAI draws inspiration from AI for Social Good (AI4SG), which focuses on broader human values beyond just ethical considerations, aiming to translate these values into positive community outcomes .\n\t\n\tThe paper highlights several sub-pillars of RAI, including:\n\t\n\t1. **Ethical Values**: These include fairness, accountability, transparency, and explainability (FATE) . These values are essential for making AI systems more accountable and trustworthy.\n\t2. **Social Good Values**: These encompass values like solidarity and community goals, such as the UN Sustainable Development Goals .\n\t3. **Institutional and Organizational Values**: The paper discusses the importance of co-production, where different practitioners and stakeholders come together to align and implement RAI values within their organizational contexts .\n\t\n\tThe primary sub-pillar emphasized in the paper is the co-production of RAI values. Co-production is described as an iterative process where organizations produce collective knowledge and align individual and team values to uphold RAI norms . This process involves multiple roles and stakeholders, and it addresses the challenges and strategies for implementing RAI values in practice .\n\t\n\tSecondary sub-pillars include the specific ethical values (FATE) and the broader social good values, which are also given significant attention but are discussed within the context of co-production and organizational implementation .\n\t\n\tThe paper's focus on co-production as a primary sub-pillar shapes its overall approach to Responsible AI by emphasizing the collaborative and iterative nature of embedding RAI values within AI/ML systems. This approach highlights the importance of understanding the diverse roles and organizational structures that influence the successful implementation of RAI values .\n\t\n\tIn summary, Responsible AI in this research paper is conceptualized through a multi-dimensional framework that includes ethical, social, and organizational values, with a strong emphasis on the co-production process to align and implement these values effectively.",
            "reference": "Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
        },
        {
            "paper_title": "On the Road to Designing Responsible AI Systems in Military Cyber Operations",
            "rai_definition_1": "The research paper by Maathuis  conceptualizes and defines Responsible AI (RAI) within the context of military Cyber Operations. The paper provides a detailed definition and an analytical model for RAI, emphasizing the integration of socio-ethical and legal principles, norms, and values throughout the lifecycle of AI systems used in military contexts.\n\t\n\t### Definition of Responsible AI\n\tThe paper defines RAI in military Cyber Operations as \"a sub-field of AI that deals with the integration of socio-ethical and legal principles, norms, and values when designing, developing, deploying, and using AI methods, techniques, and technologies embedded in different military cyber systems and processes\" . This definition underscores the importance of embedding ethical considerations and legal compliance in every phase of AI system development and deployment.\n\t\n\t### Conceptual Frameworks and Sub-Pillars\n\tThe paper categorizes RAI into broader concepts and sub-pillars, focusing on both competence/functionality and ethical considerations:\n\t\n\t1. **Competence/Functionality**:\n\t   - **Performance**: Ensuring AI systems are effective and efficient in their intended military applications.\n\t   - **Usability**: Making sure that AI systems are user-friendly and can be effectively operated by military personnel.\n\t   - **Impact**: Assessing the broader implications of AI systems on military operations and civilian infrastructure.\n\t\n\t2. **Ethical Considerations**:\n\t   - **Explainability**: AI systems should be transparent and their decision-making processes understandable to human operators.\n\t   - **Safety**: Ensuring that AI systems do not cause unintended harm and are robust against adversarial attacks.\n\t   - **Accountability**: Establishing clear lines of responsibility for the actions and decisions made by AI systems.\n\t   - **Fairness**: Ensuring that AI systems do not perpetuate biases and are equitable in their operations.\n\t   - **Privacy**: Protecting the data and privacy of individuals affected by AI systems.\n\t\n\t### Primary and Secondary Sub-Pillars\n\tThe primary sub-pillar emphasized in the paper is **Accountability**. The paper argues that military commanders are ultimately responsible for the decisions made by AI systems, especially in targeting decisions where the potential for collateral damage is significant . This focus on accountability ensures that there is a human in the loop who can be held responsible for the outcomes of AI-driven actions.\n\t\n\tSecondary sub-pillars include **Safety** and **Explainability**, which are also given considerable attention. The paper discusses the need for AI systems to be safe and reliable, particularly in the dynamic and volatile environment of cyberspace. Explainability is crucial for ensuring that military personnel can understand and trust the decisions made by AI systems .\n\t\n\t### Influence on Overall Approach\n\tThe paper's focus on these specific sub-pillars shapes its overall approach to RAI by advocating for a human-centered design that aligns with socio-ethical values and principles. This approach ensures that AI systems are not only technically proficient but also ethically sound and legally compliant. The emphasis on accountability, safety, and explainability reflects a comprehensive understanding of the complexities involved in integrating AI into military operations, aiming to mitigate risks and enhance trust in AI systems.\n\t\n\tIn summary, the paper provides a robust framework for understanding and implementing Responsible AI in military Cyber Operations, with a strong emphasis on accountability, safety, and explainability as key components of ethical AI deployment.",
            "reference": "C. Maathuis. 2022. On the Road to Designing Responsible AI Systems in Military Cyber Operations. European Conference on Cyber Warfare and Security. Retrieved from https://papers.academic-conferences.org/index.php/eccws/article/download/204/358"
        },
        {
            "paper_title": "Responsible AI for Earth Observation",
            "rai_definition_1": "The research paper conceptualizes and defines Responsible AI within the context of Earth Observation (EO) by emphasizing a comprehensive framework that integrates both technical and ethical dimensions. The paper does not provide a single explicit definition of Responsible AI but rather outlines a multifaceted approach that includes several critical components and sub-pillars.\n\t\n\tThe primary sub-pillars of Responsible AI discussed in the paper are:\n\t\n\t1. **Mitigating Unfair Bias**: This involves addressing and minimizing biases present in the algorithms and models used in AI4EO applications. Unfair bias can arise during many stages of the machine learning workflow, such as when the AI systems are trained on data that are not representative or when the data used for training contain inherent biases .\n\t\n\t2. **AI Security in EO**: This focuses on implementing measures to increase the robustness, control of uncertainty, and explainability of the AI systems and the data they process in the context of EO missions .\n\t\n\t3. **Geo-Privacy and Privacy-Preserving Issues**: This aims to find a balance between leveraging the benefits of geospatial data for various EO applications while respecting and safeguarding individuals’ privacy rights .\n\t\n\t4. **Ethical Principles in AI4EO**: This introduces the guidelines and standards that govern the maintenance of scientific excellence, data availability, and the guidance of AI usage in the context of EO research and applications .\n\t\n\t5. **AI4EO for Social Good**: This refers to applying cutting-edge AI techniques to address and contribute positively to societal challenges and global issues. It aims to leverage advanced AI4EO technologies to find innovative solutions to pressing problems, promote sustainability, and enhance the overall well-being of communities .\n\t\n\tThe paper emphasizes that Responsible AI is not merely a theoretical construct but an imperative for advancing Earth observation and geosciences. By aligning technological advances with ethical considerations, the discourse aims to contribute to the ongoing dialogue surrounding the responsible and sustainable deployment of AI in the fields of EO and geosciences .\n\t\n\tThe focus on these specific sub-pillars shapes the overall approach to Responsible AI by ensuring that ethical considerations are incorporated into every stage of AI4EO projects, from data collection to the interpretation of geospatial intuitions. This comprehensive approach underscores the importance of maintaining scientific excellence, transparency, and fairness while leveraging AI technologies to address global challenges .\n\t\n\tIn summary, the paper provides a detailed conceptual framework for Responsible AI in EO, emphasizing the importance of mitigating biases, ensuring security, preserving privacy, adhering to ethical principles, and promoting social good. These sub-pillars collectively contribute to a holistic understanding of Responsible AI and guide its practical implementation in EO applications.",
            "reference": "Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
        },
        {
            "paper_title": "A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations",
            "rai_definition_1": "The research paper conceptualizes Responsible AI (RAI) primarily through the lens of tools designed to support AI practitioners and other stakeholders in addressing ethical issues in AI system development and deployment. The paper defines RAI tools as interventions that help in mitigating potential societal impacts throughout the AI development and deployment pipeline .\n\t\n\tThe paper situates RAI tools within a broader conceptual framework that spans high-level ethical AI principles and low-level formalizations of social concepts like fairness metrics. High-level principles are seen as shaping the broader environment but are too abstract for direct application by practitioners. Conversely, low-level mathematical formalisms require substantial technical expertise to be operationalized during AI development .\n\t\n\tThe primary sub-pillars emphasized in the paper include fairness, transparency, and accountability. These sub-pillars are critical as they address the core ethical concerns that RAI tools aim to mitigate. The paper highlights that the default ways of developing AI systems are insufficient to achieve these goals on their own, necessitating the use of RAI tools to bridge the \"principles to practice\" gap .\n\t\n\tSecondary sub-pillars discussed include usability and the effectiveness of RAI tools in changing development practices and outcomes. The paper notes that while usability is a common focus in evaluations, the effectiveness of RAI tools in achieving their desired aims—such as changes in AI development and deployment practices—is often overlooked .\n\t\n\tThe paper's focus on these specific sub-pillars shapes its overall approach to Responsible AI by emphasizing the need for robust evaluation frameworks that can assess the effectiveness of RAI tools in real-world settings. This approach underscores the importance of not only developing RAI tools but also ensuring they are effective in practice, thereby contributing to a comprehensive understanding of Responsible AI .",
            "reference": "Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
        },
        {
            "paper_title": "Making Responsible AI the Norm rather than the Exception",
            "rai_definition_1": "The research paper conceptualizes Responsible AI through a comprehensive framework that includes both ethical considerations and practical implementation strategies. The paper does not provide a single explicit definition of Responsible AI but rather discusses it through various sub-pillars and broader conceptual frameworks.\n\t\n\tThe primary sub-pillar emphasized in the paper is the \"Learning, Knowledge, and Information Exchange (LKIE)\" framework. This framework is designed to accelerate organizational knowledge and ensure that insights gleaned from on-the-ground practice are shared across different arms of the US Government. The LKIE aims to reduce redundancy, accelerate the deployment of Responsible AI, and build trust with external stakeholders by leveraging collective insights and best practices .\n\t\n\tSecondary sub-pillars include:\n\t1. **The Three Ways of Responsible AI**: This framework borrows from the idea of The Three Ways of DevOps and focuses on improving the responsible posture of the overall system, providing fast, visible, and accurate feedback, and continual learning and implementation of Responsible AI .\n\t2. **Empirically-driven risk prioritization matrix**: This matrix helps in prioritizing Responsible AI practices based on the likelihood and severity of potential violations, making it easier for teams to adopt these practices without feeling overwhelmed .\n\t3. **Achieving the right level of complexity**: This principle emphasizes the importance of balancing the complexity exposed to developers, ensuring they have enough control to make necessary changes without being overburdened .\n\t\n\tThe paper's focus on these specific sub-pillars shapes its overall approach to Responsible AI by emphasizing practical, actionable measures that can be integrated into existing workflows. This approach aims to make Responsible AI a norm rather than an exception by providing concrete tools and frameworks that facilitate its implementation across various government agencies.\n\t\n\tIn summary, the paper conceptualizes Responsible AI through a multi-faceted approach that includes ethical considerations, practical frameworks, and actionable measures, with a strong emphasis on knowledge sharing and risk prioritization.",
            "reference": "A. Gupta. 2021. Making responsible AI the norm rather than the exception. arXiv:2101.11832. Retrieved from https://arxiv.org/pdf/2101.11832"
        },
        {
            "paper_title": "Responsible AI: Portraits with Intelligent Bibliometrics",
            "rai_definition_1": "The research paper conceptualizes Responsible AI by integrating it with related concepts such as trustworthy AI, explainable AI, and ethical AI. The bibliometric analysis reveals a significant emphasis on trustworthy AI, particularly concerning security and privacy considerations. Explainable AI is associated with machine learning and related techniques, while ethical AI is somewhat detached from broader societal aspects and applications .\n\t\n\tThe paper does not provide an explicit definition of Responsible AI but instead synthesizes it within a framework that encompasses conceptual principles, AI techniques, and societal applications. This synthesis is illustrated through a cohesive integration of these AI concepts, extending to encompass principles such as transparency, justice & fairness, non-maleficence, responsibility, privacy, beneficence, freedom & autonomy, trust, sustainability, dignity, and solidarity .\n\t\n\tThe primary sub-pillar emphasized in the paper is trustworthy AI, which underscores the safety, security, fairness, and privacy protection of AI systems. This concept aligns with the interest in detecting and preventing attacks and protecting data privacy, influencing the paper's approach to Responsible AI by focusing on creating a reliable and accessible AI ecosystem .\n\t\n\tSecondary sub-pillars include explainable AI, which highlights the comprehensibility of AI systems to humans, and ethical AI, which transcends technical initiatives to include societal expectations and interactions. These sub-pillars contribute to a comprehensive understanding of Responsible AI by addressing different facets of AI's impact and functionality .\n\t\n\tThe paper's focus on these specific sub-pillars shapes its overall approach to Responsible AI by emphasizing the integration of technical robustness with ethical and societal considerations, ensuring that AI systems are not only effective but also aligned with human values and societal norms .",
            "reference": "Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
        },
        {
            "paper_title": "Towards a responsible AI development lifecycle: Lessons from information security",
            "rai_definition_1": "The research paper conceptualizes and defines Responsible AI as an AI system built with the intention of minimizing potential harm. The term \"harm\" is used in accordance with Crawford's definition, encompassing both allocative and representational harms. Allocative harms refer to improper distribution of resources based on group membership, while representational harms involve the reinforcement of subordination of certain groups based on identity factors such as race, gender, and class .\n\t\n\tThe paper identifies three dominant approaches in AI ethics: fairness metrics, interpretability, and explainability. These approaches are foundational but challenging to operationalize due to computational, epistemological, and practical limitations . The proposed framework for Responsible AI development is inspired by the secure software development lifecycle from information security, aiming to address these challenges through a structured process that includes planning and review, design review, harm modeling, penetration testing, and incident response .\n\t\n\tThe primary sub-pillar emphasized in the paper is the framework's iterative nature, which allows for continuous ethical improvements without sacrificing practicality. This approach is designed to uncover, identify, and classify harms to users throughout the AI system's lifecycle, from design to deployment . Secondary sub-pillars include fairness, interpretability, and explainability, which are integrated into the framework to ensure comprehensive ethical considerations .\n\t\n\tThe focus on these specific sub-pillars and the broader conceptual framework of harm minimization shapes the paper's overall approach to Responsible AI by providing a practical, iterative method for addressing ethical issues in AI systems. This approach is intended to be adaptable and scalable, allowing organizations to integrate ethical principles into their existing AI development processes .",
            "reference": "Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
        },
        {
            "paper_title": "Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries",
            "rai_definition_1": "The research paper conceptualizes Responsible AI by emphasizing several key principles and ethical considerations. According to the paper, Responsible AI should be \"ethical by design,\" ensuring no in-built bias and guaranteeing the respect of citizens' fundamental rights while avoiding potential liability. The main principles highlighted include explainability, fairness, accountability, and transparency . \n\t\n\tThe paper also underscores the importance of the responsibility principle, which involves ensuring the availability of measures to redress adverse individual or social effects of an algorithmic system and designating a person responsible for timely remedies . \n\t\n\tThe conceptual framework for Responsible AI in this paper is thus categorized into broader ethical considerations, with a significant focus on the following sub-pillars:\n\t\n\t1. **Explainability**: Ensuring that AI systems can be understood and interpreted by humans.\n\t2. **Fairness**: Avoiding biases and ensuring equitable treatment by AI systems.\n\t3. **Accountability**: Holding entities responsible for the outcomes of AI systems.\n\t4. **Transparency**: Making the workings and data sources of AI systems clear and open.\n\t\n\tAmong these, the responsibility principle is most prominently featured, emphasizing the need for measures to address and remedy adverse effects, which contributes to a comprehensive understanding of Responsible AI by ensuring that ethical considerations are not just theoretical but actionable .\n\t\n\tSecondary sub-pillars such as fairness and explainability are also discussed, noting the challenges in achieving them due to inherent biases in datasets and the complexity of explaining AI operations . \n\t\n\tThe paper's focus on these specific sub-pillars shapes its overall approach to Responsible AI by advocating for practical, actionable measures and emphasizing the need for transparency and accountability in AI development and deployment.",
            "reference": "Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
        }
    ]
}