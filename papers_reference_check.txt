The Responsible AI Lifecycle: Socio-technical Challenges, Principles and Practices
Abstract
Artificial Intelligence (AI) has experienced cycles of progress and stagnation known as AI summers and AI winters. The release of ChatGPT in November 2022 marked the beginning of a new AI summer—a period of heightened interest, investment, and technological advancement in artificial intelligence. As AI's influence grows, so does the importance of Responsible AI, a subfield that emerged in 2019 to address technical, social, and ethical concerns in AI development, deployment, and application. Despite extensive literature, the lack of a clear and comprehensive definition for Responsible AI hinders research progress. This study addresses this gap by proposing a consensus-driven sociotechnical definition, the Responsible AI Life Cycle (RALC), derived from a systematic literature review. RALC comprises five iterative stages: 1) system design and risk assessment, 2) data validation and testing, 3) model validation and testing, 4) system monitoring, and 5) algorithmic auditing and reassessment. This comprehensive approach operationalizes social principles through technical practices in each stage of the iterative AI life cycle, considering all human stakeholders. 

1 Introduction
What is AI? 
Artificial Intelligence (AI) is the science and engineering of making intelligent machines, especially intelligent computer programs. [59] AI systematizes and automates intellectual tasks and is therefore potentially relevant to any sphere of human intellectual activity. [62] Due to the impact of AI on critical fields such as healthcare, education, transportation, manufacturing, the technology is prevalent in our day-to-day lives yielding a truly sociotechnical phenomenon. [5] In this survey, we define the term “AI System”  as a software system (possibly embedded in hardware) designed by humans to make decisions based on perception, interpretation, and reasoning from data collected about the environment. This system exhibits properties such as autonomy, adaptability, and interactivity, which are crucial for its operation and interaction within its environment. [23]

What’s changed since November 2022? 
Today we have witnessed three trends in the creation and distribution of AI technologies. 

Widespread use of AI applications. November 30, 2022 saw the debut of ChatGPT, a generative artificial intelligence chatbot developed by OpenAI that uses natural language processing to generate human-like conversational dialogue. It broke the record of user growth becoming the fastest application reaching 100 million monthly active users, a milestone it achieved in January 2023. (Reuters).This seminal event founded an entire AI ecosystem with the private industry, academia, and government all developing or adopting AI for a variety of applications. [16] An analysis from Statista reports that there was a 200 billion U.S. dollar market for AI in 2023 and is expected to grow well beyond that to over 1.8 trillion U.S. dollars by 2030. (Statista)

Center of frontier AI research shifting from academia to private industries. Although the AI ecosystem is rapidly expanding with new innovations in every sector, the private industry, especially a small number of Big Tech Firms, have been underpinning recent progress in AI. [56] According to the 2024 Stanford AI Index, private industries are dominating frontier AI research, where in 2023, there were 51 notable Machine Learning models produced from private industry compared to the 15 Machine Learning models produced from academia. (Stanford AI Index 2024) Stanford AI Index’s results are backed up by the 2024 McKinsey Global Survey on AI, which polled thousands of business executives and managers. In their survey they observed a 17 percent increase from 55 percent of their respondents claiming to have adopted AI in 2023 to 72 percent of respondents claiming to have adopted AI in 2024. Likewise, with the increase in AI adoption 44 percent of respondents experienced the negative consequences of adopting AI. (McKinsey Global AI Survey on AI 2024). The Stanford Index states that funding for generative AI surged, nearly octupling from 2022 to reach $25.2 billion (Stanford AI Index 2024). Furthermore, without a collective effort towards achieving Responsible AI principles and practices, business competition between leading AI companies such as OpenAI, Anthropic, Hugging Face, and Inflection may cause user harm, defamation, and litigation as their AI systems prove to be scandalous. (The Role of Cooperation in Responsible AI Development)

Increase in AI incidents. There is a dual narrative surrounding AI, where it can be a beneficial next step in improving decision-making capabilities, human productivity, and accuracy or potentially being an uncontrollable force for harm. [6] From 2022 to 2023, based on statistics from the AI Incident Database there was an increase of major AI incidents, from around 90 incidents in 2022 to 123 incidents in 2023. (AIID) Incidents of bias and privacy leakage have occurred in content generation models, specifically image generation models such as Stable Diffusion, DALL·E, and Google's Imagen. (A Pathway Towards Responsible AI Generated Content) Recently in 2024 several major AI incidents caught the world’s attention. In late February 2024, images showing people of color in German military uniforms from World War II that were created with Google’s Gemini chatbot have amplified concerns that artificial intelligence could add to the internet’s already vast pools of misinformation as the technology struggles with issues around race. (NYT) Then in May 2024, OpenAI launched GPT-4o with a voice feature named "Sky," which sounded remarkably similar to Johansson's voice. Despite her previous refusals to lend her voice to the project, Johansson claimed that the voice used was uncannily similar to hers, leading to accusations that OpenAI had used her voice without consent. (NYT)  This could cause a lot of harm when AI is rapidly growing economically and technologically. AI is a double-edged sword with the potential to do good and harm, so it is crucial that we pay attention to the ethical considerations to prevent global catastrophes of developing, deploying, and using AI. [55] 

Why and what is the Responsible AI field? What is the definition of Responsible? Why choose the word Responsible?
These trends demand an imminent update of Responsible Artificial intelligence (RAI), a subfield of AI. According to Cambridge Dictionary of the English Language the word responsible is defined as “to have control and authority over something or someone and the duty of taking care of it, him, or her”. (Cambridge Dictionary) Hence, RAI puts responsibility and accountability of AI systems on the people and organizations involved: for the decisions and actions of the AI applications, and for their own decision of using AI in a given application context. [4] Lu et al. [2022] emphasizes that AI ethics principles need to be operationalized into concrete patterns and best practices usable by AI developers and other stakeholders to build responsible AI systems. [1]
 
Why is this study needed? How does this study differ from other surveys?
Countless studies and research papers have been written on RAI, but there still lacks a consensus-driven definition for Responsible AI. This poses a problem for future researchers seeking to contribute to the existing Responsible AI research by building off of previous studies. Second, existing RAI definitions either lack an iterative approach, fail to consider both social and technical dimensions, or stay at a highly theoretical level. Those that meet the criteria above often focus only on one or few principles of Responsible AI (such as privacy, security, fairness, explainability, etc.), therefore not addressing RAI as a holistic subject concerning numerous AI principles. [5] 

Scope and methodology of survey? What is the research question?
We performed an in-depth systematic analysis of more than fifty of the most cited Responsible AI papers on Google Scholar. These papers represented RAI research done in many domains including healthcare, cybersecurity, education, etc. To augment and expedite information filtering and summarization, we implemented an Answer Engine based on a Retrieval-Augmented Generation (RAG) framework [57] connecting to a Large Language Model (GPT4o). The Answer Engine is capable of giving accurate references for every sentence it generates. The use of these AI tools has reduced our research time by an order of magnitude (e.g. months to weeks). 

We conducted the entire research in the following steps:

Step 1. (AI assisted) Distill the definitions of RAI from each research paper. To build off of previous literature on Responsible AI we asked the Answer Engine following research question:

RQ1: How is the term “Responsible AI” conceptualized and defined across the current body of Responsible AI research literature?

Step 2. (Manual) Formulate a RAI Life Cycle framework. We propose the five stages of the Life Cycle framework by distilling the knowledge of the AI development process. For each stage, we use the answer of RQ1 to find the most common principles from all the papers, read their detailed descriptions in the relevant papers, and determine what the set of principles are. 

Step 3. (AI assisted) Identify the known best practices for all RAI Life Cycle stages. We ask the Answer Engine an in-depth research question: 

RQ2: What are the known best practices such as methods, techniques and tools that we can apply to implement the principles for the various stages in the RAI Life Cycle framework?

We synthesize the answers, analyze the variations and trends, and determine the best practices we would recommend. 

What and why is the Responsible AI Life Cycle ? 
After a detailed analysis we propose the Responsible AI life cycle definition framework. This perspective of defining RAI allows for a consensus-driven approach that is iterative, comprehensive, and operationalizable. The Responsible AI life cycle consists of five major stages: 1) system design and risk assessment, 2) data validation and testing, 3) model validation and testing, 4) system monitoring, and 5) algorithmic auditing and reassessment. Each stage has its socio-technical challenges that are addressed by a set of AI principles, with the later stages building off the foundations set by the previous stage. These principles guide the model and application developers to take the responsibility on their users and ultimately the human society at large. They can be defined both by the social context as to how they will affect the stakeholders, but also in a technical context with a suite of methods, techniques and tools.

Who are the paper’s audiences?
AI is a largely interdisciplinary field, where it can be seen in a wide range of applications such as content recommendations, chatbots, image recognition, machine translation, fraud detection, medical diagnosis, autonomous vehicles, and more. [4] To apply RAI principles and practices to the broad and deep field of AI, the paper’s audience ranges from AI practitioners to AI researchers of all disciplines who seek to form a better understanding of Responsible AI. 

What is the paper’s organization?
Section 2 introduces the Responsible AI framework as a life cycle. Section 3 walks through the best technical practices throughout the RAI life cycle and defines the corresponding sociotechnical RAI principles and the recommended best practices. Section 4 discusses future work. Section 5 concludes with a summary and future work. 
 2. Responsible AI Life Cycle (RALC)

Responsible AI is a sociotechnical paradigm that requires consideration of both technical and societal challenges of developing, deploying, or monitoring high-stake AI applications. Responsible AI should be supported throughout all stages of the end-to-end machine learning process, from data collection and cleaning to model training, evaluation, and management. [6] Therefore, we propose the Responsible AI Life Cycle (RALC) to define Responsible AI as an iterative, comprehensive, and operationalizable process. RALC consists of five stages 1) system design and risk assessment, 2) data validation and testing, 3) model validation and testing, 4) system monitoring, and 5) algorithmic auditing and reassessment. For RALC to be comprehensive at each stage it includes RAI principles (e.g. explainability, privacy, fairness) that consider the social implications of developing, deploying, and monitoring the AI systems with respect to stakeholders involved, while also operationalizable by providing a suite of methods, techniques and tools to implement the RAI principles. Lastly, for RALC to be iterative each stage builds upon the social and technical foundations of the previous stage. For example, the data validation and testing stage should ensure data used to train the model meets the requirements before the model can be prepared for validation and testing. Furthermore, the last stage, algorithmic auditing and reassessment can cause RALC to start over and repeat the cycle at the initial stage, system design and risk assessment, forming a truly iterative framework.
2.1 Human Agencies 
The RAI life cycle requires human agency oversight at each stage to create AI systems by humans for humans. In this process there are three groups of human stakeholders concerned: 1) foundational and frontier AI model producers, 2) AI application developers, and 3) AI application users. 

These three groups can be further broken down into subgroups. AI model producers are primary-AI organizations, who have the resources, compute, and data available to produce pretrained-foundational AI frontier models that are used by AI application developers and AI application users. Primary-AI organizations are dominated by the leading technology companies such as Microsoft OpenAI, Google DeepMind, Amazon Anthropic, and Meta AI. The model producers often develop their own AI applications, such as OpenAI’s ChatGPT or Anthropic’s Claude-AI chat. AI application developers are AI-secondary organizations or individuals, who use the foundational AI models produced by the primary-AI companies in their applications, often fine-tuning or building additional layers of customization on top of the foundation models. Lastly, the majority of stakeholders are AI application users who use the AI applications produced by the model producers or the AI application developers either directly or indirectly. They provide feedback about their experiences using the applications.

Once an AI model has been deployed it becomes a tool for its producers or developers to create AI applications. These AI applications can be classified into three categories of societal risk—high, medium, and low—to perform a risk assessment. The model producers are assigned high risk because they are AI-primary companies and the failure of their model will impact both application developers and users. While the application developers can range from high risk to low risk based on their application’s scope of impact. Large organizations such as financial firms or medical institutions may be at high risk because they are in critical fields, while individual developers building local applications may be at low risk.

3 Five Stages of RALC: Social RAI principles and Best Practices
Guidelines published by both private and public sectors describing how AI should be developed and used are commonly referred to as AI principles. [2]. AI principles concern the social dimensions of Responsible AI, which is crucial to developing sociotechnical systems as outlined in [61]. However, AI principles are meaningless theoretical morals if they are not translated into practice. Currently in the literature there are a wide range of responsible practices, but they are not mapped to the appropriate context [63]. Therefore, we will first provide an abstract mapping by classifying AI principles and practices to specific stages in RALC. Then we will review each of the five stages in detail and provide a consensus-driven definition of the AI principles that were mapped to the stage and the best practices to operationalize those AI principles. 
3.1 Stage 1: System Design and Risk Assessment
We must account for three principles in this stage: 1) functionality, 2) security, and 3) legality. These three principles can be organized into a hierarchy where the AI system first has to be functional [5] then secure and finally legal. Furthermore, these three principles are fundamental to all RALC stages. 
3.1.1 Functionality 
A functional AI system is one that performs in a manner consistent with profit maximization, operating efficiency, and other key performance indicators [5]. This definition emphasizes the importance of the AI system's ability to achieve its intended objectives effectively and efficiently. There are two notable best practices to ensure an AI system is functional. 

Requirement Engineering: Accurately capturing functional and non-functional requirements is crucial, including ethical considerations like privacy, fairness, and transparency [20].
Design and Development Planning: AI system development is divided into non-AI and AI parts, covering design, implementation, testing, data engineering, feature engineering, model training, evaluation, and updates [16, 18].
3.1.2 Security 
The Security principle ensures that AI systems are protected from being attacked, co-opted, or misused by malicious actors. The goal is to prevent AI systems from causing harm to users or society by ensuring they are robust against adversarial attacks and other security vulnerabilities. To achieve this goal, we can use several best practices, derived from lessons in information security and adapted to the context of AI development.

Threat Modeling: This involves identifying and assessing potential threats to the AI system before it is fully designed. By understanding the threats early, developers can create a more secure design that mitigates these risks [51].
Design Review: Conducting a thorough review of the AI system's design helps identify potential security vulnerabilities. This step is crucial to ensure that the system's architecture is robust and can withstand various types of attacks [51].
Penetration Testing: This technique involves simulating attacks on the AI system to identify and fix vulnerabilities. Penetration testing helps in understanding how an adversary might exploit the system and allows developers to strengthen the system's defenses [51].
Incident Response: Developing a plan for responding to security incidents is essential. This includes detecting breaches, mitigating their impact, and recovering from them. An effective incident response plan ensures that the system can quickly return to normal operations after an attack [51].
Adversarial Defense: Techniques such as adversarial training, where the model is trained on adversarial examples, help in making the AI system more robust against adversarial attacks. This approach enhances the system's ability to handle unexpected inputs that could otherwise lead to incorrect or harmful outputs [47].
Uncertainty Quantification: By quantifying the uncertainty in the model's predictions, developers can better understand the confidence levels of the AI system. This helps in identifying when the system might be making unreliable predictions, thereby improving its overall security and reliability [47].
Technical Tools for Security: Utilizing technical tools designed to enhance security is important. For example, tools like Cleverhans for deep learning models and AlfaSVMLib for SVM models can test and secure systems against adversarial attacks that seek to exploit weaknesses and manipulate outputs [4].
3.1.3 Legality  
The Legality principle of an AI system refers to the requirement that AI systems must be developed and used in compliance with existing and upcoming laws. This includes ensuring that AI systems respect citizens' fundamental rights and avoid potential liability. The principle emphasizes the need for clear guidance to ensure responsible AI development and usage, particularly in high-risk applications, and to prevent illegal or negligent use of AI systems [14, 52].

There is little AI-targeted regulation, including government regulation, industry self-regulation, and international standards. Well-designed regulation can incentivize safety, security, and impact evaluation. Poor regulation can discourage innovation and increase risks. AI regulation requires detailed understanding by regulators. National regulation may lead to international competition. Reactive and slow regulation may be insufficient for AI challenges, which can operate much faster than humans [10]. Despite these challenges, countries like the UK, EU, USA, and China have proposed new AI regulations. AI rights, principles, and design measures, along with oversight and public consultation, are proposed to protect human rights [52].

To uphold legality, we can use the following best practices.

Ensure Compliance with Legal Requirements. Adhere to legal requirements such as Article 22(3) of EU’s General Data Protection Regulation (GDPR), which provides a right to contest decisions made solely by automated processes [34].
Comply with High-Risk AI Application Regulations. Ensure that AI applications falling into high-risk categories comply with the EU AI Act [27]. Participate in the National Institute of Standards and Technology's risk management framework for AI [51]. Be aware of and prepare for compliance with regulations like the US Algorithmic Accountability Act of 2022 and the forthcoming AI Bill of Rights [27].
Adopt Building Codes for AI Systems. Follow standards set by organizations like IEEE for specific sectors such as smart cities and medical device software security. Implement building codes that provide mandatory regulatory rules, ensuring independent oversight and advisory committees assess AI system compliance before launch [27].
Stay Updated with Regulatory Changes. Keep abreast of updates and clarifications in AI-related regulations, including administrative and discrimination laws [27]. Stay informed about AI regulations proposed by major jurisdictions like the UK, EU, USA, and China. Follow guidance issued for ensuring compliance with both existing and upcoming laws [52].
Develop a Clear Regulatory Strategy. Design regulatory mechanisms that incentivize investment in safety, security, and impact evaluation without stifling innovation [10].
Foster Interoperability and Adaptation. Address interoperability challenges by collaborating with stakeholders across jurisdictions. Adapt to the lengthy consultation and approval process for AI regulations, ensuring ongoing compliance and readiness for new regulatory requirements [27].
By adhering to these best practices, AI stakeholders can navigate the complex legal landscape, ensuring their AI systems are compliant, responsible, and trustworthy.
3.2 Stage 2: Data Validation and Testing
Three principles must be upheld in this stage: 1) fairness, 2) privacy and 3) transparency. The EU has led the way to establish a comprehensive mandate to handle user data with respect to the three principles, in the passing and execution of GDPR. The GDPR and other similar data regulations such as, Brazil’s Lei Geral de Proteção de Dados (LGPD), Canada’s Personal Information Protection and Electronic Documents Act (PIPEDA), and California Privacy Rights Act (CPRA) allow users to control their personal data and ultimately inspires user confidence in sharing data with private firms. 
3.2.1 Fairness
Fairness in AI can be defined as the absence of any prejudice or favoritism toward an individual or a group based on their inherent or acquired characteristics. This principle ensures that AI systems do not discriminate against certain groups or individuals and are accessible to people of any age, gender, and ability [35]. To ensure fairness in AI systems, several best practices have been developed and recommended by researchers and practitioners.

Fairness Metrics: These include individual fairness, group fairness, subgroup fairness, counterfactual fairness, equalized odds, fairness through unawareness, and predictive parity [35]. Each metric addresses different aspects of fairness. Collectively they can be used to evaluate AI models from multiple perspectives.
Bias Mitigation Algorithms: Bias mitigation can be applied at different stages of the machine learning pipeline [35]:
Pre-processing: Techniques such as reweighing, disparate impact remover, and optimized pre-processing aim to modify the training data to remove biases before model training.
In-processing: Methods like adversarial debiasing and regularization adjust the learning process to enforce fairness constraints during model training.
Post-processing: Algorithms such as reject option classification and equalized odds post-processing modify the model's outputs to ensure fairness without altering the training data or the model itself.
Causal Fairness: This approach involves using causal inference techniques to ensure that the decisions made by AI systems are not influenced by sensitive attributes. Causal learning can help identify and mitigate biases that arise from complex interactions between variables [36].
Toolkits and Frameworks: Several toolkits have been developed to help practitioners investigate and mitigate fairness issues in AI systems. Notable examples include IBM's AI Fairness 360 (AIF360) and Fairlearn. These toolkits provide a range of metrics and algorithms to assess and address bias in machine learning models.
3.2.2 Privacy
An AI system is considered privacy compliant when it adheres to principles and practices that ensure the protection of personal data and respect for individuals' privacy rights throughout its lifecycle. To ensure that an AI system is privacy compliant, several best practices can be employed. 

        1. Privacy Impact Assessments (PIAs): Conducting thorough privacy impact assessments is crucial. These assessments help identify privacy concerns and build in provisions to enable privacy controls. However, it is important to note that a one-time assessment is not sufficient for AI systems that are highly uncertain and continually learning [18].
        2. Privacy-Preserving Techniques: Implementing robust privacy-preserving techniques is essential. Techniques such as differential privacy, federated learning, and secure multiparty computation can safeguard sensitive information while maintaining the utility of the data for research purposes. Differential privacy, for instance, adds noise to the data to prevent the identification of individuals, while federated learning allows models to be trained across multiple decentralized devices without sharing raw data [47].
       3. Data Anonymization: Data anonymization methods are critical to protect individuals' identities. This includes techniques that aggregate data to prevent unique identification and ensure that sensitive attributes are not exposed [47].

3.2.3 Transparency
Transparency in AI systems refers to the clear, timely, easily understandable, and plain language explanation of what data is used, how it is collected, and how it influences the AI model's decisions. This concept is crucial for ensuring that stakeholders can make informed decisions and understand the implications of the AI system's outputs [5, 19]. Transparency involves providing detailed documentation about the datasets, including their sources, labeling processes, and any potential biases, to facilitate trust and accountability in AI applications [3, 4]. We have a few best practices to ensure data transparency. 

AI Factsheets and Model Cards: These tools provide detailed documentation about the data used for development and testing, including fairness and robustness checks, performance benchmarks, and intended uses. AI Factsheets use questionnaires to gather this information, while Model Cards advocate for quantitative evaluations broken down by individual, demographic, and other domain-relevant conditions [17]. 
Datasheets for Datasets: A Data Sheet introduces a questionnaire that captures details about the purpose and nature of a system’s data choices, including labeling, sources of noise, and the origins of the dataset. This helps in understanding the data's provenance and ensuring transparency [17].
Data Cards: Data Cards are structured summaries designed to provide essential facts about various aspects of machine learning (ML) datasets. These summaries include explanations of processes and rationales that shape the data and consequently the models, such as upstream sources, data collection and annotation methods, training and evaluation methods, intended use, and decisions affecting model performance [19]. 

3.3 Stage 3: Model Validation and Testing

In this stage, three RAI principles must be accounted for which are 1) explainability for understandability, 2) sustainability, and 3) truthfulness.
3.3.1 Explainability for Understandability
An AI system is considered explainable when it provides clear, understandable, and transparent explanations of its decision-making processes to all stakeholders. systems. eXplainable Artificial Intelligence (XAI) will be essential for users to understand, appropriately trust, and effectively manage this emerging generation of artificially intelligent partners. [46] Explainability is crucial for building trust and ensuring that stakeholders can comprehend how and why decisions are made by the AI system . To make an AI system explainable and understandable, several best practices can be used. These practices can be categorized into methodological steps, interpretability techniques, and human-centered design approaches.

Methodological Steps: When devising an approach to AI interpretability, it is crucial to consider the purpose of the model, the audience's need for explanation, and the domain's specific requirements. Preference should be given to interpretable techniques like decision trees and linear regression whenever possible, reserving complex models for cases where their capabilities are essential. For black-box models, it is important to address ethics, fairness, and safety by using XAI tools and creating an interpretability action plan. Interpretability should also account for human cognitive skills and accessibility to explanatory outcomes [2].
Interpretability Techniques:
Pre-Model Interpretability: This involves understanding the data before building the model using techniques like Principal Component Analysis (PCA), t-SNE, and clustering methods such as k-means [5].
In-Model Interpretability: This includes using intrinsicall y interpretable AI algorithms like decision trees, rule-based models, and linear regression. Constraints such as causality, sparsity, or physical conditions from domain knowledge can be imposed to achieve intrinsic interpretability [5].
Post-Model Interpretability: Also known as post-hoc interpretability, this is applied after model training and includes techniques like local explanations (LIME), saliency maps, example-based explanations, influence functions, and feature visualization [5].
Model-Specific vs. Model-Agnostic Methods: Model-specific methods are tailored to specific model classes, while model-agnostic methods can be applied to any machine learning model. Examples of model-agnostic methods include LIME and SHAP [35].
Human-Centered Design Approaches: Checklists and question banks help design explainable user interfaces by addressing user needs and XAI design factors such as input, output, performance, and ethical considerations. Conversational interfaces can be tested through Wizard of Oz studies, where users interact with a system secretly controlled by a human. Proactive informing, which involves explaining AI capabilities, limitations, ethics, decisions, potential outcomes, and data use, can increase human trust in AI systems [14]
Tools and Frameworks [35]:
IBM Watson OpenScale: This commercial solution provides tools to monitor and understand model outcomes using techniques like LIME and MACEM.
Google What-If Tool (WIT): An interactive tool that allows users to investigate model behavior and performance through a visual interface.
Google Cloud Explainable AI: A set of tools and frameworks that aid in building interpretable ML models, providing feature attribution methods like sampled Shapley, integrated gradients, and XRAI.
AI Explainability 360: An open-source software toolkit that allows exploring various explainability methods and evaluation metrics, providing a taxonomy to navigate the space of explanation methods.

3.3.2 Sustainability
Sustainability is the ability of the AI system to balance economic growth with social and environmental impacts. This involves ensuring that AI development and deployment consider ethical governance, societal well-being, and environmental protection, thereby contributing to long-term profitability and societal benefits [9]. To make an AI system sustainable, several best practices can be used. These practices focus on reducing the carbon footprint, ensuring responsible data governance, and fostering multi-stakeholder engagement.

Carbon Footprint Reduction:
CarbonTracker: Developed by the University of Copenhagen, this tool estimates the carbon emissions associated with different hardware and software configurations. It forecasts the carbon footprint of training AI models, helping organizations predict the total duration, energy, and carbon footprint of training runs [47].
Energy-efficient Algorithms: Implementing energy-efficient algorithms and optimizing the computational resources can significantly reduce the energy consumption of AI systems [47].
Open-source Solutions:
SpaceNet: This nonprofit consortium accelerates the research and application of open-source AI technology for geospatial applications. By providing open, precision-labeled datasets, SpaceNet encourages the development of algorithms designed specifically for geospatial applications, which can be used to monitor and manage environmental changes effectively [47].
Microsoft’s Planetary Computer: This platform combines a multi-petabyte catalog of global environmental data that can be queried via APIs, providing a scientific environment for users to answer global questions about environmental data. This facilitates comprehensive situation monitoring and changes over time [47].
Multi-stakeholder Engagement:
Participatory Action Research: Engaging various stakeholders such as local communities, businesses, conservation groups, and government officials in the development and implementation of AI systems ensures that the diverse interests and potential impacts are considered. This approach helps in building consensus and avoiding unintended harms [3].
Impact Assessments: Conducting well-being impact assessments, such as the IEEE 7010, helps organizations understand the variety of possible stakeholders and the intended or unintended impacts of their AI products. This process involves ongoing and iterative engagement with stakeholders to surface concerns and identify new indicators critical to a broader understanding of the social, economic, and ecological context [3].

3.3.3 Truthfulness
An AI system is truthful if it describes the literal truth about the real world. Its output is consistent with facts that most domain experts agree upon. We use the following best practices to improve this aspect. 

Truthfulness Testing. Use specialized benchmarks such as TruthfulQA to evaluate the model's ability to generate truthful responses. Check for consistency in factual information across multiple runs, and across updates after fine-tuning the model. [58]
Fact-based Retrieval. Pre-train a model using an objective function that rewards its improving internal knowledge representation and enhancing its capability to access the internal memory and retrieve relevant and factual knowledge. [53] Or if you have ground knowledge and facts about the application domains, use the RAG framework to retrieve the relevant factual context to be combined with your question as the final input to the model. 
Prompt Engineering. Use prompts that are crafted to make the model exhibiting reflective and self-examining behavior such as the curiosity-driven prompt technique. [54] 
3.4 System Monitoring
3.4.1 Maintainability
Maintainability refers to the ease with which an AI system can be modified to correct faults, improve performance, or adapt to a changed environment. This principle is crucial for ensuring that AI systems remain functional and effective over time, especially given their inherent complexity and the rapid evolution of technology [20]. The best practices for system monitoring in AI systems involve several key aspects, including continuous validation, traceability, and the use of specialized tools for monitoring data quality and model drift.

Continuous Validation: AI systems require ongoing validation to ensure they meet ethical requirements and provide intended benefits. This involves configuring the time and frequency of validation and sending version-based feedback and rebuild alerts when predefined conditions are met [14]. Continuous ethical validators are deployed to monitor and validate AI components against ethical requirements, ensuring the system behaves appropriately given the situation [14].
Traceability and Provenance: Ensuring traceability and provenance of AI models is crucial for building trust. This involves tracking the use of AI systems and maintaining model provenance to improve explainability and accountability. Version control and immutable logs are essential to prevent tampering and to provide evidence for audits [18]. Co-versioning of data, models, code, and configurations is necessary to ensure data provenance and traceability, especially when AI models are based on domain knowledge models [18].
Monitoring Tools: Several tools are available for monitoring AI models in production. AWS SageMaker Model Monitor, for instance, continuously monitors bias drift, while Qualdo monitors data quality and model drift. Azure Machine Learning uses Azure Monitor to create comprehensive monitoring data [14, 27]. These tools help in detecting and addressing issues related to model performance and ethical compliance.
Model Update and Recalibration: To maintain the reliability of AI systems, models may need to be retrained or recalibrated based on new data. This is particularly important to address potential mismatches between training data and real-world data, ensuring the model remains suitable for its intended purpose. Continuous validation and improvement of ethical principles should also occur at runtime, necessitating system-level updates to address any unethical issues [18].
3.5 Stage 5: Algorithmic Auditing and Reassessment
3.5.1 Contestability
Contestability refers to the ability of individuals to challenge and seek redress against decisions made by AI systems. This principle ensures that there is a process in place for people to contest the use or output of an AI system, particularly when it significantly impacts them. It is recognized as an important safeguard for ensuring procedural fairness and enhancing trust in AI systems [34]. To make an AI model contestable, we can use several best practices. These practices are rooted in the need for transparency, procedural fairness, and user engagement, as highlighted in various AI ethics guidelines and research.

Disclosure: Ensuring that AI systems are transparent is crucial. This involves providing clear and understandable information about how AI-based outcomes are generated, which allows individuals to challenge these outcomes effectively. The OECD's Principles on Artificial Intelligence emphasize the need for transparency to ensure that people can understand and challenge AI-based outcomes [34].
Redress: Providing users with 'outcome control,' or the ability to correct or appeal a decision, is important for improving perceptions of fairness. Research has shown that having the ability to appeal a decision can enhance individuals' perceptions of procedural fairness and the overall fairness of the outcome [34]. The procedural dimension of fairness includes the ability to contest and seek effective redress against decisions made by AI systems. This is supported by the Ethics Guidelines for Trustworthy AI, which state that fairness has both substantive and procedural dimensions, with the latter entailing the ability to contest decisions [34].
Human Intervention: The design of the contestation process should include clear instructions on how to lodge an appeal and ensure that users receive timely responses and resolutions. Access to human intervention is also critical, as it can address user dissatisfaction with automated processes [34]. The procedural justice literature indicates that having a new decision-maker assess the decision on appeal is perceived as more fair than having the same decision-maker. This practice can enhance the legitimacy and fairness of the contestation process [34].

3.5.2 Auditability
Auditability is defined as the assessment of algorithms, data, and design processes while preserving the intellectual property related to the AI systems. This involves both internal and external auditors performing the assessment and making the reports available to contribute to the trustworthiness of the technology. For AI systems that affect fundamental rights, including safety-critical applications, it is essential that they are always audited by an external third party [2]. Several best practices can be implemented to make an AI model auditable. These practices ensure transparency, accountability, and traceability, which are crucial for responsible AI.

Managing Meta-data: Organizations should manage meta-data effectively, which includes cataloging and curating data. A data catalog stores information about the data, such as the rationale for choosing a data source, the stakeholders involved, and the content stored within it. Data curation helps assess the fairness, accountability, transparency, and ethics (FATE) of system recommendations by identifying and leveraging the data within the organization [33].
Conducting Data Audits: Enhancing data auditing capabilities is another approach to establishing data provenance. Data audits assess whether the data are fit for a specific purpose and help uncover potential biases related to data processing. This process includes data profiling and impact analysis, which are essential for ensuring the fairness and accountability of AI-based systems [33].  Ensuring data traceability involves guiding data acquisition and leveraging technologies like blockchain to increase transparency. Data provenance records document the data capturing and processing entities, simplifying the audit process and enhancing transparency [33].
Version Control and Immutable Logs: Implementing version control and maintaining immutable logs are crucial for model provenance. This practice involves tracking the use of an AI system and ensuring that every piece of code and output is tagged with metadata. This helps in building trust and accountability in AI systems [18].
Continuous Monitoring and Validation: Audit is not a one-time job. There is a strong desire for continuously monitoring and validating AI systems post-deployment for ethical requirements. This includes model updates or recalibration on new data to ensure the reliability and relevance of the AI system. Continuous validation and improvement of other principles may also occur at run-time [18].

3.5.3 Accountability  
Accountability in AI systems refers to the mechanisms and practices that ensure individuals or organizations are held responsible for the actions and decisions made by AI systems. It encompasses the explainability (section 3.4.1) and auditability (section 3.6.2) principles of AI systems [2]. To ensure accountability in AI systems, several best practices have been identified across various research papers. These practices cover a range of strategies from technical implementations to organizational policies.

Minimization and Reporting of Negative Impacts: AI systems should include mechanisms for reporting actions or decisions that yield certain outcomes. This involves the identification, assessment, documentation, and minimization of potential negative impacts. Impact assessments should be carried out both prior to and during the development, deployment, and use of AI systems [2].
Trade-offs and Redress: When implementing accountability measures, any trade-offs should be ethically acceptable, reasoned, explicitly acknowledged, and documented. Additionally, mechanisms should be in place to ensure adequate redress for unforeseen adverse impacts, with special attention to vulnerable groups [2].
Co-versioning and Ethical Audit Trails: Co-versioning traces the datasets and configuration parameters used to train and evaluate models, facilitating maintenance and communication between AI and non-AI component teams. Ethical audit trails record every step of AI systems from an ethics perspective, balancing accountability and performance [16].
Accountability Knowledge Graphs: These graphs capture accountability information throughout the lifecycle of AI systems and allow for programmatic auditing. Ontologies can be used to describe and model this accountability information [16].
Principles and Guidelines: Technology companies and policy institutions have released principles and guidelines for AI use, emphasizing transparency, trust, accountability, and fairness. These guidelines suggest that AI decisions should be interpretable and explainable to trained human workers and that AI systems should explicitly identify themselves as such [17].
Design for Values Methodologies: These methodologies provide a structured way to translate abstract values into concrete norms, ensuring that fulfilling the norm adheres to the value. This involves identifying societal values, deciding on a moral deliberation approach, and linking values to formal system requirements and functionalities [23].
4 Future Work
There are two important pieces of future work. First, we are limited by the accessibility of all the papers on Responsible AI due to paywall. If we are able to access more of the recent papers and highly cited papers, we can improve the RAI Life Cycle framework efficiently using our AI assisted methodology. Second, we also have not analyzed the incidents and responses from the responsible organizations and companies. The analysis can help evaluate the efficacy of the existing practices and identify gaps and challenges in preventing, mitigating and resolving the problems that caused the incidents. These insights can be used to strengthen the RAI Life Cycle framework. 
5 Conclusion 
This survey introduces the Responsible AI Life Cycle (RALC) framework, which aims to provide a clear definition of Responsible AI, addressing the ambiguous definitions prevalent in current literature. RALC offers an iterative, comprehensive, and operationalizable approach, comprising five key stages: system design and risk assessment, data validation and testing, model validation and testing, system monitoring, and algorithmic auditing and reassessment. Each stage incorporates crucial sociotechnical principles, including functionality, security, legality, fairness, privacy, transparency, explainability, sustainability, truthfulness, maintainability, contestability, auditability, and accountability. The RALC framework prioritizes human stakeholders in AI development, offering a structured approach to implement social principles through technical practices, serving as a practical guide for AI professionals across disciplines. 

