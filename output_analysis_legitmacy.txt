An AI system is considered legitimate when it adheres to a comprehensive framework of ethical principles, governance structures, and practical implementation strategies that ensure its development and deployment are aligned with societal values and norms. 
	
	Legitimacy in AI systems is often rooted in **ethical principles** such as fairness, accountability, transparency, and explainability [1, 2, 6, 31]. These principles ensure that AI systems do not perpetuate biases, are accountable for their decisions, and are transparent and understandable to users and stakeholders [2, 6, 31]. 
	
	**Governance structures** play a crucial role in establishing legitimacy by providing oversight and regulatory compliance mechanisms. These structures ensure that AI systems adhere to responsible AI principles throughout their lifecycle, from development to deployment [14, 16, 27]. Multi-level governance frameworks are essential for setting up the regulatory and organizational frameworks needed to support responsible AI practices [14, 16].
	
	**Practical implementation strategies** are also vital for legitimacy. These include operationalizing high-level ethical principles into concrete practices that can be integrated into the AI development lifecycle [18, 20, 32]. For instance, frameworks like the "Learning, Knowledge, and Information Exchange (LKIE)" and the "Three Ways of Responsible AI" provide actionable measures to ensure responsible AI practices are embedded in organizational workflows [49]. 
	
	**Transparency and explainability** are particularly emphasized as they allow stakeholders to understand and trust the AI system's decision-making processes [17, 19, 35]. Transparency involves clear documentation and communication about the data sources, development processes, and decision-making mechanisms of AI systems [17, 19]. Explainability ensures that the decisions made by AI systems can be understood and interpreted by humans, which is crucial for building trust and ensuring accountability [35].
	
	**Accountability** mechanisms are necessary to ensure that there are clear lines of responsibility for the actions and decisions made by AI systems [6, 34, 46]. This includes having measures in place to address and remedy any adverse effects caused by the AI system, thereby ensuring that ethical considerations are not just theoretical but actionable [52].
	
	In summary, an AI system is legitimate when it integrates ethical principles, governance structures, and practical implementation strategies to ensure fairness, accountability, transparency, and explainability throughout its lifecycle. This comprehensive approach ensures that AI systems are not only technically proficient but also ethically sound and aligned with societal values and norms [1, 2, 6, 14, 16, 18, 20, 27, 31, 32, 49].