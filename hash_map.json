{
    "papers": [
        {
            "paper_title": "Responsible-AI-by-design: A pattern collection for designing responsible AI systems",
            "authors": "Q Lu, L Zhu, X Xu, J Whittle",
            "publication_info": "IEEE Software - ieeexplore.ieee.org",
            "paper_url": "https://arxiv.org/pdf/2203.00905",
            "chunks": [
                {
                    "id": "b408d418-dd6d-4b63-b38a-710a9cf8d81e",
                    "text": "Although AI has signi\ufb01cant potential to transform society, there are serious concerns about its abilityto behave and make decisions responsibly. Many ethical regulations, principles, and guidelines forresponsible AI have been issued recently. However, these principles are high-level and dif\ufb01cult toput into practice. In the meantime much effort has been put into responsible AI from the algorithmperspective, but they are limited to a small subset of ethical principles amenable to mathematicalanalysis. Responsible AI issues go beyond data and algorithms and are often at the system-levelcrosscutting many system components and the entire software engineering lifecycle. Based on theresult of a systematic literature review, this paper identi\ufb01es one missing element as the system-levelguidance \u2014 how to design the architecture of responsible AI systems. We present a summaryof design patterns that can be embedded into the AI systems as product features to contribute toresponsible-AI-by-design.Key words: Responsible AI, ethical AI, trustworthy AI, AI engineering, software architecture, MLOps, AIOps",
                    "reference": "[1] Qin Lu, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. https://arxiv.org/pdf/2203.00905"
                },
                {
                    "id": "79772aba-804e-483d-8cde-083e96366cc3",
                    "text": "Although AI has signi\ufb01cant potential and capacity to stimulate economic growth and improve productivity across agrowing range of domains, there are serious concerns about the AI systems\u2019 ability to behave and make decisions in aresponsible manner. According Gartner\u2019s recent report, 21% of organizations have already deployed or plan to deployresponsible AI technologies within the next 12 months .Many ethical principles, and guidelines have been recently issued by governments, research institutions, and compa-nies [1]. However, these principles are high-level and can hardly be used in practice by developers. Responsible AIresearch has been focusing on algorithm solutions limited to a subset of issues such as fairness[2]. Ethical issues canenter at any point of the software engineering lifecycle and are often at the system-level crosscutting many componentsof AI systems. To try to \ufb01ll the principle-algorithmic gap, some development guidelines have started to appear. However,those efforts tend to be high-level development process checklists and ad-hoc sets lacking of state-related linkages for\ufb01nal products [3].Therefore, in this paper, rather than staying at the ethical principle-level or AI algorithm-level, we take a pattern-orientedapproach and focuses on the system-level design patterns to build responsible-AI-by-design into \ufb01nal AI products. Thedesign patterns are collected based on the results of a systematic literature review (SLR) and can be embedded intothe design of AI systems as product features to contribute to responsible-AI-by-design. We identify the lifecycle of aprovisioned AI system in which the states or state transitions are associated with design patterns to show when the designpatterns can take effects. The lifecycle along with the design pattern annotations provide an responsible-AI-focusedview of system interactions and a guide to effect use of design patterns to implement responsible AI from a systemA - S 21, 2022perspective. To the best of our knowledge, this is the \ufb01rst study that provides a concrete and actionable system-leveldesign guidance for architects and developers to reference.Figure 1: Methodology.",
                    "reference": "[1] Qin Lu, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. https://arxiv.org/pdf/2203.00905"
                },
                {
                    "id": "f2d64b74-d84b-4995-9217-f794a1a35490",
                    "text": "To operationalize responsible AI, we performed an SLR to identify design patterns that architects and developers canuse during the development process.Fig. 1 illustrates the methodology. The research question is: \u201dWhat solutions forresponsible AI can be identi\ufb01ed?\u201d The research question focuses on identifying the reusable patterns for responsible AI.We used \u201dAI\u201d, \u201dResponsible\u201d, \u201dSolution\u201d as the key terms and included synonyms and abbreviations as supplementaryterms to increase the search results. The main data sources are ACM Digital Library, IEEE Xplore, Science Direct,Springer Link, and Google Scholar. The study only includes the papers that present concrete design or process solutionsfor responsible AI, and excludes the papers that only discuss high-level frameworks. The complete SLR protocol isavailable as online material . We use the ethical principles listed in Harvard University\u2019s mapping study [4]: Privacy,Accountability (professional responsibility is merged into accountability due to the overlapping de\ufb01nitions), Safety &Security, Transparency & Explainability, Fairness and Non-discrimination, Human Control of Technology, Promotionof Human Values.",
                    "reference": "[1] Qin Lu, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. https://arxiv.org/pdf/2203.00905"
                },
                {
                    "id": "630ccaad-b41b-41d3-9223-47cb8c554c3a",
                    "text": "Fig. 2 illustrates the lifecycle of a provisioned AI system using a state diagram and highlights the patterns associatingwith relevant states or transitions, which show when the design patterns could take effect. We have limited the scope tothe design patterns that can be embedded into the AI systems and provisioned supply chain tool-chain as \ufb01nal productfeatures. The best practices of the development process including some patterns related to of\ufb02ine model training is outof the scope of this paper. Before an AI system is provisioned, the supply chain information can be accessed throughbill of materials. The users can be required to provide the veri\ufb01able ethical credentials to show their capability tooperate the systems, while the users can examine the system\u2019s veri\ufb01able ethical credentials for ethical compliancechecking. Once the AI system starts serving, it is important to perform system-level simulation through an ethicaldigital twin. ethical sandbox can be used to physically separate AI components from non-AI components. When an AIsystem is requested to execute a task, decision-making is often needed before executing the task. AI component can beactivated or deactivated through AI model switcher to automatically make the decision or involve human experts toreview the suggestion. Multi-model decision maker can use different models to make a single decision and cross-checkthe results. Similarly, homogeneous redundancy can be applied to the system design to enable fault-tolerance. Both the2 A - S 21, 2022Figure 2: Lifecycle of a provisioned AI system.behaviors and decision-making outcomes of the AI system are monitored and validated through continuous ethicalvalidator. Incentives for the ethical behaviors can be maintained by an incentive registry. If the system is failed to meetthe requirements (including ethical requirements) or a near-miss is detected, the system need to be updated. Federatedlearner retrains the model locally at each client to protect data privacy. Co-versioning registry can be used to track theco-evolution of AI system components or assets. An ethical knowledge base can be built to make the ethical knowledgesystematically accessed and used when developing or updating the AI system. The AI system needs to be auditedregularly or when major-failures/near-misses occur. An ethical black box can be designed to record the critical data thatcan be kept as evidence. A global-view auditor can be built on top to provide global-view accountability when multiplesystems are involved in an accident. The stakeholders can determine to abandon the AI system if it no longer ful\ufb01ls therequirements.",
                    "reference": "[1] Qin Lu, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. https://arxiv.org/pdf/2203.00905"
                },
                {
                    "id": "5b01e982-c86c-482c-b546-1fdff5e516ec",
                    "text": "To operationalize responsible AI, Fig. 3 lists a collection of patterns for responsible-AI-by-design. The full version ofdesign patterns is available online .\u2022 Bill of materials: AI product vendors often create AI systems by assembling commercial or open source AIand/or non-AI components from third parties. The AI users often have ethical concerns about the procuredAI systems/components. Before an AI system is provisioned, the supply chain information can be accessedthrough bill of materials , which keeps a formal machine-readable record of the supply chain details ofthe components used in building an AI system, such as component name, version, supplier, dependencyrelationship, author and timestamp. The purpose of bill of material is to provide traceability and transparencyinto the components that make up AI systems so that ethical issues can be tracked and addressed. There havebeen many tools to generate SBOM for practitioners, such as Dependency-Track . To ensure traceability andintegrity, immutable data infrastructure [5] is needed to store the data of bill of materials. For example, themanufacturers of autonomous vehicles can maintain a material registry contract on blockchain to track theircomponents\u2019 supply chain information, e.g., the version and supplier of the third-party AI-based navigationcomponent. 3 A - S 21, 2022Figure 3: Operationalized design patterns for responsible AI systems.4 A - S 21, 2022\u2022 Veri\ufb01able ethical credential: Veri\ufb01able ethical credentials are cryptographically veri\ufb01able data that can beused as strong proof of ethical compliance for AI systems, components, artifacts, and stakeholders (such asdevelopers and users). Before using the provisioned AI systems, users verify the systems\u2019 ethical credential tocheck if the systems are compliant with AI ethics principles or regulations [6]. On the other hand, the usersis often required to provide the ethical credentials to use and operate the AI systems. Publicly accessibledata infrastructure needs to be built to support the generation and veri\ufb01cation for ethical credentials . Forexample, before driving an vehicle, the driver is requested to scan her/his ethical credential to show she/he hasthe capability to drive safely, while verifying the ethical credential of the vehicle\u2019s automated driving systemshown on the center console.\u2022 Ethical digital twin: Before running the provisioned AI system in a production environment, it is critical toconduct system-level simulation through an ethical digital twin running on a simulation platform to monitorthe behaviors of AI systems and predict potential ethical risks. Ethical digital twin can also be designed as acomponent at the operation infrastructure level to examine the AI systems\u2019 runtime behaviors and decisionsbased on the abstract simulation model using the real-world data. The risk assessment results can be used bythe system or users to take further actions to mitigate the potential ethical risk. For example, the manufacturersof autonomous vehicles can use the ethical digital twin to explore the limits of autonomous vehicles based onthe collected run-time data, such as NVIDIA DRIVE Sim and xFpro .\u2022 Ethical sandbox: It is risky to execute the whole system including AI components and non-AI componentsin the same environment. When an AI system is being served, ethical sandbox can be used to physicallyseparate AI components from non-AI components by running the AI component in a self-contained emulationexecution environment [7], e.g. sandboxing the unveri\ufb01ed visual perception component. The AI componentsplaced in the ethical sandbox has no access to the rest of the AI system. All the hardware and softwarefunctionality of the AI component are duplicated in the ethical sandbox. Thus, the AI component can runsafely under supervision before being deployed at scale. For example, Fastcase AI Sandbox provides asecure AI execution platform for analysing data safely in a secure environment. Maximal tolerable probabilityshould be set as an ethical margin for the sandbox against the ethical requirements. A watch dog can be addedto restrict the execution time of the AI component to avoid the potential ethical risk, e.g., only executing thevisual perception component for 10 minutes on the roads designed especially for autonomous vehicles.\u2022 AI mode switcher: When to activate AI is a major architectural design decision when designing a softwaresystem. When an AI system is making a decision, AI mode switcher enables ef\ufb01cient invocation and dismissalmechanisms for activating or stopping the AI component when needed. Kill switch is a special type ofinvocation mechanism which immediately turns off the AI component and terminates its negative effects, e.g.switching off the autopilot functionality and its internet connection. The AI component can make decisionsautomatically or provide suggestions to human experts in high risk situations. The decisions can be approvedor overridden by human expert (e.g. skipping the path suggested by the navigation system). If the systemstate after acting an AI decision is not expected by human experts, fallback can be triggered to reverse thesystem back to the previous state. A built-in guard ensures that the AI component is only being used under theprede\ufb01ned risk categories.\u2022 Multi-model decision-maker: The reliability of traditional software is dependent on the design of softwarecomponents. One of the reliability practices in the reliability community is redundancy, which can be appliedto AI components. When decisions are being made by an AI system, multi-model decision-maker can rundifferent models to make a single decision [8], e.g., using different algorithms for visual perception. Reliabilitycan be improved by using different models under different context (e.g., different user groups or regions). Inaddition, fault tolerance can be enabled by cross-checking the results given by multiple models (e.g., onlyaccepts the same results from the deployed models). IBM Watson Natural Language Understanding makepredictions using an ensemble learning framework that includes multiple emotion detection models .\u2022 Homogeneous redundancy: Ethical failures in AI systems can cause serious damage to the humans orenvironment. N-version programming is a design pattern for dealing with reliability issues of traditionalsoftware. This concept can be adapted and applied to AI system design. Homogeneous redundancy (e.g.,two brake control components) can be applied to tolerate the highly uncertain AI system components thatcan make unethical decisions or the adversary hardware components that produce malicious data or behave5 A - S 21, 2022unethically [9]. When an AI system is executing a task, a cross-check can be performed for the outputs givenby multiple redundant components of a single type.\u2022 Incentive registry: Incentives are effective in motivating AI systems to execute tasks in a responsible manner.When executing a task, an incentive registry records the rewards that are given for the behavior and decisionsand behaviors of AI systems [10], e.g., rewards for the recommended path without safety risk. There aredifferent ways to enforce the incentive mechanism, e.g., designing the incentive mechanism on blockchainbased data infrastructure that is publicly accessible, using reinforcement learning. However, it is challengingto design the mechanisms in the responsible AI context since it dif\ufb01cult to measure the ethical impact ofdecisions and behaviors of AI systems on some ethical principles (such as human values). Besides, consensusneeds to be reached on the incentive mechanism by all the stakeholders. Additionally, in some cases, ethicalprinciples are con\ufb02icting with each other, making the design of incentive mechanism harder. FLoBC is atool that uses blockchain to incentivize training contribution for federated learning.\u2022 Continuous ethical validator: AI systems often need to conduct continual learning when data drift orunethical behavior is detected in production. When an AI system executes tasks, continuous ethical validatormonitors and validates the outcomes of AI systems (e.g., the path suggested by the navigation system) againstthe ethical requirements. The outcomes of AI systems are the consequences of decisions and behaviors of thesystems, i.e., whether the AI system behaves ethically or provides the promised bene\ufb01ts in a given situation.The time and frequency of validation can be prede\ufb01ned within the continuous validator. Version-based feedbackand rebuild alert can be sent when the ethical requirements are met or breached. Incentive registry can be usedto reward or punish the ethical/unethical behavior or decisions of AI systems.\u2022 Ethical knowledge base: AI systems involve broad ethical knowledge, including AI ethics principles, regu-lations, unethical use cases, etc. Unfortunately, such ethical knowledge is scattered in different documents(e.g., AI incidents) and is usually implicit or even unknown to developers who primarily focus on the technicalaspects of AI systems and do not have ethics background. This results in negligence or ad-hoc use of relevantethical knowledge in AI system development. Ethical knowledge base is built upon a knowledge graph tomake meaningful entities, concepts and their rich semantic relationships are explicit and traceable acrossheterogeneous documents so that the ethical knowledge can be systematically accessed, analysed, used whendeveloping or updating AI systems [11]. For example, an ethical knowledge base can be used to supportcontinuous ethical risk assessment. Ethical knowledge base can be built based on the AI ethics principles,frameworks, and actual AI use cases discussed in the existing papers.\u2022 Co-versioning registry: AI systems involve different levels of dependencies and need frequent evolutionwhen data drift or unethical behavior occurs. Co-versioning of the components of AI systems or AI assetsgenerated in AI pipelines provides provenance guarantees across the entire lifecycle of AI systems. There havebeen many version control tools for managing the co-versioning of data and models, such as DVC . Whenupdating an AI system, co-versioning registry can track the co-evolution of components or AI assets. Thereare different levels of co-versioning: co-versioning of AI components and non-AI components, co-versioningof the assets within the AI components (i.e., co-versioning of data, model, code, con\ufb01gurations). A publiclyaccessible data infrastructure can be used to maintain the co-versioning registry to provide a trustworthytrace for dependencies. For example, a co-versioning registry contract can be built on blockchain to managedifferent versions of visual perception models and the corresponding training datasets.\u2022 Federated learner: Despite the widely deployed mobile or IoT devices generating massive amounts of data,data hungriness is still a challenge given the increasing concern in data privacy. When learning or updating AImodels, federated learner preserves the data privacy by performing the model training locally on the clientdevices and formulating a global model on a central server based on the local model updates , e.g., train thevisual perception model locally in each vehicle. Decentralized learning is an alternative to federated learning,which uses blockchain to remove the single point of failure and coordinate the learning process in a fullydecentralized way. In the event of negative outcomes, the responsible humans can be traced and identi\ufb01ed byan ethical black box for accountability.\u2022 Ethical black box: Black box was introduced initially for aircraft several decades ago for recording critical\ufb02ight data. The purpose of embedding an ethical black box in an AI system is to audit an AI system andinvestigate why and how the system caused an accident or a near miss. The ethical black box continuouslyrecords sensor data, internal status data, decisions, behaviors (both system and operator) and effects [12]. Forexample, an ethical black box could be built into the automated driving system to record the behaviors of the6 A - S 21, 2022system and driver and their effects. Design decisions need to be made on what data should be recorded andwhere the data should be stored (e.g. using a blockchain-based immutable log or a cloud-based data storage).\u2022 Global-view auditor: There can be more than one AI systems involved in an ethical incident (e.g. multipleautonomous vehicles in a car accident). During auditing, it is often challenging to identify the liability as thedata collected from each of the involved systems can be con\ufb02icting to each other. Global-view auditor canenable accountability by analysing the data discrepancies between the involved AI systems and identifyingthe liability for the ethical incident. This pattern can be also applied to improve the reliability an AI systemby taking the data from other systems. For example, an autonomous vehicle increases its visibility using theperception data collected from the other vehicles. [13]. All the historical data of AI systems can be recordedby an immutable log for third-party auditing.",
                    "reference": "[1] Qin Lu, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. https://arxiv.org/pdf/2203.00905"
                },
                {
                    "id": "6c024ab2-1d60-4ae8-a88d-9b438b80cbba",
                    "text": "To operationalize responsible AI, we take a pattern-oriented approach and collect a set of product design patternsthat can be embedded into an AI system as product features to enable responsible-AI-by-design. The patterns areassociated to the states or state transitions of a provisioned AI system, serving as an effective guidance for architectsand developers to design a responsible AI system. We are currently building up a responsible AI pattern cataloguethat includes multi-level governance patterns, trustworthy process patterns (i.e., best practices and techniques), andresponsible-AI-by-design product patterns.",
                    "reference": "[1] Qin Lu, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. https://arxiv.org/pdf/2203.00905"
                }
            ]
        },
        {
            "paper_title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
            "authors": "AB Arrieta, N D\u00edaz-Rodr\u00edguez, J Del Ser, A Bennetot\u2026",
            "publication_info": "Information fusion - Elsevier",
            "paper_url": "https://arxiv.org/pdf/1910.10045",
            "chunks": [
                {
                    "id": "a101fc2e-5437-4f2a-a6a1-1eec19ca353d",
                    "text": "In the last few years, Arti\ufb01cial Intelligence (AI) has achieved a notable momentum that, if harnessedappropriately, may deliver the best of expectations over many application sectors across the \ufb01eld. For thisto occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability,an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep NeuralNetworks) that were not present in the last hype of AI (namely, expert systems and rule based models).Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) \ufb01eld, which is widelyacknowledged as a crucial feature for the practical deployment of AI models. The overview presented inthis article examines the existing literature and contributions already done in the \ufb01eld of XAI, including aprospect toward what is yet to be reached. For this purpose we summarize previous efforts made to de\ufb01neexplainability in Machine Learning, establishing a novel de\ufb01nition of explainable Machine Learning thatcovers such prior conceptual propositions with a major focus on the audience for which the explainabilityis sought. Departing from this de\ufb01nition, we propose and discuss about a taxonomy of recent contributionsrelated to the explainability of different Machine Learning models, including those aimed at explainingDeep Learning methods for which a second dedicated taxonomy is built and examined in detail. Thiscritical literature analysis serves as the motivating background for a series of challenges faced by XAI,such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the conceptof Responsible Arti\ufb01cial Intelligence, namely, a methodology for the large-scale implementation of AImethods in real organizations with fairness, model explainability and accountability at its core. Ourultimate goal is to provide newcomers to the \ufb01eld of XAI with a thorough taxonomy that can serveas reference material in order to stimulate future research advances, but also to encourage experts andprofessionals from other disciplines to embrace the bene\ufb01ts of AI in their activity sectors, without anyprior bias for its lack of interpretability.Keywords: Explainable Arti\ufb01cial Intelligence, Machine Learning, Deep Learning, Data Fusion,Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, ResponsibleArti\ufb01cial Intelligence.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "3f5159f0-ad35-409e-836e-a46fac44978c",
                    "text": "Arti\ufb01cial Intelligence (AI) lies at the core of many activity sectors that have embraced new informationtechnologies [1]. While the roots of AI trace back to several decades ago, there is a clear consensus on theparamount importance featured nowadays by intelligent machines endowed with learning, reasoning andadaptation capabilities. It is by virtue of these capabilities that AI methods are achieving unprecedentedlevels of performance when learning to solve increasingly complex computational tasks, making thempivotal for the future development of the human society [2]. The sophistication of AI-powered systemshas lately increased to such an extent that almost no human intervention is required for their designand deployment. When decisions derived from such systems ultimately affect humans\u2019 lives (as in e.g.medicine, law or defense), there is an emerging need for understanding how such decisions are furnishedby AI methods [3].While the very \ufb01rst AI systems were easily interpretable, the last years have witnessed the rise ofopaque decision systems such as Deep Neural Networks (DNNs). The empirical success of Deep Learning(DL) models such as DNNs stems from a combination of ef\ufb01cient learning algorithms and their hugeparametric space. The latter space comprises hundreds of layers and millions of parameters, which makesDNNs be considered as complex black-box models [4]. The opposite of black-box-ness is transparency,i.e., the search for a direct understanding of the mechanism by which a model works [5].As black-box Machine Learning (ML) models are increasingly being employed to make importantpredictions in critical contexts, the demand for transparency is increasing from the various stakeholders inAI [6]. The danger is on creating and using decisions that are not justi\ufb01able, legitimate, or that simply donot allow obtaining detailed explanations of their behaviour [7]. Explanations supporting the output of amodel are crucial, e.g., in precision medicine, where experts require far more information from the modelthan a simple binary prediction for supporting their diagnosis [8]. Other examples include autonomousvehicles in transportation, security, and \ufb01nance, among others.In general, humans are reticent to adopt techniques that are not directly interpretable, tractable andtrustworthy [9], given the increasing demand for ethical AI [3]. It is customary to think that by focusingsolely on performance, the systems will be increasingly opaque. This is true in the sense that there is atrade-off between the performance of a model and its transparency [10]. However, an improvement in theunderstanding of a system can lead to the correction of its de\ufb01ciencies. When developing a ML model,the consideration of interpretability as an additional design driver can improve its implementability for 3reasons:\u2022 Interpretability helps ensure impartiality in decision-making, i.e. to detect, and consequently, correctfrom bias in the training dataset.\u2022 Interpretability facilitates the provision of robustness by highlighting potential adversarial perturbationsthat could change the prediction.\u2022 Interpretability can act as an insurance that only meaningful variables infer the output, i.e., guaranteeingthat an underlying truthful causality exists in the model reasoning.All these means that the interpretation of the system should, in order to be considered practical,provide either an understanding of the model mechanisms and predictions, a visualization of the model\u2019sdiscrimination rules, or hints on what could perturb the model [11].In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI(XAI) [7] proposes creating a suite of ML techniques that 1) produce more explainable models whilemaintaining a high level of learning performance (e.g., prediction accuracy), and 2) enable humans tounderstand, appropriately trust, and effectively manage the emerging generation of arti\ufb01cially intelligentpartners. XAI draws as well insights from the Social Sciences [12] and considers the psychology ofexplanation. 2Figure 1 displays the rising trend of contributions on XAI and related concepts. This literatureoutbreak shares its rationale with the research agendas of national governments and agencies. Althoughsome recent surveys [8, 13, 10, 14, 15, 16, 17] summarize the upsurge of activity in XAI across sectorsand disciplines, this overview aims to cover the creation of a complete uni\ufb01ed framework of categoriesand concepts that allow for scrutiny and understanding of the \ufb01eld of XAI methods. Furthermore, we poseintriguing thoughts around the explainability of AI models in data fusion contexts with regards to dataprivacy and model con\ufb01dentiality. This, along with other research opportunities and challenges identi\ufb01edthroughout our study, serve as the pull factor toward Responsible Arti\ufb01cial Intelligence, term by whichwe refer to a series of AI principles to be necessarily met when deploying AI in real applications. As wewill later show in detail, model explainability is among the most crucial aspects to be ensured within thismethodological framework. All in all, the novel contributions of this overview can be summarized asfollows:1. Grounded on a \ufb01rst elaboration of concepts and terms used in XAI-related research, we propose anovel de\ufb01nition of explainability that places audience (Figure 2) as a key aspect to be considered whenexplaining a ML model. We also elaborate on the diverse purposes sought when using XAI techniques,from trustworthiness to privacy awareness, which round up the claimed importance of purpose andtargeted audience in model explainability.2. We de\ufb01ne and examine the different levels of transparency that a ML model can feature by itself, aswell as the diverse approaches to post-hoc explainability, namely, the explanation of ML models thatare not transparent by design.3. We thoroughly analyze the literature on XAI and related concepts published to date, covering ap-proximately 400 contributions arranged into two different taxonomies. The \ufb01rst taxonomy addressesthe explainability of ML models using the previously made distinction between transparency andpost-hoc explainability, including models that are transparent by themselves, Deep and non-Deep (i.e.,3shallow) learning models. The second taxonomy deals with XAI methods suited for the explanation ofDeep Learning models, using classi\ufb01cation criteria closely linked to this family of ML methods (e.g.layerwise explanations, representation vectors, attention).4. We enumerate a series of challenges of XAI that still remain insuf\ufb01ciently addressed to date. Speci\ufb01-cally, we identify research needs around the concepts and metrics to evaluate the explainability of MLmodels, and outline research directions toward making Deep Learning models more understandable.We further augment the scope of our prospects toward the implications of XAI techniques in regardsto con\ufb01dentiality, robustness in adversarial settings, data diversity, and other areas intersecting withexplainability.5. After the previous prospective discussion, we arrive at the concept of Responsible Arti\ufb01cial Intelligence,a manifold concept that imposes the systematic adoption of several AI principles for AI models tobe of practical use. In addition to explainability, the guidelines behind Responsible AI establish thatfairness, accountability and privacy should also be considered when implementing AI models in realenvironments.6. Since Responsible AI blends together model explainability and privacy/security by design, we callfor a profound re\ufb02ection around the bene\ufb01ts and risks of XAI techniques in scenarios dealing withsensitive information and/or con\ufb01dential ML models. As we will later show, the regulatory pushtoward data privacy, quality, integrity and governance demands more efforts to assess the role of XAIin this arena. In this regard, we provide an insight on the implications of XAI in terms of privacy andsecurity under different data fusion paradigms.The remainder of this overview is structured as follows: \ufb01rst, Section 2 and subsections therein open adiscussion on the terminology and concepts revolving around explainability and interpretability in AI,ending up with the aforementioned novel de\ufb01nition of interpretability (Subsections 2.1 and 2.2), and ageneral criterion to categorize and analyze ML models from the XAI perspective. Sections 3 and 4 proceedby reviewing recent \ufb01ndings on XAI for ML models (on transparent models and post-hoc techniquesrespectively) that comprise the main division in the aforementioned taxonomy. We also include a reviewon hybrid approaches among the two, to attain XAI. Bene\ufb01ts and caveats of the synergies among thefamilies of methods are discussed in Section 5, where we present a prospect of general challenges andsome consequences to be cautious about. Finally, Section 6 elaborates on the concept of ResponsibleArti\ufb01cial Intelligence. Section 7 concludes the survey with an outlook aimed at engaging the communityaround this vibrant research area, which has the potential to impact society, in particular those sectors thathave progressively embraced ML as a core technology of their activity.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "f77524b1-9d38-4ed5-a743-8c14da0d8ebc",
                    "text": "Before proceeding with our literature study, it is convenient to \ufb01rst establish a common point ofunderstanding on what the term explainability stands for in the context of AI and, more speci\ufb01cally,ML. This is indeed the purpose of this section, namely, to pause at the numerous de\ufb01nitions that havebeen done in regards to this concept (what?), to argue why explainability is an important issue in AI andML (why? what for?) and to introduce the general classi\ufb01cation of XAI approaches that will drive theliterature study thereafter (how?).",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "a945f0e6-34fa-4edb-8519-3ab87bc9485d",
                    "text": "One of the issues that hinders the establishment of common grounds is the interchangeable misuse ofinterpretability and explainability in the literature. There are notable differences among these concepts.To begin with, interpretability refers to a passive characteristic of a model referring to the level at whicha given model makes sense for a human observer. This feature is also expressed as transparency. By4contrast, explainability can be viewed as an active characteristic of a model, denoting any action orprocedure taken by a model with the intent of clarifying or detailing its internal functions.To summarize the most commonly used nomenclature, in this section we clarify the distinction andsimilarities among terms often used in the ethical AI and XAI communities.\u2022 Understandability (or equivalently, intelligibility) denotes the characteristic of a model to make ahuman understand its function \u2013 how the model works \u2013 without any need for explaining its internalstructure or the algorithmic means by which the model processes data internally [18].\u2022 Comprehensibility: when conceived for ML models, comprehensibility refers to the ability of alearning algorithm to represent its learned knowledge in a human understandable fashion [19, 20, 21].This notion of model comprehensibility stems from the postulates of Michalski [22], which stated that\u201cthe results of computer induction should be symbolic descriptions of given entities, semantically andstructurally similar to those a human expert might produce observing the same entities. Components ofthese descriptions should be comprehensible as single \u2018chunks\u2019 of information, directly interpretable innatural language, and should relate quantitative and qualitative concepts in an integrated fashion\u201d.Given its dif\ufb01cult quanti\ufb01cation, comprehensibility is normally tied to the evaluation of the modelcomplexity [17].\u2022 Interpretability: it is de\ufb01ned as the ability to explain or to provide the meaning in understandableterms to a human.\u2022 Explainability: explainability is associated with the notion of explanation as an interface betweenhumans and a decision maker that is, at the same time, both an accurate proxy of the decision makerand comprehensible to humans [17].\u2022 Transparency: a model is considered to be transparent if by itself it is understandable. Since a modelcan feature different degrees of understandability, transparent models in Section 3 are divided into threecategories: simulatable models, decomposable models and algorithmically transparent models [5].In all the above de\ufb01nitions, understandability emerges as the most essential concept in XAI. Bothtransparency and interpretability are strongly tied to this concept: while transparency refers to thecharacteristic of a model to be, on its own, understandable for a human, understandability measures thedegree to which a human can understand a decision made by a model. Comprehensibility is also connectedto understandability in that it relies on the capability of the audience to understand the knowledge containedin the model. All in all, understandability is a two-sided matter: model understandability and humanunderstandability. This is the reason why the de\ufb01nition of XAI given in Section 2.2 refers to the conceptof audience, as the cognitive skills and pursued goal of the users of the model have to be taken intoaccount jointly with the intelligibility and comprehensibility of the model in use. This prominent roletaken by understandability makes the concept of audience the cornerstone of XAI, as we next elaborate infurther detail.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "e2bbf3b5-833f-4f04-abd5-a41e329c528c",
                    "text": "Although it might be considered to be beyond the scope of this paper, it is worth noting the discussionheld around general theories of explanation in the realm of philosophy [23]. Many proposals have beendone in this regard, suggesting the need for a general, uni\ufb01ed theory that approximates the structure andintent of an explanation. However, nobody has stood the critique when presenting such a general theory.For the time being, the most agreed-upon thought blends together different approaches to explanationdrawn from diverse knowledge disciplines. A similar problem is found when addressing interpretabilityin AI. It appears from the literature that there is not yet a common point of understanding on whatinterpretability or explainability are. However, many contributions claim the achievement of interpretablemodels and techniques that empower explainability.5To shed some light on this lack of consensus, it might be interesting to place the reference startingpoint at the de\ufb01nition of the term Explainable Arti\ufb01cial Intelligence (XAI) given by D. Gunning in [7]:\u201cXAI will create a suite of machine learning techniques that enables human users to understand,appropriately trust, and effectively manage the emerging generation of arti\ufb01cially intelligent partners\u201dThis de\ufb01nition brings together two concepts (understanding and trust) that need to be addressed inadvance. However, it misses to consider other purposes motivating the need for interpretable AI models,such as causality, transferability, informativeness, fairness and con\ufb01dence [5, 24, 25, 26]. We will laterdelve into these topics, mentioning them here as a supporting example of the incompleteness of the abovede\ufb01nition.As exempli\ufb01ed by the de\ufb01nition above, a thorough, complete de\ufb01nition of explainability in AIstill slips from our \ufb01ngers. A broader reformulation of this de\ufb01nition (e.g. \u201cAn explainable Arti\ufb01cialIntelligence is one that produces explanations about its functioning\u201d) would fail to fully characterize theterm in question, leaving aside important aspects such as its purpose. To build upon the completeness, ade\ufb01nition of explanation is \ufb01rst required.As extracted from the Cambridge Dictionary of English Language, an explanation is \u201cthe details orreasons that someone gives to make something clear or easy to understand\u201d [27]. In the context of anML model, this can be rephrased as: \u201dthe details or reasons a model gives to make its functioning clearor easy to understand\u201d. It is at this point where opinions start to diverge. Inherently stemming from theprevious de\ufb01nitions, two ambiguities can be pointed out. First, the details or the reasons used to explain,are completely dependent of the audience to which they are presented. Second, whether the explanationhas left the concept clear or easy to understand also depends completely on the audience. Therefore, thede\ufb01nition must be rephrased to re\ufb02ect explicitly the dependence of the explainability of the model on theaudience. To this end, a reworked de\ufb01nition could read as:Given a certain audience, explainability refers to the details and reasons a model gives to make itsfunctioning clear or easy to understand.Since explaining, as argumenting, may involve weighting, comparing or convincing an audience withlogic-based formalizations of (counter) arguments [28], explainability might convey us into the realm ofcognitive psychology and the psychology of explanations [7], since measuring whether something hasbeen understood or put clearly is a hard task to be gauged objectively. However, measuring to whichextent the internals of a model can be explained could be tackled objectively. Any means to reduce thecomplexity of the model or to simplify its outputs should be considered as an XAI approach. How bigthis leap is in terms of complexity or simplicity will correspond to how explainable the resulting modelis. An underlying problem that remains unsolved is that the interpretability gain provided by such XAIapproaches may not be straightforward to quantify: for instance, a model simpli\ufb01cation can be evaluatedbased on the reduction of the number of architectural elements or number of parameters of the modelitself (as often made, for instance, for DNNs). On the contrary, the use of visualization methods or naturallanguage for the same purpose does not favor a clear quanti\ufb01cation of the improvements gained in termsof interpretability. The derivation of general metrics to assess the quality of XAI approaches remain asan open challenge that should be under the spotlight of the \ufb01eld in forthcoming years. We will furtherdiscuss on this research direction in Section 5.Explainability is linked to post-hoc explainability since it covers the techniques used to convert anon-interpretable model into a explainable one. In the remaining of this manuscript, explainability will beconsidered as the main design objective, since it represents a broader concept. A model can be explained,but the interpretability of the model is something that comes from the design of the model itself. Bearingthese observations in mind, explainable AI can be de\ufb01ned as follows:Given an audience, an explainable Arti\ufb01cial Intelligence is one that produces details or reasons tomake its functioning clear or easy to understand. 6This de\ufb01nition is posed here as a \ufb01rst contribution of the present overview, implicitly assumes that theease of understanding and clarity targeted by XAI techniques for the model at hand reverts on differentapplication purposes, such as a better trustworthiness of the model\u2019s output by the audience.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "1bbbfa71-2ec9-4a2c-b162-074d5a215e0b",
                    "text": "As stated in the introduction, explainability is one of the main barriers AI is facing nowadays inregards to its practical implementation. The inability to explain or to fully understand the reasons bywhich state-of-the-art ML algorithms perform as well as they do, is a problem that \ufb01nd its roots in twodifferent causes, which are conceptually illustrated in Figure 2.Without a doubt, the \ufb01rst cause is the gap between the research community and business sectors,impeding the full penetration of the newest ML models in sectors that have traditionally lagged behindin the digital transformation of their processes, such as banking, \ufb01nances, security and health, amongmany others. In general this issue occurs in strictly regulated sectors with some reluctance to implementtechniques that may put at risk their assets.The second axis is that of knowledge. AI has helped research across the world with the task ofinferring relations that were far beyond the human cognitive reach. Every \ufb01eld dealing with huge amountsof reliable data has largely bene\ufb01ted from the adoption of AI and ML techniques. However, we areentering an era in which results and performance metrics are the only interest shown up in researchstudies. Although for certain disciplines this might be the fair case, science and society are far from beingconcerned just by performance. The search for understanding is what opens the door for further modelimprovement and its practical utility.The following section develops these ideas further by analyzing the goals motivating the search forexplainable AI models.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "e0c632b5-df1b-4acb-9fea-d8a909bdb711",
                    "text": "The research activity around XAI has so far exposed different goals to draw from the achievementof an explainable model. Almost none of the papers reviewed completely agrees in the goals requiredto describe what an explainable model should compel. However, all these different goals might helpdiscriminate the purpose for which a given exercise of ML explainability is performed. Unfortunately,scarce contributions have attempted to de\ufb01ne such goals from a conceptual perspective [5, 13, 24, 30].We now synthesize and enumerate de\ufb01nitions for these XAI goals, so as to settle a \ufb01rst classi\ufb01cationcriteria for the full suit of papers covered in this review:7\u2022 Trustworthiness: several authors agree upon the search for trustworthiness as the primary aim of anexplainable AI model [31, 32]. However, declaring a model as explainable as per its capabilities ofinducing trust might not be fully compliant with the requirement of model explainability. Trustwor-thiness might be considered as the con\ufb01dence of whether a model will act as intended when facing agiven problem. Although it should most certainly be a property of any explainable model, it does notimply that every trustworthy model can be considered explainable on its own, nor is trustworthinessa property easy to quantify. Trust might be far from being the only purpose of an explainable modelsince the relation among the two, if agreed upon, is not reciprocal. Part of the reviewed papers mentionthe concept of trust when stating their purpose for achieving explainability. However, as seen in Table1, they do not amount to a large share of the recent contributions related to XAI.\u2022 Causality: another common goal for explainability is that of \ufb01nding causality among data variables.Several authors argue that explainable models might ease the task of \ufb01nding relationships that, shouldthey occur, could be tested further for a stronger causal link between the involved variables [159, 160].The inference of causal relationships from observational data is a \ufb01eld that has been broadly studiedover time [161]. As widely acknowledged by the community working on this topic, causality requires awide frame of prior knowledge to prove that observed effects are causal. A ML model only discoverscorrelations among the data it learns from, and therefore might not suf\ufb01ce for unveiling a cause-effectrelationship. However, causation involves correlation, so an explainable ML model could validatethe results provided by causality inference techniques, or provide a \ufb01rst intuition of possible causal8relationships within the available data. Again, Table 1 reveals that causality is not among the mostimportant goals if we attend to the amount of papers that state it explicitly as their goal.\u2022 Transferability: models are always bounded by constraints that should allow for their seamlesstransferability. This is the main reason why a training-testing approach is used when dealing withML problems [162, 163]. Explainability is also an advocate for transferability, since it may ease thetask of elucidating the boundaries that might affect a model, allowing for a better understanding andimplementation. Similarly, the mere understanding of the inner relations taking place within a modelfacilitates the ability of a user to reuse this knowledge in another problem. There are cases in which thelack of a proper understanding of the model might drive the user toward incorrect assumptions andfatal consequences [44, 164]. Transferability should also fall between the resulting properties of anexplainable model, but again, not every transferable model should be considered as explainable. Asobserved in Table 1, the amount of papers stating that the ability of rendering a model explainable is tobetter understand the concepts needed to reuse it or to improve its performance is the second most usedreason for pursuing model explainability.\u2022 Informativeness: ML models are used with the ultimate intention of supporting decision making [92].However, it should not be forgotten that the problem being solved by the model is not equal to thatbeing faced by its human counterpart. Hence, a great deal of information is needed in order to be ableto relate the user\u2019s decision to the solution given by the model, and to avoid falling in misconceptionpitfalls. For this purpose, explainable ML models should give information about the problem beingtackled. Most of the reasons found among the papers reviewed is that of extracting information aboutthe inner relations of a model. Almost all rule extraction techniques substantiate their approach onthe search for a simpler understanding of what the model internally does, stating that the knowledge(information) can be expressed in these simpler proxies that they consider explaining the antecedent.This is the most used argument found among the reviewed papers to back up what they expect fromreaching explainable models.\u2022 Con\ufb01dence: as a generalization of robustness and stability, con\ufb01dence should always be assessedon a model in which reliability is expected. The methods to maintain con\ufb01dence under control aredifferent depending on the model. As stated in [165, 166, 167], stability is a must-have when drawinginterpretations from a certain model. Trustworthy interpretations should not be produced by modelsthat are not stable. Hence, an explainable model should contain information about the con\ufb01dence of itsworking regime.\u2022 Fairness: from a social standpoint, explainability can be considered as the capacity to reach andguarantee fairness in ML models. In a certain literature strand, an explainable ML model suggests aclear visualization of the relations affecting a result, allowing for a fairness or ethical analysis of themodel at hand [3, 100]. Likewise, a related objective of XAI is highlighting bias in the data a modelwas exposed to [168, 169]. The support of algorithms and models is growing fast in \ufb01elds that involvehuman lives, hence explainability should be considered as a bridge to avoid the unfair or unethical useof algorithm\u2019s outputs.\u2022 Accessibility: a minor subset of the reviewed contributions argues for explainability as the propertythat allows end users to get more involved in the process of improving and developing a certain MLmodel [37, 86] . It seems clear that explainable models will ease the burden felt by non-technical ornon-expert users when having to deal with algorithms that seem incomprehensible at \ufb01rst sight. Thisconcept is expressed as the third most considered goal among the surveyed literature.\u2022 Interactivity: some contributions [50, 59] include the ability of a model to be interactive with the useras one of the goals targeted by an explainable ML model. Once again, this goal is related to \ufb01elds in9which the end users are of great importance, and their ability to tweak and interact with the models iswhat ensures success.\u2022 Privacy awareness: almost forgotten in the reviewed literature, one of the byproducts enabled by ex-plainability in ML models is its ability to assess privacy. ML models may have complex representationsof their learned patterns. Not being able to understand what has been captured by the model [4] andstored in its internal representation may entail a privacy breach. Contrarily, the ability to explain theinner relations of a trained model by non-authorized third parties may also compromise the differentialprivacy of the data origin. Due to its criticality in sectors where XAI is foreseen to play a crucial role,con\ufb01dentiality and privacy issues will be covered further in Subsections 5.4 and 6.3, respectively.This subsection has reviewed the goals encountered among the broad scope of the reviewed papers.All these goals are clearly under the surface of the concept of explainability introduced before in thissection. To round up this prior analysis on the concept of explainability, the last subsection deals withdifferent strategies followed by the community to address explainability in ML models.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "49d8debf-6796-4408-b8a9-721e09fb241d",
                    "text": "The literature makes a clear distinction among models that are interpretable by design, and thosethat can be explained by means of external XAI techniques. This duality could also be regarded as thedifference between interpretable models and model interpretability techniques; a more widely acceptedclassi\ufb01cation is that of transparent models and post-hoc explainability. This same duality also appearsin the paper presented in [17] in which the distinction its authors make refers to the methods to solvethe transparent box design problem against the problem of explaining the black-box problem. Thiswork, further extends the distinction made among transparent models including the different levels oftransparency considered.Within transparency, three levels are contemplated: algorithmic transparency, decomposability andsimulatability . Among post-hoc techniques we may distinguish among text explanations, visualizations,local explanations, explanations by example, explanations by simpli\ufb01cation and feature relevance. In thiscontext, there is a broader distinction proposed by [24] discerning between 1) opaque systems, wherethe mappings from input to output are invisible to the user; 2) interpretable systems, in which users canmathematically analyze the mappings; and 3) comprehensible systems, in which the models should outputsymbols or rules along with their speci\ufb01c output to aid in the understanding process of the rationalebehind the mappings being made. This last classi\ufb01cation criterion could be considered included withinthe one proposed earlier, hence this paper will attempt at following the more speci\ufb01c one.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "4bc8072f-a360-4420-8230-09ac991af102",
                    "text": "Transparent models convey some degree of interpretability by themselves. Models belonging tothis category can be also approached in terms of the domain in which they are interpretable, namely,algorithmic transparency, decomposability and simulatability. As we elaborate next in connection toFigure 3, each of these classes contains its predecessors, e.g. a simulatable model is at the same time amodel that is decomposable and algorithmically transparent:\u2022 Simulatability denotes the ability of a model of being simulated or thought about strictly by a human,hence complexity takes a dominant place in this class. This being said, simple but extensive (i.e., withtoo large amount of rules) rule based systems fall out of this characteristic, whereas a single perceptronneural network falls within. This aspect aligns with the claim that sparse linear models are moreinterpretable than dense ones [170], and that an interpretable model is one that can be easily presented10to a human by means of text and visualizations [32]. Again, endowing a decomposable model withsimulatability requires that the model has to be self-contained enough for a human to think and reasonabout it as a whole.\u2022 Decomposability stands for the ability to explain each of the parts of a model (input, parameter andcalculation). It can be considered as intelligibility as stated in [171]. This characteristic might empowerthe ability to understand, interpret or explain the behavior of a model. However, as occurs withalgorithmic transparency, not every model can ful\ufb01ll this property. Decomposability requires everyinput to be readily interpretable (e.g. cumbersome features will not \ufb01t the premise). The addedconstraint for an algorithmically transparent model to become decomposable is that every part of themodel must be understandable by a human without the need for additional tools.\u2022 Algorithmic Transparency can be seen in different ways. It deals with the ability of the user tounderstand the process followed by the model to produce any given output from its input data. Putit differently, a linear model is deemed transparent because its error surface can be understood andreasoned about, allowing the user to understand how the model will act in every situation it mayface [163]. Contrarily, it is not possible to understand it in deep architectures as the loss landscapemight be opaque [172, 173] since it cannot be fully observed and the solution has to be approximatedthrough heuristic optimization (e.g. through stochastic gradient descent). The main constraint foralgorithmically transparent models is that the model has to be fully explorable by means of mathematicalanalysis and methods.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "f0c0ca3d-e425-4cdf-8b00-09412032a740",
                    "text": "Post-hoc explainability targets models that are not readily interpretable by design by resorting todiverse means to enhance their interpretability, such as text explanations, visual explanations, localexplanations, explanations by example, explanations by simpli\ufb01cation and feature relevance explanationstechniques. Each of these techniques covers one of the most common ways humans explain systems andprocesses by themselves.Further along this river, actual techniques, or better put, actual group of techniques are speci\ufb01edto ease the future work of any researcher that intends to look up for an speci\ufb01c technique that suits itsknowledge. Not ending there, the classi\ufb01cation also includes the type of data in which the techniques hasbeen applied. Note that many techniques might be suitable for many different types of data, althoughthe categorization only considers the type used by the authors that proposed such technique. Overall,post-hoc explainability techniques are divided \ufb01rst by the intention of the author (explanation techniquee.g. Explanation by simpli\ufb01cation), then, by the method utilized (actual technique e.g. sensitivity analysis)and \ufb01nally by the type of data in which it was applied (e.g. images).11\u2022 Text explanations deal with the problem of bringing explainability for a model by means of learning togenerate text explanations that help explaining the results from the model [169]. Text explanations alsoinclude every method generating symbols that represent the functioning of the model. These symbolsmay portrait the rationale of the algorithm by means of a semantic mapping from model to symbols.\u2022 Visual explanation techniques for post-hoc explainability aim at visualizing the model\u2019s behavior.Many of the visualization methods existing in the literature come along with dimensionality reductiontechniques that allow for a human interpretable simple visualization. Visualizations may be coupledwith other techniques to improve their understanding, and are considered as the most suitable way tointroduce complex interactions within the variables involved in the model to users not acquainted toML modeling.\u2022 Local explanations tackle explainability by segmenting the solution space and giving explanationsto less complex solution subspaces that are relevant for the whole model. These explanations can beformed by means of techniques with the differentiating property that these only explain part of thewhole system\u2019s functioning.\u2022 Explanations by example consider the extraction of data examples that relate to the result generated bya certain model, enabling to get a better understanding of the model itself. Similarly to how humansbehave when attempting to explain a given process, explanations by example are mainly centered inextracting representative examples that grasp the inner relationships and correlations found by themodel being analyzed.\u2022 Explanations by simpli\ufb01cation collectively denote those techniques in which a whole new system isrebuilt based on the trained model to be explained. This new, simpli\ufb01ed model usually attempts atoptimizing its resemblance to its antecedent functioning, while reducing its complexity, and keeping asimilar performance score. An interesting byproduct of this family of post-hoc techniques is that thesimpli\ufb01ed model is, in general, easier to be implemented due to its reduced complexity with respect tothe model it represents.\u2022 Finally, feature relevance explanation methods for post-hoc explainability clarify the inner functioningof a model by computing a relevance score for its managed variables. These scores quantify the affection(sensitivity) a feature has upon the output of the model. A comparison of the scores among differentvariables unveils the importance granted by the model to each of such variables when producing itsoutput. Feature relevance methods can be thought to be an indirect method to explain a model.The above classi\ufb01cation (portrayed graphically in Figure 4) will be used when reviewing spe-ci\ufb01c/agnostic XAI techniques for ML models in the following sections (Table 2). For each ML model, adistinction of the propositions to each of these categories is presented in order to pose an overall image ofthe \ufb01eld\u2019s trends.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "8e798f12-1897-4c24-b86a-19f72dc65f54",
                    "text": "The previous section introduced the concept of transparent models. A model is considered to betransparent if by itself it is understandable. The models surveyed in this section are a suit of transparentmodels that can fall in one or all of the levels of model transparency described previously (namely,simulatability, decomposability and algorithmic transparency). In what follows we provide reasons forthis statement, with graphical support given in Figure 5.12",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "9d4d7e5f-9a55-4efd-b4c8-02281fd40f4e",
                    "text": "Logistic Regression (LR) is a classi\ufb01cation model to predict a dependent variable (category) that isdichotomous (binary). However, when the dependent variable is continuous, linear regression wouldbe its homonym. This model takes the assumption of linear dependence between the predictors and thepredicted variables, impeding a \ufb02exible \ufb01t to the data. This speci\ufb01c reason (stiffness of the model) is theone that maintains the model under the umbrella of transparent methods. However, as stated in Section 2,explainability is linked to a certain audience, which makes a model fall under both categories dependingwho is to interpret it. This way, logistic and linear regression, although clearly meeting the characteristicsof transparent models (algorithmic transparency, decomposability and simulatability), may also demandpost-hoc explainability techniques (mainly, visualization), particularly when the model is to be explainedto non-expert audiences.The usage of this model has been largely applied within Social Sciences for quite a long time,which has pushed researchers to create ways of explaining the results of the models to non-expertusers. Most authors agree on the different techniques used to analyze and express the soundness of LR[174, 175, 176, 177], including the overall model evaluation, statistical tests of individual predictors,goodness-of-\ufb01t statistics and validation of the predicted probabilities. The overall model evaluationshows the improvement of the applied model over a baseline, showing if it is in fact improving the modelwithout predictions. The statistical signi\ufb01cance of single predictors is shown by calculating the Waldchi-square statistic. The goodness-of-\ufb01t statistics show the quality of \ufb01tness of the model to the dataand how signi\ufb01cant this is. This can be achieved by resorting to different techniques e.g. the so-calledHosmer-Lemeshow (H-L) statistic. The validation of predicted probabilities involves testing whether theoutput of the model corresponds to what is shown by the data. These techniques show mathematical waysof representing the \ufb01tness of the model and its behavior.Other techniques from other disciplines besides Statistics can be adopted for explaining these re-13gression models. Visualization techniques are very powerful when presenting statistical conclusions tousers not well-versed in statistics. For instance, the work in [178] shows that the usage of probabilities tocommunicate the results, implied that the users where able to estimate the outcomes correctly in 10% ofthe cases, as opposed to 46% of the cases when using natural frequencies. Although logistic regression isamong the simplest classi\ufb01cation models in supervised learning, there are concepts that must be takencare of.In this line of reasoning, the authors of [179] unveil some concerns with the interpretations derivedfrom LR. They \ufb01rst mention how dangerous it might be to interpret log odds ratios and odd ratios assubstantive effects, since they also represent unobserved heterogeneity. Linked to this \ufb01rst concern,[179] also states that a comparison between these ratios across models with different variables might beproblematic, since the unobserved heterogeneity is likely to vary, thereby invalidating the comparison.Finally they also mention that the comparison of these odds across different samples, groups and time isalso risky, since the variation of the heterogeneity is not known across samples, groups and time points.This last paper serves the purpose of visualizing the problems a model\u2019s interpretation might entail, evenwhen its construction is as simple as that of LR.Also interesting is to note that, for a model such as logistic or linear regression to maintain decompos-ability and simulatability, its size must be limited, and the variables used must be understandable by theirusers. As stated in Section 2, if inputs to the model are highly engineered features that are complex ordif\ufb01cult to understand, the model at hand will be far from being decomposable. Similarly, if the model isso large that a human cannot think of the model as a whole, its simulatability will be put to question.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "7500de4a-e5ca-4285-b927-d26cb3d58ba7",
                    "text": "Decision trees are another example of a model that can easily ful\ufb01ll every constraint for transparency.Decision trees are hierarchical structures for decision making used to support regression and classi\ufb01cationproblems [132, 180]. In the simplest of their \ufb02avors, decision trees are simulatable models. However,their properties can render them decomposable or algorithmically transparent.14Decision trees have always lingered in between the different categories of transparent models. Theirutilization has been closely linked to decision making contexts, being the reason why their complexityand understandability have always been considered a paramount matter. A proof of this relevance canbe found in the upsurge of contributions to the literature dealing with decision tree simpli\ufb01cation andgeneration [132, 180, 181, 182]. As noted above, although being capable of \ufb01tting every category withintransparent models, the individual characteristics of decision trees can push them toward the category ofalgorithmically transparent models. A simulatable decision tree is one that is manageable by a humanuser. This means its size is somewhat small and the amount of features and their meaning are easilyunderstandable. An increment in size transforms the model into a decomposable one since its size impedesits full evaluation (simulation) by a human. Finally, further increasing its size and using complex featurerelations will make the model algorithmically transparent loosing the previous characteristics.Decision trees have long been used in decision support contexts due to their off-the-shelf transparency.Many applications of these models fall out of the \ufb01elds of computation and AI (even informationtechnologies), meaning that experts from other \ufb01elds usually feel comfortable interpreting the outputs ofthese models [183, 184, 185]. However, their poor generalization properties in comparison with othermodels make this model family less interesting for their application to scenarios where a balance betweenpredictive performance is a design driver of utmost importance. Tree ensembles aim at overcoming sucha poor performance by aggregating the predictions performed by trees learned on different subsets oftraining data. Unfortunately, the combination of decision trees looses every transparent property, callingfor the adoption of post-hoc explainability techniques as the ones reviewed later in the manuscript.15",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "09fe67d9-d414-43db-a48d-c4bbe91f95d2",
                    "text": "Another method that falls within transparent models is that of K-Nearest Neighbors (KNN), whichdeals with classi\ufb01cation problems in a methodologically simple way: it predicts the class of a test sampleby voting the classes of its K nearest neighbors (where the neighborhood relation is induced by a measureof distance between samples). When used in the context of regression problems, the voting is replaced byan aggregation (e.g. average) of the target values associated with the nearest neighbors.In terms of model explainability, it is important to observe that predictions generated by KNN modelsrely on the notion of distance and similarity between examples, which can be tailored depending on thespeci\ufb01c problem being tackled. Interestingly, this prediction approach resembles that of experience-basedhuman decision making, which decides upon the result of past similar cases. There lies the rationaleof why KNN has also been adopted widely in contexts in which model interpretability is a requirement[186, 187, 188, 189]. Furthermore, aside from being simple to explain, the ability to inspect the reasonsby which a new sample has been classi\ufb01ed inside a group and to examine how these predictions evolvewhen the number of neighbors K is increased or decreased empowers the interaction between the usersand the model.One must keep in mind that as mentioned before, KNN\u2019s class of transparency depends on the features,the number of neighbors and the distance function used to measure the similarity between data instances.A very high K impedes a full simulation of the model performance by a human user. Similarly, the usageof complex features and/or distance functions would hinder the decomposability of the model, restrictingits interpretability solely to the transparency of its algorithmic operations.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "049cc19e-6541-4efa-9f3a-4af65cd35117",
                    "text": "Rule-based learning refers to every model that generates rules to characterize the data it is intended tolearn from. Rules can take the form of simple conditional if-then rules or more complex combinations ofsimple rules to form their knowledge. Also connected to this general family of models, fuzzy rule basedsystems are designed for a broader scope of action, allowing for the de\ufb01nition of verbally formulatedrules over imprecise domains. Fuzzy systems improve two main axis relevant for this paper. First, theyempower more understandable models since they operate in linguistic terms. Second, they perform betterthat classic rule systems in contexts with certain degrees of uncertainty. Rule based learners are clearlytransparent models that have been often used to explain complex models by generating rules that explaintheir predictions [126, 127, 190, 191].Rule learning approaches have been extensively used for knowledge representation in expert systems[192]. However, a central problem with rule generation approaches is the coverage (amount) and thespeci\ufb01city (length) of the rules generated. This problem relates directly to the intention for their use inthe \ufb01rst place. When building a rule database, a typical design goal sought by the user is to be able toanalyze and understand the model. The amount of rules in a model will clearly improve the performanceof the model at the stake of compromising its intepretability. Similarly, the speci\ufb01city of the rules playsalso against interpretability, since a rule with a high number of antecedents an/or consequences mightbecome dif\ufb01cult to interpret. In this same line of reasoning, these two features of a rule based learnerplay along with the classes of transparent models presented in Section 2. The greater the coverage orthe speci\ufb01city is, the closer the model will be to being just algorithmically transparent. Sometimes, thereason to transition from classical rules to fuzzy rules is to relax the constraints of rule sizes, since agreater range can be covered with less stress on interpretability.Rule based learners are great models in terms of interpretability across \ufb01elds. Their natural andseamless relation to human behaviour makes them very suitable to understand and explain other models.If a certain threshold of coverage is acquired, a rule wrapper can be thought to contain enough informationabout a model to explain its behavior to a non-expert user, without forfeiting the possibility of using thegenerated rules as an standalone prediction model.16",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "981bde40-1c5d-4ee7-a4db-3062c42eccc5",
                    "text": "In statistics, a Generalized Additive Model (GAM) is a linear model in which the value of thevariable to be predicted is given by the aggregation of a number of unknown smooth functions de\ufb01nedfor the predictor variables. The purpose of such model is to infer the smooth functions whose aggregatecomposition approximates the predicted variable. This structure is easily interpretable, since it allows theuser to verify the importance of each variable, namely, how it affects (through its corresponding function)the predicted output.Similarly to every other transparent model, the literature is replete with case studies where GAMsare in use, specially in \ufb01elds related to risk assessment. When compared to other models, these areunderstandable enough to make users feel con\ufb01dent on using them for practical applications in \ufb01nance[193, 194, 195], environmental studies [196], geology [197], healthcare [44], biology [198, 199] andenergy [200]. Most of these contributions use visualization methods to further ease the interpretation ofthe model. GAMs might be also considered as simulatable and decomposable models if the propertiesmentioned in its de\ufb01nitions are ful\ufb01lled, but to an extent that depends roughly on eventual modi\ufb01cationsto the baseline GAM model, such as the introduction of link functions to relate the aggregation with thepredicted output, or the consideration of interactions between predictors.All in all, applications of GAMs like the ones exempli\ufb01ed above share one common factor: under-standability. The main driver for conducting these studies with GAMs is to understand the underlyingrelationships that build up the cases for scrutiny. In those cases the research goal is not accuracy for itsown sake, but rather the need for understanding the problem behind and the relationship underneath thevariables involved in data. This is why GAMs have been accepted in certain communities as their de factomodeling choice, despite their acknowledged misperforming behavior when compared to more complexcounterparts.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "1ae79d38-38e5-477d-aa2a-6a01c545813f",
                    "text": "A Bayesian model usually takes the form of a probabilistic directed acyclic graphical model whoselinks represent the conditional dependencies between a set of variables. For example, a Bayesian networkcould represent the probabilistic relationships between diseases and symptoms. Given symptoms, thenetwork can be used to compute the probabilities of the presence of various diseases. Similar to GAMs,these models also convey a clear representation of the relationships between features and the target, whichin this case are given explicitly by the connections linking variables to each other.Once again, Bayesian models fall below the ceiling of Transparent models. Its categorization leavesit under simulatable, decomposable and algorithmically transparent. However, it is worth noting thatunder certain circumstances (overly complex or cumbersome variables), a model may loose these \ufb01rsttwo properties. Bayesian models have been shown to lead to great insights in assorted applications suchas cognitive modeling [201, 202], \ufb01shery [196, 203], gaming [204], climate [205], econometrics [206] orrobotics [207]. Furthermore, they have also been utilized to explain other models, such as averaging treeensembles [208].",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "0ca0ad85-ffc0-4a78-9694-c3369452f998",
                    "text": "When ML models do not meet any of the criteria imposed to declare them transparent, a separatemethod must be devised and applied to the model to explain its decisions. This is the purpose of post-hocexplainability techniques (also referred to as post-modeling explainability), which aim at communicatingunderstandable information about how an already developed model produces its predictions for any giveninput. In this section we categorize and review different algorithmic approaches for post-hoc explainability,discriminating among 1) those that are designed for their application to ML models of any kind; and 2)those that are designed for a speci\ufb01c ML model and thus, can not be directly extrapolated to any other17learner. We now elaborate on the trends identi\ufb01ed around post-hoc explainability for different ML models,which are illustrated in Figure 6 in the form of hierarchical bibliographic categories and summarized next:\u2022 Model-agnostic techniques for post-hoc explainability (Subsection 4.1), which can be applied seam-lessly to any ML model disregarding its inner processing or internal representations.\u2022 Post-hoc explainability that are tailored or speci\ufb01cally designed to explain certain ML models. Wedivide our literature analysis into two main branches: contributions dealing with post-hoc explainabilityof shallow ML models, which collectively refers to all ML models that do not hinge on layeredstructures of neural processing units (Subsection 4.2); and techniques devised for deep learning models,which correspondingly denote the family of neural networks and related variants, such as convolutionalneural networks, recurrent neural networks (Subsection 4.3) and hybrid schemes encompassing deepneural networks and transparent models. For each model we perform a thorough review of the latestpost-hoc methods proposed by the research community, along with a identi\ufb01cation of trends followedby such contributions.\u2022 We end our literature analysis with Subsection 4.4, where we present a second taxonomy that com-plements the more general one in Figure 6 by classifying contributions dealing with the post-hocexplanation of Deep Learning models. To this end we focus on particular aspects related to this familyof black-box ML methods, and expose how they link to the classi\ufb01cation criteria used in the \ufb01rsttaxonomy.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "ed7d9564-48fb-48b1-8ffe-cf0fe7482abb",
                    "text": "Model-agnostic techniques for post-hoc explainability are designed to be plugged to any modelwith the intent of extracting some information from its prediction procedure. Sometimes, simpli\ufb01cationtechniques are used to generate proxies that mimic their antecedents with the purpose of having somethingtractable and of reduced complexity. Other times, the intent focuses on extracting knowledge directlyfrom the models or simply visualizing them to ease the interpretation of their behavior. Following thetaxonomy introduced in Section 2, model-agnostic techniques may rely on model simpli\ufb01cation, featurerelevance estimation and visualization techniques:\u2022 Explanation by simpli\ufb01cation. They are arguably the broadest technique under the category of modelagnostic post-hoc methods. Local explanations are also present within this category, since sometimes,simpli\ufb01ed models are only representative of certain sections of a model. Almost all techniques takingthis path for model simpli\ufb01cation are based on rule extraction techniques. Among the most knowncontributions to this approach we encounter the technique of Local Interpretable Model-AgnosticExplanations (LIME) [32] and all its variations [214, 216]. LIME builds locally linear models aroundthe predictions of an opaque model to explain it. These contributions fall under explanations bysimpli\ufb01cation as well as under local explanations. Besides LIME and related \ufb02avors, another approachto rule extraction is G-REX [212]. Although it was not originally intended for extracting rules fromopaque models, the generic proposition of G-REX has been extended to also account for modelexplainability purposes [190, 211]. In line with rule extraction methods, the work in [215] presents anovel approach to learn rules in CNF (Conjunctive Normal Form) or DNF (Disjunctive Normal Form)to bridge from a complex model to a human-interpretable model. Another contribution that falls off thesame branch is that in [218], where the authors formulate model simpli\ufb01cation as a model extractionprocess by approximating a transparent model to the complex one. Simpli\ufb01cation is approached from adifferent perspective in [120], where an approach to distill and audit black box models is presented. Init, two main ideas are exposed: a method for model distillation and comparison to audit black-box riskscoring models; and an statistical test to check if the auditing data is missing key features it was trainedwith. The popularity of model simpli\ufb01cation is evident, given it temporally coincides with the most18recent literature on XAI, including techniques such as LIME or G-REX. This symptomatically revealsthat this post-hoc explainability approach is envisaged to continue playing a central role on XAI.\u2022 Feature relevance explanation techniques aim to describe the functioning of an opaque model by19ranking or measuring the in\ufb02uence, relevance or importance each feature has in the prediction output bythe model to be explained. An amalgam of propositions are found within this category, each resortingto different algorithmic approaches with the same targeted goal. One fruitful contribution to this pathis that of [224] called SHAP (SHapley Additive exPlanations). Its authors presented a method tocalculate an additive feature importance score for each particular prediction with a set of desirableproperties (local accuracy, missingness and consistency) that its antecedents lacked. Another approachto tackle the contribution of each feature to predictions has been coalitional Game Theory [225] andlocal gradients [234]. Similarly, by means of local gradients [230] test the changes needed in eachfeature to produce a change in the output of the model. In [228] the authors analyze the relations anddependencies found in the model by grouping features, that combined, bring insights about the data.The work in [173] presents a broad variety of measures to tackle the quanti\ufb01cation of the degree ofin\ufb02uence of inputs on outputs of systems. Their QII (Quantitative Input In\ufb02uence) measures accountfor correlated inputs while measuring in\ufb02uence. In contrast, in [222] the authors build upon the existingSA (Sensitivity Analysis) to construct a Global SA which extends the applicability of the existingmethods. In [227] a real-time image saliency method is proposed, which is applicable to differentiableimage classi\ufb01ers. The study in [123] presents the so-called Automatic STRucture IDenti\ufb01cation method(ASTRID) to inspect which attributes are exploited by a classi\ufb01er to generate a prediction. This method\ufb01nds the largest subset of features such that the accuracy of a classi\ufb01er trained with this subset offeatures cannot be distinguished in terms of accuracy from a classi\ufb01er built on the original feature set.In [221] the authors use in\ufb02uence functions to trace a model\u2019s prediction back to the training data, byonly requiring an oracle version of the model with access to gradients and Hessian-vector products.Heuristics for creating counterfactual examples by modifying the input of the model have been alsofound to contribute to its explainability [236, 237]. Compared to those attempting explanations bysimpli\ufb01cation, a similar amount of publications were found tackling explainability by means of featurerelevance techniques. Many of the contributions date from 2017 and some from 2018, implying that aswith model simpli\ufb01cation techniques, feature relevance has also become a vibrant subject study in thecurrent XAI landscape.\u2022 Visual explanation techniques are a vehicle to achieve model-agnostic explanations. Representativeworks in this area can be found in [222], which present a portfolio of visualization techniques to help inthe explanation of a black-box ML model built upon the set of extended techniques mentioned earlier(Global SA). Another set of visualization techniques is presented in [223]. The authors present threenovel SA methods (data based SA, Monte-Carlo SA, cluster-based SA) and one novel input importancemeasure (Average Absolute Deviation). Finally, [238] presents ICE (Individual Conditional Expecta-tion) plots as a tool for visualizing the model estimated by any supervised learning algorithm. Visualexplanations are less common in the \ufb01eld of model-agnostic techniques for post-hoc explainability.Since the design of these methods must ensure that they can be seamlessly applied to any ML modeldisregarding its inner structure, creating visualizations from just inputs and outputs from an opaquemodel is a complex task. This is why almost all visualization methods falling in this category workalong with feature relevance techniques, which provide the information that is eventually displayed tothe end user.Several trends emerge from our literature analysis. To begin with, rule extraction techniques prevailin model-agnostic contributions under the umbrella of post-hoc explainability. This could have beenintuitively expected if we bear in mind the wide use of rule based learning as explainability wrappersanticipated in Section 3.4, and the complexity imposed by not being able to get into the model itself.Similarly, another large group of contributions deals with feature relevance. Lately these techniques aregathering much attention by the community when dealing with DL models, with hybrid approaches thatutilize particular aspects of this class of models and therefore, compromise the independence of the featurerelevance method on the model being explained. Finally, visualization techniques propose interesting20ways for visualizing the output of feature relevance techniques to ease the task of model\u2019s interpretation.By contrast, visualization techniques for other aspects of the trained model (e.g. its structure, operations,etc) are tightly linked to the speci\ufb01c model to be explained.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "99b10f86-6bb5-4bf5-9422-3afb466dece2",
                    "text": "Shallow ML covers a diversity of supervised learning models. Within these models, there are strictlyinterpretable (transparent) approaches (e.g. KNN and Decision Trees, already discussed in Section 3).However, other shallow ML models rely on more sophisticated learning algorithms that require additionallayers of explanation. Given their prominence and notable performance in predictive tasks, this sectionconcentrates on two popular shallow ML models (tree ensembles and Support Vector Machines, SVMs)that require the adoption of post-hoc explainability techniques for explaining their decisions.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "6335d761-926e-4b3b-9d19-67a4677e49f1",
                    "text": "Tree ensembles are arguably among the most accurate ML models in use nowadays. Their adventcame as an ef\ufb01cient means to improve the generalization capability of single decision trees, which areusually prone to over\ufb01tting. To circumvent this issue, tree ensembles combine different trees to obtain anaggregated prediction/regression. While it results to be effective against over\ufb01tting, the combination ofmodels makes the interpretation of the overall ensemble more complex than each of its compounding treelearners, forcing the user to draw from post-hoc explainability techniques. For tree ensembles, techniquesfound in the literature are explanation by simpli\ufb01cation and feature relevance techniques; we next examinerecent advances in these techniques.To begin with, many contributions have been presented to simplify tree ensembles while maintainingpart of the accuracy accounted for the added complexity. The author from [119] poses the idea of traininga single albeit less complex model from a set of random samples from the data (ideally following the realdata distribution) labeled by the ensemble model. Another approach for simpli\ufb01cation is that in [118], inwhich authors create a Simpli\ufb01ed Tree Ensemble Learner (STEL). Likewise, [122] presents the usageof two models (simple and complex) being the former the one in charge of interpretation and the latterof prediction by means of Expectation-Maximization and Kullback-Leibler divergence. As opposed towhat was seen in model-agnostic techniques, not that many techniques to board explainability in treeensembles by means of model simpli\ufb01cation. It derives from this that either the proposed techniques aregood enough, or model-agnostic techniques do cover the scope of simpli\ufb01cation already.Following simpli\ufb01cation procedures, feature relevance techniques are also used in the \ufb01eld of treeensembles. Breiman [286] was the \ufb01rst to analyze the variable importance within Random Forests. Hismethod is based on measuring MDA (Mean Decrease Accuracy) or MIE (Mean Increase Error) of theforest when a certain variable is randomly permuted in the out-of-bag samples. Following this contribution[241] shows, in an real setting, how the usage of variable importance re\ufb02ects the underlying relationshipsof a complex system modeled by a Random Forest. Finally, a crosswise technique among post-hocexplainability, [240] proposes a framework that poses recommendations that, if taken, would convertan example from one class to another. This idea attempts to disentangle the variables importance in away that is further descriptive. In the article, the authors show how these methods can be used to elevaterecommendations to improve malicious online ads to make them rank higher in paying rates.Similar to the trend shown in model-agnostic techniques, for tree ensembles again, simpli\ufb01cation andfeature relevance techniques seem to be the most used schemes. However, contrarily to what was observedbefore, most papers date back from 2017 and place their focus mostly on bagging ensembles. Whenshifting the focus towards other ensemble strategies, scarce activity has been recently noted around theexplainability of boosting and stacking classi\ufb01ers. Among the latter, it is worth highlighting the connectionbetween the reason why a compounding learner of the ensemble produces an speci\ufb01c prediction on a givendata, and its contribution to the output of the ensemble. The so-called Stacking With Auxiliary Features(SWAF) approach proposed in [242] points in this direction by harnessing and integrating explanations in21stacking ensembles to improve their generalization. This strategy allows not only relying on the outputof the compounding learners, but also on the origin of that output and its consensus across the entireensemble. Other interesting studies on the explainability of ensemble techniques include model-agnosticschemes such as DeepSHAP [226], put into practice with stacking ensembles and multiple classi\ufb01ersystems in addition to Deep Learning models; the combination of explanation maps of multiple classi\ufb01ersto produce improved explanations of the ensemble to which they belong [243]; and recent insights dealingwith traditional and gradient boosting ensembles [287, 288].",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "777e0f89-5040-4f56-9cf9-a1b23ff8ca18",
                    "text": "Another shallow ML model with historical presence in the literature is the SVM. SVM models aremore complex than tree ensembles, with a much opaquer structure. Many implementations of post-hocexplainability techniques have been proposed to relate what is mathematically described internally inthese models, to what different authors considered explanations about the problem at hand. Technically,an SVM constructs a hyper-plane or set of hyper-planes in a high or in\ufb01nite-dimensional space, whichcan be used for classi\ufb01cation, regression, or other tasks such as outlier detection. Intuitively, a goodseparation is achieved by the hyperplane that has the largest distance (so-called functional margin) to thenearest training-data point of any class, since in general, the larger the margin, the lower the generalizationerror of the classi\ufb01er. SVMs are among the most used ML models due to their excellent predictionand generalization capabilities. From the techniques stated in Section 2, post-hoc explainability appliedto SVMs covers explanation by simpli\ufb01cation, local explanations, visualizations and explanations byexample.Among explanation by simpli\ufb01cation, four classes of simpli\ufb01cations are made. Each of them dif-ferentiates from the other by how deep they go into the algorithm inner structure. First, some authorspropose techniques to build rule based models only from the support vectors of a trained model. This isthe approach of [93], which proposes a method that extracts rules directly from the support vectors of atrained SVM using a modi\ufb01ed sequential covering algorithm. In [57] the same authors propose eclecticrule extraction, still considering only the support vectors of a trained model. The work in [94] generatesfuzzy rules instead of classical propositional rules. Here, the authors argue that long antecedents reducecomprehensibility, hence, a fuzzy approach allows for a more linguistically understandable result. Thesecond class of simpli\ufb01cations can be exempli\ufb01ed by [98], which proposed the addition of the SVM\u2019shyperplane, along with the support vectors, to the components in charge of creating the rules. His methodrelies on the creation of hyper-rectangles from the intersections between the support vectors and thehyper-plane. In a third approach to model simpli\ufb01cation, another group of authors considered addingthe actual training data as a component for building the rules. In [126, 244, 246] the authors proposed aclustering method to group prototype vectors for each class. By combining them with the support vectors,it allowed de\ufb01ning ellipsoids and hyper-rectangles in the input space. Similarly in [106], the authorsproposed the so-called Hyper-rectangle Rule Extraction, an algorithm based on SVC (Support VectorClustering) to \ufb01nd prototype vectors for each class and then de\ufb01ne small hyper-rectangles around. In[105], the authors formulate the rule extraction problem as a multi-constrained optimization to create aset of non-overlapping rules. Each rule conveys a non-empty hyper-cube with a shared edge with thehyper-plane. In a similar study conducted in [245], extracting rules for gene expression data, the authorspresented a novel technique as a component of a multi-kernel SVM. This multi-kernel method consistsof feature selection, prediction modeling and rule extraction. Finally, the study in [134] makes use of agrowing SVC to give an interpretation to SVM decisions in terms of linear rules that de\ufb01ne the space inVoronoi sections from the extracted prototypes.Leaving aside rule extraction, the literature has also contemplated some other techniques to contributeto the interpretation of SVMs. Three of them (visualization techniques) are clearly used toward explainingSVM models when used for concrete applications. For instance, [77] presents an innovative approach tovisualize trained SVM to extract the information content from the kernel matrix. They center the study22on Support Vector Regression models. They show the ability of the algorithm to visualize which of theinput variables are actually related with the associated output data. In [68] a visual way combines theoutput of the SVM with heatmaps to guide the modi\ufb01cation of compounds in late stages of drug discovery.They assign colors to atoms based on the weights of a trained linear SVM that allows for a much morecomprehensive way of debugging the process. In [116] the authors argue that many of the presentedstudies for interpreting SVMs only account for the weight vectors, leaving the margin aside. In their studythey show how this margin is important, and they create an statistic that explicitly accounts for the SVMmargin. The authors show how this statistic is speci\ufb01c enough to explain the multivariate patterns shownin neuroimaging.Noteworthy is also the intersection between SVMs and Bayesian systems, the latter being adoptedas a post-hoc technique to explain decisions made by the SVM model. This is the case of [248] and[247], which are studies where SVMs are interpreted as MAP (Maximum A Posteriori) solutions toinference problems with Gaussian Process priors. This framework makes tuning the hyper-parameterscomprehensible and gives the capability of predicting class probabilities instead of the classical binaryclassi\ufb01cation of SVMs. Interpretability of SVM models becomes even more involved when dealingwith non-CPD (Conditional Positive De\ufb01nite) kernels that are usually harder to interpret due to missinggeometrical and theoretical understanding. The work in [102] revolves around this issue with a geometricalinterpretation of inde\ufb01nite kernel SVMs, showing that these do not classify by hyper-plane marginoptimization. Instead, they minimize the distance between convex hulls in pseudo-Euclidean spaces.A difference might be appreciated between the post-hoc techniques applied to other models and thosenoted for SVMs. In previous models, model simpli\ufb01cation in a broad sense was the prominent methodfor post-hoc explainability. In SVMs, local explanations have started to take some weight among thepropositions. However, simpli\ufb01cation based methods are, on average, much older than local explanations.As a \ufb01nal remark, none of the reviewed methods treating SVM explainability are dated beyond 2017,which might be due to the progressive proliferation of DL models in almost all disciplines. Anotherplausible reason is that these models are already understood, so it is hard to improve upon what hasalready been done.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "1bb3a990-cef8-4d41-873a-f5997990498b",
                    "text": "Post-hoc local explanations and feature relevance techniques are increasingly the most adoptedmethods for explaining DNNs. This section reviews explainability studies proposed for the most usedDL models, namely multi-layer neural networks, Convolutional Neural Networks (CNN) and RecurrentNeural Networks (RNN).",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "2338b04c-9a61-4823-a783-9553b5a50fb3",
                    "text": "From their inception, multi-layer neural networks (also known as multi-layer perceptrons) have beenwarmly welcomed by the academic community due to their huge ability to infer complex relations amongvariables. However, as stated in the introduction, developers and engineers in charge of deploying thesemodels in real-life production \ufb01nd in their questionable explainability a common reason for reluctance.That is why neural networks have been always considered as black-box models. The fact that explainabilityis often a must for the model to be of practical value, forced the community to generate multipleexplainability techniques for multi-layer neural networks, including model simpli\ufb01cation approaches,feature relevance estimators, text explanations, local explanations and model visualizations.Several model simpli\ufb01cation techniques have been proposed for neural networks with one singlehidden layer, however very few works have been presented for neural networks with multiple hiddenlayers. One of these few works is DeepRED algorithm [257], which extends the decompositional approachto rule extraction (splitting at neuron level) presented in [259] for multi-layer neural network by addingmore decision trees and rules.Some other works use model simpli\ufb01cation as a post-hoc explainability approach. For instance, [56]presents a simple distillation method called Interpretable Mimic Learning to extract an interpretable model23by means of gradient boosting trees. In the same direction, the authors in [135] propose a hierarchicalpartitioning of the feature space that reveals the iterative rejection of unlikely class labels, until associationis predicted. In addition, several works addressed the distillation of knowledge from an ensemble ofmodels into a single model [80, 289, 290] .Given the fact that the simpli\ufb01cation of multi-layer neural networks is more complex as the number oflayers increases, explaining these models by feature relevance methods has become progressively morepopular. One of the representative works in this area is [60], which presents a method to decompose thenetwork classi\ufb01cation decision into contributions of its input elements. They consider each neuron as anobject that can be decomposed and expanded then aggregate and back-propagate these decompositionsthrough the network, resulting in a deep Taylor decomposition. In the same direction, the authors in [110]proposed DeepLIFT, an approach for computing importance scores in a multi-layer neural network. Theirmethod compares the activation of a neuron to the reference activation and assigns the score according tothe difference.On the other hand, some works try to verify the theoretical soundness of current explainability methods.For example, the authors in [262], bring up a fundamental problem of most feature relevance techniques,designed for multi-layer networks. They showed that two axioms that such techniques ought to ful\ufb01llnamely, sensitivity and implementation invariance, are violated in practice by most approaches. Followingthese axioms, the authors of [262] created integrated gradients, a new feature relevance method provento meet the aforementioned axioms. Similarly, the authors in [61] analyzed the correctness of currentfeature relevance explanation approaches designed for Deep Neural Networks, e,g., DeConvNet, GuidedBackProp and LRP, on simple linear neural networks. Their analysis showed that these methods do notproduce the theoretically correct explanation and presented two new explanation methods PatternNet andPatternAttribution that are more theoretically sound for both, simple and deep neural networks.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "3e32b69c-d4f9-4c6a-9d7b-8d827c20b3c6",
                    "text": "Currently, CNNs constitute the state-of-art models in all fundamental computer vision tasks, fromimage classi\ufb01cation and object detection to instance segmentation. Typically, these models are built asa sequence of convolutional layers and pooling layers to automatically learn increasingly higher levelfeatures. At the end of the sequence, one or multiple fully connected layers are used to map the outputfeatures map into scores. This structure entails extremely complex internal relations that are very dif\ufb01cultto explain. Fortunately, the road to explainability for CNNs is easier than for other types of models, as thehuman cognitive skills favors the understanding of visual data.Existing works that aim at understanding what CNNs learn can be divided into two broad categories:1) those that try to understand the decision process by mapping back the output in the input space tosee which parts of the input were discriminative for the output; and 2) those that try to delve inside thenetwork and interpret how the intermediate layers see the external world, not necessarily related to anyspeci\ufb01c input, but in general.One of the seminal works in the \ufb01rst category was [291]. When an input image runs feed-forwardthrough a CNN, each layer outputs a number of feature maps with strong and soft activations. The authorsin [291] used Deconvnet, a network designed previously by the same authors [142] that, when fed with afeature map from a selected layer, reconstructs the maximum activations. These reconstructions can givean idea about the parts of the image that produced that effect. To visualize these strongest activations inthe input image, the same authors used the occlusion sensitivity method to generate a saliency map [136],which consists of iteratively forwarding the same image through the network occluding a different regionat a time.To improve the quality of the mapping on the input space, several subsequent papers proposedsimplifying both the CNN architecture and the visualization method. In particular, [96] included a globalaverage pooling layer between the last convolutional layer of the CNN and the fully-connected layer thatpredicts the object class. With this simple architectural modi\ufb01cation of the CNN, the authors built a class24activation map that helps identify the image regions that were particularly important for a speci\ufb01c objectclass by projecting back the weights of the output layer on the convolutional feature maps. Later, in [143],the authors showed that max-pooling layers can be used to replace convolutional layers with a large stridewithout loss in accuracy on several image recognition benchmarks. They obtained a cleaner visualizationthan Deconvnet by using a guided backpropagation method.To increase the interpretability of classical CNNs, the authors in [113] used a loss for each \ufb01lter inhigh level convolutional layers to force each \ufb01lter to learn very speci\ufb01c object components. The obtainedactivation patterns are much more interpretable for their exclusiveness with respect to the different labelsto be predicted. The authors in [72] proposed visualizing the contribution to the prediction of each singlepixel of the input image in the form of a heatmap. They used a Layer-wise Relevance Propagation (LRP)technique, which relies on a Taylor series close to the prediction point rather than partial derivatives atthe prediction point itself. To further improve the quality of the visualization, attribution methods suchas heatmaps, saliency maps or class activation methods (GradCAM [292]) are used (see Figure 7). Inparticular, the authors in [292] proposed a Gradient-weighted Class Activation Mapping (Grad-CAM),which uses the gradients of any target concept, \ufb02owing into the \ufb01nal convolutional layer to produce acoarse localization map, highlighting the important regions in the image for predicting the concept.(a) Heatmap [168] (b) Attribution [293] (c) Grad-CAM [292]In addition to the aforementioned feature relevance and visual explanation methods, some worksproposed generating text explanations of the visual content of the image. For example, the authors in [91]combined a CNN feature extractor with an RNN attention model to automatically learn to describe thecontent of images. In the same line, [278] presented a three-level attention model to perform a \ufb01ne-grainedclassi\ufb01cation task. The general model is a pipeline that integrates three types of attention: the objectlevel attention model proposes candidate image regions or patches from the input image, the part-levelattention model \ufb01lters out non-relevant patches to a certain object, and the last attention model localizesdiscriminative patches. In the task of video captioning, the authors in [111] use a CNN model combinedwith a bi-directional LSTM model as encoder to extract video features and then feed these features to anLSTM decoder to generate textual descriptions.One of the seminal works in the second category is [137]. In order to analyse the visual informationcontained inside the CNN, the authors proposed a general framework that reconstruct an image from theCNN internal representations and showed that several layers retain photographically accurate informationabout the image, with different degrees of geometric and photometric invariance. To visualize the notionof a class captured by a CNN, the same authors created an image that maximizes the class score based oncomputing the gradient of the class score with respect to the input image [272]. In the same direction,the authors in [268] introduced a Deep Generator Network (DGN) that generates the most representativeimage for a given output neuron in a CNN.For quantifying the interpretability of the latent representations of CNNs, the authors in [125] used adifferent approach called network dissection. They run a large number of images through a CNN and thenanalyze the top activated images by considering each unit as a concept detector to further evaluate each25unit for semantic segmentation. This paper also examines the effects of classical training techniques onthe interpretability of the learned model.Although many of the techniques examined above utilize local explanations to achieve an overallexplanation of a CNN model, others explicitly focus on building global explanations based on locally foundprototypes. In [263, 294], the authors empirically showed how local explanations in deep networks arestrongly dominated by their lower level features. They demonstrated that deep architectures provide strongpriors that prevent the altering of how these low-level representations are captured. All in all, visualizationmixed with feature relevance methods are arguably the most adopted approach to explainability in CNNs.Instead of using one single interpretability technique, the framework proposed in [295] combinesseveral methods to provide much more information about the network. For example, combining featurevisualization (what is a neuron looking for?) with attribution (how does it affect the output?) allowsexploring how the network decides between labels. This visual interpretability interface displays differentblocks such as feature visualization and attribution depending on the visualization goal. This interfacecan be thought of as a union of individual elements that belong to layers (input, hidden, output), atoms (aneuron, channel, spatial or neuron group), content (activations \u2013 the amount a neuron \ufb01res, attribution \u2013which classes a spatial position most contributes to, which tends to be more meaningful in later layers), andpresentation (information visualization, feature visualization). Figure 8 shows some examples. Attributionmethods normally rely on pixel association, displaying what part of an input example is responsible forthe network activating in a particular way [293].(a) Neuron (b) Channel (c) Layer(a) Original image (b) Explaining electric guitar (c) Explaining acoustic guitarA much simpler approach to all the previously cited methods was proposed in LIME framework[71], as was described in Subsection 4.1 LIME perturbs the input and sees how the predictions change.In image classi\ufb01cation, LIME creates a set of perturbed instances by dividing the input image intointerpretable components (contiguous superpixels), and runs each perturbed instance through the model26to get a probability. A simple linear model learns on this data set, which is locally weighted. At the end ofthe process, LIME presents the superpixels with highest positive weights as an explanation (see Figure 9).A completely different explainability approach is proposed in adversarial detection. To understandmodel failures in detecting adversarial examples, the authors in [264] apply the k-nearest neighborsalgorithm on the representations of the data learned by each layer of the CNN. A test input image isconsidered as adversarial if its representations are far from the representations of the training images.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "3119a662-49aa-46ed-aa9b-3806f5b4ac82",
                    "text": "As occurs with CNNs in the visual domain, RNNs have lately been used extensively for predictiveproblems de\ufb01ned over inherently sequential data, with a notable presence in natural language processingand time series analysis. These types of data exhibit long-term dependencies that are complex to becaptured by a ML model. RNNs are able to retrieve such time-dependent relationships by formulating theretention of knowledge in the neuron as another parametric characteristic that can be learned from data.Few contributions have been made for explaining RNN models. These studies can be divided into twogroups: 1) explainability by understanding what a RNN model has learned (mainly via feature relevancemethods); and 2) explainability by modifying RNN architectures to provide insights about the decisionsthey make (local explanations).In the \ufb01rst group, the authors in [280] extend the usage of LRP to RNNs. They propose a speci\ufb01cpropagation rule that works with multiplicative connections as those in LSTMs (Long Short Term Memory)units and GRUs (Gated Recurrent Units). The authors in [281] propose a visualization technique based on\ufb01nite horizon n-grams that discriminates interpretable cells within LSTM and GRU networks. Followingthe premise of not altering the architecture, [296] extends the interpretable mimic learning distillationmethod used for CNN models to LSTM networks, so that interpretable features are learned by \ufb01ttingGradient Boosting Trees to the trained LSTM network under focus.Aside from the approaches that do not change the inner workings of the RNNs, [285] presents RETAIN(REverse Time AttentIoN) model, which detects in\ufb02uential past patterns by means of a two-level neuralattention model. To create an interpretable RNN, the authors in [283] propose an RNN based on SISTA(Sequential Iterative Soft-Thresholding Algorithm) that models a sequence of correlated observationswith a sequence of sparse latent vectors, making its weights interpretable as the parameters of a principledstatistical model. Finally, [284] constructs a combination of an HMM (Hidden Markov Model) and anRNN, so that the overall model approach harnesses the interpretability of the HMM and the accuracy ofthe RNN model.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "2d054d34-0d54-4467-9764-beed966c426f",
                    "text": "The use of background knowledge in the form of logical statements or constraints in KnowledgeBases (KBs) has shown to not only improve explainability but also performance with respect to purelydata-driven approaches [297, 298, 299]. A positive side effect shown is that this hybrid approach providesrobustness to the learning system when errors are present in the training data labels. Other approacheshave shown to be able to jointly learn and reason with both symbolic and sub-symbolic representationsand inference. The interesting aspect is that this blend allows for expressive probabilistic-logical reasoningin an end-to-end fashion [300]. A successful use case is on dietary recommendations, where explanationsare extracted from the reasoning behind (non-deep but KB-based) models [301].Future data fusion approaches may thus consider endowing DL models with explainability by external-izing other domain information sources. Deep formulation of classical ML models has been done, e.g. inDeep Kalman \ufb01lters (DKFs) [302], Deep Variational Bayes Filters (DVBFs) [303], Structural VariationalAutoencoders (SVAE) [304], or conditional random \ufb01elds as RNNs [305]. These approaches providedeep models with the interpretability inherent to probabilistic graphical models. For instance, SVAEcombines probabilistic graphical models in the embedding space with neural networks to enhance theinterpretability of DKFs. A particular example of classical ML model enhanced with its DL counterpart is27Deep Nearest Neighbors DkNN [264], where the neighbors constitute human-interpretable explanationsof predictions. The intuition is based on the rationalization of a DNN prediction based on evidence.This evidence consists of a characterization of con\ufb01dence termed credibility that spans the hierarchy ofrepresentations within a DNN, that must be supported by the training data [264].A different perspective on hybrid XAI models consists of enriching black-box models knowledgewith that one of transparent ones, as proposed in [24] and further re\ufb01ned in [169] and [307]. In particular,this can be done by constraining the neural network thanks to a semantic KB and bias-prone concepts[169], or by stacking ensembles jointly encompassing white- and black-box models [307].Other examples of hybrid symbolic and sub-symbolic methods where a knowledge-base tool orgraph-perspective enhances the neural (e.g., language [308]) model are in [309, 310]. In reinforcementlearning, very few examples of symbolic (graphical [311] or relational [75, 312]) hybrid models exist,while in recommendation systems, for instance, explainable autoencoders are proposed [313]. A speci\ufb01ctransformer architecture symbolic visualization method (applied to music) pictorially shows how soft-maxattention works [314]. By visualizing self-reference, i.e., the last layer of attention weights, arcs showwhich notes in the past are informing the future and how attention is skip over less relevant sections.Transformers can also help explain image captions visually [315].Another hybrid approach consists of mapping an uninterpretable black-box system to a white-box twinthat is more interpretable. For example, an opaque neural network can be combined with a transparentCase Based Reasoning (CBR) system [316, 317]. In [318], the DNN and the CBR (in this case a kNN) arepaired in order to improve interpretability while keeping the same accuracy. The explanation by exampleconsists of analyzing the feature weights of the DNN which are then used in the CBR, in order to retrievenearest-neighbor cases to explain the DNNs prediction.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "6cf665d1-cb09-42f2-b985-562ffe87c60a",
                    "text": "DL is the model family where most research has been concentrated in recent times and they havebecome central for most of the recent literature on XAI. While the division between model-agnosticand model-speci\ufb01c is the most common distinction made, the community has not only relied on thiscriteria to classify XAI methods. For instance, some model-agnostic methods such as SHAP [224] arewidely used to explain DL models. That is why several XAI methods can be easily categorized indifferent taxonomy branches depending on the angle the method is looked at. An example is LIMEwhich can also be used over CNNs, despite not being exclusive to deal with images. Searching withinthe alternative DL taxonomy shows us that LIME can explicitly be used for Explaining a Deep NetworkProcessing, as a kind of Linear Proxy Model. Another type of classi\ufb01cation is indeed proposed in [13]with a segmentation based on 3 categories. The \ufb01rst category groups methods explaining the processing ofdata by the network, thus answering to the question \u201cwhy does this particular input leads to this particularoutput?\u201d. The second one concerns methods explaining the representation of data inside the network, i.e.,answering to the question \u201cwhat information does the network contain?\u201d. The third approach concerns28models speci\ufb01cally designed to simplify the interpretation of their own behavior. Such a multiplicity ofclassi\ufb01cation possibilities leads to different ways of constructing XAI taxonomies.(a) (b)Figure 11 shows the alternative Deep Learning taxonomy inferred from [13]. From the latter, it can bededuced the complementarity and overlapping of this taxonomy to Figure 6 as:\u2022 Some methods [272, 280] classi\ufb01ed in distinct categories (namely feature relevance for CNN andfeature relevance for RNN) in Figure 6 are included in a single category (Explanation of Deep NetworkProcessing with Salience Mapping) when considering the classi\ufb01cation from [13].\u2022 Some methods [82, 144] are classi\ufb01ed on a single category (Explanation by simpli\ufb01cation for Multi-Layer Neural Network) in Figure 6 while being in 2 different categories (namely, Explanation of DeepNetwork Processing with Decision Trees and Explanation of Deep Network Representation with theRole of Representation Vectors) in [13], as shown in Figure 11.A classi\ufb01cation based on explanations of model processing and explanations of model representationis relevant, as it leads to a differentiation between the execution trace of the model and its internal datastructure. This means that depending of the failure reasons of a complex model, it would be possible topick-up the right XAI method according to the information needed: the execution trace or the data structure.This idea is analogous to testing and debugging methods used in regular programming paradigms [346].",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "78b309f2-3a32-48a3-aac6-538c303eb93c",
                    "text": "We now capitalize on the performed literature review to put forward a critique of the achievements,trends and challenges that are still to be addressed in the \ufb01eld of explainability of ML and data fusionmodels. Actually our discussion on the advances taken so far in this \ufb01eld has already anticipated someof these challenges. In this section we revisit them and explore new research opportunities for XAI,identifying possible research paths that can be followed to address them effectively in years to come:29\u2022 When introducing the overview in Section 1 we already mentioned the existence of a tradeoff betweenmodel interpretability and performance, in the sense that making a ML model more understandablecould eventually degrade the quality of its produced decisions. In Subsection 5.1 we will stress on thepotential of XAI developments to effectively achieve an optimal balance between the interpretabilityand performance of ML models.\u2022 In Subsection 2.2 we stressed on the imperative need for reaching a consensus on what explainabilityentails within the AI realm. Reasons for pursuing explainability are also assorted and, under ourown assessment of the literature so far, not unambiguously mentioned throughout related works. InSubsection 5.2 we will further delve into this important issue.\u2022 Given its notable prevalence in the XAI literature, Subsections 4.3 and 4.4 revolved on the explainabilityof Deep Learning models, examining advances reported so far around a speci\ufb01c bibliographic taxonomy.We go in this same direction with Subsection 5.3, which exposes several challenges that hold in regardsto the explainability of this family of models.\u2022 Finally, we close up this prospective discussion with Subsections 5.4 to 5.8, which place on the tableseveral research niches that despite its connection to model explainability, remain insuf\ufb01ciently studiedby the community.Before delving into these identi\ufb01ed challenges, it is important to bear in mind that this prospectivesection is complemented by Section 6, which enumerates research needs and open questions related toXAI within a broader context: the need for responsible AI.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "db45a836-4e58-4b35-8c30-c2791906ada1",
                    "text": "The matter of interpretability versus performance is one that repeats itself through time, but as anyother big statement, has its surroundings \ufb01lled with myths and misconceptions.As perfectly stated in [347], it is not necessarily true that models that are more complex are inherentlymore accurate. This statement is false in cases in which the data is well structured and features at ourdisposal are of great quality and value. This case is somewhat common in some industry environments,since features being analyzed are constrained within very controlled physical problems, in which all ofthe features are highly correlated, and not much of the possible landscape of values can be explored inthe data [348]. What can be hold as true, is that more complex models enjoy much more \ufb02exibility thantheir simpler counterparts, allowing for more complex functions to be approximated. Now, returning tothe statement \u201cmodels that are more complex are more accurate\u201d, given the premise that the functionto be approximated entails certain complexity, that the data available for study is greatly widespreadamong the world of suitable values for each variable and that there is enough data to harness a complexmodel, the statement presents itself as a true statement. It is in this situation that the trade-off betweenperformance and interpretability can be observed. It should be noted that the attempt at solving problemsthat do not respect the aforementioned premises will fall on the trap of attempting to solve a problem thatdoes not provide enough data diversity (variance). Hence, the added complexity of the model will only\ufb01ght against the task of accurately solving the problem.In this path toward performance, when the performance comes hand in hand with complexity, in-terpretability encounters itself on a downwards slope that until now appeared unavoidable. However,the apparition of more sophisticated methods for explainability could invert or at least cancel that slope.Figure 12 shows a tentative representation inspired by previous works [7], in which XAI shows itspower to improve the common trade-off between model interpretability and performance. Another aspectworth mentioning at this point due to its close link to model interpretability and performance is theapproximation dilemma: explanations made for a ML model must be made drastic and approximateenough to match the requirements of the audience for which they are sought, ensuring that explanationsare representative of the studied model and do not oversimplify its essential features.30",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "22c8bdab-b01c-46c1-88e8-e634ba64db1b",
                    "text": "The literature clearly asks for an uni\ufb01ed concept of explainability. In order for the \ufb01eld to thrive,it is imperative to place a common ground upon which the community is enabled to contribute newtechniques and methods. A common concept must convey the needs expressed in the \ufb01eld. It shouldpropose a common structure for every XAI system. This paper attempted a new proposition of a conceptof explainability that is built upon that from Gunning [7]. In that proposition and the following strokes tocomplete it (Subsection 2.2), explainability is de\ufb01ned as the ability a model has to make its functioningclearer to an audience. To address it, post-hoc type methods exist. The concept portrayed in this surveymight not be complete but as it stands, allows for a \ufb01rst common ground and reference point to sustaina pro\ufb01table discussion in this matter. It is paramount that the \ufb01eld of XAI reaches an agreement in thisrespect combining the shattered efforts of a widespread \ufb01eld behind the same banner.Another key feature needed to relate a certain model to this concrete concept is the existence of ametric. A metric, or group of them should allow for a meaningful comparison of how well a model \ufb01tsthe de\ufb01nition of explainable. Without such tool, any claim in this respect dilutes among the literature, notproviding a solid ground on which to stand. These metrics, as the classic ones (accuracy, F1, sensitivity...),should express how well the model performs in a certain aspect of explainability. Some attempts have beendone recently around the measurement of XAI, as reviewed thoroughly in [349, 350]. In general, XAImeasurements should evaluate the goodness, usefulness and satisfaction of explanations, the improvementof the mental model of the audience induced by model explanations, and the impact of explanations on theperformance of the model and on the trust and reliance of the audience. Measurement techniques surveyedin [349] and [350] (e.g., goodness checklist, explanation satisfaction scale, elicitation methods for mentalmodels, computational measures for explainer \ufb01delity, explanation trustworthiness and model reliability)seem to be a good push in the direction of evaluating XAI techniques. Unfortunately, conclusions drawnfrom these overviews are aligned with our prospects on the \ufb01eld: more quanti\ufb01able, general XAI metricsare really needed to support the existing measurement procedures and tools proposed by the community.This survey does not tackle the problem of designing such a suite of metrics, since such a task shouldbe approached by the community as a whole prior acceptance of the broader concept of explainabil-ity, which on the other hand, is one of the aims of the current work. Nevertheless, we advocate forfurther efforts towards new proposals to evaluate the performance of XAI techniques, as well as compar-ison methodologies among XAI approaches that allow contrasting them quantitatively under different31application context, models and purposes.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "81a0bb5f-0c46-492e-bc1c-51e1fab43183",
                    "text": "While many efforts are currently being made in the area of XAI, there are still many challenges to befaced before being able to obtain explainability in DL models. First, as explained in Subsection 2.2, thereis a lack of agreement on the vocabulary and the different de\ufb01nitions surrounding XAI. As an example,we often see the terms feature importance and feature relevance referring to the same concept. This iseven more obvious for visualization methods, where there is absolutely no consistency behind what isknown as saliency maps, salient masks, heatmaps, neuron activations, attribution, and other approachesalike. As XAI is a relatively young \ufb01eld, the community does not have a standardized terminology yet.As it has been commented in Subsection 5.1, there is a trade-off between interpretability and accuracy[13], i.e., between the simplicity of the information given by the system on its internal functioning, andthe exhaustiveness of this description. Whether the observer is an expert in the \ufb01eld, a policy-maker or auser without machine learning knowledge, intelligibility does not have to be at the same level in orderto provide the audience an understanding [6]. This is one of the reasons why, as mentioned above, achallenge in XAI is establishing objective metrics on what constitutes a good explanation. A possibilityto reduce this subjectivity is taking inspiration from experiments on human psychology, sociology orcognitive sciences to create objectively convincing explanations. Relevant \ufb01ndings to be considered whencreating an explainable AI model are highlighted in [12]: First, explanations are better when constrictive,meaning that a prerequisite for a good explanation is that it does not only indicate why the model made adecision X, but also why it made decision X rather than decision Y. It is also explained that probabilitiesare not as important as causal links in order to provide a satisfying explanation. Considering that black boxmodels tend to process data in a quantitative manner, it would be necessary to translate the probabilisticresults into qualitative notions containing causal links. In addition, they state that explanations areselective, meaning that focusing solely on the main causes of a decision-making process is suf\ufb01cient. Itwas also shown that the use of counterfactual explanations can help the user to understand the decision ofa model [40, 42, 351].Combining connectionist and symbolic paradigms seems a favourable way to address this challenge[169, 299, 312, 352, 353]. On one hand, connectionist methods are more precise but opaque. On the otherhand, symbolic methods are popularly considered less ef\ufb01cient, while they offer a greater explainabilitythus respecting the conditions mentioned above:\u2022 The ability to refer to established reasoning rules allows symbolic methods to be constrictive.\u2022 The use of a KB formalized e.g. by an ontology can allow data to be processed directly in a qualitativeway.\u2022 Being selective is less straightforward for connectionist models than for symbolic ones.Recalling that a good explanation needs to in\ufb02uence the mental model of the user, i.e. the repre-sentation of the external reality using, among other things, symbols, it seems obvious that the use ofthe symbolic learning paradigm is appropriate to produce an explanation. Therefore, neural-symbolicinterpretability could provide convincing explanations while keeping or improving generic performance[297].As stated in [24], a truly explainable model should not leave explanation generation to the users asdifferent explanations may be deduced depending on their background knowledge. Having a semanticrepresentation of the knowledge can help a model to have the ability to produce explanations (e.g., innatural language [169]) combining common sense reasoning and human-understandable features.Furthermore, until an objective metric has been adopted, it appears necessary to make an effort torigorously formalize evaluation methods. One way may be drawing inspiration from the social sciences,e.g., by being consistent when choosing the evaluation questions and the population sample used [354].32A \ufb01nal challenge XAI methods for DL need to address is providing explanations that are accessiblefor society, policy makers and the law as a whole. In particular, conveying explanations that requirenon-technical expertise will be paramount to both handle ambiguities, and to develop the social right tothe (not-yet available) right for explanation in the EU General Data Protection Regulation (GDPR) [355].",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "9285cb32-5a74-40ad-a2aa-abe269ebb9da",
                    "text": "Nothing has been said about con\ufb01dentiality concerns linked to XAI. One of the last surveys verybrie\ufb02y introduced the idea of algorithm property and trade secrets [14]. However, not much attentionhas been payed to these concepts. If con\ufb01dential is the property that makes something secret, in theAI context many aspects involved in a model may hold this property. For example, imagine a modelthat some company has developed through many years of research in a speci\ufb01c \ufb01eld. The knowledgesynthesized in the model built might be considered to be con\ufb01dential, and it may be compromised evenby providing only input and output access [356]. The latter shows that, under minimal assumptions,data model functionality stealing is possible. An approach that has served to make DL models morerobust against intellectual property exposure based on a sequence of non accessible queries is in [357].This recent work exposes the need for further research toward the development of XAI tools capable ofexplaining ML models while keeping the model\u2019s con\ufb01dentiality in mind.Ideally, XAI should be able to explain the knowledge within an AI model and it should be able toreason about what the model acts upon. However, the information revealed by XAI techniques can be usedboth to generate more effective attacks in adversarial contexts aimed at confusing the model, at the sametime as to develop techniques to better protect against private content exposure by using such information.Adversarial attacks [358] try to manipulate a ML algorithm after learning what is the speci\ufb01c informationthat should be fed to the system so as to lead it to a speci\ufb01c output. For instance, regarding a supervisedML classi\ufb01cation model, adversarial attacks try to discover the minimum changes that should be appliedto the input data in order to cause a different classi\ufb01cation. This has happened regarding computer visionsystems of autonomous vehicles; a minimal change in a stop signal, imperceptible to the human eye, ledvehicles to detect it as a 45 mph signal [359]. For the particular case of DL models, available solutionssuch as Cleverhans [360] seek to detect adversarial vulnerabilities, and provide different approachesto harden the model against them. Other examples include AlfaSVMLib [361] for SVM models, andAdversarialLib [362] for evasion attacks. There are even available solutions for unsupervised ML, likeclustering algorithms [363].While XAI techniques can be used to furnish more effective adversarial attacks or to reveal con\ufb01dentialaspects of the model itself, some recent contributions have capitalized on the possibilities of GenerativeAdversarial Networks (GANs [364]), Variational Autoencoders [365] and other generative models towardsexplaining data-based decisions. Once trained, generative models can generate instances of what theyhave learned based on a noise input vector that can be interpreted as a latent representation of the data athand. By manipulating this latent representation and examining its impact on the output of the generativemodel, it is possible to draw insights and discover speci\ufb01c patterns related to the class to be predicted.This generative framework has been adopted by several recent studies [366, 367] mainly as an attributionmethod to relate a particular output of a Deep Learning model to their input variables. Another interestingresearch direction is the use of generative models for the creation of counterfactuals, i.e., modi\ufb01cationsto the input data that could eventually alter the original prediction of the model [368]. Counterfactualprototypes help the user understand the performance boundaries of the model under consideration forhis/her improved trust and informed criticism. In light of this recent trend, we de\ufb01nitely believe that thereis road ahead for generative ML models to take their part in scenarios demanding understandable machinedecisions.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "23b70f17-8eec-4ea0-8b72-590139d938a1",
                    "text": "Safety issues have also been studied in regards to processes that depend on the output of AI models,such as vehicular perception and self-driving in autonomous vehicles, automated surgery, data-based33support for medical diagnosis, insurance risk assessment and cyber-physical systems in manufacturing,among others [369]. In all these scenarios erroneous model outputs can lead to harmful consequences,which has yielded comprehensive regulatory efforts aimed at ensuring that no decision is made solely onthe basis of data processing [3].In parallel, research has been conducted towards minimizing both risk and uncertainty of harmsderived from decisions made on the output of a ML model. As a result, many techniques have beenreported to reduce such a risk, among which we pause at the evaluation of the model\u2019s output con\ufb01denceto decide upon. In this case, the inspection of the share of epistemic uncertainty (namely, the uncertaintydue to lack of knowledge) of the input data and its correspondence with the model\u2019s output con\ufb01dencecan inform the user and eventually trigger his/her rejection of the model\u2019s output [370, 371]. To this end,explaining via XAI techniques which region of the input data the model is focused on when producing agiven output can discriminate possible sources of epistemic uncertainty within the input domain.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "cb7d07db-af53-4884-a0fb-dc339bf4708c",
                    "text": "When shifting the focus to the research practices seen in Data Science, it has been noted thatreproducibility is stringently subject not only to the mere sharing of data, models and results to thecommunity, but also to the availability of information about the full discourse around data collection,understanding, assumptions held and insights drawn from model construction and results\u2019 analyses [372].In other words, in order to transform data into a valuable actionable asset, individuals must engage incollaborative sense-making by sharing the context producing their \ufb01ndings, wherein context refers to setsof narrative stories around how data were processed, cleaned, modeled and analyzed. In this discoursewe \ufb01nd also an interesting space for the adoption of XAI techniques due to their powerful ability todescribe black-box models in an understandable, hence conveyable fashion towards colleagues fromSocial Science, Politics, Humanities and Legal \ufb01elds.XAI can effectively ease the process of explaining the reasons why a model reached a decision in anaccessible way to non-expert users, i.e. the rationale explanation. This con\ufb02uence of multi-disciplinaryteams in projects related to Data Science and the search for methodologies to make them appraise theethical implications of their data-based choices has been lately coined as Critical Data studies [373]. Itis in this \ufb01eld where XAI can signi\ufb01cantly boost the exchange of information among heterogeneousaudiences about the knowledge learned by models.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "baa868cd-19e2-43fd-bbd5-282625b7abae",
                    "text": "We envision an exciting synergy between the XAI realm and Theory-guided Data Science, a paradigmexposed in [374] that merges both Data Science and the classic theoretical principles underlying theapplication/context where data are produced. The rationale behind this rising paradigm is the need for data-based models to generate knowledge that is the prior knowledge brought by the \ufb01eld in which it operates.This means that the model type should be chosen according to the type of relations we intend to encounter.The structure should also follow what is previously known. Similarly, the training approach should notallow for the optimization process to enter regions that are not plausible. Accordingly, regularizationterms should stand the prior premises of the \ufb01eld, avoiding the elimination of badly represented truerelations for spurious and deceptive false relations. Finally, the output of the model should inform abouteverything the model has come to learn, allowing to reason and merge the new knowledge with what wasalready known in the \ufb01eld.Many examples of the implementation of this approach are currently available with promising results.The studies in [375]-[382] were carried out in diverse \ufb01elds, showcasing the potential of this new paradigmfor data science. Above all, it is relevant to notice the resemblance that all concepts and requirements ofTheory-guided Data Science share with XAI. All the additions presented in [374] push toward techniquesthat would eventually render a model explainable, and furthermore, knowledge consistent. The conceptof knowledge from the beginning, central to Theory-guided Data Science, must also consider how34the knowledge captured by a model should be explained for assessing its compliance with theoreticalprinciples known beforehand. This, again, opens a magni\ufb01cent window of opportunity for XAI.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "2ef7a7ed-33de-44e1-9ec9-18fcd288db40",
                    "text": "Recent surveys have emphasized on the multidisciplinary, inclusive nature of the process of makingan AI-based model interpretable. Along this process, it is of utmost importance to scrutinize and take intoproper account the interests, demands and requirements of all stakeholders interacting with the system tobe explained, from the designers of the system to the decision makers consuming its produced outputsand users undergoing the consequences of decisions made therefrom.Given the con\ufb02uence of multiple criteria and the need for having the human in the loop, someattempts at establishing the procedural guidelines to implement and explain AI systems have been recentlycontributed. Among them, we pause at the thorough study in [383], which suggests that the incorporationand consideration of explainability in practical AI design and deployment work\ufb02ows should comprisefour major methodological steps:1. Contextual factors, potential impacts and domain-speci\ufb01c needs must be taken into account whendevising an approach to interpretability: These include a thorough understanding of the purpose forwhich the AI model is built, the complexity of explanations that are required by the audience, and theperformance and interpretability levels of existing technology, models and methods. The latter pose areference point for the AI system to be deployed in lieu thereof.2. Interpretable techniques should be preferred when possible: when considering explainability in thedevelopment of an AI system, the decision of which XAI approach should be chosen should gaugedomain-speci\ufb01c risks and needs, the available data resources and existing domain knowledge, and thesuitability of the ML model to meet the requirements of the computational task to be addressed. It is inthe con\ufb02uence of these three design drivers where the guidelines postulated in [383] (and other studiesin this same line of thinking [384]) recommend \ufb01rst the consideration of standard interpretable modelsrather than sophisticated yet opaque modeling methods. In practice, the aforementioned aspects(contextual factors, impacts and domain-speci\ufb01c needs) can make transparent models preferableover complex modeling alternatives whose interpretability require the application of post-hoc XAItechniques. By contrast, black-box models such as those reviewed in this work (namely, supportvector machines, ensemble methods and neural networks) should be selected only when their superiormodeling capabilities \ufb01t best the characteristics of the problem at hand.3. If a black-box model has been chosen, the third guideline establishes that ethics-, fairness- and safety-related impacts should be weighed. Speci\ufb01cally, responsibility in the design and implementation ofthe AI system should be ensured by checking whether such identi\ufb01ed impacts can be mitigated andcounteracted by supplementing the system with XAI tools that provide the level of explainabilityrequired by the domain in which it is deployed. To this end, the third guideline suggests 1) a detailedarticulation, examination and evaluation of the applicable explanatory strategies, 2) the analysis ofwhether the coverage and scope of the available explanatory approaches match the requirements ofthe domain and application context where the model is to be deployed; and 3) the formulation ofan interpretability action plan that sets forth the explanation delivery strategy, including a detailedtime frame for the execution of the plan, and a clearance of the roles and responsibilities of the teaminvolved in the work\ufb02ow.4. Finally, the fourth guideline encourages to rethink interpretability in terms of the cognitive skills,capacities and limitations of the individual human. This is an important question on which studieson measures of explainability are intensively revolving by considering human mental models, theaccessibility of the audience to vocabularies of explanatory outcomes, and other means to involve theexpertise of the audience into the decision of what explanations should provide.35We foresee that the set of guidelines proposed in [383] and summarized above will be complementedand enriched further by future methodological studies, ultimately heading to a more responsible use of AI.Methodological principles ensure that the purpose for which explainability is pursued is met by bringingthe manifold of requirements of all participants into the process, along with other universal aspects ofequal relevance such as no discrimination, sustainability, privacy or accountability. A challenge remainsin harnessing the potential of XAI to realize a Responsible AI, as we discuss in the next section.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "424af93c-8066-4880-bf16-458b3397c7b4",
                    "text": "Over the years many organizations, both private and public, have published guidelines to indicatehow AI should be developed and used. These guidelines are commonly referred to as AI principles, andthey tackle issues related to potential AI threats to both individuals and to the society as a whole. Thissection presents some of the most important and widely recognized principles in order to link XAI \u2013which normally appears inside its own principle \u2013 to all of them. Should a responsible implementationand use of AI models be sought in practice, it is our \ufb01rm claim that XAI does not suf\ufb01ce on its own. Otherimportant principles of Arti\ufb01cial Intelligence such as privacy and fairness must be carefully addressedin practice. In the following sections we elaborate on the concept of Responsible AI, along with theimplications of XAI and data fusion in the ful\ufb01llment of its postulated principles.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "f11d3353-c891-48cc-80cb-dfa01e39ec7a",
                    "text": "A recent review of some of the main AI principles published since 2016 appears in [385]. In thiswork, the authors show a visual framework where different organizations are classi\ufb01ed according to thefollowing parameters:\u2022 Nature, which could be private sector, government, inter-governmental organization, civil society ormultistakeholder.\u2022 Content of the principles: eight possible principles such as privacy, explainability, or fairness. Theyalso consider the coverage that the document grants for each of the considered principles.\u2022 Target audience: to whom the principles are aimed. They are normally for the organization thatdeveloped them, but they could also be destined for another audience (see Figure 2).\u2022 Whether or not they are rooted on the International Human Rights, as well as whether they explicitlytalk about them.For instance, [386] is an illustrative example of a document of AI principles for the purpose ofthis overview, since it accounts for some of the most common principles, and deals explicitly withexplainability. Here, the authors propose \ufb01ve principles mainly to guide the development of AI withintheir company, while also indicating that they could also be used within other organizations and businesses.The authors of those principles aim to develop AI in a way that it directly reinforces inclusion, givesequal opportunities for everyone, and contributes to the common good. To this end, the following aspectsshould be considered:\u2022 The outputs after using AI systems should not lead to any kind of discrimination against individualsor collectives in relation to race, religion, gender, sexual orientation, disability, ethnic, origin or anyother personal condition. Thus, a fundamental criteria to consider while optimizing the results of an AIsystem is not only their outputs in terms of error optimization, but also how the system deals with thosegroups. This de\ufb01nes the principle of Fair AI. 36\u2022 People should always know when they are communicating with a person, and when they are commu-nicating with an AI system. People should also be aware if their personal information is being usedby the AI system and for what purpose. It is crucial to ensure a certain level of understanding aboutthe decisions taken by an AI system. This can be achieved through the usage of XAI techniques. Itis important that the generated explanations consider the pro\ufb01le of the user that will receive thoseexplanations (the so-called audience as per the de\ufb01nition given in Subsection 2.2) in order to adjust thetransparency level, as indicated in [45]. This de\ufb01nes the principle of Transparent and Explainable AI.\u2022 AI products and services should always be aligned with the United Nation\u2019s Sustainable DevelopmentGoals [387] and contribute to them in a positive and tangible way. Thus, AI should always generatea bene\ufb01t for humanity and the common good. This de\ufb01nes the principle of Human-centric AI (alsoreferred to as AI for Social Good [388]).\u2022 AI systems, specially when they are fed by data, should always consider privacy and security standardsduring all of its life cycle. This principle is not exclusive of AI systems since it is shared with manyother software products. Thus, it can be inherited from processes that already exist within a company.This de\ufb01nes the principle of Privacy and Security by Design, which was also identi\ufb01ed as one ofthe core ethical and societal challenges faced by Smart Information Systems under the ResponsibleResearch and Innovation paradigm (RRI, [389]). RRI refers to a package of methodological guidelinesand recommendations aimed at considering a wider context for scienti\ufb01c research, from the perspectiveof the lab to global societal challenges such as sustainability, public engagement, ethics, scienceeducation, gender equality, open access, and governance. Interestingly, RRI also requires openness andtransparency to be ensured in projects embracing its principles, which links directly to the principle ofTransparent and Explainable AI mentioned previously.\u2022 The authors emphasize that all these principles should always be extended to any third-party (providers,consultants, partners...).Going beyond the scope of these \ufb01ve AI principles, the European Commission (EC) has recentlypublished ethical guidelines for Trustworthy AI [390] through an assessment checklist that can becompleted by different pro\ufb01les related to AI systems (namely, product managers, developers and otherroles). The assessment is based in a series of principles: 1) human agency and oversight; 2) technicalrobustness and safety; 3) privacy and data governance; 4) transparency, diversity, non-discrimination andfairness; 5) societal and environmental well-being; 6) accountability. These principles are aligned withthe ones detailed in this section, though the scope for the EC principles is more general, including anytype of organization involved in the development of AI.It is worth mentioning that most of these AI principles guides directly approach XAI as a key aspectto consider and include in AI systems. In fact, the overview for these principles introduced before [385],indicates that 28 out of the 32 AI principles guides covered in the analysis, explicitly include XAI as acrucial component. Thus, the work and scope of this article deals directly with one of the most importantaspects regarding AI at a worldwide level.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "ce3a01b7-f6a0-4a6c-860d-f52b4d56dd09",
                    "text": "As mentioned in the previous section, there are many critical aspects, beyond XAI, included withinthe different AI principles guidelines published during the last decade. However, those aspects are notcompletely detached from XAI; in fact, they are intertwined. This section presents two key componentswith a huge relevance within the AI principles guides, Fairness and Accountability. It also highlights howthey are connected to XAI. 37",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "1575eb53-1f7e-416a-a71a-36ff65a23878",
                    "text": "It is in the identi\ufb01cation of implicit correlations between protected and unprotected features whereXAI techniques \ufb01nd their place within discrimination-aware data mining methods. By analyzing howthe output of the model behaves with respect to the input feature, the model designer may unveil hiddencorrelations between the input variables amenable to cause discrimination. XAI techniques such as SHAP[224] could be used to generate counterfactual outcomes explaining the decisions of a ML model whenfed with protected and unprotected variables.Recalling the Fair AI principle introduced in the previous section, [386] reminds that fairness is adiscipline that generally includes proposals for bias detection within datasets regarding sensitive data thataffect protected groups (through variables like gender, race...). Indeed, ethical concerns with black-boxmodels arise from their tendency to unintentionally create unfair decisions by considering sensitive factorssuch as the individual\u2019s race, age or gender [391]. Unfortunately, such unfair decisions can give rise todiscriminatory issues, either by explicitly considering sensitive attributes or implicitly by using factorsthat correlate with sensitive data. In fact, an attribute may implicitly encode a protected factor, as occurswith postal code in credit rating [392]. The aforementioned proposals centered on fairness aspects permitto discover correlations between non-sensitive variables and sensitive ones, detect imbalanced outcomesfrom the algorithms that penalize a speci\ufb01c subgroup of people (discrimination), and mitigate the effectof bias on the model\u2019s decisions. These approaches can deal with:\u2022 Individual fairness: here, fairness is analyzed by modeling the differences between each subject and therest of the population.\u2022 Group fairness: it deals with fairness from the perspective of all individuals.\u2022 Counterfactual fairness: it tries to interpret the causes of bias using, for example, causal graphs.The sources for bias, as indicated in [392], can be traced to:\u2022 Skewed data: bias within the data acquisition process.\u2022 Tainted data: errors in the data modelling de\ufb01nition, wrong feature labelling, and other possible causes.\u2022 Limited features: using too few features could lead to an inference of false feature relationships thatcan lead to bias.\u2022 Sample size disparities: when using sensitive features, disparities between different subgroups caninduce bias.\u2022 Proxy features: there may be correlated features with sensitive ones that can induce bias even when thesensitive features are not present in the dataset.The next question that can be asked is what criteria could be used to de\ufb01ne when AI is not biased. Forsupervised ML, [393] presents a framework that uses three criteria to evaluate group fairness when thereis a sensitive feature present within the dataset:\u2022 Independence: this criterion is ful\ufb01lled when the model predictions are independent of the sensitivefeature. Thus, the proportion of positive samples (namely, those ones belonging to the class of interest)given by the model is the same for all the subgroups within the sensitive feature.\u2022 Separation: it is met when the model predictions are independent of the sensitive feature given thetarget variable. For instance, in classi\ufb01cation models, the True Positive (TP) rate and the False Positive(FP) rate are the same in all the subgroups within the sensitive feature. This criteria is also known asEqualized Odds. 38\u2022 Suf\ufb01ciency: it is accomplished when the target variable is independent of the sensitive feature giventhe model output. Thus, the Positive Predictive Value is the same for all subgroups within the sensitivefeature. This criteria is also known as Predictive Rate Parity.Although not all of the criteria can be ful\ufb01lled at the same time, they can be optimized together inorder to minimize the bias within the ML model.There are two possible actions that could be used in order to achieve those criteria. On one hand,evaluation includes measuring the amount of bias present within the model (regarding one of the criteriaaforementioned). There are many different metrics that can be used, depending on the criteria considered.Regarding independence criterion, possible metrics are statistical parity difference or disparate impact.In case of the separation criterion, possible metrics are equal opportunity difference and average oddsdifference [393]. Another possible metric is the Theil index [394], which measures inequality both interms of individual and group fairness.On the other hand, mitigation refers to the process of \ufb01xing some aspects in the model in order toremove the effect of the bias in terms of one or several sensitive features. Several techniques exist withinthe literature, classi\ufb01ed in the following categories:\u2022 Pre-processing: these groups of techniques are applied before the ML model is trained, looking toremove the bias at the \ufb01rst step of the learning process. An example is Reweighing [395], whichmodi\ufb01es the weights of the features in order to remove discrimination in sensitive attributes. Anotherexample is [396], which hinges on transforming the input data in order to \ufb01nd a good representationthat obfuscates information about membership in sensitive features.\u2022 In-processing: these techniques are applied during the training process of the ML model. Normally,they include Fairness optimization constraints along with cost functions of the ML model. An exampleis Adversarial Debiasing, [397]. This technique optimizes jointly the ability of predicting the targetvariable while minimizing the ability of predicting sensitive features using a GAN.\u2022 Post-processing: these techniques are applied after the ML model is trained. They are less intrusivebecause they do not modify the input data or the ML model. An example is Equalized Odds [393]. Thistechniques allows to adjust the thresholds in the classi\ufb01cation model in order to reduce the differencesbetween the TP rate and the FP rate for each sensitive subgroup.Even though these references apparently address an AI principle that appears to be independent ofXAI, the literature shows that they are intertwined. For instance, the survey in [385] evinces that 26 outof the 28 AI principles that deal with XAI, also talk about fairness explicitly. This fact elucidates thatorganizations usually consider both aspects together when implementing Responsible AI.The literature also exploses that XAI proposals can be used for bias detection. For example, [398]proposes a framework to visually analyze the bias present in a model (both for individual and groupfairness). Thus, the fairness report is shown just like the visual summaries used within XAI. Thisexplainability approach eases the understanding and measurement of bias. The system must report thatthere is bias, justify it quantitatively, indicate the degree of fairness, and explain why a user or groupwould be treated unfairly with the available data. Similarly, XAI techniques such as SHAP [224] could beused to generate counterfactual outcomes explaining the decisions of a ML model when fed with protectedand unprotected variables. By identifying implicit correlations between protected and unprotected featuresthrough XAI techniques, the model designer may unveil hidden correlations between the input variablesamenable to cause discrimination.Another example is [399], where the authors propose a fair-by-design approach in order to developML models that jointly have less bias and include as explanations human comprehensible rules. Theproposal is based in self-learning locally generative models that use only a small part of the wholedataset available (weak supervision). It \ufb01rst \ufb01nds recursively relevant prototypes within the dataset, and39extracts the empirical distribution and density of the points around them. Then it generates rules in anIF/THEN format that explain that a data point is classi\ufb01ed within a speci\ufb01c category because it is similarto some prototypes. The proposal then includes an algorithm that both generates explanations and reducesbias, as it is demonstrated for the use case of recidivism using the Correctional Offender ManagementPro\ufb01ling for Alternative Sanctions (COMPAS) dataset [400]. The same goal has been recently pursued in[401], showing that post-hoc XAI techniques can forge fairer explanations from truly unfair black-boxmodels. Finally, CERTIFAI (Counterfactual Explanations for Robustness, Transparency, Interpretability,and Fairness of Arti\ufb01cial Intelligence models) [402] uses a customized genetic algorithm to generatecounterfactuals that can help to see the robustness of a ML model, generate explanations, and examinefairness (both at the individual level and at the group level) at the same time.Strongly linked to the concept of fairness, much attention has been lately devoted to the concept ofdata diversity, which essentially refers to the capability of an algorithmic model to ensure that all differenttypes of objects are represented in its output [403]. Therefore, diversity can be thought to be an indicatorof the quality of a collection of items that, when taking the form of a model\u2019s output, can quantify theproneness of the model to produce diverse results rather than highly accurate predictions. Diversity comesinto play in human-centered applications with ethical restrictions that permeate to the AI modeling phase[404]. Likewise, certain AI problems (such as content recommendation or information retrieval) alsoaim at producing diverse recommendations rather than highly-scoring yet similar results [405, 406]. Inthese scenarios, dissecting the internals of a black-box model via XAI techniques can help identifying thecapability of the model to maintain the input data diversity at its output. Learning strategies to endow amodel with diversity keeping capabilities could be complemented with XAI techniques in order to shedtransparency over the model internals, and assess the effectiveness of such strategies with respect to thediversity of the data from which the model was trained. Conversely, XAI could help to discriminate whichparts of the model are compromising its overall ability to preserve diversity.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "a52dd930-c98f-46ad-9d92-ac849c866bc6",
                    "text": "Regarding accountability, the EC [390] de\ufb01nes the following aspects to consider:\u2022 Auditability: it includes the assessment of algorithms, data and design processes, but preserving theintellectual property related to the AI systems. Performing the assessment by both internal and externalauditors, and making the reports available, could contribute to the trustworthiness of the technology.When the AI system affects fundamental rights, including safety-critical applications, it should alwaysbe audited by an external third party.\u2022 Minimization and reporting of negative impacts: it consists of reporting actions or decisions that yielda certain outcome by the system. It also comprises the assessment of those outcomes and how torespond to them. To address that, the development of AI systems should also consider the identi\ufb01cation,assessment, documentation and minimization of their potential negative impacts. In order to minimizethe potential negative impact, impact assessments should be carried out both prior to and during thedevelopment, deployment and use of AI systems. It is also important to guarantee protection for anyonewho raises concerns about an AI system (e.g., whistle-blowers). All assessments must be proportionateto the risk that the AI systems pose.\u2022 Trade-offs: in case any tension arises due to the implementation of the above requirements, trade-offscould be considered but only if they are ethically acceptable. Such trade-offs should be reasoned,explicitly acknowledged and documented, and they must be evaluated in terms of their risk to ethicalprinciples. The decision maker must be accountable for the manner in which the appropriate trade-offis being made, and the trade-off decided should be continually reviewed to ensure the appropriatenessof the decision. If there is no ethically acceptable trade-off, the development, deployment and use ofthe AI system should not proceed in that form. 40\u2022 Redress: it includes mechanisms that ensure an adequate redress for situations when unforeseen unjustadverse impacts take place. Guaranteeing a redress for those non-predicted scenarios is a key to ensuretrust. Special attention should be paid to vulnerable persons or groups.These aspects addressed by the EC highlight different connections of XAI with accountability. First,XAI contributes to auditability as it can help explaining AI systems for different pro\ufb01les, includingregulatory ones. Also, since there is a connection between fairness and XAI as stated before, XAI canalso contribute to the minimization and report of negative impacts.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "676f4f06-04e4-4f92-9f4e-9133be66833b",
                    "text": "The ever-growing number of information sources that nowadays coexist in almost all domains ofactivity calls for data fusion approaches aimed at exploiting them simultaneously toward solving a learningtask. By merging heterogeneous information, data fusion has been proven to improve the performance ofML models in many applications, such as industrial prognosis [348], cyber-physical social systems [407]or the Internet of Things [408], among others. This section speculates with the potential of data fusiontechniques to enrich the explainability of ML models, and to compromise the privacy of the data fromwhich ML models are learned. To this end, we brie\ufb02y overview different data fusion paradigms, and lateranalyze them from the perspective of data privacy. As we will later, despite its relevance in the context ofResponsible AI, the con\ufb02uence between XAI and data fusion is an uncharted research area in the currentresearch mainstream.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "9eb886e9-f185-4054-bee5-51060b118a06",
                    "text": "We depart from the different levels of data fusion that have been identi\ufb01ed in comprehensive surveyson the matter [409, 410, 411, 412]. In the context of this subsection, we will distinguish among fusion atdata level, fusion at model level and fusion at knowledge level. Furthermore, a parallel categorization canbe established depending on where such data is processed and fused, yielding centralized and distributedmethods for data fusion. In a centralized approach, nodes deliver their locally captured data to a centralizedprocessing system to merge them together. In contrast, in a distributed approach, each of the nodes mergesits locally captured information, eventually sharing the result of the local fusion with its counterparts.Fusion through the information generation process has properties and peculiarities depending onthe level at which the fusion is performed. At the so-called data level, fusion deals with raw data. Asschematically shown in Figure 13, a fusion model at this stage receives raw data from different informationsources, and combines them to create a more coherent, compliant, robust or simply representative data\ufb02ow. On the other hand, fusion at the model level aggregates models, each learned from a subset of thedata sets that were to be fused. Finally, at the knowledge level the fusion approach deals with knowledge inthe form of rules, ontologies or other knowledge representation techniques with the intention of mergingthem to create new, better or more complete knowledge from what was originally provided. Structuredknowledge information is extracted from each data source and for every item in the data set using multipleknowledge extractors (e.g. a reasoning engine operating on an open semantic database). All producedinformation is then fused to further ensure the quality, correctness and manageability of the producedknowledge about the items in the data set.Other data fusion approaches exist beyonds the ones represented in Figure 13. As such, data-levelfusion can be performed either by a technique speci\ufb01cally devoted to this end (as depicted in Figure 13.b)or, instead, performed along the learning process of the ML model (as done in e.g. DL models). Similarly,model-level data fusion can be made by combining the decisions of different models (as done in treeensembles).",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "320d910d-d2a2-4157-897b-b773cd59a24a",
                    "text": "In the next subsection we examine other data fusion approaches that have recently come into scenedue to their implications in terms of data privacy: 41\u2022 In Big Data fusion (Figure 13.d), local models are learned on a split of the original data sources, eachsubmitted to a Worker node in charge of performing this learning process (Map task). Then, a Reducenode (or several Reduce nodes, depending on the application) combines the outputs produced by eachMap task. Therefore, Big Data fusion can be conceived as a means to distribute the complexity of learn-ing a ML model over a pool of Worker nodes, wherein the strategy to design how information/modelsare fused together between the Map and the Reduce tasks is what de\ufb01nes the quality of the \ufb01nallygenerated outcome [413].\u2022 By contrast, in Federated Learning [414, 415, 416], the computation of ML models is made on datacaptured locally by remote client devices (Figure 13.e). Upon local model training, clients transmitencrypted information about their learned knowledge to a central server, which can take the form oflayer-wise gradients (in the case of neural ML models) or any other model-dependent content alike. Thecentral server aggregates (fuses) the knowledge contributions received from all clients to yield a sharedmodel harnessing the collected information from the pool of clients. It is important to observe that noclient data is delivered to the central server, which elicits the privacy-preserving nature of FederatedLearning. Furthermore, computation is set closer to the collected data, which reduces the processinglatency and alleviates the computational burden of the central server.\u2022 Finally, Multiview Learning [417] constructs different views of the object as per the informationcontained in the different data sources (Figure 13.f). These views can be produced from multiplesources of information and/or different feature subsets [418]. Multiview Learning devises strategiesto jointly optimize ML models learned from the aforementioned views to enhance the generalizationperformance, specially in those applications with weak data supervision and hence, prone to modelover\ufb01tting. This joint optimization resorts to different algorithmic means, from co-training to co-regularization [419].",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "11422480-d1ba-4c90-a2dc-060c276add62",
                    "text": "AI systems, specially when dealing with multiple data sources, need to explicitly include privacyconsiderations during the system\u2019s life cycle. This is specially critical when working with personal data,42because respecting people\u2019s right to privacy should always be addressed. The EC highlights that privacyshould also address data governance, covering the quality and integrity of the used data [390]. It shouldalso include the de\ufb01nition of access protocols and the capability to process data in a way that ensuresprivacy. The EC guide breaks down the privacy principle into three aspects:\u2022 Privacy and data protection: they should be guaranteed in AI systems throughout its entire lifecycle. Itincludes both information provided by users and information generated about those users derived fromtheir interactions with the system. Since digital information about a user could be used in a negativeway against them (discrimination due to sensitive features, unfair treatment...), it is crucial to ensureproper usage of all the data collected.\u2022 Quality and integrity of data: quality of data sets is fundamental to reach good performance with AIsystems that are fueled with data, like ML. However, sometimes the data collected contains sociallyconstructed biases, inaccuracies, errors and mistakes. This should be tackled before training any modelwith the data collected. Additionally, the integrity of the data sets should be ensured.\u2022 Access to data: if there is individual personal data, there should always be data protocols for datagovernance. These protocols should indicate who may access data and under which circumstances.The aforementioned examples from the EC shows how data fusion is directly intertwined with privacyand with fairness, regardless of the technique employed for it.Notwithstanding this explicit concern from regulatory bodies, loss of privacy has been compromisedby DL methods in scenarios where no data fusion is performed. For instance, a few images are enoughto threaten users\u2019 privacy even in the presence of image obfuscation [420], and the model parameters ofa DNN can be exposed by simply performing input queries on the model [356, 357]. An approach toexplain loss of privacy is by using privacy loss and intent loss subjective scores. The former provides asubjective measure of the severity of the privacy violation depending on the role of a face in the image,while the latter captures the intent of the bystanders to appear in the picture. These kind of explanationshave motivated, for instance, secure matching cryptographic protocols for photographer and bystanders topreserve privacy [356, 421, 422]. We de\ufb01nite advocate for more efforts invested in this direction, namely,in ensuring that XAI methods do not pose a threat in regards to the privacy of the data used for trainingthe ML model under target.When data fusion enters the picture, different implications arise with the context of explainabilitycovered in this survey. To begin with, classical techniques for fusion at the data level only deal with dataand have no connection to the ML model, so they have little to do with explainability. However, theadvent of DL models has blurred the distinction between information fusion and predictive modeling. The\ufb01rst layers of DL architectures are in charge of learning high-level features from raw data that possessrelevance for the task at hand. This learning process can be thought to aim at solving a data level fusionproblem, yet in a directed learning fashion that makes the fusion process tightly coupled to the task to besolved.In this context, many techniques in the \ufb01eld of XAI have been proposed to deal with the analysis ofcorrelation between features. This paves the way to explaining how data sources are actually fused throughthe DL model, which can yield interesting insights on how the predictive task at hand induces correlationsamong the data sources over the spatial and/or time domain. Ultimately, this gained information on thefusion could not only improve the usability of the model as a result of its enhanced understanding by theuser, but could also help identifying other data sources of potential interest that could be incorporated tothe model, or even contribute to a more ef\ufb01cient data fusion in other contexts.Unfortunately, this previously mentioned concept of fusion at data level contemplates data undercertain constraints of known form and source origin. As presented in [423], the Big Data era presentsan environment in which these premises cannot be taken for granted, and methods to board Big Datafusion (as that illustrated in Figure 13.d) have to be thought. Conversely, a concern with model fusion43context emerges in the possibility that XAI techniques could be explanatory enough to compromise thecon\ufb01dentiality of private data. This could eventually occur if sensitive information (e.g. ownership) couldbe inferred from the explained fusion among protected and unprotected features.When turning our prospects to data fusion at model level, we have already argued that the fusion of theoutputs of several transparent models (as in tree ensembles) could make the overall model opaque, therebymaking it necessary to resort to post-hoc explainability solutions. However, model fusion may entail otherdrawbacks when endowed with powerful post-hoc XAI techniques. Let us imagine that relationships ofa model\u2019s input features have been discovered by means of a post-hoc technique) and that one of thosefeatures is hidden or unknown. Will it be possible to infer another model\u2019s features if that previous featurewas known to be used in that model? Would this possibility uncover a problem as privacy breaches incases in which related protected input variables are not even shared in the \ufb01rst place?To get the example clearer, in [424] a multiview perspective is utilized in which different single views(representing the sources they attend to) models are fused. These models contain among others, cell-phonedata, transportation data, etc. which might introduce the problem that information that is not even sharedcan be discovered through other sources that are actually shared. In the example above, what if instead offeatures, a model shares with another a layer or part of its architecture as in Federated Learning? Wouldthis sharing make possible to infer information from that exchanged part of its model, to the extent ofallowing for the design of adversarial attacks with better success rate upon the antecedent model?If focused at knowledge level fusion, a similar reasoning holds: XAI comprises techniques that extractknowledge from ML model(s). This ability to explain models could have an impact on the necessity ofdiscovering new knowledge through the complex interactions formed within ML models. If so, XAI mightenrich knowledge fusion paradigms, bringing the possibility of discovering new knowledge extractorsof relevance for the task at hand. For this purpose, it is of paramount importance that the knowledgeextracted from a model by means of XAI techniques can be understood and extrapolated to the domainin which knowledge extractors operate. The concept matches with ease with that of transfer learningportrayed in [425]. Although XAI is not contemplated in the surveyed processes of extracting knowledgefrom models trained in certain feature spaces and distributions, to then be utilized in environments whereprevious conditions do not hold, when deployed, XAI can pose a threat if the explanations given about themodel can be reversely engineered through the knowledge fusion paradigm to eventually compromise, forinstance, the differential privacy of the overall model.The distinction between centralized and distributed data fusion also spurs further challenges inregards to privacy and explainability. The centralized approach does not bring any further concernsthat those presented above. However, distributed fusion does arise new problems. Distributed fusionmight be applied for different reasons, mainly due to environmental constraints or due to security orprivacy issues. The latter context may indulge some dangers. Among other goals (e.g. computationalef\ufb01ciency), model-level data fusion is performed in a distributed fashion to ensure that no actual data isactually shared, but rather parts of an ML model trained on local data. This rationale lies at the heartof Federated Learning, where models exchange locally learned information among nodes. Since datado not leave the local device, only the transmission of model updates is required across distributeddevices. This lightens the training process for network-compromised settings and guarantees data privacy[416]. Upon the use of post-hoc explainability techniques, a node could disguise sensitive informationabout the local context in which the received ML model part was trained. In fact, it was shown that ablack-box model based on a DNN from which an input/output query interface is given can be used toaccurately predict every single hyperparameter value used for training, allowing for potential privacy-related consequences [357, 420, 421]. This relates to studies showing that blurring images does notguarantee privacy preservation.Data fusion, privacy and model explainability are concepts that have not been analysed together so far.From the above discussion it is clear that there are unsolved concerns and caveats that demand furtherstudy by the community in forthcoming times. 44",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "3bdb3f1e-df50-4b9a-bda9-e975168bda14",
                    "text": "While increasingly more organizations are publishing AI principles to declare that they care aboutavoiding unintended negative consequences, there is much less experience on how to actually implementthe principles into an organization. Looking at several examples of principles declared by differentorganizations [385], we can divide them into two groups:\u2022 AI-speci\ufb01c principles that focus on aspects that are speci\ufb01c to AI, such as explainability, fairness andhuman agency.\u2022 End-to-end principles that cover all aspects involved in AI, including also privacy, security and safety.The EC Guidelines for Trustworthy AI are an example of end-to-end principles [390], while those ofTelefonica (a large Spanish ICT company operating worldwide) are more AI-speci\ufb01c [386]. For example,safety and security are relevant for any connected IT system, and therefore also for AI systems. Thesame holds for privacy, but it is probably true that privacy in the context of AI systems is even moreimportant than for general IT systems, due to the fact that ML models need huge amounts of data andmost importantly, because XAI tools and data fusion techniques pose new challenges to preserve theprivacy of protected records.When it comes to implement the AI Principles into an organization, it is important to operationalizethe AI-speci\ufb01c parts and, at the same time, leverage the processes already existing for the more genericprinciples. Indeed, in many organizations there already exist norms and procedures for privacy, securityand safety. Implementing AI principles requires a methodology such as that presented in [386] that breaksdown the process into different parts. The ingredients of such a methodology should include, at least:\u2022 AI principles (already discussed earlier), which set the values and boundaries.\u2022 Awareness and training about the potential issues, both technical and non-technical.\u2022 A questionnaire that forces people to think about certain impacts of the AI system (impact explanation).This questionnaire should give concrete guidance on what to do if certain undesired impacts aredetected.\u2022 Tools that help answering some of the questions, and help mitigating any problems identi\ufb01ed. XAItools and fairness tools fall in this category, as well as other recent proposals such as model cards [426].\u2022 A governance model assigning responsibilities and accountabilities (responsibility explanation). Thereare two philosophies for governance: 1) based on committees that review and approve AI developments,and 2) based on the self-responsibility of the employees. While both are possible, given the factthat agility is key for being successful in the digital world, it seems wiser to focus on awareness andemployee responsibility, and only use committees when there are speci\ufb01c, but important issues.From the above elaborations, it is clear that the implementation of Responsible AI principles incompanies should balance between two requirements: 1) major cultural and organizational changesneeded to enforce such principles over processes endowed with AI functionalities; and 2) the feasibilityand compliance of the implementation of such principles with the IT assets, policies and resources alreadyavailable at the company. It is in the gradual process of rising corporate awareness around the principlesand values of Responsible AI where we envision that XAI will make its place and create huge impact.",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                },
                {
                    "id": "aafc9153-ef32-499b-8805-7bc4089420a6",
                    "text": "This overview has revolved around eXplainable Arti\ufb01cial Intelligence (XAI), which has been identi\ufb01edin recent times as an utmost need for the adoption of ML methods in real-life applications. Our study45has elaborated on this topic by \ufb01rst clarifying different concepts underlying model explainability, as wellas by showing the diverse purposes that motivate the search for more interpretable ML methods. Theseconceptual remarks have served as a solid baseline for a systematic review of recent literature dealing withexplainability, which has been approached from two different perspectives: 1) ML models that featuresome degree of transparency, thereby interpretable to an extent by themselves; and 2) post-hoc XAItechniques devised to make ML models more interpretable. This literature analysis has yielded a globaltaxonomy of different proposals reported by the community, classifying them under uniform criteria.Given the prevalence of contributions dealing with the explainability of Deep Learning models, we haveinspected in depth the literature dealing with this family of models, giving rise to an alternative taxonomythat connects more closely with the speci\ufb01c domains in which explainability can be realized for DeepLearning models.We have moved our discussions beyond what has been made so far in the XAI realm toward the conceptof Responsible AI, a paradigm that imposes a series of AI principles to be met when implementing AImodels in practice, including fairness, transparency, and privacy. We have also discussed the implicationsof adopting XAI techniques in the context of data fusion, unveiling the potential of XAI to compromisethe privacy of protected data involved in the fusion process. Implications of XAI in fairness have alsobeen discussed in detail. This vision of XAI as a core concept to ensure the aforementioned principles forResponsible AI is summarized graphically in Figure 14.Our re\ufb02ections about the future of XAI, conveyed in the discussions held throughout this work,agree on the compelling need for a proper understanding of the potentiality and caveats opened up byXAI techniques. It is our vision that model interpretability must be addressed jointly with requirementsand constraints related to data privacy, model con\ufb01dentiality, fairness and accountability. A responsibleimplementation and use of AI methods in organizations and institutions worldwide will be only guaranteedif all these AI principles are studied jointly.AcknowledgmentsAlejandro Barredo-Arrieta, Javier Del Ser and Sergio Gil-Lopez would like to thank the BasqueGovernment for the funding support received through the EMAITEK and ELKARTEK programs. JavierDel Ser also acknowledges funding support from the Consolidated Research Group MATHMODE(IT1294-19) granted by the Department of Education of the Basque Government. Siham Tabik, SalvadorGarcia, Daniel Molina and Francisco Herrera would like to thank the Spanish Government for its fundingsupport (SMART-DaSCI project, TIN2017-89517-P), as well as the BBVA Foundation through its Ayudas46Fundaci\u00b4on BBVA a Equipos de Investigaci\u00b4on Cient\u00b4\u0131\ufb01ca 2018 call (DeepSCOP project). This work wasalso funded in part by the European Union\u2019s Horizon 2020 research and innovation programme AI4EUunder grant agreement 825619. We also thank Chris Olah, Alexander Mordvintsev and Ludwig Schubertfor borrowing images for illustration purposes. Part of this overview is inspired by a preliminary work ofthe concept of Responsible AI: R. Benjamins, A. Barbado, D. Sierra, \u201cResponsible AI by Design\u201d, toappear in the Proceedings of the Human-Centered AI: Trustworthiness of AI Models & Data (HAI) trackat AAAI Fall Symposium, DC, November 7-9, 2019 [386].",
                    "reference": "[1] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, Elsevier. arXiv:1910.10045. Retrieved from https://arxiv.org/pdf/1910.10045."
                }
            ]
        },
        {
            "paper_title": "Principles to practices for responsible AI: closing the gap",
            "authors": "D Schiff, B Rakova, A Ayesh, A Fanti\u2026",
            "publication_info": "arXiv preprint arXiv \u2026 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2006.04707",
            "chunks": [
                {
                    "id": "4b0d1ad3-7fec-4590-8eb0-41e4ccf3860f",
                    "text": ". Companies have considered adoption of various high-level arti\ufb01cial intelligence (AI) principles for responsible AI, butthere is less clarity on how to implement these principles as orga-nizational practices. This paper reviews the principles-to-practicesgap. We outline \ufb01ve explanations for this gap ranging from a disci-plinary divide to an overabundance of tools. In turn, we argue thatan impact assessment framework which is broad, operationalizable,\ufb02exible, iterative, guided, and participatory is a promising approachto close the principles-to-practices gap. Finally, to help practitionerswith applying these recommendations, we review a case study of AI\u2019suse in forest ecosystem restoration, demonstrating how an impact as-sessment framework can translate into effective and responsible AIpractices.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "64def63e-55dd-4fc8-89d9-1ff20d445427",
                    "text": "Arti\ufb01cial intelligence (AI) is already in use across many areas of so-cial and economic life, and new opportunities for AI to contribute tosocial good (AI4SG) have also been proposed and developed [13].For example, efforts like Microsoft\u2019s AI for Earth program highlightthe potential of AI to address the United Nation\u2019s Sustainable Devel-opment Goals (SDGs). However, many challenges face the practicalimplementation of AI for social good efforts. Similarly, in the \ufb01eldof fairness, accountability, and transparency of AI, decades of re-search has only recently begun to be more thoroughly incorporatedinto practical settings, and many questions remain. In this paper wereview challenges in translating principles into practices and proposerecommendations towards closing this gap.After introducing prior work on responsible AI principles and con-cerns about the practical application of these principles in Section 1,Section 2 proposes \ufb01ve explanations for the principles-to-practicesgap. We discuss the complexity of AI\u2019s impacts, confusion about thedistribution of accountability, a social technical disciplinary divide,identifying and using tools, and organizational processes and normsas key issues in this gap.In light of these concerns, Section 3 proposes the criteria of aframework that could help organizations turn responsible AI prin-ciples into practices. We propose that impact assessment is a promis-ing approach towards meeting these criteria, as it has the potentialto be suf\ufb01ciently broad, operationalizable, \ufb02exible, iterative, guided,and participatory. As an exemplar, we focus on the new Institute of Electrical and Electronics Engineerings (IEEE) 7010-2020 Rec-ommended Practice for Assessing the Impact of Autonomous andIntelligent Systems on Human Well-being (henceforth IEEE 7010).IEEE 7010 is a standard that assesses the well-being implications ofAI and employs a well-being impact assessment to do so. Finally,to help practitioners apply these recommendations. Section 4 ap-plies a well-being impact assessment framework to a case study. Thecase study reviews challenges with AI\u2019s use in ecosystem restora-tion and afforestation \u2013 an important aspect related to several SDGs\u2013 and demonstrates how an impact assessment framework may helpto close the principles-to-practices gap in this case.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "2f1ad42f-9935-49b4-89cb-26b8c8bbc92a",
                    "text": "As of 2019, more than 20 \ufb01rms have produced frameworks, princi-ples, guidelines, and policies related to the responsible developmentand use of arti\ufb01cial intelligence (AI). These documents are meant toaddress many of the social and ethical issues that surround AI, rang-ing from labor displacement [3] and algorithmic bias [7] to privacy,an increasingly important issue in the context of the COVID-19 virus[44]. These governance documents typically address a set of socialand ethical concerns, propose principles in response, and in somecases offer concrete reforms or internal governance strategies.Research on the various AI documents produced by \ufb01rms alongwith government actors and non-governmental associations has iden-ti\ufb01ed clear consensus in organizations\u2019 ethical priorities [21, 23, 41].The social and ethical concerns highlighted most often surround gen-eral concern for public, customer, and employee welfare; algorithmicbias and fairness; transparency and explainability; trust in AI; andreliability and safety of AI products [61]. While this scholarship of-ten focuses on identifying consensus across organizations [34], it hasalso examined how companies de\ufb01ne their responsibilities [29] andwhether there are issues neglected across documents.Importantly, a key focus of the documents is on presenting a set ofhigh-level principles for responsible AI. For example, Google\u2019s AIprinciples include \u201cBe socially bene\ufb01cial,\u201d \u201cAvoid creating or rein-forcing AI bias,\u201d \u201cBe built and tested for safety,\u201d and \u201cBe accountableto people,\u201d among other principles [28]. OpenAI discusses its focuson \u201cBroadly Distributed Bene\ufb01ts,\u201d \u201cLong-Term Safety,\u201d \u201cTechnicalLeadership,\u201d and its \u201cCooperative Orientation\u201d [50]. However, theseand other high-level responsible AI principles can often be vague,host a multitude of possible interpretations, and may be dif\ufb01cult totranslate into everyday practices.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "61970f12-4e09-483b-ad21-2e4d3b1246ac",
                    "text": "Many scholars have raised concerns that companies less often pro-vide detailed prescriptions of policies or practices meant to ensurethat these principles are adhered to [29, 34, 61]. In some cases, com-panies have established relatively clear strategies. Proposed prac-tices include training, hiring, algorithm development frameworks andtools, and governance strategies. For example, Vodafone\u2019s AI Frame-work provides some detail on speci\ufb01c actions it will take, such asadhering to its Code of Conduct and privacy commitments [70]. SAPproposes as part of its Guiding Principles an AI Ethics Steering Com-mittee and an AI Ethics Advisory Panel [59]. IBM\u2019s Everyday Ethicsfor AI provides a set of recommended actions and questions for itsemployees to address key concerns [33].On the other hand, some principles are not accompanied by clearexpressions of changes to practice. For example, documents fromTieto, Futurice, and Salesforce focus on abstract principles and com-mitments. Futurice proposes to \u201cavoid creating or reinforcing bias,\u201dwhile Salesforce claims \u201cwe test models with diverse data sets, seekto understand their impact, and build inclusive teams\u201d and Tietostates it is \u201ccommitted to harness AI for good, for the planet and hu-mankind\u201d [25, 57, 68]. These and other generic principles like \u201cBesocially bene\ufb01cial\u201c beg the question of how exactly the companiesare carrying out their commitments.In the best case, companies may still be in the process of workingout the details or may have communicated their intended strategies inother venues, for example, by publishing tools for responsible prac-tice. Nevertheless, remaining at the \u201cmission statement\u201d level andthe lack of practical detail are worrisome. We believe that the ques-tion of translating high-level principles into effective and responsiblepractices is a critical priority in the near-term future for AI. Closingthe principles-to-practices gap is worthy of attention by companiesdeveloping AI, by those who might procure and deploy AI systems,and by other stakeholders and the public more broadly.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "721f9ae3-1eed-4f55-9746-a3ccc185a9e9",
                    "text": "Despite \ufb01rms\u2019 efforts towards establishing some design and devel-opment principles for AI systems, several breaches of law and thepublic trust have been reported in the last few years. Companies havecome under signi\ufb01cant scrutiny, in some cases facing signi\ufb01cant neg-ative media attention, along with customer criticism and employeepetitions, walkouts, and resignations [66].The question then is why principles have seemingly not been trans-lated into effective practices? In fact, the process of translation is nei-ther obvious nor automatic as clear. According to Mittelstadt (2019)\u201cnorms and requirements can rarely be logically deduced withoutaccounting for speci\ufb01c elements of the technology, application, con-text of use, or relevant local norms\u201d. Barring practical guidance andabsent \u201cempirically proven methods in real-world development con-texts,\u201d [41] claims of responsible AI may amount to no more just that\u2014 claims.As a result, some criticisms more deeply impugn the motives of\ufb01rms. Greene et al. [29] argue that companies attempt to shift re-sponsibility onto designers and experts in order to minimize scrutinyof business decisions. Similarly, Hagendorff [31] argues that compa-nies are often driven by an economic logic, and that \u201cengineers anddevelopers are neither systematically educated about ethical issues,nor are they empowered, for example by organizational structures,to raise ethical concerns.\u201d On this account, companies may be strate-gically promoting their principles to ameliorate customer trust and reputational concerns. In this way, they can appear actively engagedregarding AI\u2019s ethical risks in the public eye, but while framing is-sues so as to minimize genuine accountability.While some of these deeper criticisms may be true in part or forsome organizations, we think a more multifaceted and charitable in-terpretation [1, 6] is both appropriate and likely to be bene\ufb01cial to-ward seeking positive change. Organizations are best understood notas monolithic single actors, but as multiple coordinating and com-peting coalitions of individuals [39]. Individuals within a single or-ganization may have multiple or competing preferences and roles.Organizational motives should therefore be considered a complexcomposition of genuine ethical concern, economic logic, signalingand framing strategies, and promotion of both internal and externalchanges [61].Researchers who have noticed the principles-to-practices gap havebegun proposing strategies [14], often aimed at companies. Theseproposals include changes to software mechanisms (such as audittrails), hardware mechanisms (such as secure hardware enclaves),and institutional mechanisms (such as red team exercises) [8]. Thiswork highlights that it is not only technical practices that must adapt,but also organizational practices.Among the most comprehensive work assessing the principles-to-practices gap is the review by Morley et al. (2019), which system-atically explores existing responsible AI tools and methodologiesmapped against six components of the AI development lifecycle: 1)business and use-case development, 2) design phase, 3) training andtest data procurement, 4) building, 5) testing, 6) deployment, and 7)monitoring [42]. They identify 106 such tools and methodologies.Some such methods are relatively narrower in scope, such as thosesurrounding explainable AI [30], bias [7], or procurement (e.g., theAI-RFX Procurement Framework).Other methodologies adopt a broader scope of focus, including im-pact assessments like the ISO 26000 Framework for Social Responsi-bility [72] and IEEE 7010 [43]. Relevant methods and approaches forresponsible AI also come from outside of the AI domain and includeprivacy-by-design [48], value-sensitive design [24], the ResponsibleResearch and Innovation (RRI) approach [63], and numerous others.In fact, the plethora of possible tools is itself a challenge which wediscuss more in Section 2.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "838d76b6-28ce-425d-bcee-c1943f3a1e5e",
                    "text": "In short, despite the urgent attention to responsible AI in recent years,there are already many existing frameworks and a growing set of newmethods aimed at addressing core ethical issues. Why then does theissue of translating principles to practices seem intractable? We offera few candidate explanations that are neither exhaustive nor mutuallyexclusive.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "1f635668-b3e0-453c-ad31-e4ac9be73b9c",
                    "text": "AI\u2019s impacts on human well-being \u2013 positive or negative \u2013 are morecomplex than is sometimes assumed. Site-based research has iden-ti\ufb01ed that engineers are often focused on single products and thephysical harm they may cause rather than broader kinds of harms,such as social, emotional, or economic harms [69]. Even as con-versations surrounding responsible AI increase, most work centersaround a relatively small subset of issues, most often bias [5] andtransparency [2] in particular AI models. This approach involves ex-posing and then attempting to mitigate bias in algorithms as well astrying to improve interpretability or explainability given the black-boxed nature of certain AI models which can make decision-makingprocesses opaque. Other commonly-emphasized issues include pri-vacy, reliability, and safety.However, these prominent issues most familiar to engineers stillconstitute only a subset of social and ethical risks and impacts re-lated to AI. Indeed, AI can be understood to impact a wide vari-ety of aspects of human well-being, such as human rights, inequal-ity, human-human relationships, social and political cohesion, psy-chological health, and more. AI can also impact natural ecosystemsand animal life. Moreover, many of these harms do not arise in astraightforward way from a single AI product, but from many AI sys-tems in\ufb02uencing human social and economic life together and overtime.AI is not the only technology with complex implications on humanwell-being. Yet its rapid rise is leading to calls for urgency, and someaspects of AI surface a unique combination of ethical concerns [13].For example, compared to other general-purpose technologies likeelectricity or the internet, AI is notable for its autonomy, its capac-ity to \u2018learn,\u2019 and its power in making accurate predictions, all whileembedded in software and ambient systems and therefore invisibleto many affected by it. As a result, AI systems are becoming increas-ingly ubiquitous, and can act in the aggregate to in\ufb02uence human andsocial well-being in subtle but pervasive ways.For example, algorithms on social media designed to steer con-sumers to entertaining video clips have also led to so-called \ufb01lterbubbles that may foster political polarization, misinformation andpropaganda, targeting of minority groups, and election interference.AI as instantiated in autonomous vehicles has potentially massive im-plications for physical infrastructure, energy and environment, traf\ufb01cfatalities, work productivity, urban design, and unemployment [4]. Inshort, addressing AI principles in full seriousness requires an expan-sive scope of attention to the full set of issues in\ufb02uencing humanwell-being. This requires looking well beyond a narrow set of topicssuch as bias, transparency, privacy, or safety and treating them as in-dependent issues. Instead, the full range of topics and their complexinterdependencies needs to be understood. However, such a task canbe enormously dif\ufb01cult.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "b155e217-e7f8-476e-9b9a-6634b014275c",
                    "text": "It is clear that responsibly designing and applying AI is thereforeboth a technical challenge and a social one (implicating social, eco-nomic, and policy questions). For example, creating a facial recogni-tion system for policing that minimizes racial bias (by some technicalmeasure) is inseparable from questions on the legitimacy of the useof that system in a particular social and policy setting. However thequestion of distributing accountability for addressing these issues re-mains open and contested. Engineers and computer scientists maysee their responsibility as focused on the quality and safety of a par-ticular product rather than on larger scale social issues, and may beunaware of the wider set of implications [20]. Business managersand companies may see their responsibility as \ufb01duciary, in produc-ing high-quality products and revenue. This potentially creates holesin responsibility for addressing key well-being impacts of AI.In addition to uncertainty regarding one\u2019s scope of professional ac-countability, engineers and computer scientists who focus on designof systems may have limited in\ufb02uence within their organizations.They may expect business managers, liability of\ufb01cers, or corporate social responsibility staff to assess broader social and ethical issues.Social scientists and ethicists tapped speci\ufb01cally for these issues may\ufb01nd themselves similarly handicapped, perhaps in an external advi-sory role without real say. The result is the \u2018many hands\u2019 problem,where responsibility for responsible AI is distributed and muddled[22]. The many stakeholders involved in shaping AI need to be bothfunctionally able and willing to resolve the accountability questionwith a concrete division of labor. If companies fail to resolve thesechallenges, they may continue to face public scrutiny as well as \ufb01-nancial and legal risks and reputational harms. Moreover, they mayharm their employees, consumers, or the public. Figuring out howto distribute responsibility for AI\u2019s impacts on well-being is there-fore as critical as it is dif\ufb01cult. It may involve challenging long-heldassumptions and shifting norms.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "659075ca-8a60-4146-afe7-ada0b6b85dff",
                    "text": "Another related challenge is the plurality of professional disciplineswith roles to play in responsible AI. Discourse on responsible AI hasbeen advanced not only by engineers and computer scientists, butalso by sociologists, ethicists, historians and philosophers of technol-ogy, policy scholars, political decision-makers, journalists, membersof the public, and more. Yet the composition of these diverse stake-holders directs attention to the likelihood that they may bring verydifferent perspectives to the table. They may differ in their techni-cal and ethical education, their framing of problems and solutions,their attitudes and values towards responsible AI, and their norms ofcommunication.Consider attempts to apply the principle of fairness in attemptingto minimize bias. Arguably, a thoughtful AI engineer today mightidentify a normative principle like \u2019fairness,\u2019 speci\ufb01ed in a corporateresponsible AI policy, pick a plausible fairness metric to instantiateit (noting there are ineliminable trade-offs between different metrics[12]), apply it, and communicate these decisions transparently [37].However, even these laudable efforts cannot begin to satisfy the ex-tensive societal questions related to fairness, discrimination, and in-equality that trouble many social scientists and ethicists.More speci\ufb01cally, approaching social issues like bias and fairnesstoo narrowly leads to what Selbst et al. (2018) call category or ab-straction errors. For example, computer scientists and engineers de-veloping AI systems can fail to consider how an AI system will beimplemented in different social contexts, in\ufb02uence human behaviorin those contexts, or lead to long-term ripple effects, all of whichcan threaten the assumptions on which the AI system is built. Thisis especially dif\ufb01cult as predicting a technology\u2019s usage and impactis known by historians of science and technology to be dif\ufb01cult [71].More fundamentally, AI developers may err in even considering so-cial concepts like fairness to be computationally de\ufb01nable and tech-nically soluble [65].Consider an algorithm designed to minimize racial bias that is usedto inform a judge\u2019s decision about criminal sentencing. An algorithmdesigned and trained on test data from one jurisdiction may translatepoorly to another region. It may in\ufb02uence the judge\u2019s decisions inunexpected ways, as a judge may overtrust or undertrust the algo-rithm, or even hold values contrary to those re\ufb02ected in the algorithm.For example, the algorithm may favor predictive accuracy, while thejudge favors leniency and second chances. The consequences forcriminal justice outcomes when such a system is used in complexcontexts is unclear, and may feed back in unexpected or problematicways if an AI is trained on data the system has itself helps to gener-ate. To reiterate, there are many questions about responsible AI thatcannot be straightforwardly addressed with a narrow technical lens.On the other hand, social scientists may bring a lens that is broaderbut faces an inverse problem to the problem faced by engineers.Frameworks for considering social and ethical consequences of AImore in line with the thinking of social scientists can be unhelpfullycomplex and vague, and therefore fail to translate into action. Forexample, ethicists recognize that concepts like justice are complex,while political scientists know that values surrounding justice are po-litically contested. Yet AI engineers must de\ufb01ne some measure ofjustice to implement it.In addressing issues like inequality, social scientists may proposelarge structural changes to economic and social systems, some ofwhich are dif\ufb01cult to achieve (e.g., reforming the motives of corpora-tions) and others possibly far-fetched (e.g., changing the structure ofcapitalism). These structural changes may be signi\ufb01cantly outside ofthe scope of control of AI engineers. Also unhelpful are conceptionsof AI based on sweeping, overly futuristic, or unrealistic generaliza-tions. These abstractions can fail to provide the speci\ufb01city needed tothink clearly about addressing harms to human well-being. Again,while the intentions may be laudable, translating them to practicecan be unfeasible or at best unclear. In the best case, it is dif\ufb01cultto resolve the awkwardness of attempting to apply technical \ufb01xes tofundamentally socio-technical problems. Something is lost in trans-lation.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "586ae1b7-72b0-4a31-a03f-92cdacd98608",
                    "text": "As we have seen, there are already many tools and methodologiesfor addressing responsible development and use of AI. While creat-ing more and better such tools and methodologies is a worthy pursuit,in one sense there are already too many. Even those tools that do ex-ist have arguably not been tested suf\ufb01ciently to demonstrate whichare most effective and in which contexts [41]. An over-abundanceproblem makes it dif\ufb01cult for individuals to sort through and assessthe utility of a given tool, or to weigh it against the many other avail-able tools. People\u2019s time, attention, and cognitive capacity is limited,leading to search and transaction cost problems. As a result, individ-uals and organizations may fail to take advantage of the useful toolsand methodologies that are already out there.In addition, many tools and methodologies are not supported bypractical guidance [42]. A published journal paper or open sourcecode may explain basic functionality but not contain suf\ufb01cient in-structions to apply, customize, or troubleshoot tools and methodolo-gies, especially in a variety of organizational contexts and use cases.This means that only tools that are well-documented, perhaps thosecreated by well-resourced companies or universities and backed upby online communities, may be feasible to use. Individuals withouthigh levels of expertise and speci\ufb01c training may have little luck evenwith these prominent tools.Further, because of the disciplinary divide, methodologies devel-oped in part or in whole by disciplines outside of engineering andcomputer science (such as responsible research and design ethics)may have a harder time gaining traction. If these extra-disciplinaryideas are not documented and translated for use in AI developmentsettings, there may be little uptake. More work is needed to test toolsempirically, to streamline access and guidance, and to help with sort-ing between tools and methods. Organizations may need an overarch-ing framework to help integrate these lower-level tools and method-ologies.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "aabb5ef7-cad2-4749-b81a-5af9093a8468",
                    "text": "The last explanation for the principles-to-practices that we discussis how organizations structure their job responsibilities and work-\ufb02ow related to AI. Again related to the disciplinary divide, a majorconcern is that the computer scientists and engineers more directlyresponsible for an AI system\u2019s development may be functionally sep-arated from other workers likely to be tasked with thinking about thesystem\u2019s broader implications \u2013 such as higher-level business man-agers, the C-suite, and corporate social responsibility and compliancestaff. For simplicity, we refer to these crassly as \u2018technical\u2019 and \u2018non-technical\u2019 teams.For instance, several companies have proposed external AI ethicsadvisory or governance boards. External boards (and likely internalones) may constitute functionally distinct units of the organizationthat interact only occasionally with primary AI system designers.The same functional separation may apply even when non-technicalteams are internal to an organization.Non-technical employees may have limited ability to understandor modify an AI system\u2019s design if interaction with technical teamshappens at an arm\u2019s distance. Staff without disciplinary expertise inengineering and computer science and even those with technical ex-pertise but not involved in the system\u2019s creation may not be able toimagine improvements to the system\u2019s development or deployment.They may make underinformed or overly simplistic decisions, for ex-ample, prohibiting the use of an AI system that could be modi\ufb01ed; orrecommending the use of an AI system when they do not fully under-stand its risks. This functional separation therefore limits their abilityto support responsible AI development that adequately considers thefull range of impacts on human well-being.On the other hand, engineers and computer scientists in techni-cal teams may also not be privy to the deliberations of their non-technical counterparts if there is functional organizational separation.If technical employees are exempt from this dialogue, they will notbe able to participate in how their colleagues weigh considerations ofcorporate responsibility, pro\ufb01t, policy, and social and ethical impacts.They may not learn how to incorporate these concepts and trade-offsinto their design processes. Technical teams may also fail to imag-ine ways in which the system they are creating could be improved, orhow other systems, tools, or methodologies could be applied to bettersafeguard and improve human well-being. In sum, functional sepa-ration of technical and non-technical experts in organizations limitsthe potential to communicate effectively, understand issues robustly,and respond to considerations of AI\u2019s impacts on well-being.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "f6693258-b5eb-4563-a855-9c7bf49f5c64",
                    "text": "In this section, we have reviewed \ufb01ve sets of concerns that we be-lieve help to explain why AI principles do not easily translate intoconcrete and effective practices: 1) that AI\u2019s social and ethical impli-cations for human well-being are broader, more complex, and moreunpredictable than is often understood; 2) that accountability for eth-ical consequences is divided and muddled; 3) that the orientations ofexperts in different disciplines lead to emphases that are too narrow,too broad, and generally dif\ufb01cult for translation and interdisciplinarycommunication; 4) that existing methodologies and tools for respon-sible AI are hard to access, evaluate, and apply effectively; and 5) thatorganizational practices and norms which divide technical from non-technical teams minimizes the chance of developing well-consideredAI systems that can safeguard and improve human well-being.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "2c2f74a8-e39d-44c7-a926-28f80214ffab",
                    "text": "Given the proposed explanations above, how can we begin to closethe principles-to-practices gap? We think an overarching frame-work for responsible AI development can help to streamline practiceand leverage existing tools and methodologies. What would be thedesiderata of such a framework for responsible AI? As a startingpoint and based on the identi\ufb01ed gaps, we suggest the following:\u2022 Broad: it should consider AI\u2019s impacts expansively, across manydifferent ethical issues and aspects of social and economiclife. Narrower tools and methodologies such as bias mitigation,privacy-by-design, and product design documentation [27, 40] canthen be subsumed under this more comprehensive framework. Forexample, after identifying the scope of an AI system\u2019s impactsto human-wellbeing, designers could determine which lower-levelsub-tools and methodologies are relevant.\u2022 Operationalizable: it should enable users to cast conceptual prin-ciples and goals into speci\ufb01c strategies that can be implemented inreal-world systems. This includes identifying relevant actions anddecisions, assigned to the appropriate stage of an AI\u2019s lifecycle,e.g., use case conception, system development, deployment, mon-itoring. This also means identifying accountable parties for thesedecisions at multiple levels of governance \u2014 engineers, design-ers, lawyers, executives. This helps to ensure that accountabilityfor actions is assigned to those with the capacity to implementthem.\u2022 Flexible: it should be able to adapt to a wide variety of AI systems,use cases, implementation contexts, and organizational settings. A\ufb02exible framework has greater applicability to more kinds of AIsystems and use cases, allowing for shared language and learning,while enabling suf\ufb01ciently customization.\u2022 Iterative: it should be applied throughout the lifecycle of an AIsystem and repeatedly as the AI system, implementation context,or other external factors change, not only at one point. ResponsibleAI is not one-and-done.\u2022 Guided: it should be easy to access and understand, with suf\ufb01cientdocumentation for users of moderate skill to apply, customize, andtroubleshoot across different contexts. It should also be tested indifferent contexts with evidence of effectiveness made public.\u2022 Participatory: it should incorporate the perspectives and inputfrom stakeholders from a range of disciplines as well as those thatmay be impacted by the AI system, especially the public. Trans-lating principles \u201cinto business models, work\ufb02ows, and productdesign\u201d will be an ongoing effort that requires engineers, com-puter scientists, social scientists, lawyers, members of the public,and others to work together [36].A framework that meets these criteria balances the need for techni-cal speci\ufb01city with an equally important need for conceiving of AI\u2019simpacts on human well-being in their full breadth and complexity,understanding we cannot fully predict all of AI\u2019s possible rami\ufb01ca-tions. That is, while some prominent strategies for responsible AIassume there are only a small set of issues to address, such as bias,transparency, and privacy, we have argued that AI\u2019s impacts are morecomplex. We think that impact assessments are a promising strategy towardsachieving these criteria [9]. Impact assessments have been used his-torically in human rights [36], in regulatory contexts [53], and morerecently to study the impact of AI or algorithms [9, 55]. We focusspeci\ufb01cally on the recently published IEEE 7010 standard as an ex-emplar [43, 60] created speci\ufb01cally to assess AI\u2019s impacts on hu-man well-being. We argue below that impact assessments like thewell-being impact assessment from the IEEE 7010 standard could beadopted by companies pursuing responsible AI development as wellas incorporated into the institutions which train future practitioners.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "e6176ae0-3c72-4486-8dfd-250eda40c5f4",
                    "text": "Building on the IEEE 7010 standard, a well-being impact assessmentis an iterative process that entails (1) internal analysis, (2) user andstakeholder engagement, and (3) data collection, among other activi-ties. Internal analysis involves broadly assessing the possible harms,risks, and intended and unintended users and uses of an AI system.Here, developers and managers of an AI system carefully consider awide range of an AI system\u2019s potential impacts on human well-being,not limited to prominent issues like privacy, bias, or transparency.Critically, assessing impacts requires not just speculating aboutimpacts, but measuring them. Therefore, the user and stakeholderengagement stages of the assessment include learning from users ofAI systems as well as others more indirectly impacted to determinehow the system impacts their well-being. When developers have ac-cess to users, this may include asking them about possible or actualpsychological impacts, economic impacts, changes to relationships,work-life balance, or health. . This is again in contrast to strategieswhich focus solely on technical \ufb01xes to issues like bias or privacyduring the design stage alone and fail to account for the broader uni-verse of well-being implications.Finally, data collection based on the continuous assessment of theidenti\ufb01ed possible impacts is key. Here we refer to the collection andtracing of data related to the impact assessment, which may exceedcollection of data related to the development of the AI system itself.Data can be collected through user surveys, focus, groups, publicly-available data sources, or directly as system outputs. In sum, we pro-pose that adherence to \u2013 and rigorous documentation of \u2013 impact as-sessments will contribute to the continuous improvement of AI sys-tems by ensuring that organizations are better able to understand andaddress AI\u2019s many impacts on human well-being.Not all tools entitled \u2018impact assessment\u2019 meet our de\ufb01nition.Many existing tools consider only a small scope of possible impacts.Some fail to measure impacts at all, instead focusing on anticipatingimpacts assumed to be important and applying best practices to avoidassociated harms. Inversely, some tools that are not labelled \u2018impactassessments\u2019 might be classi\ufb01ed as such under our de\ufb01nition, suchas the European Commission\u2019s Ethics Guidelines for Trustworthy AI[19]. Notably, some frameworks have been proposed by the publicsector (i.e., governments) and others by non-governmental organiza-tions and companies.Why is impact assessment as we have de\ufb01ned it a promisingapproach? First, it can be highly broad, measuring many aspectsof human social and economic well-being and even environmen-tal impacts. IEEE 7010 is an exemplar in its breadth. It identi\ufb01estwelve domains as part of its well-being impact assessment: affect,community, culture, education, economy, environment, health, hu-man settlements, government, psychological/mental well-being, andwork [43]. For an AI system like a chatbot or autonomous vehi-cle, the impact assessment may lead to identi\ufb01cation of numerousareas of concern like social relationships, the environment, psycho-logical health, and economy. Thus while other responsible AI ap-proaches take into account a far narrower range of concerns, em-phasizing largely bias and transparency of algorithms, IEEE 7010\u2019swell-being impact assessment is far more broad-ranging.Impact assessments can also be highly operationalizable into spe-ci\ufb01c strategies. In IEEE 7010, overarching domains like human well-being or environmental impacts are not just stated in abstract terms,but are measured through speci\ufb01c indicators based on rigorous re-search. The strategy involves internal analysis, followed by user andstakeholder engagement, both used to determine domains where anAI system can impact human well-being. Next, AI system creatorscan identify measurable indicators related to each domain, followedby measuring the impacts of their AI system on the selected indica-tors. For example, through using the well-being impact assessment,an AI developer might identify the environment as an important con-cern, and increases in air pollution as a speci\ufb01c possible impact. Us-ing validated indicators to measure air pollution, the developer couldthen assess whether air pollution has increased or decreased.Next, given the extensive range of possible impacts that can bemeasured, there is also ample room to customize an impact assess-ment and make it suf\ufb01ciently \ufb02exible for particular use cases andorganizational contexts. ALGO-CARE is one such example of animpact assessment applied speci\ufb01cally to algorithms in the criminaljustice system [51]. ALGO-CARE considers context-speci\ufb01c issueslike whether human of\ufb01cers retain decision-making discretion and ifproposed AI tools improve the criminal justice system in a demon-strable way. Similarly, users of IEEE 7010 would \ufb01nd that their im-pact assessment approach could be customized to focus on issuesranging from housing to human rights. Impact assessments also typ-ically leave room to determine which actions are taken in responseto identi\ufb01ed impacts, meaning these responses can be applied in aniterative fashion, not only during the design phase. For example,concerns about impacts of an AI system on pollution could lead tochanges not only during an AI system\u2019s design, but also in terms ofits implementation in the real world. Moreover, it is impossible to \ufb01n-ish an impact assessment only during the design stage, as it requiresmeasuring its impacts in real-world settings.However, this breadth and \ufb02exibility suggest to us that guidanceis the most challenging issue currently. Simply, there is no one-to-one mapping from identi\ufb01ed impacts or problems with AI systemsto individual technical or implementation \u2018\ufb01xes\u2019 and creating sucha comprehensive mapping is likely not plausible. The breadth andcomplexity of an AI well-being impact assessment demonstrate thedif\ufb01culties for any actor who attempts to single-handedly close thegap from principles to practices. Thus, we propose that developingguidance for particular sectors, types of AI systems, and use casesis a necessary and ongoing effort which could leverage a participa-tory process-driven impact assessment approach that engages differ-ent groups of stakeholders. In particular, developers, policymakers, philosophers, intended and unintended users of the technology be-ing developed, and others could equally contribute in the AI impactassessment process, such as through interviews, focus groups, andparticipatory design methods [64].We are hopeful that more scholars and organizations focused onresponsible uses of AI will adopt an assessment approach that mea-sures a wide range of impacts on human well-being and meets the cri-teria identi\ufb01ed above. A key aspect of creating the supportive struc-ture for effective impact assessments will be adopting new educa-tional practices in institutions of higher education as well as organi-zational changes in \ufb01rms. We turn to these issues brie\ufb02y.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "32ebb8e0-6a0b-4eaf-b9ed-ed8601b5d0e4",
                    "text": "Educational institutions also have an important role to play. Educa-tional systems have undertaken meaningful efforts aimed at increas-ing ethical sensitivity and decision-making, but have not yet madethe changes needed to support responsible AI practice. Of around200 AI/ML/data science courses reviewed by Saltz et al. (2019), lit-tle more than 1 in 10 mentioned ethics in their syllabus or course de-scription. Those that did focused overwhelmingly on bias, fairness,and privacy [58]. While courses focused speci\ufb01cally on AI ethicscover a wider set of issues including consequences of algorithms,technically tractable issues like bias and privacy are still prominent[26]. We suggest that AI ethics education focus not solely on a fewprominent or technically tractable issues nor on general awarenessbuilding alone, but also on impact assessment as an overarchingframework to understand AI\u2019s impacts on human well-being.AI ethics and design courses should also recruit and serve studentsof social sciences and humanities (and other \u2018non-technical\u2019 \ufb01elds).Calls for more STEM education for these individuals often result inthem taking a small number of basic computer science or statisticscourses. We believe that more fundamental interaction with AI sys-tems is important to build capacity in these students, who should be\u201ccapable of grasping technical details\u201d in order to translate abstractprinciples and concepts from these \ufb01elds into concrete computer anddata ethics practices [31]. In turn, students in social scientists andhumanities can help to expand the scope of thinking of their coun-terparts in engineering and computer science. For example, AI ethicsand design courses can facilitate interdisciplinary teamwork that in-volves the use of impact assessments. Such an approach would allowstudents to understand the range of AI\u2019s impacts and practice apply-ing relevant tools and methodologies in response. Interdisciplinaryteaming could also occur through student extracurricular clubs andcontests (not limited to grand prizes) to encourage this kind of cross-disciplinary learning and practice.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "69d225ce-4ded-4deb-ad25-1e1d0166ba7b",
                    "text": "Analogous to the educational setting, companies developing or de-ploying AI should move towards the integration of technical and non-technical teams rather than functional separation of roles, for reasonsdiscussed in the previous section. These integrated teams could in-clude technical developers as well as other individuals tasked withconsidering impacts of an AI system who may have social science,humanities, business, law, or ethics expertise, or who can represent atypical user\u2019s perspective effectively. Such a change requires estab-lishing practices that are integrated with engineering and softwarelifecycles and part of the ongoing dialogue characteristic of devel-opment processes. Already, organizations have proposed includinga residential non-technical thinker tasked with responsible AI \u2014 an\u2018ethics engineer\u2019 or \u2019responsible AI champion\u2019 [52].However, we would urge that these integrated teams not remainat an arm\u2019s distance in a way that maintains bifurcated expertise ar-eas and roles. Instead, technical and non-technical team membersshould aim learn each other\u2019s languages and work jointly. For ex-ample, an AI development team could include ethnographers, policyscholars, or philosophers, all tasked with applying a broad impact as-sessment as the AI system is being created and implemented. Whilethese changes to organizational practice may be dif\ufb01cult, requiringindividuals to stretch their boundaries, we believe that a deep levelof integration is necessary to bridge the disciplinary divide.Organizations could also engage in interdisciplinary and interde-partmental cross-training, potentially supported by responsible AIchampions or external experts. For example, organizations could fa-cilitate red team exercises [8] or hypothetical case studies that drawon the impact assessment approach. Practicing even on hypotheti-cal cases allows social science-oriented practitioners and technically-oriented practitioners to learn from one another about how they cande\ufb01ne problems, consider solutions, de\ufb01ne terminology, etc. This canhelp diverse disciplinary practitioners begin to learn and establishcommon language and identify gaps and opportunities in each other\u2019spractice.In summary, we have argued that impact assessments are a promis-ing strategy to address the gaps between principles and effectivepractices for responsible AI. However, applying an impact assess-ment might feel like an abstract exercise to those who have not doneit. To demonstrate how closing the principles-to-practices gaps withan impact assessment might occur, we move now to a case study.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "b2141956-e032-4171-90f5-73f22131fc2a",
                    "text": "In this section, we set out to explore how the recommendations in-troduced above could be implemented within a particular setting. Wehope this case study will help practitioners in adapting our research\ufb01ndings to the unique sociotechnical context within which their ownwork is situated. In the example case study below, we look at AIsystems that are being used to address forest ecosystem restoration.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "377e2b60-80da-4608-bce7-27493fb91a61",
                    "text": "As is characteristic of the SDGs, achieving goals in one area \u2013 likethe environment \u2013 also has effects on multiple other goals, such asaddressing health and poverty targets. Forest restoration is one suchaspect of the SDGs. While it has clear importance to SDG 13 (Cli-mate Action) and SDG 12 (Responsible Consumption and Produc-tion), forest ecosystem restoration is addressed most directly by SDG15 (Life on Land). SDG 15 states a global ambition to \u201cProtect, re-store and promote sustainable use of terrestrial ecosystems, sustain-ably manage forests, combat deserti\ufb01cation, and halt and reverse landdegradation and halt biodiversity loss\u201d [45].Forest ecosystem restoration is therefore essential for many rea-sons. Forests have the most species diversity on the planet, with some80% of land-based species. Forests also reduce the risk of natural dis-asters such as \ufb02oods, droughts, and landslides and help protect wa-tersheds [45]. Further, forests are critical for mitigating land-basedcarbon emissions by increasing carbon sequestration, critical for cli-mate change prevention goals [46]. Project Drawdown, for example,has calculated that the restoration and protection of tropical forestscould lead to 61.23 gigatons of carbon reduction by 2050 [17]. Achieving these goals requires the restoration of forest ecosystemsthrough the cultivation of trees, known as afforestation [16]. Appliedafforestation projects typically involve three stages - planning, execu-tion, and monitoring of ecosystem restoration. Several AI technolo-gies have been used in afforestation efforts and their use is increasing.During planning, AI systems have been used to predict forest carbonsequestration potential through the use of satellite and drone imagedata [47, 54]. AI can also facilitate execution of afforestation throughcomputer vision algorithms used in identifying appropriate plantingsites, monitoring plant health, and analyzing trends [18]. Lastly, inthe monitoring stage of restoration projects, AI can be used to iden-tify where deforestation may have been conducted illegally [32, 38],as well as assess risks due to \ufb01re, disease, insects, or other causes[62].",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "30181d9c-1db7-4d3c-984b-7cde0bde0ad7",
                    "text": "While AI thus has great potential to contribute to SDG efforts andsocial good in this case, there are complications with translating theaforementioned goals into responsible practices. We focus on onespeci\ufb01c challenge leading to a gap in responsible AI practice \u2013 theissue of multi-stakeholder coordination.According to international governance efforts like the UN SDGs,the UN Forum on Forests, Agenda 21, and the Future We Want (theoutcome document of the Rio+20 Conference) there is a need forholistic, multi-stakeholder engagement to address forest ecosystemrestoration adequately [10, 67]. This is due to the existence of mul-tiple groups with critical interests in forest ecosystems. Local com-munities and businesses may engage in harvesting timber, farming,and industrial exploitation to produce resources and support localeconomies. Natives living off the land have an essential stake, as theirlivelihood may depends on hunting animals and harvesting plants andother materials. Government of\ufb01cials tasked with maintaining forestsor woodlands need to monitor the quantity and kind of trees to har-vest, and NGOs focused on conservationism may attend to animallife and biodiversity as well. Finally, policymakers must also worryabout carbon sequestration and climate change efforts.Though the goals of these groups are not always in con\ufb02ict, theycan come from different perspectives and have competing priorities.Therefore, AI-driven systems used for afforestation that do not takeinto account these \u201cmultiple ecological, economic, social and cul-tural roles\u201d important to various stakeholders [45] may lead to blindspots and unintended harms. For example, an AI system that usesimaging data to determine carbon sequestration potential could op-timize climate change goals in a narrow sense, but fail to accountfor social-ecological aspects of the land important to indigenousgroups, or ignore endangered species important to conservationists.This could engender a lack of coordination and collaboration amongstakeholders and lead to costly delays and con\ufb02ict, as parties are un-willing to accept afforestation efforts or even work actively againstthem.As a result, carbon sequestration targets optimized in the shortterm could fall short in the long term as afforestation progress fails totranslate into a sustainably managed multi-stakeholder effort. Failingto develop and implement AI systems for ecosystem restoration in aparticipatory fashion is thus an example of how the laudable goal ofimproving environmental well-being can fail to translate into respon-sible and effective practices.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                },
                {
                    "id": "f97ef9d8-5caa-4f7f-8478-cf6d47e400d8",
                    "text": "It is therefore important for developers of AI systems to consider thatnumerous groups have stakes in forest ecosystem restoration. As dis-cussed by Rolnick et al. in the case of AI [56], \u201cEach stakeholderhas different interests, and each often has access to a different por-tion of the data that would be useful for impactful [machine learn-ing] applications. Interfacing between these different stakeholders isa practical challenge for meaningful work in this area.\u201d Landowners,policymakers, public and private sector organizations, local commu-nities, and others need to have a voice in the application of AI toforest ecosystem restoration.How would AI impact assessments such as the IEEE 7010 well-being impact assesssment help in this instance? As discussed in Sec-tion 3, the assessment process involves a broad internal analysisby the organizations developing AI systems for forest ecosystemrestoration. This would involve trying to understand the variety ofpossible stakeholders and intended or unintended impacts of theirproducts. A company that develops AI to identify target areas for af-forestation given carbon sequestration potential might recognize pos-sible impacts on species diversity, the local economy, and the generalwell-being of native groups.In order to have a more accurate picture \u2013 as well as to build con-sensus among stakeholders \u2013 the company would then begin the userand stakeholder engagement process. This would involve talking tolocal governments procuring the company\u2019s AI systems about theneed for a holistic implementation of afforestation efforts. Critically,it would involve soliciting the input of the numerous stakeholdersmentioned such as conservation groups, landowners, scientists, gov-ernment of\ufb01cials, local businesses, and native populations. For ex-ample, a method like participatory action research or other partici-patory design methods [64] could be used to facilitate this engage-ment.This process, which should be ongoing and iterative throughoutthe management of the forest ecosystem, should surface a numberof clear concerns about possible implications of the afforestation ef-forts. For example, the company may have originally been optimiz-ing a target through their AI system such as SDG indicators 15.1.1,\u201dForest area as a proportion of total land area,\u201d or 15.3.1, \u201dProportionof land that is degraded over total land area.\u201d However, the impactassessment process should lead to the \ufb02exible identi\ufb01cation of newindicators critical to having a broader understanding of the broadersocial, economic, and ecological context.These new indicators \u2013 re\ufb02ecting economic, health, and gover-nance concerns as well as environmental ones \u2013 could include, forexample, SDG indicators 3.3.5, \u201dNumber of people requiring inter-ventions against neglected tropical diseases,\u201d 1.5.2, \u201dDirect disastereconomic loss in relation to global gross domestic product,\u201d or 11.3.2\u201dProportion of cities with direct participation structure of civil soci-ety in urban planning and management that operate regularly anddemocratically.\u201d These and other indicators, not necessarily pickedfrom the SDG indicators, would therefore operationalize possibledimensions and impacts of the forest ecosystem management effort\u2013 disease, natural disasters, and participatory governance \u2013 as spe-ci\ufb01c measurable indicators. The company in collaboration with part-ners would endeavor to measure these impacts, not merely carbonsequestration or forest area as a proportion of land area.Finally, the company in collaboration with partners, would haveseveral ways to use this new and deeper understanding of the well-being implications of their AI system. One such approach would beembedding this expert domain knowledge garnered from the partici- patory process into the architecture of the AI system itself [35]. Forexample, an AI system that previously optimized carbon sequestra-tion potential as part of its objective function could incorporate newdata regarding tropical diseases or natural disasters as additional con-straints or targets in the optimization of its model.However, not all efforts to address the identi\ufb01ed well-being im-pacts need be strictly technical in nature. Changes to organizationalpractices and governance strategies are likely called for. For example,the company might \ufb01nd that accounting for species diversity directlywithin the model is not suf\ufb01ciently nuanced. Instead, the companycould bring initial recommendations about carbon sequestration tar-get areas to a multi-stakeholder governance board. The board couldthen offer feedback on the suitability of the recommendations givenspecies diversity or native land usage. While the impact assessmentprocess and identi\ufb01cation of solutions would initially feel unfamiliarand complex, the company would gradually develop best practicesand guidance towards a more responsible application of its AI sys-tem for forest ecosystem restoration.While this case study \u2013 the use of AI for forest ecosystem restora-tion \u2013 is based on real uses of AI and associated real-world chal-lenges, the speci\ufb01c indicators and actions taken by the company arehypothetical. We do not mean to suggest that there are not compa-nies or governments already taking thoughtful approaches to multi-stakeholder governance in this area. However, to the best of the au-thors\u2019 knowledge, current sustainability efforts have not yet incor-porated impact assessments of AI-driven technological solutions ap-plied to ecosystem restoration. We hope this case study helps demon-strate how impact assessments are a promising tool to close theprinciples-to-practices gap towards responsible AI.ConclusionIn this paper, we reviewed and synthesized explanations for the gapbetween high-level responsible AI principles and the capacity to im-plement those principles in practice. We identi\ufb01ed \ufb01ve explanationsfor the gap, related to the complexity of AI\u2019s impacts on well-being,the distribution of accountability, socio-technical and disciplinary di-vides, a lack of clarity and guidance around tool usage, and func-tional separations within organizations that preclude effective inter-disciplinary practices.Next, we considered the criteria of a framework likely to help closethe principles-to-practices gap, and identi\ufb01ed that impact assessmentis one such approach. An impact assessment approach to responsibleAI, unlike some alternative approaches, has the potential to be broad,operationalizable, \ufb02exible, iterative, guided, and participatory. Afterreviewing the bene\ufb01ts of impact assessment and the well-being im-pact assessment approach of IEEE 7010, we suggested changes thateducational institutions and companies can make to supportive effec-tive and responsible AI practices.Finally, we considered the use of AI in forest ecosystem restora-tion efforts. In the face of complex impacts and possible con\ufb02ictsbetween stakeholders that could inhibit sustainable forest manage-ment efforts, impact assessment offers promise for those wishing toclose the principles-to-practices gap.",
                    "reference": "[1] David Schiff, Beena Ammanath, Amanda Ayesh, Antonietta Fanti, Andrew Zaldivar. 2020. Principles to practices for responsible AI: closing the gap. arXiv:2006.04707. Retrieved from https://arxiv.org/pdf/2006.04707"
                }
            ]
        },
        {
            "paper_title": "Responsible AI by design in practice",
            "authors": "R Benjamins, A Barbado, D Sierra",
            "publication_info": "arXiv preprint arXiv:1909.12838 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/1909.12838",
            "chunks": [
                {
                    "id": "eb0b93bf-131e-42b3-be3d-e49f99284aee",
                    "text": "",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "01f3d6b7-98dd-4283-b21c-d3467a834aa7",
                    "text": "Artificial Intelligence (AI) is on the rise. It can be applied to  many  different  domains  such  as  content  recommenda-tions,  chatbots,  image  recognition,  machine  translation, fraud  detection,  medical  diagnosis,  autonomous  vehicles, legal, education, transport, and logistics to name just a few. It  can  not  only  be  used  for  business,  but  also  for  social purposes  such  as  better  understanding  and  reducing  the impact  of  climate  change,  natural  disasters,  and  migration (Globalpulse 2017).   However, recently several concerns have been expressed about  the  use  of  AI,  in  particular  related  to  potential  dis-crimination  (bias,  discrimination,  predictive  parity) (O\u2019Neill  2016),  interpretability  of  algorithmic  conclusions (explainability,  black  box  problem)  (Samek,  Wiegand, M\u00fcller  2017,  Guidotti  et  al  2018,  Pedreschi  et  al  2018) transparency of data used (Gross-Brown 2015), impact on jobs  (Manyika  and  Sneader  2018)  liability  questions (Kingston 2016), and malicious use of the technology (Pis-tono and Yampolskiy, 2016).    With the increasing uptake of AI technologies in organi-zations, many start to worry about those concerns, and are wondering how to prepare themselves to avoid unintended negative  consequences.  While  some  of  the  concerns  are outside  the  scope  of  private  enterprises  and  require  gov- ernments to act (such as impact on jobs, liability and mali-cious  use),  others  need  to  be  addressed  at  the  individual company level.  In this paper, we will describe how a large organization as  Telefonica  (24  countries,  127.000  employees,  343  mil-lion  accesses,  52  billion  in  revenues)  is  approaching  this problem.  In  the  next  section  we  briefly  describe  related work.  Then  we  present  the  AI  principles  that  Telefonica has  developed  for  responsible  AI.  Next,  we  present  the \u201cResponsible  AI  by  Design\u201d methodology  for  implement-ing  the  Principles  in  our  organization.  After  that,  we  pre-sent  our  experience  with  the  implementation  of  the  meth-odology in our organization. Then we summarize the chal-lenges  we  encountered  as  well  as  some  topics  that  need further research. Finally, we present a conclusion.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "c0943427-a665-4944-94e3-25dd16fb0953",
                    "text": "Several  works  have  been  published  about  the  ethical  and societal impact of  AI. (Anderson and Anderson 2007) de-scribe  how  machine  behavior  is  ethically  acceptable  by human users. (Boden et al 2017) proposes a set of princi-ples to be included in a potential regulation for the design of robots. (Bryson and Winfield 2017) propose the use of standardization such that AI and Autonomous systems ex-hibit the required algorithmic transparency. (Dignum 2018, 2017)  distinguishes  between  Ethics  by  /  in  /  for  Design, where ethics in design refers to \u201censuring that development processes  are  aligned  with  ethical  principles.\u201d  (Gumbus and Grodzinsky 2015) review the relation of AI algorithms with undesired discrimination and relate that to privacy of people in a HR context of organizations.     However,  few  publications  exist  on  how  companies should go about the creation of AI systems while respect-ing  the  ethical  and  societal  implications  they  might  have. The work that comes closest  to this is IBM\u2019s work on AI and  Ethics  (IBM  2018a).  The  European  Commission  has published  Ethics  guidelines  for  trustworthy  AI  (EC  2018) proposing  an  assessment  check  list  for  AI  practitioners based  on  seven  principles:  Human  agency  and  oversight; technical  robustness  and  safety;  privacy  and  data  govern-ance;  transparency,  diversity,  non-discrimination  and  fair-ness; societal and environmental wellbeing; accountability.   The principles we are presenting are broadly in line with the EC principles, even though the EC  published them six months later. The main differences stem from:  \u2022  Our principles focus on the aspects that make AI different from other technologies unless they play a key role for AI such as privacy and security, in which case we refer to existing company practices rather  than  defining  things  again.  The  EC  guide-lines are more inclusive of all aspects involved in end-to-end AI.   \u2022  The  EC  guidelines  aim  to  cover  any  sector, whereas  our  principles-  while  broader  than  only for  telecoms-  do  not  cover  aspects  clearly  out  of the  telecom  scope.  This  means  for  instance,  that there  is  no  AI  principle  for  \u201csafety\u201d  or  \u201crobust-ness\u201d. Of course, safety and robustness are key el-ements  in  our  operations  (e.g.  of  mobile  anten-nas), but this is not specifically related to AI. \u2022  As discussed in this paper, our principles are part of  a  methodology  which  includes  a  governance model.  Some  aspects  of  the  EC  guidelines  are captured  in  this  governance  model,  such  as  \u201cac-countability\u201d.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "82c6702e-68b5-4b10-8454-871517c56cd1",
                    "text": "As  many  businesses,  Telefonica  is  strongly  committed  to respecting Human Rights, as is stated in its Business Prin-ciples and Human Rights Policy. This includes a commit-ment to developing products and services aimed at making the world a better place to live and mitigating any negative impacts  technology  may  have  on  society  or  the  environ-ment.  Technology  should  contribute  to  making  society more inclusive and offer better opportunities for all, and AI can contribute to these goals. In order to guide the organization in its uptake of AI and Data  across  the  business,  Telefonica  has  published  its \u201cPrinciples  of  AI\u201d  (Telefonica  2018).  The  principles  in-clude: \u2022  Fair AI seeks to ensure that the applications of AI technology  lead  to  fair  results.  This  means  that they should not lead to discriminatory impacts on people (Stoyanovich, Abiteboul,  Miklau 2016) in relation  to  race,  ethnic  origin,  religion,  gender, sexual orientation, disability or any other personal condition.  When  optimizing  a  machine  learning algorithm, we must take into account not only the performance  in  terms  of  error  optimization,  but also  the  impact  of  the  algorithm  in  the  specific domain. \u2022  Transparent  and  Explainable  AI  means  to  be explicit  about  the  kind  of  personal  and/or  non-personal data the AI systems uses as well as about the purpose the data is used for. When people di-rectly  interact  with  an  AI  system,  it  should  be  clear  to  the  users  that  this  is  the  case.  When  AI systems take, or support, decisions, a certain level of  understanding  of  how  the  conclusions  are  ar-rived  at  needs  to  be  ensured  (Samek,  Wiegand, M\u00fcller  2017),  by  generation  explanations  about how they reached that decision, like is illustrated in  (Gilpin  et  al.,  2019)  for  the  particular  case  of supervised  machine  learning.  Those  explanations should  always  consider  the  user  profile  to  adjust them to the transparency level required, as stated in (Theodorou, Wortham, Bryson, 2017). This al-so applies in case of using third-party AI technol-ogy. \u2022  Human-centric  AI  means  that  AI  should  be  at the service of society and generate tangible bene-fits for people. AI systems should always stay un-der  human  control  and  be  driven  by  value-based considerations.  AI  used  in  products  and  services should  in  no  way  lead  to  a  negative  impact  on human rights or the achievement of the UN\u2019s Sus-tainable Development Goals. \u2022  Privacy  and  Security  by  Design  means  that when  creating  AI  systems,  which  are  fueled  by data, privacy and security aspects are an inherent part of the system\u2019s lifecycle. This maximizes re-specting  people\u2019s  right  to  privacy  and  their  per-sonal data. Notice that the data used in AI systems can be personal or anonymous/aggregated. Notice also  that  this  principle  is  broader  applicable  than only  to  AI  systems,  and  most  organizations  al-ready  have  processes  in  place  to  ensure  proper privacy and security.  The  Principles  are  by  extension  also  applicable  when working with partners and third parties.  Those Principles are based on a broad consensus in ex-pert  communities  that  have  sparked  profound  conversa-tions  and  discussions  about  the  impact  of  AI  in  society. One  just  has  to  read  the  national  strategies  of  different countries on AI. There are other concerns about AI such as the  impact  of  jobs,  liability,  malicious  use  or  AI  for  war-fare,  but  these  fall  outside  the  acting  scope  of  private  or-ganizations and are in the realm of governments and insti-tutions.  It  is  one  thing  to  come  up  with  a  set of guiding  princi-ples  for  how  to  design,  develop  and  use  AI  in  organiza-tions, but it is another thing how to implement those prin-ciples across organizations such that they have the desired effect. While there is significant consensus on the concerns of AI underlying the principles, there is less experience on how this can be practically implemented.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "bf3be9c3-bf73-49e1-8360-a033b6e9e9a2",
                    "text": "Given how new the use of AI is in organizations, specific new training material needs to be developed for answering questions  related  to  Transparent  &  Explainable  AI.  Some of the questions are not specific to AI and we can refer to existing training material in the organization. This includes the Principles related to Human-centric AI and Privacy & Security  by  Design.  For  some  questions  such  as  those  re-lated to \u201chuman-centric AI\u201d, \u201cPrivacy\u201d and \u201cthird-parties\u201d it is possible to refer to existing templates (ADAPT 2017, ICO,  Hind  2018)  that  can  be  reused  for  training  purposes and  for  the  actual  work  as  well.  In  Table  2,  we  show  the essential content of the different training components to be developed.  Ideally,  to  ensure  \u201cexplainable\u201d  AI,  technical tools  would  be  needed,  in  addition  to  training,  to  support developers.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "955eabec-e0d5-490a-9356-78b56e32e877",
                    "text": "",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "38157a76-5a34-457a-a48e-3dd2fed6781a",
                    "text": "We have taken the principles a step further and have turned them  into  a  methodology  for  creating  Responsible  AI  by Design, in the tradition of Privacy and Security by Design.  The methodology consists of the following ingredients \u2022  The  AI  principles  setting  the  values  and  bounda-ries \u2022  A set of questions and check points, ensuring that all AI principles have been considered in the crea-tion process \u2022  Tools that help answering some of the questions, and help mitigating any problems identified \u2022  Training, both technical and non-technical \u2022  A  governance  model  assigning  responsibilities and accountabilities Designing  the  methodology  requires  a  cross-enterprise initiative involving different departments such as Engineer-ing, Security, Legal, Business, IT, Human Resources, Pro-curement, as well as an endorsement of top management.  Table  1  below  illustrates  how  the  Principles  are  imple-mented  in  the  organization.    For  each  principle,  several questions are defined that must be answered by the respec-tive  responsible  persons.  Several  of  the  questions  require certain  understanding  of  AI  and  Machine  Learning,  and therefore specific tools and training are required. The next sections will indicate the details and proposals for each of the ingredients mentioned before.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "d816d3b2-4810-4a9d-9c10-9dd4a96190ed",
                    "text": "Many of the tools required to support \u201cResponsible AI by Design\u201d  are  still  in  early  stage,  though  they  are  being de-veloped  quickly  and  its  expected  that  the  companies  in-clude  them  soon  in  their  processes.  The  technical  tools proposal that appears within this methodology spans from the usage of stand-alone tools to the reference of more aca-demic research solutions in order to offer a complete port-folio that could be useful to all the profiles within the com-pany. Table  3  Summarizes  the  key  functionalities  of  the  re-quired tools.  Fair AI Regarding Fair AI, some of solutions exist for internal use such  as  Facebook\u2019s  Fairness  Flow  tool  (CNET  2018), while  others  such  as  IBM\u2019s  Fairness  360  toolkit  (IBM 2018b) and Accenture\u2019s AI Fairness tool (Accenture 2018) are for any organization to use. Some tools are open source such  as  IBM\u2019s  tool,  a  tool  from  Pymetrics  (2018),  and  a tool  from  the  University  of  Chicago  (Aequitas  2018).  As these  tools  focus  on  \u201cfair  AI\u201d  their  mission  is  helping  to detect  and  avoid  undesired  discrimination  through  bias. Typical  functionalities  include:  detecting  bias  in  data  sets related  to  sensitive  data  (impacting  protected  groups),  de-tecting  correlations  in  data  sets  between  normal  variables and  sensitive  variables,  detecting  unbalanced  outcomes  of algorithms  within  sub  groups  of  the  population  and  miti-gating the effect of the bias. The concept of Fair AI has obtained a lot of relevance in the last years. Many new studies are published every year dealing with the problem unfair AI and how to mitigate the inclusion  of  new  biases  in  the  decision-making  process. There is some confusion about why bias arises and even if the bias is something natural that must be preserved in or-der to maintain the \u201cpurity\u201d of original data. The potential causes  of  bias  were  first  registered  by  (Selbst,  Barocas 2016).  They  listed  some  causes  why  a  machine  learning model can give unfair results\u2022  Skewed data: Incorrect assumptions about the da-ta generation process. It occurs when the data ac-quisition process is biased. \u2022  Tainted  data:  Incorrect  problem  definition  and target labeling.  \u2022  Limited features: When the number of features is so  limited  that  bias  is  induced  in  some  sensitive attributes \u2022  Sample  size  disparities:  Unequal  sample  sizes  of different sensitive groups  \u2022  Proxy  features:  Presence  of  correlated  variables within  the  problem  that  induces  bias  even  when the sensitive features are removed. There are several definitions of Fairness provided in the literature: unawareness, group fairness, individual fairness and  counterfactual  fairness.  The  definition  of  fairness through  unawareness  consists  of  removing  the  sensitive variable  from  input  data.  This  can  be  insufficient  because the presence of proxy features implicitly maintains the in-formation of the deleted sensitive variable. Group  fairness deals with Fairness from the perspective of all individuals, while individual fairness tries to model the differences be-tween each subject with the rest of population. The case of counterfactual  fairness  goes  one  step  beyond  trying  to  in-terpret the causes of bias via causal graphs. (Hardt et al. 2016) propose a framework for group fair-ness  in  which  three  different  criteria  can  be  employed  to evaluate a supervised ML model in terms of Fairness: \u2022  Independence: It is achieved when the model pre-diction  is  independent  of  the  sensitive  variable, that  is,  the  proportion  of  Positive  samples  given by the model is the same for all sensitive groups. \u2022  Separation:  Also  known  as  Equalized  Odds.  It  is achieved when the model prediction is independ-ent of the sensitive variable given the target varia-ble,  that  is,  when  the  TP  (true  positive)  rate  and the  FP  (false  positive)  rate  are  equal  in  all  sensi-tive groups, respectively. \u2022  Sufficiency: Also known as Predictive Rate  Pari-ty. It is achieved when the target variable is inde-pendent of the sensitive attribute given the model output, that is, when the Positive Predictive Value is the same in all sensitive groups. It  is  impossible  to  achieve  all  three  criteria  at  the  same time, but they can be optimized jointly in order to optimal-ly mitigate bias in ML models. In  terms  of  Fairness  in  ML,  two  main  actions  can  be considered:  evaluation  and  mitigation.  The  former  is  the process  of  measuring  and  quantifying  the  amount  of  bias present  in  the  model  (in  terms  of  one  or  several  criteria), and  the  latter  is  the  process  of  fixing  some  aspects  of  the model  in  order  to  reduce  or  remove  the  effect  of  bias  in terms of one or several sensitive attributes. For  evaluation,  several  metrics  have  been  proposed  in the last years for the different fairness criteria. In the case of  the  Independence  criterion,  possible  metrics  include statistical parity difference or disparate impact for the case of  independence.  For  the  Separation  criterion,  possible metrics include equal opportunity difference and the aver-age odds difference (Hardt et al. 2016).  Another metric is the Theil index (Speicher el al. 2018)  which measures the inequality not only in terms of the group fairness, but also in terms of the individual fairness. For  the  mitigation  phase,  several  techniques  can  be found  in  the  literature.  In  this  part,  there  are  three  main groups of techniques. \u2022  Pre-processing:  These  techniques  are  applied  be-fore  the  machine  learning  algorithm  is  trained  in \u2022  order  to  remove  biases  in  the  very  early  stage  of the learning process. In-processing:  These  techniques  are  applied  dur-ing the training process by including Fairness op-timizations  constraints  along  with  cost  functions in ML models. \u2022  Post-processing:  Employed  after  the  algorithm  is built,  these  are  the  less  intrusive  techniques  be-cause they don\u2019t modify the input data or the ML algorithm.  This  technique  is  especially  adequate for mitigating biases in models that already exist. Each  technique  applies  the  mitigation  process  in differ-ent  phases  of  a  typical  analytics  pipeline  and  the  choice must be made to fit the particular case, but in terms of per-formance  it  is  better  to  apply  pre-processing  or  in-processing techniques. A  very  simple  pre-processing  technique  is  Reweighing (Kamiran  and  Calders,  2012)  consisting  of  modifying  the weights  of  samples  in  order  to  remove  discrimination  in sensitive attributes. Another example is the technique pro-posed by (Zemel et al. 2013) in which they transform input data  in order to find a good representation that obfuscates information about the membership in the sensitive groups. Another popular algorithm to mitigate bias is Adversari-al Debiasing (Zhang et al.  2018). This in-processing tech-nique tries to maximize the ability of predicting the target variable  while  minimizing  the  ability  of  predicting  sensi-tive variables through GAN. Also, the case of Equalized Odds post-processing (Hardt et al. 2016) is a good example of mitigation through a post-processing  technique.  In  their  proposal,  they  try  to  adjust the  thresholds  in  a  classification  model  in  order  to  reduce the  differences  between  the  True  Positive  Rate  and  False Positive Rate for each sensitive group. Transparent & Explainable AI (xAI) Explainable AI is something that has existed in the litera-ture for many years. Expert Systems were among the first AI systems that dealt with it, trying to use some of the mul-tiples rules generated as a base for the explanations provid-ed.  This  was  the  case  of  MYCIN  (one  of  the  first  Expert Systems  for  diagnosing  meningitis,  Clancey  1983)  which was able to give the following types of explanations:  why was  a  given  fact  used?  Why  was  a  given  fact  not  used? How  was  a  given  conclusion  reached?  How  was  it  that another  conclusion  was  not  reached?  The  conclusions were explained in a human-understandable manner by trac-ing back from the conclusions to the initial state. In  the  context  of  Machine  Learning,  there  are  several proposals for xAI. Supervised ML models can be classified into two groups: white box models and black box models. On  the  one  hand,  white  box models,  such  as  simple  deci-sion trees or linear regressions, are models that can gener-ate comprehensive explanations based on the model itself. On  the  other  hand,  black  box  models,  such  as  complex deep  learning  (DL)  architectures,  can't  provide  direct  ex-planations  for  the  decision  taken  by  the  system  in  a  way  that is comprehensive for a human being. The main issue is that there is a tradeoff between complexity and explainabil-ity: more complex models can potentially be more precise, but  in  exchange  the  model  is  opaquer.  To  be  able  to  use more complex models while being able to generate expla-nations  that  can  be  understood,  there  are  different  pro-posals available depending on the explanations desired. One example is explaining why a data point is classified within one of the categories used by the supervised model. For that purpose, there are libraries such as LIME (Ribeiro, Singh, Guestrin, 2016) that do not need information about the  model  itself  (model-agnostic).  Other  proposals  like Layer-wise  Relevance  Propagation  (LRP)  (Bach  et  al., 2015)  obtain,  for  deep  learning  models,  the  contributions of the different input features to a classification by aggre-gating the contributions of each of the layers present. This second  group  of  solutions  are  known  as  model  specific. (Samek, Wiegand, M\u00fcller 2017)  provide several examples of  applying  LRP  to  image  recognition.  A  system  trained with pixels of an image for classifying it as containing a \"a cat\" or \"not a cat\", will highlight the relevant pixels associ-ated  to  the  category  chosen.  Then,  a  person  can  see  the conclusion (e.g. a cat) and can inspect the highlighted pix-els to see if they really corresponded to the pixels of the cat (and  thus,  the  system  can  be  trusted).  Otherwise,  if  they correspond  to  something  not  related  to  a  cat,  even  if  the accuracy of the algorithm is high, the system should not be trusted.  This  approach  can  be  applied  to  a  variety  of  do-mains and data structures (tabular, texts...). There are many situations when the desired explanation is not about an individual classification but about how the whole  model  works  and  what  the  main  features  are  that have  contributed  to  the  training  phase.  To  address  that, there  are  several  solutions,  such  as  surrogate  models  im-plemented  through  libraries  such  as  Skater  (Oracle  2017). This  solution  uses  a  white  box  model  such  as  a  decision tree  trained  with  the  predicted  outputs  of  the  black  box model  (instead  of  the  real  values)  while  using  the  same input features. This trained white box model then serves as a  simple  but  useful  way  to  understand  the  most  relevant features of the black box model, regardless of the complex-ity of the model itself. See (Guidotti et al 2018) for a sur-vey of methods for explaining black box models. Though most of the research work addresses supervised learning, there are also proposals in the scientific literature for other kind of models such as for reinforcement learning (Anderson et al., 2019). Finally, some xAI tools developed by  big  tech  companies  are  What-If  (Google,  2018)  and Interpret (Microsoft, 2019). Privacy & Security by Design Regarding  the  Privacy  &  Security  by  design  principle,  an example of a tool that contributes to privacy by testing the risk of re-identification in case the data set is anonymized, even if differential privacy techniques are used, is (Dwork, Roth, 2014). Related to security, one aspect to consider is if the sys-tem is robust against attacks that seek to exploit weakness-es and manipulate its outputs. There are techniques, such as Adversarial Examples (Goodfellow, Shlens, Szegedy, 2014), that seek to cheat and manipulate the outputs of a model. In the case of a supervised ML, this takes place by checking the minimum changes in the input data that would cause different classifications. This has happened, for example, with computer vision systems of autonomous vehicles; with slight changes in a stop sign, which go un-noticed by the human eye, the systems detected them in-stead as speed limit signs of 45 mph (Eykholt et al., 2018). Some tools available to test and even secure the system against those attacks are for example Cleverhans (Papernot et al, 2016) for deep learning models, AlfaSVMLib (Xiao et al., 2015) for SVM models, AdversarialLib for evasion attacks (Biggio, Corona et al., 2013), and even for unsu-pervised learning such as clustering algorithms (Biggio, Pillai et al., 2013).",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "b757b3c5-7124-45e5-aa07-166e40fc7b23",
                    "text": "The  implementation  of  the  methodology  is  no  different from  implementing  other  technology-related  policies  such as Security & Privacy by Design, but given how unknown the  innerworkings  of  AI  are  for  the  wider  audience,  it might cause additional challenges.  Firstly,  the  company  needs  to  start  an  awareness  cam-paign explaining what AI is, why and how it is used in the company,  and  what  the  challenges  are.  The  campaign  in-troduces the Principles, the methodology, the training pro-gram and the tools.  Secondly,  the  training  program  starts  with  the  people closest  to  designing  and  developing  products  &  services that  use  AI,  including  the  procurement  people  who  deal with buying technology from third parties. In a later phase, training can be extended more widely across the organiza-tion, but always starting with people who are most likely to get involved in AI-related initiatives. Training ranges from very  technical  training  to  non-technical  training,  for  in-stance non-technical training for call center or social media agents who might receive customer inquiries related to AI. We  have  developed  an  online  course  of  about  one  hour where we explain to employees  basic AI concepts such as Machine  Learning,  the  potential  undesired  impact  of  AI such  as  bias,  the  AI  principles,  technical  tools  and  ques-tionnaires,  the  governance  model,  and  several  practical examples of assessing products against the principles.  De-pending on the role employees have, they will see more or less technical details.  As discussed (see Table 1 and Table 3), several specific tools are needed to support the implementation of Respon-sible AI in an organization. Without those tools, it is very hard  to  make  justifiable  statements  about  fairness  or  ex-plainability.  We believe that as a start, self-control is more effective than  control  through  specific  committees.  Therefore,  we start with an agile governance model, delegating as much as possible responsibility to the people responsible for the concerned products and services, and providing  them with support in case a question is raising concern. Making peo-ple  aware  of  the  concerns  and  providing  them  with  con-crete  training  and  tools  will  help  changing  the  culture  to creating  Responsible  AI  by Design.  In  this  way,  we  man-age to \u201csensibilize\u201d the organization without imposing a lot of controls. Such awareness is also important for potential future  regulation.  The  governance  model  only  covers  the part  that  is  specific  for  AI.  For  privacy  and  security  mat-ters, existing governance models are used.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "8117acf6-f4ac-4833-8241-2adaf9150b13",
                    "text": "",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "bc975c71-9845-453f-ae15-564bf97c99a9",
                    "text": "Flow  tool  (CNET  2018), while  others  such  as  IBM\u2019s  Fairness  360  toolkit  (IBM 2018b) and Accenture\u2019s AI Fairness tool (Accenture 2018) are for any organization to use. Some tools are open source such  as  IBM\u2019s  tool,  a  tool  from  Pymetrics  (2018),  and  a tool  from  the  University  of  Chicago  (Aequitas  2018).  As these  tools  focus  on  \u201cfair  AI\u201d  their  mission  is  helping  to detect  and  avoid  undesired  discrimination  through  bias. Typical  functionalities  include:  detecting  bias  in  data  sets related  to  sensitive  data  (impacting  protected  groups),  de-tecting  correlations  in  data  sets  between  normal  variables and  sensitive  variables,  detecting  unbalanced  outcomes  of algorithms  within  sub  groups  of  the  population  and  miti-gating the effect of the bias. The concept of Fair AI has obtained a lot of relevance in the last years. Many new studies are published every year dealing with the problem unfair AI and how to mitigate the inclusion  of  new  biases  in  the  decision-making  process. There is some confusion about why bias arises and even if the bias is something natural that must be preserved in or-der to maintain the \u201cpurity\u201d of original data. The potential causes  of  bias  were  first  registered  by  (Selbst,  Barocas 2016).  They  listed  some  causes  why  a  machine  learning model can give unfair results\u2022  Skewed data: Incorrect assumptions about the da-ta generation process. It occurs when the data ac-quisition process is biased. \u2022  Tainted  data:  Incorrect  problem  definition  and target labeling.  \u2022  Limited features: When the number of features is so  limited  that  bias  is  induced  in  some  sensitive attributes \u2022  Sample  size  disparities:  Unequal  sample  sizes  of different sensitive groups  \u2022  Proxy  features:  Presence  of  correlated  variables within  the  problem  that  induces  bias  even  when the sensitive features are removed. There are several definitions of Fairness provided in the literature: unawareness, group fairness, individual fairness and  counterfactual  fairness.  The  definition  of  fairness through  unawareness  consists  of  removing  the  sensitive variable  from  input  data.  This  can  be  insufficient  because the presence of proxy features implicitly maintains the in-formation of the deleted sensitive variable. Group  fairness deals with Fairness from the perspective of all individuals, while individual fairness tries to model the differences be-tween each subject with the rest of population. The case of counterfactual  fairness  goes  one  step  beyond  trying  to  in-terpret the causes of bias via causal graphs. (Hardt et al. 2016) propose a framework for group fair-ness  in  which  three  different  criteria  can  be  employed  to evaluate a supervised ML model in terms of Fairness: \u2022  Independence: It is achieved when the model pre-diction  is  independent  of  the  sensitive  variable, that  is,  the  proportion  of  Positive  samples  given by the model is the same for all sensitive groups. \u2022  Separation:  Also  known  as  Equalized  Odds.  It  is achieved when the model prediction is independ-ent of the sensitive variable given the target varia-ble,  that  is,  when  the  TP  (true  positive)  rate  and the  FP  (false  positive)  rate  are  equal  in  all  sensi-tive groups, respectively. \u2022  Sufficiency: Also known as Predictive Rate  Pari-ty. It is achieved when the target variable is inde-pendent of the sensitive attribute given the model output, that is, when the Positive Predictive Value is the same in all sensitive groups. It  is  impossible  to  achieve  all  three  criteria  at  the  same time, but they can be optimized jointly in order to optimal-ly mitigate bias in ML models. In  terms  of  Fairness  in  ML,  two  main  actions  can  be considered:  evaluation  and  mitigation.  The  former  is  the process  of  measuring  and  quantifying  the  amount  of  bias present  in  the  model  (in  terms  of  one  or  several  criteria), and  the  latter  is  the  process  of  fixing  some  aspects  of  the model  in  order  to  reduce  or  remove  the  effect  of  bias  in terms of one or several sensitive attributes. For  evaluation,  several  metrics  have  been  proposed  in the last years for the different fairness criteria. In the case of  the  Independence  criterion,  possible  metrics  include statistical parity difference or disparate impact for the case of  independence.  For  the  Separation  criterion,  possible metrics include equal opportunity difference and the aver-age odds difference (Hardt et al. 2016).  Another metric is the Theil index (Speicher el al. 2018)  which measures the inequality not only in terms of the group fairness, but also in terms of the individual fairness. For  the  mitigation  phase,  several  techniques  can  be found  in  the  literature.  In  this  part,  there  are  three  main groups of techniques. \u2022  Pre-processing:  These  techniques  are  applied  be-fore  the  machine  learning  algorithm  is  trained  in \u2022  order  to  remove  biases  in  the  very  early  stage  of the learning process. In-processing:  These  techniques  are  applied  dur-ing the training process by including Fairness op-timizations  constraints  along  with  cost  functions in ML models. \u2022  Post-processing:  Employed  after  the  algorithm  is built,  these  are  the  less  intrusive  techniques  be-cause they don\u2019t modify the input data or the ML algorithm.  This  technique  is  especially  adequate for mitigating biases in models that already exist. Each  technique  applies  the  mitigation  process  in differ-ent  phases  of  a  typical  analytics  pipeline  and  the  choice must be made to fit the particular case, but in terms of per-formance  it  is  better  to  apply  pre-processing  or  in-processing techniques. A  very  simple  pre-processing  technique  is  Reweighing (Kamiran  and  Calders,  2012)  consisting  of  modifying  the weights  of  samples  in  order  to  remove  discrimination  in sensitive attributes. Another example is the technique pro-posed by (Zemel et al. 2013) in which they transform input data  in order to find a good representation that obfuscates information about the membership in the sensitive groups. Another popular algorithm to mitigate bias is Adversari-al Debiasing (Zhang et al.  2018). This in-processing tech-nique tries to maximize the ability of predicting the target variable  while  minimizing  the  ability  of  predicting  sensi-tive variables through GAN. Also, the case of Equalized Odds post-processing (Hardt et al. 2016) is a good example of mitigation through a post-processing  technique.  In  their  proposal,  they  try  to  adjust the  thresholds  in  a  classification  model  in  order  to  reduce the  differences  between  the  True  Positive  Rate  and  False Positive Rate for each sensitive group. Transparent & Explainable AI (xAI) Explainable AI is something that has existed in the litera-ture for many years. Expert Systems were among the first AI systems that dealt with it, trying to use some of the mul-tiples rules generated as a base for the explanations provid-ed.  This  was  the  case  of  MYCIN  (one  of  the  first  Expert Systems  for  diagnosing  meningitis,  Clancey  1983)  which was able to give the following types of explanations:  why was  a  given  fact  used?  Why  was  a  given  fact  not  used? How  was  a  given  conclusion  reached?  How  was  it  that another  conclusion  was  not  reached?  The  conclusions were explained in a human-understandable manner by trac-ing back from the conclusions to the initial state. In  the  context  of  Machine  Learning,  there  are  several proposals for xAI. Supervised ML models can be classified into two groups: white box models and black box models. On  the  one  hand,  white  box models,  such  as  simple  deci-sion trees or linear regressions, are models that can gener-ate comprehensive explanations based on the model itself. On  the  other  hand,  black  box  models,  such  as  complex deep  learning  (DL)  architectures,  can't  provide  direct  ex-planations  for  the  decision  taken  by  the  system  in  a  way  that is comprehensive for a human being. The main issue is that there is a tradeoff between complexity and explainabil-ity: more complex models can potentially be more precise, but  in  exchange  the  model  is  opaquer.  To  be  able  to  use more complex models while being able to generate expla-nations  that  can  be  understood,  there  are  different  pro-posals available depending on the explanations desired. One example is explaining why a data point is classified within one of the categories used by the supervised model. For that purpose, there are libraries such as LIME (Ribeiro, Singh, Guestrin, 2016) that do not need information about the  model  itself  (model-agnostic).  Other  proposals  like Layer-wise  Relevance  Propagation  (LRP)  (Bach  et  al., 2015)  obtain,  for  deep  learning  models,  the  contributions of the different input features to a classification by aggre-gating the contributions of each of the layers present. This second  group  of  solutions  are  known  as  model  specific. (Samek, Wiegand, M\u00fcller 2017)  provide several examples of  applying  LRP  to  image  recognition.  A  system  trained with pixels of an image for classifying it as containing a \"a cat\" or \"not a cat\", will highlight the relevant pixels associ-ated  to  the  category  chosen.  Then,  a  person  can  see  the conclusion (e.g. a cat) and can inspect the highlighted pix-els to see if they really corresponded to the pixels of the cat (and  thus,  the  system  can  be  trusted).  Otherwise,  if  they correspond  to  something  not  related  to  a  cat,  even  if  the accuracy of the algorithm is high, the system should not be trusted.  This  approach  can  be  applied  to  a  variety  of  do-mains and data structures (tabular, texts...). There are many situations when the desired explanation is not about an individual classification but about how the whole  model  works  and  what  the  main  features  are  that have  contributed  to  the  training  phase.  To  address  that, there  are  several  solutions,  such  as  surrogate  models  im-plemented  through  libraries  such  as  Skater  (Oracle  2017). This  solution  uses  a  white  box  model  such  as  a  decision tree  trained  with  the  predicted  outputs  of  the  black  box model  (instead  of  the  real  values)  while  using  the  same input features. This trained white box model then serves as a  simple  but  useful  way  to  understand  the  most  relevant features of the black box model, regardless of the complex-ity of the model itself. See (Guidotti et al 2018) for a sur-vey of methods for explaining black box models. Though most of the research work addresses supervised learning, there are also proposals in the scientific literature for other kind of models such as for reinforcement learning (Anderson et al., 2019). Finally, some xAI tools developed by  big  tech  companies  are  What-If  (Google,  2018)  and Interpret (Microsoft, 2019). Privacy & Security by Design Regarding  the  Privacy  &  Security  by  design  principle,  an example of a tool that contributes to privacy by testing the risk of re-identification in case the data set is anonymized, even if differential privacy techniques are used, is (Dwork, Roth, 2014). Related to security, one aspect to consider is if the sys-tem is robust against attacks that seek to exploit weakness-es and manipulate its outputs. There are techniques, such as Adversarial Examples (Goodfellow, Shlens, Szegedy, 2014), that seek to cheat and manipulate the outputs of a model. In the case of a supervised ML, this takes place by checking the minimum changes in the input data that would cause different classifications. This has happened, for example, with computer vision systems of autonomous vehicles; with slight changes in a stop sign, which go un-noticed by the human eye, the systems detected them in-stead as speed limit signs of 45 mph (Eykholt et al., 2018). Some tools available to test and even secure the system against those attacks are for example Cleverhans (Papernot et al, 2016) for deep learning models, AlfaSVMLib (Xiao et al., 2015) for SVM models, AdversarialLib for evasion attacks (Biggio, Corona et al., 2013), and even for unsu-pervised learning such as clustering algorithms (Biggio, Pillai et al., 2013).  Implementation of the methodology The  implementation  of  the  methodology  is  no  different from  implementing  other  technology-related  policies  such as Security & Privacy by Design, but given how unknown the  innerworkings  of  AI  are  for  the  wider  audience,  it might cause additional challenges.  Firstly,  the  company  needs  to  start  an  awareness  cam-paign explaining what AI is, why and how it is used in the company,  and  what  the  challenges  are.  The  campaign  in-troduces the Principles, the methodology, the training pro-gram and the tools.  Secondly,  the  training  program  starts  with  the  people closest  to  designing  and  developing  products  &  services that  use  AI,  including  the  procurement  people  who  deal with buying technology from third parties. In a later phase, training can be extended more widely across the organiza-tion, but always starting with people who are most likely to get involved in AI-related initiatives. Training ranges from very  technical  training  to  non-technical  training,  for  in-stance non-technical training for call center or social media agents who might receive customer inquiries related to AI. We  have  developed  an  online  course  of  about  one  hour where we explain to employees  basic AI concepts such as Machine  Learning,  the  potential  undesired  impact  of  AI such  as  bias,  the  AI  principles,  technical  tools  and  ques-tionnaires,  the  governance  model,  and  several  practical examples of assessing products against the principles.  De-pending on the role employees have, they will see more or less technical details.  As discussed (see Table 1 and Table 3), several specific tools are needed to support the implementation of Respon-sible AI in an organization. Without those tools, it is very hard  to  make  justifiable  statements  about  fairness  or  ex-plainability.  We believe that as a start, self-control is more effective than  control  through  specific  committees.  Therefore,  we start with an agile governance model, delegating as much as possible responsibility to the people responsible for the concerned products and services, and providing  them with support in case a question is raising concern. Making peo-ple  aware  of  the  concerns  and  providing  them  with  con-crete  training  and  tools  will  help  changing  the  culture  to creating  Responsible  AI  by Design.  In  this  way,  we  man-age to \u201csensibilize\u201d the organization without imposing a lot of controls. Such awareness is also important for potential future  regulation.  The  governance  model  only  covers  the part  that  is  specific  for  AI.  For  privacy  and  security  mat-ters, existing governance models are used. Challenges and research needs Fairness One of the main challenges around Fair AI is the evangeli-zation  of  users,  developers  and  companies  about  the  need for  Fairness  treatment  in  ML  processes.  Most  people  be-lieve  that  bias  is  something  intrinsic  to  the  nature  of  pro-cess that generates data and must not be altered. As an ex-ample,  if  we  examine  the  proportion  of  men  and  women studying engineering, we will find that, in general, women are  more  inclined  to  other  type  of  studies.  We  can  think that  people  chose  whatever  they  want  (and  it\u2019s  true)  and data  do  not  suffer  from  any  type  of  human  bias,  but  the truth  is  that  all  decisions  are  strongly  affected  by  society and stereotypes that determine the shape of data in the ac-quisition process.   Fair AI has been one of the most important topics in the last  years.  Many  studies  and  new  techniques  have  arisen around  how  to  make  fair  models.  The  constant  increasing of work and new perspectives about the problem has creat-ed  a  need  for  a  unified  framework  for  Fairness  in  AI  in order to simplify the process of evangelization and imple-mentation, especially in the industry.   Several techniques as disparate impact or Equal Oppor-tunity  difference  are  intended  to  measure  specific  aspects of  Fairness  in  ML  models.  Other  metrics  like  Theil  index are useful for group fairness and individual fairness but in general,  the  need  of  a  unified  metric  for  this  purpose  is more  evident  every  day.  With  the  inclusion  of  unified technique of Fairness, the process of evaluation and correc-tion would be easier to implement and compare.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "faa15e55-799a-4512-8bc6-321d9c9918e1",
                    "text": "Though  not  every  domain  is  equal,  and  explainability  is more  relevant  for  applications  such  as  medical  diagnosis than  for  recommendations  of  movies,  most  domains,  in-cluding telecommunications, require some kind of explain-ability.  The  type  of  explanation  should  be  adapted  to  the different  profiles  and  transparency  levels  required  by  the domain and, sometimes, regulations. For example, accord-ing  to  GDPR,  Article  13  users  have  the  right  to  know \"meaningful information about the logic involved\" (GDPR, 2018) when personal data  is used, and thus,  implying  that explainability  is  compulsory  for  any  organization  deliver-ing products and services in Europe in that scenario.   Besides the explicit requirements within regulation, xAI could  be  provided  for  all  profiles  that  have  a  relationship with the AI system, including technical profiles (Data Sci-entists, DevOps...), stakeholders (in case of private compa-nies), domain experts (such as doctors for medical AI ap-plications),  regulators  and  auditors,  and  end  users  of  the system (like patients in case of medical applications). This poses a challenge for xAI since most of the aforementioned tools  assist  in  providing  a  fixed  type  of  explanation, whereas ideally explanations should be tailored to the level of transparency required and to the specific profile at hand. It  would  be  even  better  if  the  explanations  generated, would  take into account  specific  domain knowledge, such that  they  go  beyond mere rankings  of  features  in  the  case of supervised ML.   There  are  some  research  proposals  that  deal  with  this, such  as  (Hind,  Wei,  Campbell,  Codella,  2019)  where  a supervised  model  is  trained,  not  only  with  the  labels  to predict, but also with the domain explanations they should provide. In this approach domain explanations are linked to particular  combinations  of  feature  values  and  their  corre-sponding  prediction.  Any  time  the  model  predicts  a  new instance, it will include an explanation. This approach can be used to train a model with the adequate explanations for a  transparency  level,  like,  for  instance,  the  transparency level  required  for  an  end  user\ufffc.  As  a  consequence,  the user will receive explanations according to the data points, but the inner information of the model itself will be still be unknown to them, and thus, it offers a privacy-conserving explainability method.    Another proposal tackling the same problem is presented in  (Doran,  Schulz,  Besold,  2017)  where  the  relevant  fea-tures  detected  in  a  supervised  ML  model  are  combined with a knowledge-based reasoning system in order to pro-vide human comprehensible explanations.   While those proposals are promising, they are still early-stage research, and therefore hard to automate for inclusion in production environments.   Another  important  challenge  regarding  xAI,  mentioned by (Guidotti et al 2018), is that there is still not an availa-ble  formalism  to  define  a  common  reference  for  what  an explanation should be. There are some criteria to consider while evaluating an explanation (it should be concise, con-sistent, should consider anomalous situations\u2026) but that is still  not  enough  to  set  a  general  reference  to  build  them. Also,  the  article  mentions  that  explanations  are  still  con-strained  to  the  available  features  of  a  data  set  (for  super-vised ML) but they do not consider other relevant features  such  as  latent  variables,  correlated  information  that  is  not explicitly present in the dataset, etc... So, building explana-tions based only on those features might be too simplistic.   Other  recent  proposals  are  related  to  the  definition  of methodologies  and  procedures  to  document  and  standard-ize the development of an AI software product in order to reflect  how  company  AI  principles  have  been  addressed. (Hind  et  al.,  2018)  suggests  creating  documentation,  in-spired by the SDoCs (supplier's declaration of conformity, which describes the lineage of a product including its safe-ty and performance tests), which contain the purpose, per-formance, safety, security and the origin of information of an  AI  product,  and  that  can  be  examined  by  clients.  This serves as  a mechanism for the  builder of an AI system or service  to communicate how all relevant issues have been addressed in the product or service.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "3b34dd6a-e256-4ec0-a95b-8971262fd4ad",
                    "text": "A challenge for a methodology for Responsible AI by De-sign  is  the  trade-off  between  innovation  speed  and  risks. The speed of technological changes and their uptake in the market  dictate  a  fast  incorporation  of  AI  in  products  and services.  However,  risk-reducing  methodologies  usually slow down the innovation pace due to controls by commit-tees.  That  is  the  reason  why we  have  chosen  to  start  with self-control by using the methodology as an awareness and training  tool  for  our  employees.  It  may  delay  slightly  the development  and  go-to-market  processes,  but  the  likeli-hood of social acceptance of the solutions is expected to be higher, and the risk for committing an error with negative impacts, will be reduced.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "b3bed2bf-6627-4129-aee3-79b345095d7e",
                    "text": "Currently, at Telef\u00f3nica, there are some products and uses cases that address the AI principles of the company.    Regarding  fairness,  Telef\u00f3nica  developed  a  tool  coined Luca  Ethics  that  allows  to  audit  and  mitigate  bias  in  any existing  classification  ML  model.  The  methodology  has two  main  phases:  the  first  one  consists  of  quantifying  the bias  present  in  the  model  and  the  second  one  consists  of applying  post-processing  techniques  to  mitigate  the  bias. For  bias  measurement,  several  state-of-the-art  techniques are  employed  like  statistical  parity,  average  odds  differ-ence,  predictive  parity  difference  or  mutual  information. Once the bias is quantified,  we  optimize  the classification threshold  of  each  sensitive  group  individually  in  order  to satisfy  the  equal  opportunity  criterion  (Hardt  et  al.  2016) while achieving the best possible model performance.   Regarding transparency and data privacy, Telef\u00f3nica has created  a  \u201ctransparency  center\u201d  for  users  of  Aura,  Tele-fonica\u2019s  virtual  assistant.  This  center  allows  customers  to have direct  access to, and  control over their personal  data held  by  the  company  and  used  by  the  system.  Another product  related  to  privacy  is  Spectra,  a  software  tool  for complex  and  robust  data  anonymization,  along  with  a  pa-tented  tool  to  avoid  the  risk  of  re-identifying  anonymized data. In the area of xAI, as an example we mention two prod-ucts  that  include  an  explainability  feature:  A  device  rec-ommender for mobile handsets, and Luca Comms for ana-lyzing  organizational  communications  patterns.  Device Recommender  is  a  software  product  that  recommends  a device to a specific user using techniques of reinforcement learning  (contextual  bandits)  combined  with  market  re-search information elicited by a conjoint analysis. The rec-ommender  system  includes  an  automatic  natural-language generated explanation that incorporates the key features of why that device was considered. The second example, Lu-ca  Comms,  is  a  big  data  analytics  service  that  identifies opportunities  for  business  improvement  within  the  com-plexity  of  enterprise  communication  data.  Luca  Comms includes a patent-pending (Barbado et al., 2019) xAI solu-tion  for  unsupervised  ML  anomaly  detection  models (OneClass  SVM)  for  local  explanations  (why  a  particular data point is anomalous) using model specific information and providing counterfactual explanations through a visual interface.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                },
                {
                    "id": "1246a119-c146-4516-88a0-7b78babef202",
                    "text": "In  this  paper,  we  have  discussed  a  methodology  called \u201cResponsible AI by Design\u201d that can be used by organiza-tions who plan to use AI at a large scale, and want to avoid unintentionally  creating undesired side-effects such as un-fair  discrimination,  or  not  being  able  to  understand  the conclusions of an AI system. The methodology starts with defining a set of overall AI principles that state the values and  boundaries.  Other  ingredients  include  awareness  & training, a set of questions to be asked in the development process, specific tools to help answering some of the ques-tions,  and  a  governance  process  defining  responsibilities and accountability. It is the combination of those different elements that makes the methodology successful. With the expected  massive  uptake  of  AI,  we  think  that  such  meth-odologies are becoming increasingly important, and practi-cal experience needs to be shared to ensure the sustainabil-ity  of  AI.  Like  many  other  organizations,  we  have  only started the journey. We do therefore not consider our prin-ciples  and  methodology  as  final  standards  for  ethical  AI, but  as  a  starting  point  that  will  evolve  as  more  collective experience becomes available.",
                    "reference": "[1] Ricardo Baeza-Yates Benjamins, Almudena Barbado, and David Sierra. 2019. Responsible AI by design in practice. arXiv:1909.12838. Retrieved from https://arxiv.org/pdf/1909.12838"
                }
            ]
        },
        {
            "paper_title": "Socially responsible ai algorithms: Issues, purposes, and challenges",
            "authors": "L Cheng, KR Varshney, H Liu",
            "publication_info": "Journal of Artificial Intelligence Research - jair.org",
            "paper_url": "https://www.jair.org/index.php/jair/article/download/12814/26713/",
            "chunks": [
                {
                    "id": "f4cee667-0919-4a9b-adb9-9b499f8b497a",
                    "text": "",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "6ea826e2-179b-4c67-9e87-5b83b6a4966e",
                    "text": "Arti\ufb01cial intelligence (AI) has had and will continue to have a central role in countlessaspects of life, livelihood, and liberty. AI is bringing forth a sea-change that is not onlylimited to technical domains, but is a truly sociotechnical phenomenon a\ufb00ecting healthcare,education, commerce, \ufb01nance, and criminal justice, not to mention day-to-day life. AI o\ufb00ersboth promise and perils. A report published by Martha Lane Fox\u2019s Doteveryone think tank(Miller C, 2019) reveals that 59% of tech workers have worked on products they felt harmfulto society, and more than 25% of workers in AI who had such an experience quit their jobsas a result. This was particularly marked in relation to AI products. The rise of activism \u2013which has been regarded as one of the current few mechanisms to keep big tech companiesin check (Schwab, 2021) \u2013 against negative social impacts of big tech have brought SocialResponsibility of AI into the spotlight of the media, the general public, and AI technologistsand researchers (Abdalla & Abdalla, 2020). Even researchers in universities and researchinstitutes are trying hard to rectify the mistakes made by algorithms. Stanford\u2019s COVID-19vaccine allocation algorithm, for example, prioritizes older employees over front-line workers(Asimov, 2020), turning much of our attention again to the transparency and fairness ofAI. Research directed towards developing fair, transparent, accountable, and ethical AI al-gorithms has burgeoned with a focus on decision-making algorithms such as scoring orclassi\ufb01cation to mitigate unwanted bias and achieve fairness (Jagadish, 2019). However,this narrow subset of research risks blinding us to the challenges and opportunities thatare presented by the full scope of AI. To identify potential higher-order e\ufb00ects on safety,privacy, and society at large, it is critical to think beyond algorithmic bias, to capture allthe connections among di\ufb00erent aspects related to AI algorithms. Therefore, this surveycomplements prior work through a holistic understanding of the relations between AI sys-tems and humans. In this work, we begin by introducing an inclusive de\ufb01nition of SocialResponsibility of AI. Drawing on theories in business research, we then present a pyramidof Social Responsibility of AI that outlines four speci\ufb01c AI responsibilities in a hierarchy.This is adapted from the pyramid proposed for Corporate Social Responsibility (CSR) byCarroll et al. (1991). In the second part of the survey, we review major aspects of AI algo-rithms and provide a systematic framework \u2013 Socially Responsible AI Algorithms (SRAs)\u2013 that aims to understand the connections among these aspects. In particular, we examinethe subjects and causes of socially indi\ufb00erent AI algorithms , de\ufb01ne the objectives, andintroduce the means by which we can achieve SRAs. We further discuss how to leverageSRAs to improve daily life of human beings and address challenging societal issues throughprotecting, informing, and preventing/mitigating. We illustrate these ideas using recentstudies on several emerging societal challenges. The survey concludes with open problemsand challenges in SRAs.Di\ufb00erences from Existing Surveys. Some recent surveys focus on speci\ufb01c topics suchas bias and fairness (Mehrabi, Morstatter, Saxena, Lerman, & Galstyan, 2019; Caton &Haas, 2020), interpretability/explainability (Carvalho, Pereira, & Cardoso, 2019; Tjoa &Guan, 2020), and privacy-preservation (Beigi & Liu, 2020; Dwork, 2008). These surveyssuccessfully draw great attention to the social responsibility of AI, leading to further de-velopments in this important line of research. However, as indispensable components ofsocially responsible AI, these topics have been presented in their own self-contained ways.These works pave the way for looking at socially responsible AI holistically. Therefore, oursurvey aims to frame socially responsible AI with a more systematic view that goes beyonddiscussion of each independent line of research. We summarize our contributions as follows:\u2022 We formally de\ufb01ne social responsibility of AI with three speci\ufb01ed dimensions: principles,means, and objectives. We then propose the pyramid of social responsibility of AI, de-scribing its four fundamental responsibilities: functional, legal, ethical, and philanthropicresponsibilities. The pyramid embraces the entire range of AI responsibilities involvinge\ufb00orts from various disciplines.\u2022 We propose a systematic framework that discusses the essentials of socially responsibleAI algorithms (SRAs) \u2013 including its subjects, causes, means, and objectives \u2013 and theroles of SRAs in protecting, informing users, and preventing them from negative impactof AI. This framework subsumes existing topics such as fairness and interpretability.\u2022 We look beyond prior research in socially responsible AI and identify an extensive list ofopen problems and challenges, ranging from understanding why we need AI systems toshowing the need to de\ufb01ne new AI ethics principles and policies. We hope our discussionscan spark future research on SRAs.Intended Audience and Paper Organization. This survey is intended for AI re-searchers, AI technologists, researchers, and practitioners from other disciplines who wouldlike to contribute to making AI more socially responsible with their expertise. The rest ofthe survey is organized as follows: Section 2 introduces the de\ufb01nition and the pyramid ofsocial responsibility of AI, and compares de\ufb01nitions of similar concepts. Section 3 discussesthe framework of socially responsible algorithms and its essentials, followed by Section 4that illustrates the roles of SRAs using several emerging societal issues as examples. Section5 details the open problems and challenges that socially responsible AI currently confronts.The last section concludes the survey.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "2e9c1388-dd35-4a52-81da-86ac9871e921",
                    "text": "Social Responsibility of AI includes e\ufb00orts devoted to addressing both technical and societalissues. While similar concepts (e.g., \u201cEthical AI\u201d) repeatedly appear in the news, magazines,and scienti\ufb01c articles, \u201cSocial Responsibility of AI\u201d has yet to be properly de\ufb01ned. In thissection, we \ufb01rst attempt to provide an inclusive de\ufb01nition and then propose the Pyramidof Social Responsibility of AI to outline the various responsibilities of AI in a hierarchy:functional responsibilities, legal responsibilities, ethical responsibilities, and philanthropicresponsibilities. At last, we compare \u201cSocially Responsible AI\u201d with similar concepts.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "42b3c146-645e-4166-b286-021292f3bb25",
                    "text": "Here, we de\ufb01ne three dimensions of Social Responsibility of AI: the principles lay the foun-dations for ethical AI systems; the means to reach the overarching goal of Social Respon-sibility of AI is to develop Socially Responsible AI Algorithms; and the objective of SocialResponsibility of AI is to improve both AI\u2019s capability and humanity with the second beingthe proactive goal.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "4e5f8926-1a0a-4ecc-a425-f87fc0205fd1",
                    "text": "Social Responsibility of AI should be framed in such a way that the entire range of AIresponsibilities are embraced. Adapting Carroll\u2019s Pyramid of CSR (Carroll et al., 1991) inthe AI context, we suggest four kinds of social responsibilities that constitute the SocialResponsibility of AI: functional, legal, ethical, and philanthropic responsibilities, as shownin Figure 1. By modularizing AI responsibilities, we hope to help AI technologists andFigure 1: The pyramid of Social Responsibility of AI, adapted from the Pyramid of CSRby Carroll et al. (1991).researchers to reconcile these obligations and simultaneously ful\ufb01ll all the components inthe pyramid. All of these responsibilities have always existed, but functional responsibilitieshave been the main consideration until recently. Each type of responsibility requires closeconsideration.The pyramid portrays the four components of Social Responsibility of AI, beginning withthe basic building block notion that the functional competence of AI undergirds all else.Functional responsibilities require AI systems to perform in a manner consistent with pro\ufb01tsmaximization, operating e\ufb03ciency, and other key performance indicators. Meanwhile, AIis expected to obey the law, which codi\ufb01es the acceptable and unacceptable behaviorsin our society. That is, legal responsibilities require AI systems to perform in a mannerconsistent with expectations of government and law. All AI systems should at least meetthe minimal legal requirements. At its most fundamental level, ethical responsibilities arethe obligation to do what is right, just, and fair, and to prevent or mitigate negativeimpact on stakeholders (e.g., users, the environment). To ful\ufb01ll its ethical responsibilities,AI systems need to perform in a manner consistent with societal expectations and ethicalnorms, which cannot be compromised in order to achieve AI\u2019s functional responsibilities.Finally, in philanthropic responsibilities, AI systems are expected to be good AI citizens andto contribute to tackling societal challenges such as cancer and climate change. Particularly,it is important for AI systems to perform in a manner consistent with the philanthropicand charitable expectations of society to enhance people\u2019s quality of life. The distinguishingfeature between ethical and philanthropic responsibilities is that the latter are not expectedin an ethical sense. For example, while communities desire AI systems to be applied toConceptsRobust AIEthical AITrustworthy AIFair AISafe AIDependable AI De\ufb01nitionsAI systems with the ability \u201cto cope with errors during exe-cution and cope with erroneous input\u201d (Wikipedia, 2021a).AI systems that do what is right, fair, and just. Preventharm.AI systems that achieve their full potential if trust can be es-tablished in the development, deployment, and use (Thiebes,Lins, & Sunyaev, 2020).AI systems absent from \u201cany prejudice or favoritism towardan individual or a group based on their inherent or acquiredcharacteristics\u201d (Mehrabi et al., 2019).AI systems deployed in ways that do not harm humanity(Feige, 2019).AI systems that focus on reliability, veri\ufb01ability, explainabil-ity, and security (Singh, Vatsa, & Ratha, 2021).Human-centered AI AI systems that are \u201ccontinuously improving because of hu-man input while providing an e\ufb00ective experience betweenhuman and robot\u201d .Table 1: De\ufb01nitions of concepts similar to Socially Responsible AI.humanitarian projects or purposes, they do not regard the AI systems as unethical if theydo not provide such services. We explore the nature of Social Responsibility of AI byfocusing on its components to help AI technologists to reconcile these obligations. Thoughthese four components are depicted as separate concepts, they are not mutually exclusive.It is necessary for AI technologists and researchers to recognize that these obligations arein a constant but dynamic tension with one another.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "861da3c7-38d7-4fb9-8e8c-6922aa03c87a",
                    "text": "Based on De\ufb01nition 1 and the pyramid of socially responsibility of AI, we compare SociallyResponsible AI with other similar concepts, as illustrated in Table 1. The results showthat Socially Responsible AI holds a systematic view that subsumes existing concepts andfurther considers the fundamental responsibilities of AI systems \u2013 to be functional andlegal, as well as their philanthropic responsibilities \u2013 to be able to improve life quality ofIn the rest of this survey, we focuswell beings and address challenging societal issues.our discussions on the ethical (Section 3, essentials of SRAs) and philanthropic (Section 4,roles of SRAs) responsibilities of AI given that both the functional and legal responsibilitiesare the usual focuses in AI research and development. An overview of SRAs research isillustrated in Figure 2, which we will refer back to throughout the remainder of the survey.Importantly, in our view, the essentials of SRAs work toward ethical responsibilities, andtheir roles in society encompass both ethical and philanthropic responsibilities.Figure 2: An overview of SRAs Research.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "04fc7aee-1f7b-4561-a4be-642da2a03441",
                    "text": "The role of AI technologists and researchers carries a number of responsibilities. The mostobvious is developing accurate, reliable, and trustworthy algorithms that can be dependedon by their users. Yet, this has never been a trivial task. For example, due to the varioustypes of human biases, e.g., con\ufb01rmation bias, gender bias, and anchoring bias, AI technol-ogists and researchers often inadvertently inject these same kinds of bias into the developedalgorithms, especially when using machine learning techniques. For example, supervisedmachine learning is a common technique for learning and validating algorithms throughmanually annotated data, loss functions, and related evaluation metrics. Numerous uncer-tainties \u2013 e.g., imbalanced data, ill-de\ufb01ned criteria for data annotation, over-simpli\ufb01ed lossfunctions, and unexplainable results \u2013 potentially lurk in this \u201cbeautiful\u201d pipeline and willeventually lead to negative outcomes such as biases and discrimination. With the growingreliance on AI in almost any \ufb01eld in our society, we must bring upfront the vital questionabout how to develop Socially Responsible AI Algorithms. While conclusive answers are yetto be found, we attempt to provide a systematic framework of SRAs (illustrated in Figure3) to discuss the components of AI\u2019s ethical responsibilities, the roles of SRAs in terms ofAI\u2019s philanthropic and ethical responsibilities, and the feedback from users routed back asinputs to SRAs. We hope to broaden future discussions on this subject. In this regard, wede\ufb01ne SRAs as follows:De\ufb01nition 2 (Socially Responsible AI Algorithms) Socially Responsible AI Algo-rithms are the intelligent algorithms that prioritize the needs of all stakeholders as thehighest priority, especially the minoritized and disadvantaged users, in order to make justand trustworthy decisions. These obligations include protecting and informing users; pre-venting and mitigating negative impact; and maximizing the long-term bene\ufb01cial impact.Socially Responsible AI Algorithms constantly receive feedback from users to continuallyaccomplish the expected social values.In this de\ufb01nition, we highlight that the functional (e.g., maximizing pro\ufb01ts) and societal(e.g., transparency) objectives are integral parts of AI algorithms. SRAs aim to be sociallyresponsible while still meeting and exceeding business objectives.Figure 3: The framework of Socially Responsible AI Algorithms (SRAs). It consists ofthe essentials (i.e., the internal mechanisms) of SRAs (left), their roles (right),and feedback received from end users for helping SRAs gradually achieve theexpected social values (bottom). The essentials of SRAs center on the ethicalresponsibilities of AI and the roles of SRAs require philanthropic responsibilitiesand ethical responsibilities.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "2a56c506-76ae-45e7-9deb-ad366a6da0f2",
                    "text": "Every human being can be a potential victim of socially indi\ufb00erent AI algorithms. Mir-roring society, the ones who su\ufb00er the most, both in frequency and severity, are minoritiesand disadvantaged groups such as black, indigenous and people of color (BIPOC), and fe-males. For example, Google mislabeled an image of two black people as \u201cgorillas\u201d (Guynn,2015) and more frequently showed ads of high-paying jobs to males than females (Carpen-ter, 2015). Similar gender bias was also found in Facebook algorithms behind the job ads(Horwitz, 2021). In domains with high-stakes decisions, e.g., \ufb01nancial services, healthcare,and criminal justice, it is not uncommon to identify instances where socially indi\ufb00erent AIalgorithms favor privileged groups. For example, the algorithm used in Correctional Of-fender Management Pro\ufb01ling for Alternative Sanctions (COMPAS) was found almost twiceas likely to mislabel a black defendant as a future risk than a white defendant (Angwin, Lar-son, Mattu, & Kirchner, 2016). Identifying the subjects of socially indi\ufb00erent AI algorithmsdepends on the context. In another study, the journalistic organization ProPublica investi-gated algorithms that determine online prices for Princeton Review\u2019s tutoring classes. Theresults showed that people who lived in higher income areas were charged twice as muchas the general public and than people living in a zip code with high population density.Asians were 1.8 times more likely to pay higher price, regardless of their income (Angwin &Larson, 2015). Analogously, these AI algorithms might put poor people who cannot a\ufb00ordinternet service at disadvantage because they simply have never seen such data samples inthe training process.When it comes to purpose-driven collection and use of data, each individual can be thesubject of socially indi\ufb00erent AI algorithms. Users\u2019 personal data are frequently collectedand used without their consent. Such data includes granular details such as contact infor-mation, online browsing and session record, social media consumption, location and so on.While most of us are aware of our data being used, few have controls to where and howthe data is used, and by whom. The misuse of data and lack of knowledge causes users tobecome the victims of privacy-leakage and distrust.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "8adb2692-9788-48d7-8b6d-37afc36fcd9b",
                    "text": "There are many potential factors that can cause AI algorithms to be socially indi\ufb00erent.Here, we list several causes that have been frequently discussed in literature (Mehrabiet al., 2019; Getoor, 2019). They are formalization, measuring errors, bias, privacy, andcorrelation versus causation.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "591e6002-a8f0-4596-84a0-76ce2c02e81e",
                    "text": "AI algorithms encompass data formalization, label formalization, formalization of loss func-tion and evaluation metrics. We unconsciously make some frame of reference commitmentto each of these formalizations. Firstly, the social and historical context are often left outwhen transforming raw data into numerical feature vectors. Therefore, AI algorithms aretrained on pre-processed data with important contextual information missing. Secondly,data annotation can be problematic for a number of reasons. For example, what are thecriteria? Who de\ufb01nes the criteria? Who are the annotators? How can it be ensured thatthey all follow the criteria? What we have for model training are only proxies of the truelabels (Getoor, 2019). Ill-formulated loss functions can also result in socially indi\ufb00erent AIalgorithms. Many loss functions in the tasks are over-simpli\ufb01ed to solely focus on maxi-mizing pro\ufb01ts and minimizing losses. The concerns of unethical optimization are recentlydiscussed by Beale, Battey, Davison, and MacKay (2019). Unknown to AI systems, certainstrategies in the optimization space that are considered as unethical by stakeholder may beselected to satisfy the simpli\ufb01ed task requirements. Lastly, the use of inappropriate bench-marks for evaluation may push algorithms away from the overarching goal of the task andfuel injustice.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "a2f79e21-ce8d-4ce2-907c-c08e354ae496",
                    "text": "Another cause of socially indi\ufb00erent AI algorithms is the errors when measuring algorithmperformance. When reporting results, researchers typically proclaim the proposed algo-rithms can achieve certain accuracy or F1 scores. However, this is based on assumptionsthat the training and test samples are representative of the target population and theirdistributions are similar enough. Yet, how often does the assumption hold in practice? Asillustrated in Figure 4, with non-representative samples, the learned model can achieve zerotraining error and perform well on the testing data at the initial stage. However, with moredata being tested later, the model performance deteriorates because the learned model doesnot represent the true model.Figure 4: An example of measuring errors. The green line denotes the learned model andthe blue one is the true model. \u2018+\u2019 and \u2018-\u2019 represent training data belonging todi\ufb00erent classes; \u2018X\u2019 represents testing data. Image taken from Getoor\u2019s slides for2019 IEEE Big Data keynote (Getoor, 2019) with permission.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "5e9d612c-344a-42d1-9b57-1fa53cf1a4f5",
                    "text": "Bias is one of the most discussed topics regarding responsible AI. We here focus on the databias, automation bias, and algorithmic bias (Getoor, 2019).Data Bias. Data, especially big data, is often heterogeneous \u2013 data with high variabil-ity of types and formats, e.g., text, image, and video. The availability of multiple datasources brings unprecedented opportunities as well as unequivocally presented challenges(Li, Cheng, Wang, Morstatter, Trevino, Tang, & Liu, 2017). For instance, high-dimensionaldata such as text is infamous for the danger of over\ufb01tting and the curse of dimensionality.Additionally, it is rather challenging to \ufb01nd subset of features that are predictive but un-correlated. The required number samples for generalization also grows proportionally withfeature dimension. One example is how the U.S. National Security Agency tried to use AIalgorithms to identify potential terrorists. The Skynet project collected cellular networktra\ufb03c in Pakistan and extracted 80 features for each cell phone user with only 7 knownterrorists (Gershgorn, 2019). The algorithm ended up identifying an Al Jazeera reportercovering Al Qaeda as a potential terrorist. Data heterogeneity is also against the well knowni.i.d. assumption in most learning algorithms (Li et al., 2017). Therefore, training thesealgorithms on heterogeneous data can result in undesired results. Imbalanced subgroupsis another source of data bias. As illustrated in (Mehrabi et al., 2019), regression analysisbased on the subgroups with balanced \ufb01tness level suggests positive correlation betweenBMI and daily pasta calorie intake whereas that based on less balanced data shows almostno relationship.Automation Bias. This type of bias refers to our preference to results suggested by au-tomated decision-making systems while ignoring the contradictory information. Too muchreliance on the automated systems without sparing additional thoughts in making \ufb01naldecisions, we might end up abdicating decision responsibility to AI algorithms.Algorithmic Bias. Algorithmic bias regards biases added purely by the algorithm itself(Baeza-Yates, 2018). Some algorithms are inadvertently taught prejudices and unethicalbiases by societal patterns hidden in the data. Typically, models \ufb01t better to featuresthat frequently appear in the data. For example, an automatic AI recruiting tool willFigure 5: Confounders are common reasons for spurious correlation between two variablesthat are not causally connected.learn to make decisions for a given applicant of a software engineer position using observedpatterns such as \u201cexperience\u201d, \u201cprogramming skills\u201d, \u201cdegree\u201d, and \u201cpast projects\u201d. For aposition where gender disparity is large, the algorithms mistakenly interpret this collectiveimbalance as a useful pattern in the data rather than undesirable noise that should havebeen discarded. Algorithmic bias is systematic and repeatable error in an AI system thatcreates discriminated outcome, e.g., privileging wealthy users over others. It can amplify,operationalize, and even legitimize institutional bias (Getoor, 2019).",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "ab681611-0507-4675-b7f7-fe3a5e40be45",
                    "text": "Data is the fuel and new currency that has empowered tremendous progress in AI research.Search engines have to rely on data to craft precisely personalized recommendation thatimproves the online experience of consumers, including online shopping, book recommen-dation, entertainment, and so on. However, users\u2019 data are frequently misused withoutthe consent and awareness of users. One example is the Facebook-Cambridge Analyticalscandal (Wikipedia, 2021b) where millions of Facebook users\u2019 personal data was collectedby Cambridge Analytica (Wikipedia, 2021c), without their consent. In a more recent study(Caba\u02dcnas, Cuevas, Arrate, & Cuevas, 2020), researchers show that Facebook allows adver-tisers to exploit its users\u2019 sensitive information for tailored ad campaigns. To make thingsworse, users often have no clue about where, how, and why their data is being used, and bywhom. The lack of knowledge and choice over their data causes users to undervalue theirpersonal data, and further creates issues such as privacy and distrust.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "764f1d0b-9c5f-4d1c-afbb-4886491c2d25",
                    "text": "AI Algorithms can become socially indi\ufb00erent when correlation is misinterpreted as causa-tion. For example, in the diagram in Figure 5, we observe a strong correlation between theelectric bill of an ice cream shop and ice cream sales. Apparently, high electric bill cannotcause the ice cream sales to increase. Rather, weather is the common cause of electric billand the sale, i.e., high temperature causes high electric bill and the increased ice cream sales.Weather \u2013 the confounder \u2013 creates a spurious correlation between electric bill and ice creamsales. Causality is a generic relationship between the cause and the outcome (Guo, Cheng,Li, Hahn, & Liu, 2020). While correlation helps with prediction, causation is important fordecision making. One typical example is Simpson\u2019s Paradox (Blyth, 1972). It describes aphenomenon where a trend or association observed in subgroups maybe opposite to thatobserved when these subgroups are aggregated. For instance, in the study of analyzing theFigure 6: The objectives of Socially Responsible AI Algorithms.sex bias in graduation admissions at UC Berkeley (Bickel, Hammel, & O\u2019Connell, 1975),the admission rate was found higher in male applicants when using the entire data. How-ever, when the admission data were separated and analyzed over the departments, femalecandidates had equal or even higher admission rate over male candidates.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "7fd5834f-5171-493d-b812-62904962c323",
                    "text": "Essentially, the goal is to (re)build trust in AI. By de\ufb01nition, trust is the \u201c\ufb01rm belief in thereliability, truth or ability of someone or something\u201d . It is a high-level concept that needsto be speci\ufb01ed by more concrete objectives. We here discuss the SRAs objectives that havebeen discussed comparatively frequently in literature. They are fairness, transparency, andsafety as illustrated in Figure 6.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "7134c036-0279-4a43-85c0-c4c587850218",
                    "text": "Fairness in AI has gained substantial attentions in both research and industry since 2010.For decades, researchers found it rather challenging to present a uni\ufb01ed de\ufb01nition of fairnessin part because fairness is a societal and ethical concept. This concept is mostly subjective,changes over social context, and evolves over time, making fairness a rather challenginggoal to achieve in practice. Because SRAs is a decision-making process commensurate withsocial values, we here adopt a fairness de\ufb01nition in the context of decision-making:De\ufb01nition 3 (Fairness) \u201cFairness is the absence of any prejudice or favoritism towardan individual or a group based on their inherent or acquired characteristics\u201d (Mehrabi et al.,2019).Note that even an ideally \u201cfair\u201d AI system de\ufb01ned in a speci\ufb01c context might still lead tobiased decisions as the entire decision making process involves numerous elements such aspolicy makers and environment. While the concept of fairness is di\ufb03cult to pin down, unfair-ness/bias/discrimination might be easier to identify. There are six types of discrimination(Mehrabi et al., 2019). Direct discrimination results from protected attributes of individualswhile indirect discrimination from seemingly neural and non-protected attributes. Systemicdiscrimination relates to policies that may show discrimination against subgroups of pop-ulation. Statistical discrimination occurs when decision makers use average statistics torepresent individuals. Depending whether the di\ufb00erences amongst di\ufb00erent groups can bejusti\ufb01ed or not, we further have explainable and unexplainable discrimination.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "8b7b8c6b-e82c-4207-856a-30f160a7d34a",
                    "text": "Transparency is another important but quite ambiguous concept. This is partly becauseAI alone can be de\ufb01ned in more than 70 ways (Legg, Hutter, et al., 2007). When we seeka transparent algorithm, we are asking for an understandable explanation of how it works(Yeo, 2020): What does the training set look like? Who collected the data? What isthe algorithm doing? There are mainly three types of transparency with regard to humaninterpretability of AI algorithms (Weller, 2017): For a developer, the goal of transparencyis to understand how the algorithm works and get a sense of why; for a deployer who ownsand releases the algorithm to the public, the goal of transparency is to make the consumersto feel safe and comfortable to use the system; and what transparency means to a user isunderstanding what the AI system is doing and why. We may further di\ufb00erentiate globaltransparency from local transparency, the former aims to explain the entire system whereasthe latter explains a decision within a particular context.Yet, at the same time, disclosures about AI can pose potential risks: explanations can behacked and releasing additional information may make AI more vulnerable to attacks. It isbecoming clear that transparency is often bene\ufb01cial but not universally good (Weller, 2017).The AI \u201ctransparency paradox\u201d encourages di\ufb00erent parties of AI systems to think morecarefully about how to balance the transparency and the risks it poses. We can also see re-lated discussions in recent work such as (Slack, Hilgard, Jia, Singh, & Lakkaraju, 2020). Thepaper studied how the widely recognized interpretable algorithms LIME (Ribeiro, Singh,& Guestrin, 2016) and SHAP (Lundberg & Lee, 2017) could be hacked. As the authorsillustrated, explanations can be purposefully manipulated, leading to a loss of trust not onlyin the model but also in its explanations (Slack et al., 2020). Consequently, while workingtowards the goal of transparency, we must also recognize that privacy and security are theindispensable conditions we need to satisfy.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "772a4326-9b05-41d0-8d5c-db42df146e93",
                    "text": "Because AI systems operate in a world with much uncertainty, volatility, and \ufb02ux, anotherobjective of SRAs is to be safe, accurate, and reliable (Varshney & Alemzadeh, 2017). Thereare four operational objectives relevant to Safety: accuracy, reliability, security, and robust-ness (Leslie, 2019). In machine learning, accuracy is typically measured by error rate or thefraction of instances for which the algorithm produces an incorrect output. As a standardperformance metric, accuracy should be the fundamental component to establishing theapproach to safe AI. It is necessary to specify a proper performance measure for evaluatingany AI systems. For instance, when data for classi\ufb01cation tasks is extremely imbalanced,precision, recall, and F1 scores are more appropriate than accuracy. The objective of relia-bility is to ensure that AI systems behave as we anticipate. It is a measure of consistencyand is important to establish con\ufb01dence in the safety of AI systems. Security encompassesthe protection of information integrity, con\ufb01dentiality, and continuous functionality to itsusers. Under harsh conditions (e.g., adversarial attack, perturbations, and implementationerror), AI systems are expected to functions reliably and accurately, i.e., Robustness.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "3976e100-6f64-476d-b807-d137f26dd361",
                    "text": "In this section, we review four primary machine learning techniques and statistical methodsfor achieving the goals of SRAs \u2013 interpretability and explainability, adversarial machinelearning, causal learning, and uncertainty quanti\ufb01cation. Existing surveys have conductedcomprehensive reviews on each of these techniques: e.g., interpretablity (Carvalho et al.,2019; Tjoa & Guan, 2020), causal learning (Guo et al., 2020; Yao, Chu, Li, Li, Gao, &Zhang, 2020), adversarial machine learning (Chakraborty, Alam, Dey, Chattopadhyay, &Mukhopadhyay, 2018; Akhtar & Dasgupta, 2019), and uncertainty quanti\ufb01cation (Kabir,Khosravi, Hosen, & Nahavandi, 2018). We thereby focus on the basics and the most fre-quently discussed methods in each means.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "70097c2a-64f0-490c-81c1-4704e665a8ca",
                    "text": "Interpretability and explanability are the keys to increasing transparency of AI algorithms.This is extremely important when we leverage these algorithms for high-stakes predictionapplications, which deeply impact people\u2019s lives (Rudin, 2019). Existing work in machinelearning interpretability can be categorized according to di\ufb00erent criteria. Depending onwhen the interpretability methods are applicable (before, during, or after building the ma-chine learning model), we have pre-model (before), in-model (during), and post-model (af-ter) interpretability. Pre-model techniques are only applicable to the data itself. It requiresan in-depth understanding of the data before building the model, e.g., sparsity and dimen-sionality. Therefore, it is closely related to data interpretability (Carvalho et al., 2019), inwhich classic descriptive statistics and data visualization methods are often used, includ-ing Principal Component Analysis (Wold, Esbensen, & Geladi, 1987) and t-SNE (Maaten& Hinton, 2008), and clustering methods such as k-means (Hartigan & Wong, 1979). In-model interpretability asks for intrinsically interpretable AI algorithms (e.g., Yang, Yang,Dyer, He, Smola, & Hovy, 2016), we can also refer to it as intrinsic interpretability. Itcan be achieved through imposition of constraints on the model such as causality, sparsity,or physical conditions from domain knowledge (Rudin, 2019). In-model interpretabilityanswers question how the model works (Lipton, 2018). Decision trees, rule-based models,linear regression, attention network, and disentangled representation learning are in-modelinterpretability techniques. Post-model interpretability, or post-hoc interpretability (e.g.,Mordvintsev, Olah, & Tyka, 2015; Ribeiro et al., 2016), is applied after model training.It answers the question what else can the model tell us (Lipton, 2018). Post-model in-terpretability include local explanations (Ribeiro et al., 2016), saliency maps (Simonyan,Vedaldi, & Zisserman, 2013), example-based explanations (Kim, Khanna, & Koyejo, 2016),in\ufb02uence functions (Koh & Liang, 2017), feature visualization (Erhan, Bengio, Courville,& Vincent, 2009), and explaining by base interpretable models (Craven & Shavlik, 1996).Another criterion to group current interpretability techniques is model-speci\ufb01c vs model-agnostic. Model-speci\ufb01c interpretation is based on internals of a speci\ufb01c model (Molnar,2020). To illustrate, the coe\ufb03cients of a linear regression model belong to model-speci\ufb01cinterpretation. Model-agnostic methods do not have access to the model inner workings,rather, they are applied to any machine learning model after it has been trained. Essen-tially, the goal of interpretability is to help the user understand the decisions made bythe machine learning models through the tool explanation. There are pragmatic and non-pragmatic theories of explanation. The former indicates that explanation should be a goodanswer that can be easily understood by the audience. The non-pragmatic theory empha-sizes the correctness of the answer to the why-question. Both need to have the followingproperties (Robnik-\u02c7Sikonja & Bohanec, 2018): expressive power, translucency, portability,and algorithmic complexity.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "d0929901-5234-4d91-b758-960136d1b509",
                    "text": "Machine learning models, especially deep learning models, are vulnerable to crafted adver-sarial examples, which are imperceptible to human eyes but can easily fool deep neuralnetworks (NN) in the testing/deploying stage (Yuan, He, Zhu, & Li, 2019). Adversarialexamples have posed great concerns in the security and integrity of various applications.Adversarial machine learning, therefore, closely relates to the robustness of SRAs.The security of any machine learning model is measured with regard to the adversarialgoals and capabilities (Chakraborty et al., 2018). Identifying the threat surface (Papernot,McDaniel, Sinha, & Wellman, 2016a) of an AI system built on machine learning models iscritical to understand where and how an adversary may subvert the system under attack.For example, the attack surface in a standard automated vehicle system can be de\ufb01nedwith regard to the data processing pipeline. Typically, there are three types of attacks theattack surface can identify: evasion attack \u2013 the adversary attempts to evade the systemby manipulating malicious samples during testing phase, poisoning attack \u2013 the adversaryattempts to poison the training data by injecting carefully designed samples into the learn-ing process, and exploratory attack \u2013 it tries to collect as much information as possibleabout the learning algorithm of the underlying system and pattern in training data. De-pending on the amount of information available to an adversary about the system, we cande\ufb01ne di\ufb00erent types of adversarial capabilities. In the training phase (i.e., training phasecapabilities), there are three broad attack strategies: (1) data injection. The adversary canonly augment new data to the training set; (2) data modi\ufb01cation. The adversary has fullaccess to the training data; and (3) logic corruption. The adversary can modify the learningalgorithm. In the testing phase (i.e., testing phase capabilities), adversarial attacks focus onproducing incorrect outputs. For white-box attack, an adversary has full knowledge aboutthe model used for prediction: algorithm used in training, training data distribution, andthe parameters of the fully trained model. The other type of attack is black-box attack,which, on the contrary, assumes no knowledge about the model and only uses historicalinformation or information about the settings. The primary goal of black-box attack is totrain a local model with the data distribution, i.e., non-adaptive attack, and with carefullyselected dataset by querying the target model, i.e., adaptive attack.Exploratory attacks do not have access to the training data but aim to learn the currentstate by probing the learner. Commonly used techniques include model inversion attack(Fredrikson, Lantz, Jha, Lin, Page, & Ristenpart, 2014; Fredrikson, Jha, & Ristenpart,2015), model extraction using APIs (Tram`er, Zhang, Juels, Reiter, & Ristenpart, 2016),and inference attack (Ateniese, Mancini, Spognardi, Villani, Vitali, & Felici, 2015; Shokri,Stronati, Song, & Shmatikov, 2017). The popular attacks are evasion attacks where ma-licious inputs are craftily manipulated so as to fool the model to make false predictions.Poisoning attacks, however, modify the input during the training phase to obtain the de-sired results. Some of the well-known techniques are generative adversarial network (GAN)(Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, & Bengio, 2014),adversarial examples generation (including training phase modi\ufb01cation, e.g., Barreno, Nel-son, Sears, Joseph, & Tygar, 2006, and testing phase modi\ufb01cation, e.g., Papernot, McDaniel,Wu, Jha, & Swami, 2016b), GAN-based attack in collaborative deep learning (Hitaj, Ate-niese, & Perez-Cruz, 2017), and adversarial classi\ufb01cation (Dalvi, Domingos, Sanghai, &Verma, 2004).",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "3341808a-384a-45b5-afc5-918b8a0672d5",
                    "text": "Causal inference and reasoning is a critical ingredient for AI to achieve human-level in-telligence, an overarching goal of Socially Responsible AI. The momentum of integratingcausality into responsible AI is growing, as witnessed by a number of works (e.g., Kusner,Loftus, Russell, & Silva, 2017; Xu, Wu, Yuan, Zhang, & Wu, 2019; Holzinger, Langs, Denk,Zatloukal, & M\u00a8uller, 2019) studying SRAs through causal learning methods.Basics of Causal Learning. The two fundamental frameworks in causal learning arestructural causal models (Pearl, 2009) and potential outcome (Rubin, 1974). Structuralcausal models rely on the causal graph, which is a special class of Bayesian network withedges denoting causal relationships. A more structured format is referred to as structuralequations. One of the fundamental notions in structural causal models is the do-calculus(Pearl, 2009), an operation for intervention. The di\ufb03culty to conduct causal study is thedi\ufb00erence between the observational and interventional distribution, the latter describeswhat the distribution of outcome Y is if we were to set covariates X = x. Potentialoutcome framework interprets causality as given the treatment and outcome, we can onlyobserve one potential outcome. The counterfactuals \u2013 potential outcome that would havebeen observed if the individual had received a di\ufb00erent treatment \u2013 however, can never beobserved in reality. These two frameworks are the foundations of causal e\ufb00ect estimation(estimating e\ufb00ect of a treatment) and causal discovery (learning causal relations amongstdi\ufb00erent variables).Many important concepts in causal inference have been adapted to AI such as interven-tion and counterfactual reasoning. Here, we introduce the causal concept most frequentlyused in SRAs \u2013 propensity score, de\ufb01ned as \u201cconditional probability of assignment to aparticular treatment given a vector of observed covariates\u201d (Rosenbaum & Rubin, 1983). Apopular propensity-based approach is Inverse Probability of Treatment Weighting (Hirano,Imbens, & Ridder, 2003). To synthesize a randomized control trial (Rubin, 1974), it usescovariate balancing to weigh instances based on their propensity scores and the probabilityof an instance to receive the treatment. Let t and x be the treatment assignment andcovariate of instance i, the weight w is typically computed by the following formula:w = tP (t |x ) + 1 \u2212 t1 \u2212 P (t |x ) , (1)where P (t |x ) quanti\ufb01es the propensity score. The weighted average of the observed out-comes for the treatment and control groups are de\ufb01ned as\u02c6\u03c4 = 1n (cid:88) w y \u2212 1n (cid:88) w y , (2)where n and n denote the sizes of the treated and controlled groups.Causal Learning for SRAs. Firstly, it is becoming increasingly popular to use causalmodels to solve fairness-related issues. For example, the subject of causality and its impor-tance to address fairness issue was discussed in (Loftus, Russell, Kusner, & Silva, 2018).Causal models can also be used to discover and eliminate discrimination to make decisionsthat are irrespective of sensitive attributes, on individual-, group-, and system-level, see,e.g., (Zhang, Wu, & Wu, 2016; Zhang & Wu, 2017; Nabi & Shpitser, 2018). Secondly,bias alleviation is another \ufb01eld where causal learning methods are frequently discussed anda\ufb00ect many machine learning applications at large. The emerging research on debiasing rec-ommender system (Wang, Golbandi, Bendersky, Metzler, & Najork, 2018b; Wang, Liang,Charlin, & Blei, 2018c; Joachims, Swaminathan, & Schnabel, 2017) can serve as one exam-ple. Due to the biased nature of user behavior data, recommender systems inevitably involvewith various discrimination-related issues: recommending less career coaching services andhigh-paying jobs to women (Lambrecht & Tucker, 2019; Datta, Tschantz, & Datta, 2015),recommending more male-authored books (Ekstrand, Tian, Kazi, Mehrpouyan, & Kluver,2018), and minorities are less likely to become social in\ufb02uencers (Karimi, G\u00b4enois, Wagner,Singer, & Strohmaier, 2018; Stoica, Riederer, & Chaintreau, 2018). Gender and ethnicbiases were even found in a broader context, e.g., word embeddings trained on 100 years oftext data (Garg, Schiebinger, Jurafsky, & Zou, 2018). Causal approaches such as (Yang &Feng, 2020) aim to mitigate such bias in word embedding relations.Thirdly, causal learning methods also have had discernible achievements in transparency,especially the interpretability of black-box algorithms. Causality is particularly desiredsince these algorithms only capture correlations not real causes (Mora\ufb00ah, Karami, Guo,Raglin, & Liu, 2020). Further, it has been suggested that counterfactual explanations arethe highest level of interpretability (Pearl, 2018). For model-based interpretations, causalinterpretability aims to explain the causal e\ufb00ect of a model component on the \ufb01nal decision(Chattopadhyay, Manupriya, Sarkar, & Balasubramanian, 2019; Para\ufb01ta & Vitri`a, 2019;Narendra, Sankaran, Vijaykeerthy, & Mani, 2018). One example to di\ufb00erentiate it fromtraditional interpretability is only causal interpretability is able to answer question such as\u201cWhat is the e\ufb00ect of the n-th \ufb01lter of the m-th layer of a neural network on the predictionof the model?\u201d. Counterfactual explanations is a type of example-based explanations, inwhich we look for data instances that can explain the underlying data distributions. Coun-terfactual explanations are human friendly, however, it is possible to have di\ufb00erent trueversions of explanations for the predicted results, i.e., the Rashomon e\ufb00ect (Molnar, 2020).Studies such as (Wachter, Mittelstadt, & Russell, 2017; Grath, Costabello, Van, Sweeney,Kamiab, Shen, & Lecue, 2018; Liu, Kailkhura, Loveland, & Han, 2019) are proposed to ad-dress this issue. For detailed discussion on causal interpretability, please refer to (Mora\ufb00ahet al., 2020). Lastly, causal learning is inherently related to the robustness or adaptabilityof AI systems, which have been noted to lack the capability of reacting to new circum-stances they are not trained for. Causal relationship, however, is expected to be invariantand robust across environments (Pearl, 2019, 2009). This complements intensive earliere\ufb00orts toward \u201ctransfer learning\u201d, \u201cdomain adaptation\u201d, and \u201clifelong learning\u201d (Chen &Liu, 2018). Some current work seeking to extrapolate the relationship between AI robust-ness and causality includes the independent causal mechanism principle (Peters, Janzing,& Sch\u00a8olkopf, 2017; Sch\u00a8olkopf, 2019), invariant prediction (Arjovsky, Bottou, Gulrajani, &Lopez-Paz, 2019), and disentangled causal mechanism (Suter, Miladinovic, Sch\u00a8olkopf, &Bauer, 2019; Bengio, Deleu, Rahaman, Ke, Lachapelle, Bilaniuk, Goyal, & Pal, 2019).",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "82a96220-2773-4200-85e2-a6253618ef93",
                    "text": "AI research continues to develop new state-of-the-art algorithms with superior performanceand large-scaled datasets with high quality. Even using the best models and training data, itis still infeasible for AI systems to cover all the potential situations when deployed into real-world applications. As a matter of fact, AI systems always encounter new samples that aredi\ufb00erent from those used for training. The core question is how to leverage the strengths ofthese uncertainties. Recent research, e.g., (Bhatt, Antor\u00b4an, Zhang, Liao, Sattigeri, Fogliato,Melancon, Krishnan, Stanley, Tickoo, et al., 2020), has advocated to measure, communi-cate, and use uncertainty as a form of transparency. There are also tools such as IBM\u2019sUncertainty Quanti\ufb01cation 360 to provide AI practitioners access to related resources ascommon practices for AI transparency. Consequently, uncertainty quanti\ufb01cation plays acrucial role in the optimization and decision-making process in SRAs. There are typicallytwo kinds of uncertainties in risk analysis processes: \ufb01rst, the aleatory uncertainty describesthe inherent randomness of systems. For example, an AI system can present di\ufb00erent resultseven with the same set of inputs. The uncertainty arises from underlying random variationswithin the data. Second, the epistemic uncertainty represents the e\ufb00ect of an unknown phe-nomenon or an internal parameter. The primary reason leading to this type of uncertaintyis the lack of observed data. As the variation among the data in aleatory uncertainty isoften observable, we can well quantify the uncertainty and assess the risks. Quanti\ufb01cationof epistemic uncertainty is more challenging because AI systems are forced to extrapolateover unseen situations (St\u02daahl, Falkman, Karlsson, & Mathiason, 2020). In the literatureof uncertainty quanti\ufb01cation, one of the most widely recognized techniques are predictionintervals (PI). For neural-network-based models, PI can be categorized into multi-step PIconstruction methods (e.g., Bayesian method) and direct PI construction methods (e.g.,lower upper bound estimation). Here, we brie\ufb02y discuss several methods in each category.Please refer to the survey (Kabir et al., 2018) for more details.Multi-Step Prediction Intervals Construction Methods. Delta method, Bayesianmethod, Mean-Variance Estimation method, and Bootstrap method are the four conven-tional multi-step methods reported in literature. Delta method constructs PIs throughnonlinear regression using Tylor series expansion. Particularly, we linearize neural networkmodels through optimization by minimizing the error-based loss function, sum square error.Under the assumption that uncertainty is from normal and homogeneous distribution, wethen employ standard asymptotic theory to construct PIs. Delta method has been usedin numerous case studies, e.g., (Lu & Viljanen, 2009; Ho, Xie, Tang, Xu, & Goh, 2001).Bayesian learning provides a natural framework for constructing PIs (Kasiviswanathan &Figure 7: Illustration of what Socially Responsible AI Algorithms (SRAs) can do. It re-quires philanthropic responsibilities and ethical responsibilities.Sudheer, 2016; Ungar, De Veaux, & Rosengarten, 1996) as it optimizes the posterior distri-bution of parameters from the assumed prior distribution. Despite its high generalizationpower, Bayesian techniques are limited by large computational complexity due to the calcu-lation of Hessian matrix. Bootstrap method is the most popular among the four conventionalmulti-step PI construction methods (Errouissi, Cardenas-Barrera, Meng, Castillo-Guerra,Gong, & Chang, 2015; Zio, 2006; Dybowski & Roberts, 2001). It includes smooth, para-metric, wild, pairs, residual, Gaussian process, and other types of bootstrap techniques. InNN-based pairs bootstrap algorithm, for example, the key is to generate bootstrapped pairsby uniform sampling with replacement from the original training data. The estimation isthen conducted for a single bootstrapped dataset (Kabir et al., 2018).Direct Prediction Intervals Construction Methods. This category of methods cantackle some of the limitations in previous methods, such as high demanding in computa-tional power and stringent assumptions. When NN models are constructed through directtraining without any assumptions, they can provide more adaptive and smarter PIs for anydistribution of targets (Chu, Li, Pedro, & Coimbra, 2015). Lower Upper Bound estimationmethod is such a technique that can be applied to arbitrary distribution of targets withmore than one order reduced computation time. It directly calculates the lower and theupper bounds through trained NNs. Initially, Lower Upper Bound estimation NNs are opti-mized with the coverage width-based criterion, which presents several limitations. With allthe bene\ufb01ts of the original Lower Upper Bound estimation method, the NN-based DirectInterval Forecasting method (Wan, Xu, Pinson, Dong, & Wong, 2013) has much shortercomputation time and narrower PIs credited to the improved cost function and the reducedaverage coverage error. Other approaches for improving the cost function of Lower Up-per Bound estimation include the normalized root-mean-square width and particle swarmoptimization (Quan, Srinivasan, & Khosravi, 2014), optimal system by (Hosen, Khosravi,Nahavandi, & Creighton, 2014), the independent width and penalty factors (Khosravi,Nahavandi, Srinivasan, & Khosravi, 2014), the deviation from mid-interval consideration(Mar\u00b4\u0131n, Valencia, & S\u00b4aez, 2016), and the deviation information-based criterion (Zhang, Wu,Wong, Xu, Dong, & Iu, 2014).",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "85a70b45-dce6-4f62-b446-f30425fbe8fe",
                    "text": "So far, we have introduced the essentials of SRAs to achieve the expected ethical responsibil-ities. But pragmatic questions regarding their intended use remain: How to operationalizeSRAs? What can SRAs eventually do for societal well-being to address societal challenges?Both ethical and philanthropic responsibilities are indispensable ingredients of the answers.While the ultimate goal of SRAs is to do good and be a good AI citizen, their ethical re-sponsibilities should be ensured \ufb01rst. When AI fails to ful\ufb01ll its ethical responsibilities,its philanthropic bene\ufb01ts can be insigni\ufb01cant. For instance, despite the immense publicgood of COVID-19 vaccines, there has been great controversy about algorithms for theirdistribution, which have been shown to be inequitable (Branswell, 2021). Some argue thatdistribution algorithms should prioritize saving more lives and bringing the economy backmore rapidly (Cowen, 2020); they support such an \u2018unfair\u2019 allocation, but we would arguethat that is not unfairness, but simply a di\ufb00erence of values and ethics. In our view, rolesof SRAs are expected to encompass both ethical and philanthropic responsibilities. In thissurvey, we describe three dimensions that SRAs can help with to improve the quality ofhuman life as illustrated in Figure 7: Protect (e.g., protect users\u2019 personal information),Inform (e.g., fake news early detection), and Prevent/Mitigate (e.g., cyberbullying miti-gation). We illustrate each dimension with research \ufb01ndings in several emerging societalissues. Particularly, for protecting dimension, we focus on privacy preserving and data dig-nity; for informing and preventing/mitigating dimensions, we discuss three societal issuesthat raise growing concerns recently: disinformation, abusive language, and unwanted bias.Because there are many various forms of abusive language such as hate speech and pro-fanity, and the body of work related to each form is vast and diverse, spanning multipleinterconnected disciplines, this survey uses the form of cyberbullying as a representative forthe illustrations.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "5d039fd8-b4bf-461f-a5b9-c3b7b6bd36b2",
                    "text": "The protecting dimension aims to cover or shield humans (especially the most vulnerable orat-risk) from harm, injury, and negative impact of AI systems, in order to intervene. Thiscan be the protection of users\u2019 personal data and their interactions with AI systems. Twotypical examples are privacy preserving and data dignity.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "416fa0f8-76ed-485a-ae93-8d7b97d5f9df",
                    "text": "The capability of deep learning models has been greatly improved by the emerging pow-erful infrastructures such as clouds and collaborative learning for model training. Thefuel of this power, however, comes from data, particularly sensitive data. This has raisedgrowing privacy concerns such as illegitimate use of private data and the disclosure of sen-sitive data (Boulemtafes, Derhab, & Challal, 2020; Beigi & Liu, 2020). Existing threatsagainst privacy are typically from attacks such as the adversarial examples we discussedin Sec. 3.4.2. Speci\ufb01cally, there are direct information exposure (e.g., untrusted clouds),which is caused by direct intentional or unintentional data breaches, and indirect (inferred)information exposure (e.g., parameter inference), which is caused by direct access to themodel or output. Existing privacy-preserving mechanisms can be classi\ufb01ed into three cat-egories, namely, private data aggregation methods, private training, and private inference(Mirshghallah, Taram, Vepakomma, Singh, Raskar, & Esmaeilzadeh, 2020).Data aggregation methods are either context-free or context-aware. A context-free ap-proach such as di\ufb00erential privacy (Dwork, 2008), is unaware of the context or what the datawill be used for. Context-aware approach such as information-theoretic privacy (Schaefer,Boche, Khisti, & Poor, 2017), on the other hand, is aware of the context in which the datawill be used. A na\u00a8\u0131ve technique for privacy protection is to remove identi\ufb01ers from data,such as name, address, and zip code. It has been used for protecting patients\u2019 informa-tion while processing their medical records, but the results are unsatisfying (Sweeney, 2002;Narayanan & Shmatikov, 2008; Homer, Szelinger, Redman, Duggan, Tembe, Muehling,Pearson, Stephan, Nelson, & Craig, 2008). The k-Anonymity method can prevent informa-tion from re-identi\ufb01cation by showing at least k samples with exact same set of attributesfor given combination of attributes that the adversary has access to (Sweeney, 2002). Themost commonly used data aggregation method is di\ufb00erential privacy, which aims to estimatethe e\ufb00ect of removing an individual from the dataset and keep the e\ufb00ect of the inclusion ofone\u2019s data small. Some notable work includes the Laplace mechanism (Dwork, McSherry,Nissim, & Smith, 2006), di\ufb00erential privacy with Advanced Composition (Dwork, Roth,et al., 2014), and local di\ufb00erential privacy (Kairouz, Oh, & Viswanath, 2014; Erlingsson,Pihur, & Korolova, 2014).Information-theoretic privacy is a context-aware approach that explicitly models thedataset statistics. By contrast, context-free methods assume worse-case dataset statisticsand adversaries. This line of research was studied by Diaz, Wang, Calmon, and Sankar(2019), Pinceti, Kosut, and Sankar (2019), and Varodayan and Khisti (2011). The sec-ond type of privacy-preserving mechanism works during the training phase. Establishedwork in private training is mostly used to guarantee di\ufb00erential privacy or semantic se-curity and encryption (Goldwasser & Micali, 1984). The two most common methods forencryption are homomorphic encryption (Gentry, 2009) and secure multi-party computa-tion (Makri, Rotaru, Smart, & Vercauteren, 2019). The third type of privacy-preservingmechanism works during the inference phase. It aims at the trained systems that are de-ployed to o\ufb00er inference-as-a-service (Mirshghallah et al., 2020). Most methods in privateinference are similar to those in private training, except for the information-theoretic pri-vacy (Malekzadeh, Clegg, Cavallaro, & Haddadi, 2019, 2018, 2020). It is typically used too\ufb00er information-theoretic mathematical or empirical evidence of how these methods oper-ate to improve privacy. There is also work using di\ufb00erential privacy (Wang, Zhang, Bao,Zhu, Cao, & Yu, 2018a), homomorphic encryption (Gilad-Bachrach, Dowlin, Laine, Lauter,Naehrig, & Wernsing, 2016; Chabanne, de Wargny, Milgram, Morel, & Prou\ufb00, 2017), andsecure multi-party computation (Liu, Juuti, Lu, & Asokan, 2017).",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "56226ab0-2759-4f5b-9075-27e6fd8ad742",
                    "text": "Beyond privacy preserving, what is more urgent to accomplish is data dignity. It allowsusers to have absolute control to how their data is being used and they are paid accordingly(Getoor, 2019). Data dignity encompasses the following aspects (Grover, 2020):\u2022 To help users objectively determine the bene\ufb01ts and risks associated with their digitalpresence and personal data.\u2022 To let users control how their data will be used and the purpose of using the data.\u2022 To allow users to negotiate the terms of using their data.\u2022 To give users complete right and autonomy to be found, analyzed, or forgotten, apartfrom the fundamental right over their data.There are business models such as the Microsoft Data Bank designed to give users thecontrol of their data and those shared by the Art of Research (Hart, 2019) about howpeople can buy and sell their personal data.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "3e236c12-3115-4d94-9703-80cc4c8cb4d0",
                    "text": "The informing dimension aims to deliver the facts or information to users, particularly thepotential negative results, in a timely way. We illustrate it with a focus on the discussionsof detecting disinformation, cyberbullying, and bias.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "13cba9ab-4c3e-4ef7-be41-5fc90da52435",
                    "text": "Disinformation is false information that is deliberately created and spread to deceive people,a social group, organization, or country (Pacepa & Rychlak, 2013). The online informationecosystem is never short of disinformation and misinformation, and the growing concernshave been raised recently. Tackling disinformation is rather challenging mainly because(1) disinformation exists almost in all domains; (2) it is ever-changing with new problems,challenges, and threats emerging every day; (3) it entails the joint e\ufb00orts of interdisciplinaryresearch \u2013 computer science, social science, politics, policy making, and psychology, cogni-tive science (Bhattacharjee, Shu, Gao, & Liu, 2020). Accurate and e\ufb03cient identi\ufb01cationof disinformation is the core to combat disinformation. Existing prominent approaches fordisinformation detection primarily rely on news content, social context, user comments,fact-checking tools, and explainable and cross-domain detection.Early work on disinformation detection has been focused on hand-crafted features ex-tracted from text, such as lexical and syntactic features (Feng, Banerjee, & Choi, 2012; Ott,Choi, Cardie, & Hancock, 2011). Apart from text, online platforms also provide abundantsocial information that can be leveraged to enrich the textual features, e.g., number of re-tweets and likes on Twitter. Informed by theories in social science and network science,another line of work exploits social network information to improve the detection perfor-mance. Common features are social context (Shu, Wang, & Liu, 2019), user pro\ufb01le (Shu,Wang, & Liu, 2018), user engagement (Shu, Mahudeswaran, Wang, Lee, & Liu, 2020), andrelationships among news articles, readers, and publishers (Della Vedova, Tacchini, Moret,Ballarin, DiPierro, & de Alfaro, 2018). A unique function of online platforms is that theyallow users to interact through comments. Recent work has shown that user comments canprovide weak supervision signal for identifying the authenticity of news articles, which en-ables early detection of disinformation (Shu, Zheng, Li, Mukherjee, Awadallah, Ruston, &Liu, 2020). When the user comments are unavailable, it is possible to learn users\u2019 responseto news articles and then generate user responses (Qian, Gong, Sharma, & Liu, 2018).Fact-checking can be achieved manually or automatically. Manual fact-checking relies ondomain experts or crowdsourced knowledge from users. Automatic fact-checking uses struc-ture knowledge bases such as knowledge graph to verify the authenticity of news articles,see, e.g., (Ciampaglia, Shiralkar, Rocha, Bollen, Menczer, & Flammini, 2015). Beyondwithin-domain detection, other tasks such as cross-domain detection (Janicka, Pszona, &Wawer, 2019), explanation (Shu, Cui, Wang, Lee, & Liu, 2019), and causal understandingof fake news dissemination (Cheng, Guo, Shu, & Liu, 2021) have also been discussed inliterature.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "81712a0d-da93-4bfa-a135-f972af1a5991",
                    "text": "Cyberbullying di\ufb00ers from other forms of abusive language in that it is not an one-o\ufb00incident but \u201caggressively intentional acts carried out by a group or an individual usingelectronic forms of contact, repeatedly or over time against victims who cannot easily defendthemselves\u201d (Smith, Mahdavi, Carvalho, Fisher, Russell, & Tippett, 2008). The increasinglyreported number of cyberbullying cases on social media and the resulting detrimental impacthave raised great concerns in society. Cyberbullying detection is regularly \ufb01gured as a binaryclassi\ufb01cation problem. While it shares some similarities with document classi\ufb01cation, itshould be noted that cyberbullying identi\ufb01cation is inherently more complicated than simplyidentifying oppressive content (Salawu, He, & Lumsden, 2017).Distinct characteristics of cyberbullying such as power imbalance and repetition of ag-gressive acts are central to marking a message or a social media session (Cheng, Silva, Hall,& Liu, 2020b) as cyberbullying. Several major challenges in cyberbullying detection havebeen discussed in literature such as the formulation of the unique bullying characteristics,e.g., repetition, data annotation, and severe class imbalance. Depending on the employedfeatures, established work can be classi\ufb01ed into four categories: content-based, sentiment-based, user-based, and network-based methods. Features extracted from social media con-tent are lexical items such as keywords, Bag of Words, pronoun and punctuation. Empiricalevaluations have shown that textual features are the most informative predictors for cyber-bullying detection (Cheng, Shu, Wu, Silva, Hall, & Liu, 2020a). For instance, using numberof o\ufb00ensive terms as content features is e\ufb00ective in detecting o\ufb00ensive and cursing behav-ior (Dinakar, Jones, Havasi, Lieberman, & Picard, 2012; Dadvar, Trieschnigg, Ordelman,& de Jong, 2013; Kontostathis, Reynolds, Garron, & Edwards, 2013); Computing contentsimilarity between tweets from di\ufb00erent users can help capture users\u2019 personality traits andpeer in\ufb02uence, two important factors of cyberbullying occurrences (Cheng, Li, Silva, Hall,& Liu, 2019a). Sentiment-based features typically include key-words, phrases and emojis,and they are often combined with content-based features (Dani, Li, & Liu, 2017). A notablework (Xu, Jun, Zhu, & Bellmore, 2012) identi\ufb01ed seven types of emotions in tweets suchas anger, empathy, and fear. User-based features are typical characteristics of users, e.g.,personality (e.g., hostility), demographics (e.g., age), and user activity (e.g., active users(Balakrishnan, 2015)). Hostility and neuroticism are found to be strongly related to cyber-bullying behavior (Biel, Aran, & Gatica-Perez, 2011; Mishna, Khoury-Kassabri, Gadalla,& Daciuk, 2012). Further, gender and age are indicative of cyberbullying in certain cases(Al-garadi, Varathan, & Ravana, 2016). Network-based features measure the sociabilityof online users, e.g., number of friends, followers, and network embeddedness (Cheng, Li,Silva, Hall, & Liu, 2019b; Huang, Singh, & Atrey, 2014). In addition, a number of methodsseek to capture the temporal dynamics to characterize the repetition of cyberbullying, suchas (Cheng, Guo, Silva, Hall, & Liu, 2019; Chen & Li, 2020; Ge, Cheng, & Liu, 2021; Cheng,Guo, Silva, Hall, & Liu, 2021).",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "23a86124-b38d-413b-afe3-c3cc5cdb0b14",
                    "text": "Compared to the well-de\ufb01ned notions of fairness, bias detection is much less studied andthe solution is not as straightforward as it may seem (Fu, Huang, & Singh, 2020). Thechallenges arise from various perspectives. First, the data and algorithms used to make adecision are often not available to policy makers or enforcement agents. Second, algorithmsare becoming increasingly complex and the uninterpretability limits an investigator\u2019s abilityto identify systematic discrimination through analysis of algorithms. Rather, they have toexamine the output from algorithms to check for anomalous results, increasing the di\ufb03cultyand uncertainty of the task.Data exploratory analysis is a simple but e\ufb00ective tool to detect data bias. In this initialstep of data analysis, we can use basic data statistics and visual exploration to understandwhat is in a dataset and the characteristics of the data. For algorithmic bias, one of theearliest methods is to compare the selection rate of di\ufb00erent groups. Discrimination is highlypossible if the selection rate for one group is su\ufb03ciently lower than that for other groups. Forexample, the US Equal Employment Opportunity Commission (EEOC) advocates the \u201cfour-\ufb01fths rule\u201d or \u201c80% rule\u201d (Feldman, Friedler, Moeller, Scheidegger, & Venkatasubramanian,2015) to identify a disparate impact. Suppose Y denotes a binary class (e.g., hire or not),A is the protected attribute (e.g., gender), a dataset presents disparate impact ifP r(Y = 1|A = 0)P r(Y = 1|A = 1) \u2264 \u03c4 = 0.8. (3)However, statistical disparity does not necessarily indicate discrimination. If one group hasdisproportionately more quali\ufb01ed members, we may expect the di\ufb00erences between groupsin the results.A more frequently used approach is regression analysis (Ayres, 2010), which is per-formed to examine the likelihood of favorable (or adverse) decisions across groups basedon sensitive attributes. A signi\ufb01cant, non-zero coe\ufb03cient of the sensitive attributes givena correctly speci\ufb01ed regression signals the presence of discrimination. However, we cannotguarantee to observe all the factors the decision maker considers. Therefore, instead ofusing rate at which decisions are made (e.g., the loan approval rates), bias detection can bebased on the success rate of the decisions (e.g., the payback rate of the approved applicants(Becker, 1993)), i.e., the outcome test. Another less popular statistical approach for biasdetection is benchmarking. The major challenge of benchmarking analysis is identifyingthe distribution of the sensitive attributes of the benchmark population where sensitiveattributes are unlikely to in\ufb02uence the identi\ufb01cation of being at-risk. Some solutions canbe seen in (McConnell & Scheidegger, 2001; Lange, Blackman, & Johnson, 2001). Re-cently, AI researchers have developed tools to automatically detect bias. For instance,drawing on techniques in natural language processing and moral foundation theories, thetool by Mokhberian, Abeliuk, Cummings, and Lerman (2020) can understand structureand nuances of content consistently showing up on left-leaning and right-leaning news sites,aiming to help consumers better prepare for unfamiliar news source. In earlier e\ufb00orts, aninternational research group launched a non-pro\ufb01t organization Project Implicit in 1998aimed at detecting implicit social bias.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "938d4eb3-71e6-4bee-829a-11dc107d0f37",
                    "text": "If both of the \ufb01rst two dimensions fail, we may rely on the last dimension to prevent/mitigatethe negative impact of socially indi\ufb00erent AI algorithms on the end-users. We continue thediscussions about disinformation, cyberbullying, and bias, with a focus on the preventionand mitigation strategies.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "90fecb0b-2944-4e29-83a4-53715aa50354",
                    "text": "Preventing the generation/spread of disinformation and mitigating its negative impact isan urgent task because disinformation typically spread faster (Vosoughi, Roy, & Aral, 2018)than normal information due to the catchy news content and the ranking algorithms oper-ating behind the online news platforms. To increase user engagement, social recommendersystems are designed to recommend popular posts and trending content. Therefore, disin-formation often gains more visibility. An e\ufb00ective approach for disinformation mitigationis to govern this visibility of news, e.g., recommendation and ranking based algorithms.Mitigation also relates to early detection.Network intervention can slow down the spread of disinformation by in\ufb02uencing theexposed users in a social network. For example, we can launch a counter-cascade thatconsists of fact-checked version of false news articles. This is commonly referred to as thein\ufb02uence limitation or minimization problem (Bhattacharjee et al., 2020). Given a networkwith accessible counter-cascade, the goal is to \ufb01nd a (minimum) set of nodes in this networksuch that the e\ufb00ect of the original cascade can be minimized. A variety of approximationalgorithms (Budak, Agrawal, & El Abbadi, 2011; Nguyen, Yan, Thai, & Eidenbenz, 2012)have been proposed to solve the NP-hard problem and the variants. When applied todisinformation mitigation, they seek to inoculate as many nodes as possible in a shortperiod of time. It is possible to extend the two cascades into tasks with multiple cascades,where we can further consider the di\ufb00erent priorities of these cascades, i.e., each cascadein\ufb02uences the node in the network di\ufb00erently (Tong, Du, & Wu, 2018). The second methodfor disinformation mitigation is content \ufb02agging: social media platforms allow users to \u2018\ufb02ag\u2019or \u2018report\u2019 a news content if they \ufb01nd it o\ufb00ensive, harmful, and/or false. Big social mediacompanies such as Facebook hired professional moderators to manually investigate and/orremove these content. However, considering the millions of news generated/spread everyminute (Marr, 2021), it is impractical for these moderators to manually review all the news.The solution comes to the crowd wisdom \u2013 users can choose to \u2018\ufb02ag\u2019 the content if it violatesthe community guidelines of the platform. Some platforms can further provide feedback forthese users about if their fact-check is correct or not. User behavior is an e\ufb00ective predictorfor disinformation detection (Cheng et al., 2021), therefore, the third prevention methodleverages the di\ufb00erences between user behaviors to identify susceptible or gullible users. Forexample, it is shown in (Rajabi, Shehu, & Purohit, 2019) that groups of vulnerable Twitterusers can be identi\ufb01ed in fake news consumption. Other studies (Guess, Nagler, & Tucker,2019) also suggest that older people are more likely to spread disinformation.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "26b2b4f9-6893-42a8-969f-e937e312f451",
                    "text": "In contrast to the large amount of work in cyberbullying detection, e\ufb00orts for its preven-tion and mitigation have been a few. Some research suggests that prevention/mitigationstrategies are de\ufb01ned at di\ufb00erent levels (Smith et al., 2008). At technological level, we canconsider providing parental control service, \ufb01rewall blocking service, online services rules,text-message control, and mobile parental control, e.g., KnowBullying and BullyBlocker(Silva, Rich, & Hall, 2016). Another e\ufb00ective tool is psychological approach, such as talk-ing and listening to cyber-victims, providing counseling services, encouraging victims tomake new relations and join social clubs. At education level, we are responsible to edu-cate end-users, help improve their technical and cognitive skills. At administrative level,it is important for organizations and government to develop policies to regulate using freeservice and enhance workplace environment. Therefore, the goal of cyberbullying preven-tion/mitigation can only be accomplished with interdisciplinary collaborations, e.g., psy-chology, public health, computer science, and other behavioral and social sciences (Kraft &Wang, 2009). One example is that computer and social scientists attempted to understandbehavior of users in realistic environments by designing social media site for experimenta-tion such as controlled study and post-study survey (DiFranzo, Taylor, Kazerooni, Wherry,& Bazarova, 2018; Ashktorab, 2017).Existing solutions to preventing cyberbullying can report/control/warn about messagecontent (e.g., Dinakar et al., 2012; Vishwamitra, Zhang, Tong, Hu, Luo, Kowalski, & Mazer,2017), provide support for victims (e.g., Vishwamitra et al., 2017), and educate both victimsand bullies (e.g., Dinakar et al., 2012). A variety of anti-bully apps are also availableto promote well-being of users. For example, NoMoreBullyingMe App provides onlinemeditation techniques to support victims; \u201cHonestly\u201d App (Shaul, 2015) encourages users toshare positive responses with each other (e.g., sing a song). However, current cyberbullyingprevention strategies often do not work as desired because of the complexity and nuancewith which adolescents bully others online (Ashktorab & Vitak, 2016).",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "15deadd4-c837-4868-97a9-540a216cfe5a",
                    "text": "Prior approaches for bias mitigation focus on either designing fair machine learning al-gorithms or theorizing on the social and ethical aspects of machine learning discrimina-tion (Caton & Haas, 2020). From the technical aspect, approaches to fairness typicallycan be categorized into pre-processing (prior to modelling), in-processing (at the point ofmodelling), and post-processing (after modelling). One condition to use pre-processing ap-proaches is that the algorithm is allowed to modify the training data (Bellamy, Dey, Hind,Ho\ufb00man, Houde, Kannan, Lohia, Martino, Mehta, Mojsilovi\u00b4c, et al., 2019). We can thentransform the data to remove the discrimination (d\u2019Alessandro, O\u2019Neil, & LaGatta, 2017).In-processing approaches eliminate bias by modifying algorithms during the training pro-cess (d\u2019Alessandro et al., 2017). We can either incorporate fairness notion into the objectivefunction or impose fairness constraint (Bellamy et al., 2019; Berk, Heidari, Jabbari, Joseph,Kearns, Morgenstern, Neel, & Roth, 2017). When neither training data nor model can bemodi\ufb01ed, we can use post-processing approaches to reassign the predicted labels based on ade\ufb01ned function and a holdout set which was not used in the model training phase (Bellamyet al., 2019; Berk et al., 2017). Most of these approaches are built on the notion of protectedor sensitive variables that de\ufb01ne the (un)privileged groups. Commonly used protected vari-ables are age, gender, marital status, race, and disabilities. A shared characteristic of thesegroups is they are disproportionately (less) more likely to be positively classi\ufb01ed. Fair-ness measures are important to quantify fairness in the development of fairness approaches.However, creating generalized notions of fairness quanti\ufb01cation is a challenging task (Caton& Haas, 2020). Depending on the protected target, fairness metrics are usually designed forindividual fairness (e.g., every one is treated equally), group fairness (e.g., di\ufb00erent groupssuch as women vs men are treated equally), or subgroup fairness. Drawing on theories incausal inference, individual fairness also includes counterfactual fairness which describesthat a decision is fair towards an individual if the result was same when s/he had taken adi\ufb00erent sensitive attribute (Kusner et al., 2017).Recent years have witnessed immense progress of fair machine learning \u2013 a variety ofmethods have been proposed to address bias and discrimination over di\ufb00erent applications.We focus on two mainstream methods: fair classi\ufb01cation and fair regression. A review ofmachine learning fairness can be referred to (Mehrabi et al., 2019; Caton & Haas, 2020).(1) Fair Classi\ufb01cation. For a (binary) classi\ufb01er with sensitive variable S, the target variableY , and the classi\ufb01cation score R, general fairness desiderata have three \u201cnon-discrimination\u201dFair machine learning algorithms need to adopt/create speci\ufb01c fairness de\ufb01nitions that\ufb01t into context (Goel, Yaghini, & Faltings, 2018; Kamishima, Akaho, Asoh, & Sakuma,2012; Krasanakis, Spyromitros-Xiou\ufb01s, Papadopoulos, & Kompatsiaris, 2018; Menon &Williamson, 2018; Calders & Verwer, 2010). Common methods in fair classi\ufb01cation includeblinding (Calmon, Wei, Vinzamuri, Ramamurthy, & Varshney, 2017; Hardt, Price, & Sre-bro, 2016), causal methods (Galhotra, Brun, & Meliou, 2017; Johnson, Foster, & Stine,2016), transformation (Feldman et al., 2015; Dwork, Hardt, Pitassi, Reingold, & Zemel,2012; Wei, Ramamurthy, & Calmon, 2020), sampling and subgroup analysis (Celis, Desh-pande, Kathuria, & Vishnoi, 2016; Dwork, Immorlica, Kalai, & Leiserson, 2018), adversariallearning (Feng, Yang, Lyu, Tan, Sun, & Wang, 2019; Xu et al., 2019; Sattigeri, Ho\ufb00man,Chenthamarakshan, & Varshney, 2019), reweighing (Jiang & Nachum, 2020; Calders &Verwer, 2010), and regularization and constraint optimization (Kamishima et al., 2012;Bechavod & Ligett, 2017; Cheng, Mosallanezhad, Silva, Hall, & Liu, 2021).(2) Fair Regression. The goal of fair regression is to jointly minimize the di\ufb00erence betweentrue and predicted values and ensure fairness. It follows the general formulation of fair clas-si\ufb01cation but with continuous rather than binary/categorical target variable. Accordingly,the fairness de\ufb01nition, metrics, and the basic algorithms are adapted from classi\ufb01cationto regression. For example, it is suggested using statistical parity and bounded-group-lossmetrics to measure fairness in regression (Agarwal, Dud\u00b4\u0131k, & Wu, 2019). Bias in linear re-gression is considered as the e\ufb00ects of a sensitive attribute on the target variable through themean di\ufb00erence between groups and AUC metrics (Calders, Karim, Kamiran, Ali, & Zhang,2013). One commonly used approach in fair regression is regularization, e.g., (Kamishimaet al., 2012; Berk et al., 2017).Apart from fair machine learning, algorithm operators are encouraged to share enoughdetails about how research is carried out to allow others to replicate it. This is a leap formitigating bias as it helps end-users with di\ufb00erent technical background to understand howthe algorithm works before making any decision. It is also suggested that AI technologistsFigure 8: Primary challenges and open problems we confront in developing SRAs. Somechallenges relate to SRAs\u2019 internal mechanisms that ful\ufb01ll AI\u2019s ethical responsi-bilities whilst others relate to SRAs\u2019 roles to which both ethical and philanthropicresponsibilities are the keys.and researchers develop a bias impact statement as a self-regulatory practice. It can helpprobe and avert any potential biases that are injected into or resultant from algorithmic de-cision (Lee, Resnick, & Barton, 2019). Some example questions in the statement are \u201cWhatwill the automated decision do?\u201d, \u201cHow will potential bias be detected?\u201d, and \u201cWhat arethe operator incentives\u201d. In algorithm design, researchers are also responsible to encouragethe role of diversity within the team, training data, and the level of cultural sensitivity. The\u201cdiversity-in-design\u201d mechanism aims to take deliberate and transparent actions to addressthe upfront cultural biases and stereotypes. Furthermore, we might also consider updat-ing nondiscrimination and other civil rights laws to interpret and redress online disparateimpacts (Lee et al., 2019). An example of such consideration is to unambiguously de\ufb01nethe thresholds and parameters for the disparate treatments of protected groups before thealgorithm design.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "98cbef73-326f-4706-98e3-d57f6046036a",
                    "text": "This survey reveals that the current understanding of SRAs is insu\ufb03cient and future e\ufb00ortsare in great need. Here, we describe several primary challenges, as summarized in Figure8, in an attempt to broaden the discussions on future directions and potential solutions.Causal Learning. The correlation fallacy causes AI algorithms to meet with fundamentalobstacles in order to commit social responsibility. These obstacles are robustness, explain-ability, and cause-e\ufb00ect connections (Pearl, 2019). The era of big data has changed theways of learning causality, and meanwhile, causal learning becomes an indispensable ingre-dient for AI systems to achieve human-level intelligence. There are a number of bene\ufb01ts toincorporate causality in the next-generation of AI. For example, teaching AI algorithms tounderstand \u201cwhy\u201d can help them transfer their knowledge to di\ufb00erent but similar domains.Early e\ufb00orts in SRAs attempted to employ causal learning concept and methods such asintervention, counterfactual, do-calculus, propensity scoring to address fairness (e.g., coun-terfactual fairness) and interpretability (causal interpretability) issues. They have shownprominent results in these tasks.Context Matters. Context is the core to SRAs due to its inherently elaborate nature,e.g., the \u201cTransparency Paradox\u201d. Understanding and quantifying the relationships amongthe various principles (some are tradeo\ufb00s and some are not), e.g., fairness, transparency,and safety, have to be placed in speci\ufb01c context. One such context is the social context.Existing SRAs (e.g., fair machine learning), once introduced into a new social context,may render current technical interventions ine\ufb00ective, inaccurate, and even dangerouslymisguided (Selbst, Boyd, Friedler, Venkatasubramanian, & Vertesi, 2019). A recent study(S\u00a8uhr, Hilgard, & Lakkaraju, 2020) found that while fair ranking algorithms such as Det-Greedy (Geyik, Ambler, & Kenthapadi, 2019) help increase the exposure of minority candi-dates, their e\ufb00ectiveness is limited by the job contexts in which employers have a preferenceto particular genders. How to properly integrate social context into SRAs is still an openproblem. Algorithmic context (e.g., supervised learning, unsupervised learning, and rein-forcement learning) is also extremely important when designing SRAs for the given data.A typical example is the feedback loop problem in predictive policing (Ensign, Friedler,Neville, Scheidegger, & Venkatasubramanian, 2018). A subtle algorithmic choice can havehuge rami\ufb01cations on the results. Consequently, we need to understand the algorithmic con-text to make the right algorithmic choices when designing socially responsible AI systems.Designing context-aware SRAs is the key to achieving Social Responsibility of AI.Responsible Model Release and Governance. Nontransparent model reporting isone of the main causes of AI indi\ufb00erent behaviors. As a critical step to clarify the in-tended use cases of AI systems and the contexts for which they are well suited, responsiblemodel release and governance has been receiving growing attentions from both industry andacademia. One role of SRAs is to bring together the tools, solutions, practices, and peopleto govern the built AI systems across its life cycle (Dobrin, 2020). At this early stage,some research results suggested that released models be accompanied by documentationdetailing various characteristics of the systems, e.g., what it does, how it works, and whyit matters. For example, the AI FactSheets (Arnold, Bellamy, Hind, Houde, Mehta, Mo-jsilovi\u00b4c, Nair, Ramamurthy, Olteanu, Piorkowski, et al., 2019) advocates to use a factsheetcompleted and voluntarily released by AI developers to increase the transparency of theirservices. A similar concept is model cards (Mitchell, Wu, Zaldivar, Barnes, Vasserman,Hutchinson, Spitzer, Raji, & Gebru, 2019), short documents that provide benchmarkedevaluation for the trained AI models in a variety of conditions, e.g., di\ufb00erent cultural ordemographic groups. Typically, a model card should include the model details, intendeduse, evaluation metrics, training/evaluation data, ethical considerations, and caveats andrecommendations. To help increase transparency, manage risk, and build trust in AI, AItechnologists and researchers are responsible to address various challenges faced in creatinguseful AI release documentation (Hind, Houde, Martino, Mojsilovic, Piorkowski, Richards,& Varshney, 2020) and develop e\ufb00ective AI governance tools.AI Defenses. Developing AI systems that outwit malicious AI is still at an early stage(Knight, 2020). Since we have not fully understood how AI systems work, they are not onlyvulnerable to attack but also likely to fail in surprising ways (Yuan et al., 2019; Chakrabortyet al., 2018). As a result, it is critical and urgent to work on designing systems that areprovably robust to help ensure that the AI systems are not vulnerable to adversaries. Atleast two capabilities an \u201cAI \ufb01rewall\u201d needs to be equipped with: one capability is toprobe an AI algorithm for weaknesses (e.g., perturb the input of an AI system to make itmisbehave) and the other one is to automatically intercept potentially problematic inputs.Some big tech companies have started building their own AI defenses to identify the weakspots, e.g., the \u201cred team\u201d in Facebook, the software framework released by Microsoft,Nvidia, IBM, and 9 other companies. AI defenses re\ufb02ect the fundamental weakness inmodern AI and make AI systems more robust and intelligent.AI Ethics Principles and Policies. Current AI principles and policies for ethical prac-tice have at least two common criticisms: (1) they are too vaguely formulated to prove to behelpful in guiding practice; and (2) they are de\ufb01ned primarily by AI researchers and pow-erful people with mainstream populations in mind (Young, Magassa, & Friedman, 2019).For the \ufb01rst criticism, to help operationalize AI principles in practice and organizationsconfront inevitable value trade-o\ufb00s, it has been suggested to rede\ufb01ne AI principles based onphilosophical theories in applied ethics (Canca, 2020). Particularly, it categorizes publishedAI principles (e.g., fairness, accountability, and transparency) into three widely used coreprinciples in applied ethics: autonomy, bene\ufb01cence (avoiding harm and doing good), andjustice. The core principles \u201cinvoke those values that theories in moral and political philos-ophy argue to be intrinsically valuable, meaning their value is not derived from somethingelse\u201d (Canca, 2020). Existing AI principles are instrumental principles that \u201cbuild on con-cepts whose values are derived from their instrumental e\ufb00ect in protecting and promotingintrinsic values\u201d (Canca, 2020). Operationazable AI principles help e\ufb00ectively put ethicalAI in practice and reduce the responsible AI Gap in companies. To address the secondcriticism, we need to best elicit the inputs and values of diverse voices from the Subjects ofSRAs, i.e., the minority and disadvantaged groups, and incorporate their perspectives intothe tech policy document design process. If we align values of AI systems through a panelof people (who are compensated for doing this), they too can in\ufb02uence the system behavior,and not just the powerful people or AI researchers.Understanding Why. Many AI systems are designed and developed without fully un-derstanding why: What do we wish the AI system do? This is often the reason that thesesystems fail to represent the goals of the real tasks, a primary source of AI risks. Theproblem can become more challenging when the AI system is animated through a numberof lines of code that lack nuance, creating a machine that does not align with our trueintentions. As the \ufb01rst step, understanding why clearly de\ufb01nes our social expectation of AIsystems and paves way for more speci\ufb01c questions such as \u201cWhat is the problem? Who willde\ufb01ne it? and what are the right people to include?\u201d. Answering why helps us e\ufb00ectivelyabolish the development of socially indi\ufb00erent AI systems in the \ufb01rst place and also helpsunderstand the kinds of deception an AI system may learn by itself.Long-Term E\ufb00ect. SRAs include social concepts such as fairness that can evolve overtime along with the constant changes of human values and social dynamics. This raises theconcerns about the commitment SRAs need to ful\ufb01ll in the long term. For example, despitethe various types of fairness de\ufb01nitions, once introduced into the dimension of time, thenumber of fairness de\ufb01nitions may be explosive. In addition, current fairness criteria maybe considered as unfair in the future. Fairness criteria are essentially designed to promotelong-term well-being. However, even a static fairness notion can fail to protect the targetgroups when there is a feedback loop in the overall system (Liu, Dean, Rolf, Simchowitz,& Hardt, 2018). How to build AI systems that can commit long-term responsibility isextremely challenging and rarely studied thus far. Initial results of long-term fairness (Liuet al., 2018; Hu & Chen, 2018) highlight the importance of measurement and temporalmodeling in the evaluation of fairness criteria.Humans in the Loop. While existing techniques in SRAs have indeed made signi\ufb01cantprogress towards responsible AI systems, their usefulness can be limited in some settingswhere the decisions made are actually poorer for every individual. For issues of fairness inprediction, for example, many \ufb01ndings (e.g., Pfohl, Foryciarz, & Shah, 2020) have shownthe concerns about the fairness-performance trade-o\ufb00: the imposition of fairness comes ata cost to model performance. Predictions are less reliable and moreover, di\ufb00erent notionsof fairness can make approaches to fairness con\ufb02ict with one another. Having human inthe loop matters when it comes to contextualizing the objectives of SRAs, especially forhigh-stake decisions. For instance, there are situations where the cut-o\ufb00 values of fairnessfor two subgroups are di\ufb00erent, and humans can help calibrate the di\ufb00erences.Responsible AI Gap in Industry. The far-reaching e\ufb00ect of reputational damage andemployee disengagement result from AI misbehavior has forced company executives to beginunderstanding the risks of poorly designed AI systems and the importance of SRAs. Whileseeing many potential bene\ufb01ts of developing responsible AI systems such as increasingmarket share and long-term pro\ufb01tability, companies lack the knowledge of how to cross the\u201cResponsible AI Gap\u201d between principles and tangible actions (Mills, Baltassis, Santinelli,Carlisi, Duranton, & Gallego, 2020). This is partly because companies view responsibleAI solely as risk-avoidance mechanism and overlook its \ufb01nancial rewards. To capture thebene\ufb01ts of responsible AI in companies\u2019 day-to-day business, companies need to go farbeyond SRAs and examine every aspect of the end-to-end AI systems. A recent article(Mills et al., 2020) suggested six basic steps to bridge the gulf between responsible AI andthe reality: Empower responsible AI leadership, Develop principles, policies, and training,Establish human and AI governance, Conduct Responsible AI reviews, Integrate tools andmethods, and Build and test a response plan. Even though the gap might be huge, smalle\ufb00orts built over time can let SRAs achieve a transformational impact on the businesses.Interdisciplinary Research. Current public dialog on SRAs has been focused on a narrowsubset of \ufb01elds, blinding us to the opportunities presented by interdisciplinary research. It isnecessary to work with researchers from di\ufb00erent disciplines whose contributions are sorelyneeded, e.g., psychologist, social scientist, educators, and humanities. Non-pro\ufb01t organiza-tions are both the bene\ufb01ciaries and benefactors of SRAs. In partnering with non-pro\ufb01ts andsocial enterprises will not only unleash AI\u2019s potential for bene\ufb01ting societal well-being, butalso let AI technologists and researchers have the opportunity to encounter the real prob-lems we are currently facing. A better understanding of what problems need to be solvedhelps identify SRAs that need to be created. Moreover, as big tech companies bankrollmore work of academic researchers, much of ethics-based research gets concentrated in thehands of a few companies that can a\ufb00ord it (Rivero, 2020). This is problematic because weare over reliant on the same companies that are producing socially indi\ufb00erent AI systems.We need interdisciplinary and decentralized research to create SRAs and simultaneouslyachieve the four levels in the pyramid of Social Responsibility of AI.SRAs for Social Good. The last challenge regards the intended use of SRAs. WhenSRAs are leveraged to uplift humanity, a trust in AI is further enhanced. There has beena burgeoning of AI-for-social-good movement that produces AI algorithms to help reducepoverty, hunger, inequality, injustice, climate change, ill health, and other causes of humansu\ufb00ering (Varshney, 2019). Compared to deploying cutting-edge AI systems to solve thesecritical issues, a more urgent question to examine is \u201cWhat makes an AI project good\u201d in or-der to prevent the detrimental consequences of AI. In addition to Protecting, Informing, andPreventing, social good applications also relate closely to Fundraise and Greenlight (Siegel,2020). Applying SRAs to target solicitations for donations largely helps with fundraising fornon-pro\ufb01ts, charitable organizations, and universities. Greenlight describes how SRAs canhelp allocate grants and other types of resources by predicting the success rates of projectproposals. It plays an important role in improving execution e\ufb00ectiveness of organizations.Developing social good applications that leverage power of SRAs to bene\ufb01t society is anequally endeavor for AI technologists and researchers.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                },
                {
                    "id": "a5233a62-14aa-434d-a7ab-2f2e7a6bc6a3",
                    "text": "This survey examines multiple dimensions of research in Social Responsibility of AI, seek-ing to broaden the current discussions primarily focused on decision-making algorithms thatperform scoring and classi\ufb01cation tasks. We argue that having a full scope of AI to capturethe connections among all the major dimensions is the key to Socially Responsible AI Al-gorithms (SRAs). This work starts with an inclusive de\ufb01nition of Social Responsibility ofAI, highlighting the principles (e.g., Fairness, Inclusiveness), means (e.g., SRAs), and ob-jective (e.g., improving humanity). To better frame the Social Responsibility of AI, we alsointroduce the pyramid with four-level responsibilities of AI systems: functional responsi-bilities, legal responsibilities, ethical responsibilities, and philanthropic responsibilities. Wethen focus our discussions on how to achieve Social Responsibility of AI via the proposedframework SRAs. In the de\ufb01nition of SRAs, we emphasize that the functional and societalaspects are integral parts of AI algorithms. Given that both the functional and legal respon-sibilities are the usual focuses in AI research and development, we particularly investigatethe essentials to achieve AI\u2019s ethical responsibilities: the subjects, causes, objectives, andmeans. For the intended use (i.e., roles) of SRAs, we discuss the need of philanthropic andethical responsibilities for AI systems to protect and inform users, and prevent/mitigate thenegative impact. We conclude with several open problems and major challenges in SRAs.At this pivotal moment in the development of AI, it is of vital importance to discuss AIethics and specify Social Responsibility of AI. Drawing from the theory of moral license(List & Momeni, 2018) \u2013 when humans are good, we give ourselves moral license to be bad\u2013 we argue that simply asking AI to do good is insu\ufb03cient and ine\ufb03cient, and more can bedone for AI technologists and researchers to develop socially responsible AI systems. Wehope this work can propel future research in various \ufb01elds to tackle together the challengesand steer a course towards a bene\ufb01cial AI future.AcknowledgementsThis material is based upon work supported by, or in part by, the U.S. Army ResearchLaboratory (ARL), the U.S. Army Research O\ufb03ce (ARO), the O\ufb03ce of Naval Research(ONR) under contract/grant numbers W911NF2110030, W911NF2020124, and N00014-21-1-4002, as well as by the National Science Foundation (NSF) grants 1909555 and 2036127.We thank Dr. Lise Getoor and Dr. Hosagrahar V. Jagadish for their invaluable suggestions.",
                    "reference": "[1] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially responsible AI algorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence Research. Retrieved from https://www.jair.org/index.php/jair/article/download/12814/26713/."
                }
            ]
        },
        {
            "paper_title": "The role and challenges of education for responsible AI",
            "authors": "V Dignum",
            "publication_info": "London Review of Education - discovery.ucl.ac.uk",
            "paper_url": "https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf",
            "chunks": [
                {
                    "id": "a7af8ae7-bbf4-479e-8c9a-759d542ef612",
                    "text": "Artificial intelligence (AI) is impacting education in many different ways. From virtual assistants for personalized education, to student or teacher tracking systems, the potential benefits of AI for education often come with a discussion of its impact on  privacy  and  well-being.  At  the  same  time,  the  social  transformation  brought about by AI requires reform of traditional education systems. This article discusses what a responsible, trustworthy vision for AI is and how this relates to and affects education.IntroductionDigital  transformation  is  currently  seen  as  the  driving  force  of  global,  innovative, inclusive  and  sustainable  growth.  Education  is  necessary  to  prepare  people  for  the opportunities and challenges of globalization and the digital revolution and to ensure that  everyone  can  fully  participate  in,  benefit  from  and  adapt  to  new  occupations and  skills  needs.  In  many  cases,  artificial  intelligence  (AI)  is  considered  to  provide an essential contribution to ensure the inclusive and cohesive societies and resilient economies that are expected to arise from this digital transformation. However, given the wide range of views on the role and possibilities of AI in society, it is not always clear how to bring AI and education together towards these aims. Even at the beginning of the twenty-first century, Aiken and Epstein (2000: 164) stated: \u2018Unless we seriously discuss  our  philosophical  premises  before  AI  moves  in  any  significant  way  into  the classroom, we will limit the scope, effectiveness and positive contributions that AI can bring  to  learning.\u2019  Understanding  the  possibilities  and  limits  of  AI  and  ensuring  the necessary human skills need to be the focus of education and training programmes at different levels. Nevertheless, and even though the quantity and quality of robust evidence that shows that well-designed AI does work in education is increasing (Luckin and Cukurova, 2019), the field of education often questions the educational value of technology (Selwyn, 2015; Slay et\u00a0al., 2008).There is a general feeling that the current educational system is not generating enough  experts,  and  everyday  users  are  increasingly  not  able  to  understand  the systems  with  which  they  are  faced.  These  issues  have  been  extensively  studied  by others. This article is not about all the potential impacts and uses of AI in education, but it focuses on what a responsible, trustworthy vision for AI is and how this relates to and affects education and learning studies. Even though science, technology, engineering and mathematics (STEM) education is necessary, responsible AI renders the need for education  in  arts  and  humanities  even  more  necessary.  In  a  world  where  machines can find (all) answers, it becomes imperative that all people are well trained in asking questions and evaluating answers.The many faces of AIUnderstanding  the  role  of  education  in  the  realization  of  the  digital  transformation fuelled by AI requires that we first are able to understand what AI is and what makes AI different from other technologies, so specific educational changes are needed. The recent success of AI, and the hype around it, have created a plethora of definitions, ranging from ultra-simplified ones, such as that recently put forward in a White Paper by the European Commission (2020: 2) \u2013 \u2018AI is a collection of technologies that combine data, algorithms and computing power\u2019 \u2013 to purely magical ones. Depending on the focus  and  the  context,  Theodorou  and  Dignum  (2020)  have  identified  the  following ways in which AI has been viewed: \u00a01.  A  (computational)  technology  that  is  able  to  infer  patterns  and  possibly  draw conclusions from data. (Currently AI technologies are often based on machine learning and/or neural networking based paradigms.)2.  A  next  step  in  the  digital  transformation.  This  view  brings  under  the  general denominator  of  AI  many  different  technologies,  from  robotics  to  the  Internet of  Things,  and  from  data  analytics  to  cybersecurity,  the  result  of  which  is  that everything is considered to be AI.3.  A field of scientific research. This is the original reference, and it is still predominant in  academia.  The  field  of  AI  includes  the  study  of  theories  and  methods  for adaptability, interaction and autonomy of machines (virtual or embedded).4.  An (autonomous) entity (for example, when one refers to \u2018an\u2019 AI). This is the most usual reference in media and science fiction, endowing AI with all-knowing, all-powerful qualities and bringing with it the (dystopic) view of magical powers and the feeling that AI happens to us without us having any power to control it.A  more  informed  view  describes  AI  as  a  software  system  (possibly  embedded  in hardware) designed by humans that, given a complex goal, is able to take a decision based  on  a  process  of  perception,  interpretation  and  reasoning  based  on  data collected about the environment and that meets the properties of:  \u2022 autonomy, meaning that the system is able to deliberate and act with the intent of reaching some task-specific goal without external control  \u2022 adaptability,  meaning  that  the  system  is  able  to  sense  its  environment  and update its behaviour to changes in the environment \u2022 interactivity,  meaning  that  the  system  acts  in  a  physical  or  digital  dimension where people and other systems coexist. Even  though  many  AI  systems  currently  only  exhibit  some  of  these  properties,  it  is their combination that is at the basis of the current interest in, and results of, AI and that fuels the public\u2019s fears and expectations (Dignum, 2019). The scientific study of AI includes several approaches and techniques, including not only machine learning (deep  learning  and  reinforcement  learning  are  specific  examples),  which  is  currently the approach to AI that is most visible and successful, but also reasoning (for example, planning,  scheduling,  knowledge  representation,  search  and  optimization),  multi-agent  systems  (for  example,  distributed  problem  solving  and  communication)  and intelligent robotics (for example, control, perception, sensors and actuators, and the integration of physical/hardware components).Responsible AITo  understand  the  societal  impact  of  AI,  one  needs  to  realize  that  AI  systems  are more than just the sum of their software components. AI systems are fundamentally socio-technical, including the social context in which they are developed, used and acted upon, with its variety of stakeholders, institutions, cultures, norms and spaces. That  is,  it  is  fundamental  to  recognize  that,  when  considering  the  effects  and  the governance  of  AI  technology,  or  the  artefact  that  embeds  that  technology,  the technical component cannot be separated from the socio-technical system (Hendler and Mulvehill, 2016; Dignum, 2019). This system includes people and organizations in  many  different  roles  (developer,  manufacturer,  user,  bystander,  policymaker  and so  on),  their  interactions,  and  the  procedures  and  processes  that  organize  those interactions. Guidelines, principles and strategies must focus on this socio-technical view of AI. In fact, it is not the AI artefact or application that is ethical, trustworthy or responsible.  Rather,  it  is  the  people  and  organizations  that  create,  develop  or  use these systems that should take responsibility and act in consideration of human values and ethical principles, such that the overall system and its results can be trusted by society.  The  ethics  of  AI  is  not,  as  some  may  claim,  a  way  to  give  machines  some kind of \u2018responsibility\u2019 for their actions and decisions and, in the process, discharge people  and  organizations  of  their  responsibility.  On  the  contrary,  AI  ethics  requires more responsibility and accountability from the people and organizations involved: for the decisions and actions of the AI applications and for their own decision to use AI in a given application context. The  processes  by  which  systems  are  developed  entail  a  long  list  of  decisions by  designers,  developers  and  other  stakeholders,  many  of  them  of  a  societal,  legal or ethical nature. Typically, many different options and decisions are taken during the design process, and in many cases there is not one clear \u2018right\u2019 choice. These decisions cannot just be left to be made by those who engineer the systems, nor to those who manufacture  or  use  them;  they  require  societal  awareness  and  informed  discussion. Determining which decisions an AI system can take, and deciding how to develop such systems, are at the core of a responsible approach to AI. At the very least, responsible AI means that these choices and decisions must be explicitly reported and open to inspection. The ART (accountability, responsibility, transparency) principles for responsible and trustworthy AI (Dignum, 2019), depicted in Figure 1, are therefore meant for the Figure 1: The ART of AI (Source: Dignum, 2019)whole AI socio-technical system, rather than being focused on the software. In other words, addressing ART will require a socio-technical approach to design, deployment and use of systems, interweaving software solutions with governance and regulation. The  ART  principles  for  responsible  AI  can  be  summarized  as  the  following requirements for the socio-technical AI system: \u2022 Accountability  refers  to  the  necessity  for  the  system  to  provide  account,  that is,  to  explain  and  justify  its  decisions  to  users  and  other  relevant  actors.  To ensure accountability, decisions should be derivable from, and explained by, the decision-making  mechanisms  used.  It  also  requires  that  the  moral  values  and societal norms that inform the purpose of the system, as well as their operational interpretations, have been elicited in an open way involving all stakeholders. \u2022 Responsibility  refers  to  the  role  of  people  themselves  in  their  relation  to  AI systems. As the chain of responsibility grows, means are needed to link an AI system\u2019s decisions to its input data and to the actions of stakeholders involved in the system\u2019s decision. Responsibility is not just about making rules to govern intelligent  machines.  but  also  about  the  role  of  people  and  institutions  in  the effects of developing and using the system. \u2022 Transparency  indicates  the  capability  to  describe,  inspect  and  reproduce  the mechanisms  through  which  AI  systems  make  decisions  and  learn  to  adapt  to their environment and the provenance and dynamics of the data that is used and created by the system. Moreover, trust in the system will improve if we can ensure openness  of  affairs  in  all  that  is  related  to  the  system.  As  such,  transparency is also about being explicit and open about choices and decisions concerning data  sources,  development  processes  and  stakeholders.  Stakeholders  should also be involved in decisions on all models that use human data, affect human beings or have other morally significant impacts.Responsible AI and educationCurrent awareness of ethical issues relating to AI and education are mostly restricted to  privacy,  security  and  the  appropriate  uses  of  personal  data.  In  addition  to  these, in  settings  where  there  is  an  increasing  availability  of  virtual  assistants  supporting students\u2019 learning processes or teachers\u2019 performance, concerns are growing around the  impact  of  such  assistants  on  the  learner  (Tuomi,  2018;  Popenici  and  Kerr,  2017). Students  may  also  find  it  difficult  to  experience  or  believe  in  success  when  using  a learning  companion  that  remembers  and  reminds  the  student  of  their  past  failures (Luckin et\u00a0al., 2016). A study reported in Hastie et\u00a0al. (2016) shows worse results with the  empathic  version  of  an  agent  that  reminded  students  of  what  they  had  learnt. Wearables and smart assistants keep track of many of our activities and those of our children.  A  visit  to  the  gym,  therapist  or  supermarket,  or  a  quiet  afternoon  at  home, can  produce  psychometric,  physiological,  financial,  emotional  and  social  information that could be used to build an affective user model and, as such, potentially improve personalized and appropriate responses (Richards, 2017). However, the sharing of user models that capture an individual\u2019s inner thoughts and feelings could potentially impact that individual if revealed to their employer, family, friends or to the wider public. When we consider the context of learning and education, such monitoring could be seen as a way to identify bullying and supporting children to cope (Hall et\u00a0al., 2009). Hudlicka  (2016)  identifies  several  ethical  issues  that  go  beyond  the  general concerns of data privacy and that are specific to virtual agents. These concerns include affective privacy (the right to keep your thoughts and emotions to yourself), emotion induction (changing how someone feels) and virtual relationships (where the human enters  a  relationship  with  the  agent).  Concerning  student  data  and  student  privacy, potential conflicts can arise from the proliferation of systems for continuous evaluation of students. In fact, even if parents want to protect the personal data of their children, they also want access to that data themselves, while children and young people may want privacy from their parents. For example, in the Netherlands, the Magister tracking system used by most secondary schools provides parents with direct access to their children\u2019s results and activities at school. Often, the parent will know before the child whether they passed or failed a test. In a study, 70 per cent of parents indicated that they thought that their child was not bothered by the fact that parents receive school information about their child through the tracking system. The parents also thought that  their  involvement  increased  by  having  direct  access  to  information  on  grades, homework  and  absence  (Boon,  2018).  However,  concerns  over  breaching  children\u2019s right to privacy have been raised in parliament and by national ombudsmen (Kok, 2018). In fact, adolescents need a certain degree of freedom to learn to take responsibility. Schools  and  parents  must  allow  that  space.  Dealing  with  these  conflicts  of  interest, and the protection of the rights of vulnerable people such as children and the mentally disabled, will need carefully designed standards and legislation. Potential  social  and  ethical  issues  are  raised  as  educational  technology  is increasingly endowed with more smart functionalities (Richards and Dignum, 2019). A  literature  review  focusing  on  the  use  of  (humanoid)  robots  in  the  classroom  has analysed  their  ethical  impact  along  four  themes:  (1)  privacy;  (2)  replacing  humans; (3)  effects  on  children;  and  (4)  responsibility  (Serholt  et\u00a0 al.,  2017).  At  the  same time,  many  fundamental  questions  about  AI  \u2013  on  the  nature  of  intelligence,  how to  balance  individual  and  collective  interests,  how  to  solve  ethical  dilemmas  and how automation will impact the labour market \u2013 cannot be answered by technology alone. These questions require interdisciplinary approaches and, as such, a change in the role and content of educational programmes (Dwivedi et\u00a0al., 2019). AI offers the  potential  for  augmentation  and  potential  replacement  of  human  tasks  and activities  within  a  wide  range  of  applications.  The  current  pace  of  change  for  AI innovation  is  high,  requiring  societal,  institutional  and  technological  adjustments, and new opportunities for continued innovation across different domains, including business and management, government, public sector and science and technology. In order to navigate this potential, explore opportunities and mediate challenges, it is essential to integrate humanities and social sciences into the conversation on law, economics,  ethics  and  the  impact  of  AI  and  digital  technology.  Only  together  can we chart a way forward into a beneficial and trustworthy future with our increasingly algorithmic societal systems. The rapidly growing capabilities and increasing presence of AI-based systems in  our  lives  raise  pressing  questions  about  the  impact,  governance,  ethics  and accountability of these technologies around the world (Dwivedi et\u00a0al., 2019). How can decisions be made on when, what for and how AI should be applied? How can the variety of views and requirements of people who use, interact with and are impacted by  these  technologies  be  integrated?  How  do  we  harness  the  potential  of  AI systems, while ensuring that they do not exacerbate existing inequalities and biases, or  even  create  new  ones?  These  questions  cannot  be  answered  from  a  computer science or engineering perspective alone. In fact, we can say that AI is no longer an engineering discipline, but requires a broad involvement of different disciplines and participants.  Here  is  where  education  and  learning  studies  have  an  important  role to play. The learning sciences field is interdisciplinary and encompasses psychology, sociology,  computer  science,  education  and  cognitive  science.  Bringing  learning studies together with AI research and development will increase the understanding of teaching and learning within those who develop AI, which can contribute to better machine-learning techniques and applications. At the same time, such collaborations contribute to the ability of education specialists, teachers and learners to understand and  be  confident  using  AI  (Luckin  and  Cukurova,  2019).  However,  most  current  AI and robotics curricula worldwide provide engineers with a too-narrow task view. The wide impact of AI on society, as argued in Dignum (2020), requires a broadening of engineering education to include: 1.  analysis  of  the  distributed  nature  of  AI  applications  as  these  integrate  socio-technical systems and the complexity of human\u2013agent interaction 2.  reflection  on  the  meaning  and  global  effect  of  the  autonomous,  emergent, decentralized, self-organizing character of distributed learning entities and how they operate 3.  incremental design and development frameworks, and the unforeseen positive and negative influence of individual decisions at a system level, as well as how these impact human rights, democracy and education 4.  the  consequences  of  inclusion  and  diversity  in  design,  and  how  these  inform processes and results 5.  understanding  of  governance  and  normative  issues,  not  only  in  terms  of competences and responsibilities, but also in the case of views on health, safety, risks, explanations and accountability 6.  the underlying societal, legal and economic models of socio-technical systems. Broadening  AI  and  engineering  curricula  is  possibly  also  a  way  to  attract  a  more diverse  student  population.  When  AI  curricula  are  known  to  be  transdisciplinary,  it can be expected that female students, who traditionally choose humanities and social sciences subjects over engineering ones, may be motivated to choose AI. In parallel, humanities and social sciences curricula also need to include subjects on the theory and practice of AI. For example, law curricula need to prepare law experts on how to address legal and regulatory issues around AI.Guidelines and regulatory frameworks for AIThere  is  increasing  recognition  that  AI  should  be  developed  with  a  focus  on  the human consequences as well as the economic benefits. As such, most guidelines and recommendations explicitly consider the need for education and training, in particular to ensure the technical skills needed to drive the role of AI in the digital transformation, even  though  they  are  particularly  short  on  providing  concrete  approaches  for educational transformation.Governance  is  necessary  for  the  reduction  of  incidents,  to  ensure  trust,  and for  the  long-term  stability  of  society  through  the  use  of  well-established  tools  and design  practices.  Well-designed  regulations  do  not  eliminate  innovation,  as  some claim; instead, they can enhance innovation through the development and promotion of  both  socio-legal  and  technical  means  to  enforce  compliance  (Monett  and  Lewis, 2017). Moreover, policy is needed to ensure human responsibility for the development and deployment of intelligent systems, filling the gap that emerges from the increased automation of decision making. The ultimate aim of regulation is to ensure our \u2013 and our societies\u2019 \u2013 well-being in a sustainable world. That is, research, development and use of AI should always be done in a responsible way \u2013 what is often referred to as responsible AI. When developing intelligent systems, besides the obvious necessity to meet legal obligations, societal values and moral considerations must also be taken into account, weighing the respective priorities of values held by different stakeholders in various multicultural contexts. Human responsibility to ensure flourishing and well-being of our societies should always be at the core of any technological development (Floridi et\u00a0al., 2018).According to the Organisation for Economic Co-operation and Development (OECD,  2019:  n.p.):  \u2018Governments  should  take  steps,  including  through  social dialogue, to ensure a fair transition for workers as AI is deployed, such as through training  programmes  along  the  working  life,  support  for  those  affected  by displacement, and access to new opportunities in the labour market.\u2019 The  European  High  Level  Expert  Group  on  AI  devotes  a  whole  chapter  of  its recommendations document to skills and education, encouraging member states to increase digital literacy across Europe, to provide data literacy education to government agencies, to ensure a leading role for Europe in fundamental AI research programmes, retaining  and  attracting  world-class  researchers  and  to  ensure  the  upskilling  and reskilling  of  the  current  workforce  (European  Commission,  n.d.).  This  means  that proficiency in education in STEM subjects can position students to be competitive in the workforce. In the view of the High Level Expert Group: Conscious  and  well-informed  children  and  other  individuals  will  create a  solid  foundation  for  responsible  and  positive  uses  of  AI  systems  and digital  technologies  more  generally,  and  strengthen  their  personal skills  on  cognitive,  social  and  cultural  levels.  This  will  not  only  increase the  available  talent  pool,  but  also  foster  the  relevance  and  quality  of research and innovation of AI systems for society as a whole. (European Commission,\u00a0n.d.)At the same time, as suggested by the G20, ongoing training in the workplace should be reinforced to help workers adapt (Twomey, 2018). It is not only that a human impact review  should  be  part  of  the  AI  development  process  and  that  a  workplace  plan for  managing  disruption  and  transitions  should  be  part  of  the  deployment  process, but  governments  should  also  plan  for  transition  support  as  jobs  disappear  or  are significantly changed.Besides the need for increased attention to AI technology skills, several groups recognize  the  need  for  introducing  ethics  into  STEM  education  (Villani,  2018,  Taebi et\u00a0al., 2019). This is aligned with the overall view that ensuring students are prepared for  the  changing  labour  market  will  be  the  main  challenge  for  education  curricula. Curricula  should  focus  on  the  development  of  those  skills  that  are  likely  to  remain in  demand  (sometimes  referred  to  as  \u2018twenty-first-century  skills\u2019)  and  thus  prioritize teaching critical thinking, problem solving and teamwork across subject areas and at all  education  levels,  from  kindergarten  to  university.  Teaching  students  to  become analytical thinkers, problem solvers and good team members will allow them to remain competitive in the job market, even as the nature of work changes. The role of education is also recognized as a way to build and sustain trust in AI systems, alongside reliability, accountability, processes to monitor and evaluate the integrity of AI systems over time, and the tools and techniques ensuring compliance with norms and standards (Jobin, 2019). Challenges for educationAt  all  levels  and  in  all  domains,  businesses  and  governments  are,  or  will  soon  be, applying AI solutions to a myriad of products and services. It is fundamental that the general public moves from passively adopting or rejecting technology to being in the forefront of the innovation process, demanding and reflecting on the potential results and reach of AI. The success of AI is therefore no longer a matter of financial profit alone, but of how it connects directly to human well-being. Putting human well-being at  the  core  of  development  provides  not  only  a  sure  recipe  for  innovation,  but  also both a realistic goal and a concrete means to measure the impact of AI. The  extent  to  which  various  technologies  are  adopted  and  supported  in educational  institutions  is  often  politically  driven  or  at  least  constrained  (Selwyn, 2020;  Jandric\u00b4,  2017).  While  governments  may  launch  grand-sounding  initiatives such as the \u2018Digital Education Revolution\u2019 (Buchanan, 2011), often the aims are to provide all students with computers and schools with access to networks and digital resources and to equip teachers, students and parents to use digital technologies. While laudable, these aims are not sufficient, and they are certainly not concerned with the use of state-of-the-art technology or new pedagogies. Moreover, teachers often do not have permission, interest, capacity or energy to participate in trials and experiments  with  advanced  technology  applications  in  the  classroom,  even  when the  materials  are  carefully  aligned  to  the  national  curriculum  and  designed  with teachers  to  cover  concepts  and  topics  that  students  find  challenging  (Jacobson et\u00a0al., 2016). Before committing to a future where AI pervades learning, educationalists and technologists need to guide society and governments to understand the potential social  and  ethical  implications  of  this  technology.  Rather  than  worrying  about  AI taking over the world (or at least the classroom), as in the fear of the technological singularity warned of by some (Bostrom, 2016), the main concern should be people\u2019s readiness  to  blindly  accept  the  technology  as  a  given,  which  leads  to  practical problems, including misuse, overreliance, ignorance or underutilization, abuse and use without concern for the consequences (Parasuraman and Riley, 1997). It is also striking  to  see  that  most  guidelines  and  principles  that  have  sprouted  in  the  last couple  of  years,  from  governments,  policy  organizations,  social  agencies  or  tech companies,  are  mostly  lacking  in  concrete  proposals  for  education,  even  though most  recognize  that  education  will  play  an  important  role  in  ensuring  trustworthy and responsible AI.From  an  education  perspective,  a  pressing  question  is  how  to  ensure  the knowledge and skills to develop and deploy AI systems that align with fundamental human principles and values, and with our legal system, and that serve the common good. As industry, research, the public sector and society in general are increasingly experimenting  with,  and  applying,  AI  across  many  different  domains,  governments and  policymakers  are  looking  at  AI  governance,  that  is,  the  means  to  shape  the process  of  decision  making  in  ways  that  ensure  public  safety,  social  stability  and continued innovation. For policy to deliver that goal, it needs to be supported by a solid  knowledge  not  only  of  the  technical  aspects  of  AI,  but  also  of  its  implications for law, ethics, economy and society. This requires a multidisciplinary approach to the study and development of AI. The digital transformation of society is possibly the main challenge of this century. By the end of 2013, those who have grown up in a digital world started to outnumber those who had to adapt to it. However, capacity building to ensure that everybody is able to contribute to the digital ecosystem, and to fully participate in the workforce, is lagging behind, and current education curricula are perhaps not the most suitable to meet the demands of future work.Considering  that,  as  the  media  theorist  Marshall  McLuhan  is  claimed  to  have said, \u2018the tools that we shape will thereafter shape us\u2019, the digital ecosystem will bring about a redefinition of fundamental human values, including our current understanding of work and wealth. In order to ensure the skills needed for resilient and sustainable capacity building for the digital ecosystem, the following aspects must be central in education curricula across the world: \u2022 Collaborate: The digital ecosystem makes possible, and assumes, collaboration across distance, time, cultures and contexts. The world is indeed a village, and all  of  us  are  the  inhabitants  of  this  village.  Skills  are  needed  to  interact,  build relationships and show the self-awareness needed to work effectively with others in person and virtually, across cultures. \u2022 Question: AI systems are great at finding answers and will do this increasingly well. It is up to us to ask the right questions and to critically evaluate results to contribute to responsible implementation of solutions. \u2022 Imagine:  Skills  to  approach  problem-solving  creatively,  using  empathy,  logic and novel thinking, are needed. For this, it is imperative to nurture humanities education and to include humanities subjects in all technology curricula. \u2022 Learn  to  learn:  The  ability  to  adapt  and  pick  up  new  skills  quickly  is  vital  for success, requiring us to continuously learn and grow and to adapt to change. Being  able  to  understand  what  it  is  necessary  to  know,  and  knowing  when  to apply a particular concept, as well as knowing how to do it, are key to continuous success.The  digital  age  is  a  time  for  reinvention  and  creativity.  Capacity  building  must embrace these skills alongside technological expertise. This shows that the traditional separation  between  humanities,  arts,  social  sciences  and  STEM  is  not  suitable  for the needs of the digital age. More than multidisciplinary, future students need to be transdisciplinary \u2013 to be proficient in a variety of intellectual frameworks beyond the disciplinary perspectives. In fact, we have stated that artificial intelligence is not a pure STEM  discipline  (Dignum,  2019,  2020);  it  is  in  essence  transdisciplinary  and  requires a spectrum of capabilities not covered by current education curricula. It is urgent we redesign studies. This will provide a unique opportunity to truly achieve inclusion and diversity across academic fields.AcknowledgementsThis  work  was  partially  supported  by  the  Wallenberg  AI,  Autonomous  Systems  and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.Notes on the contributorVirginia Dignum is Professor of Responsible Artificial Intelligence at Ume\u00e5 University, Sweden and Scientific Director of WASP-HS, the Wallenberg Program on Humanities and Society for AI, Autonomous Systems and Software. She is a Fellow of the European Artificial  Intelligence  Association,  member  of  several  policy  advice  groups  including the  European  High  Level  Expert  Group  on  AI,  and  author  of  Responsible  Artificial Intelligence: Developing and using AI in a responsible way (Springer, 2019).Declarations and conflict of interestsThe author declares no conflict of interest with this work.References",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                },
                {
                    "id": "1f655fe7-35b8-4583-91e7-913b8d35b070",
                    "text": "Artificial intelligence (AI) is impacting education in many different ways. From virtual assistants for personalized education, to student or teacher tracking systems, the potential benefits of AI for education often come with a discussion of its impact on  privacy  and  well-being.  At  the  same  time,  the  social  transformation  brought about by AI requires reform of traditional education systems. This article discusses what a responsible, trustworthy vision for AI is and how this relates to and affects education.",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                },
                {
                    "id": "838ea89e-4da2-469b-9fe5-05f869770246",
                    "text": "Digital  transformation  is  currently  seen  as  the  driving  force  of  global,  innovative, inclusive  and  sustainable  growth.  Education  is  necessary  to  prepare  people  for  the opportunities and challenges of globalization and the digital revolution and to ensure that  everyone  can  fully  participate  in,  benefit  from  and  adapt  to  new  occupations and  skills  needs.  In  many  cases,  artificial  intelligence  (AI)  is  considered  to  provide an essential contribution to ensure the inclusive and cohesive societies and resilient economies that are expected to arise from this digital transformation. However, given the wide range of views on the role and possibilities of AI in society, it is not always clear how to bring AI and education together towards these aims. Even at the beginning of the twenty-first century, Aiken and Epstein (2000: 164) stated: \u2018Unless we seriously discuss  our  philosophical  premises  before  AI  moves  in  any  significant  way  into  the classroom, we will limit the scope, effectiveness and positive contributions that AI can bring  to  learning.\u2019  Understanding  the  possibilities  and  limits  of  AI  and  ensuring  the necessary human skills need to be the focus of education and training programmes at different levels. Nevertheless, and even though the quantity and quality of robust evidence that shows that well-designed AI does work in education is increasing (Luckin and Cukurova, 2019), the field of education often questions the educational value of technology (Selwyn, 2015; Slay et\u00a0al., 2008).There is a general feeling that the current educational system is not generating enough  experts,  and  everyday  users  are  increasingly  not  able  to  understand  the systems  with  which  they  are  faced.  These  issues  have  been  extensively  studied  by others. This article is not about all the potential impacts and uses of AI in education, but it focuses on what a responsible, trustworthy vision for AI is and how this relates to and affects education and learning studies. Even though science, technology, engineering and mathematics (STEM) education is necessary, responsible AI renders the need for education  in  arts  and  humanities  even  more  necessary.  In  a  world  where  machines can find (all) answers, it becomes imperative that all people are well trained in asking questions and evaluating answers.",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                },
                {
                    "id": "17283b54-b19c-4544-b408-21f4a9e73a74",
                    "text": "Understanding  the  role  of  education  in  the  realization  of  the  digital  transformation fuelled by AI requires that we first are able to understand what AI is and what makes AI different from other technologies, so specific educational changes are needed. The recent success of AI, and the hype around it, have created a plethora of definitions, ranging from ultra-simplified ones, such as that recently put forward in a White Paper by the European Commission (2020: 2) \u2013 \u2018AI is a collection of technologies that combine data, algorithms and computing power\u2019 \u2013 to purely magical ones. Depending on the focus  and  the  context,  Theodorou  and  Dignum  (2020)  have  identified  the  following ways in which AI has been viewed: \u00a01.  A  (computational)  technology  that  is  able  to  infer  patterns  and  possibly  draw conclusions from data. (Currently AI technologies are often based on machine learning and/or neural networking based paradigms.)2.  A  next  step  in  the  digital  transformation.  This  view  brings  under  the  general denominator  of  AI  many  different  technologies,  from  robotics  to  the  Internet of  Things,  and  from  data  analytics  to  cybersecurity,  the  result  of  which  is  that everything is considered to be AI.3.  A field of scientific research. This is the original reference, and it is still predominant in  academia.  The  field  of  AI  includes  the  study  of  theories  and  methods  for adaptability, interaction and autonomy of machines (virtual or embedded).4.  An (autonomous) entity (for example, when one refers to \u2018an\u2019 AI). This is the most usual reference in media and science fiction, endowing AI with all-knowing, all-powerful qualities and bringing with it the (dystopic) view of magical powers and the feeling that AI happens to us without us having any power to control it.A  more  informed  view  describes  AI  as  a  software  system  (possibly  embedded  in hardware) designed by humans that, given a complex goal, is able to take a decision based  on  a  process  of  perception,  interpretation  and  reasoning  based  on  data collected about the environment and that meets the properties of:  \u2022 autonomy, meaning that the system is able to deliberate and act with the intent of reaching some task-specific goal without external control  \u2022 adaptability,  meaning  that  the  system  is  able  to  sense  its  environment  and update its behaviour to changes in the environment \u2022 interactivity,  meaning  that  the  system  acts  in  a  physical  or  digital  dimension where people and other systems coexist. Even  though  many  AI  systems  currently  only  exhibit  some  of  these  properties,  it  is their combination that is at the basis of the current interest in, and results of, AI and that fuels the public\u2019s fears and expectations (Dignum, 2019). The scientific study of AI includes several approaches and techniques, including not only machine learning (deep  learning  and  reinforcement  learning  are  specific  examples),  which  is  currently the approach to AI that is most visible and successful, but also reasoning (for example, planning,  scheduling,  knowledge  representation,  search  and  optimization),  multi-agent  systems  (for  example,  distributed  problem  solving  and  communication)  and intelligent robotics (for example, control, perception, sensors and actuators, and the integration of physical/hardware components).",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                },
                {
                    "id": "90d5bd07-f6e3-480a-9652-29802aa69c77",
                    "text": "To  understand  the  societal  impact  of  AI,  one  needs  to  realize  that  AI  systems  are more than just the sum of their software components. AI systems are fundamentally socio-technical, including the social context in which they are developed, used and acted upon, with its variety of stakeholders, institutions, cultures, norms and spaces. That  is,  it  is  fundamental  to  recognize  that,  when  considering  the  effects  and  the governance  of  AI  technology,  or  the  artefact  that  embeds  that  technology,  the technical component cannot be separated from the socio-technical system (Hendler and Mulvehill, 2016; Dignum, 2019). This system includes people and organizations in  many  different  roles  (developer,  manufacturer,  user,  bystander,  policymaker  and so  on),  their  interactions,  and  the  procedures  and  processes  that  organize  those interactions. Guidelines, principles and strategies must focus on this socio-technical view of AI. In fact, it is not the AI artefact or application that is ethical, trustworthy or responsible.  Rather,  it  is  the  people  and  organizations  that  create,  develop  or  use these systems that should take responsibility and act in consideration of human values and ethical principles, such that the overall system and its results can be trusted by society.  The  ethics  of  AI  is  not,  as  some  may  claim,  a  way  to  give  machines  some kind of \u2018responsibility\u2019 for their actions and decisions and, in the process, discharge people  and  organizations  of  their  responsibility.  On  the  contrary,  AI  ethics  requires more responsibility and accountability from the people and organizations involved: for the decisions and actions of the AI applications and for their own decision to use AI in a given application context. The  processes  by  which  systems  are  developed  entail  a  long  list  of  decisions by  designers,  developers  and  other  stakeholders,  many  of  them  of  a  societal,  legal or ethical nature. Typically, many different options and decisions are taken during the design process, and in many cases there is not one clear \u2018right\u2019 choice. These decisions cannot just be left to be made by those who engineer the systems, nor to those who manufacture  or  use  them;  they  require  societal  awareness  and  informed  discussion. Determining which decisions an AI system can take, and deciding how to develop such systems, are at the core of a responsible approach to AI. At the very least, responsible AI means that these choices and decisions must be explicitly reported and open to inspection. The ART (accountability, responsibility, transparency) principles for responsible and trustworthy AI (Dignum, 2019), depicted in Figure 1, are therefore meant for the Figure 1: The ART of AI (Source: Dignum, 2019)whole AI socio-technical system, rather than being focused on the software. In other words, addressing ART will require a socio-technical approach to design, deployment and use of systems, interweaving software solutions with governance and regulation. The  ART  principles  for  responsible  AI  can  be  summarized  as  the  following requirements for the socio-technical AI system: \u2022 Accountability  refers  to  the  necessity  for  the  system  to  provide  account,  that is,  to  explain  and  justify  its  decisions  to  users  and  other  relevant  actors.  To ensure accountability, decisions should be derivable from, and explained by, the decision-making  mechanisms  used.  It  also  requires  that  the  moral  values  and societal norms that inform the purpose of the system, as well as their operational interpretations, have been elicited in an open way involving all stakeholders. \u2022 Responsibility  refers  to  the  role  of  people  themselves  in  their  relation  to  AI systems. As the chain of responsibility grows, means are needed to link an AI system\u2019s decisions to its input data and to the actions of stakeholders involved in the system\u2019s decision. Responsibility is not just about making rules to govern intelligent  machines.  but  also  about  the  role  of  people  and  institutions  in  the effects of developing and using the system. \u2022 Transparency  indicates  the  capability  to  describe,  inspect  and  reproduce  the mechanisms  through  which  AI  systems  make  decisions  and  learn  to  adapt  to their environment and the provenance and dynamics of the data that is used and created by the system. Moreover, trust in the system will improve if we can ensure openness  of  affairs  in  all  that  is  related  to  the  system.  As  such,  transparency is also about being explicit and open about choices and decisions concerning data  sources,  development  processes  and  stakeholders.  Stakeholders  should also be involved in decisions on all models that use human data, affect human beings or have other morally significant impacts.",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                },
                {
                    "id": "2ddd7983-9024-401b-8c4e-bc1da229325e",
                    "text": "Current awareness of ethical issues relating to AI and education are mostly restricted to  privacy,  security  and  the  appropriate  uses  of  personal  data.  In  addition  to  these, in  settings  where  there  is  an  increasing  availability  of  virtual  assistants  supporting students\u2019 learning processes or teachers\u2019 performance, concerns are growing around the  impact  of  such  assistants  on  the  learner  (Tuomi,  2018;  Popenici  and  Kerr,  2017). Students  may  also  find  it  difficult  to  experience  or  believe  in  success  when  using  a learning  companion  that  remembers  and  reminds  the  student  of  their  past  failures (Luckin et\u00a0al., 2016). A study reported in Hastie et\u00a0al. (2016) shows worse results with the  empathic  version  of  an  agent  that  reminded  students  of  what  they  had  learnt. Wearables and smart assistants keep track of many of our activities and those of our children.  A  visit  to  the  gym,  therapist  or  supermarket,  or  a  quiet  afternoon  at  home, can  produce  psychometric,  physiological,  financial,  emotional  and  social  information that could be used to build an affective user model and, as such, potentially improve personalized and appropriate responses (Richards, 2017). However, the sharing of user models that capture an individual\u2019s inner thoughts and feelings could potentially impact that individual if revealed to their employer, family, friends or to the wider public. When we consider the context of learning and education, such monitoring could be seen as a way to identify bullying and supporting children to cope (Hall et\u00a0al., 2009). Hudlicka  (2016)  identifies  several  ethical  issues  that  go  beyond  the  general concerns of data privacy and that are specific to virtual agents. These concerns include affective privacy (the right to keep your thoughts and emotions to yourself), emotion induction (changing how someone feels) and virtual relationships (where the human enters  a  relationship  with  the  agent).  Concerning  student  data  and  student  privacy, potential conflicts can arise from the proliferation of systems for continuous evaluation of students. In fact, even if parents want to protect the personal data of their children, they also want access to that data themselves, while children and young people may want privacy from their parents. For example, in the Netherlands, the Magister tracking system used by most secondary schools provides parents with direct access to their children\u2019s results and activities at school. Often, the parent will know before the child whether they passed or failed a test. In a study, 70 per cent of parents indicated that they thought that their child was not bothered by the fact that parents receive school information about their child through the tracking system. The parents also thought that  their  involvement  increased  by  having  direct  access  to  information  on  grades, homework  and  absence  (Boon,  2018).  However,  concerns  over  breaching  children\u2019s right to privacy have been raised in parliament and by national ombudsmen (Kok, 2018). In fact, adolescents need a certain degree of freedom to learn to take responsibility. Schools  and  parents  must  allow  that  space.  Dealing  with  these  conflicts  of  interest, and the protection of the rights of vulnerable people such as children and the mentally disabled, will need carefully designed standards and legislation. Potential  social  and  ethical  issues  are  raised  as  educational  technology  is increasingly endowed with more smart functionalities (Richards and Dignum, 2019). A  literature  review  focusing  on  the  use  of  (humanoid)  robots  in  the  classroom  has analysed  their  ethical  impact  along  four  themes:  (1)  privacy;  (2)  replacing  humans; (3)  effects  on  children;  and  (4)  responsibility  (Serholt  et\u00a0 al.,  2017).  At  the  same time,  many  fundamental  questions  about  AI  \u2013  on  the  nature  of  intelligence,  how to  balance  individual  and  collective  interests,  how  to  solve  ethical  dilemmas  and how automation will impact the labour market \u2013 cannot be answered by technology alone. These questions require interdisciplinary approaches and, as such, a change in the role and content of educational programmes (Dwivedi et\u00a0al., 2019). AI offers the  potential  for  augmentation  and  potential  replacement  of  human  tasks  and activities  within  a  wide  range  of  applications.  The  current  pace  of  change  for  AI innovation  is  high,  requiring  societal,  institutional  and  technological  adjustments, and new opportunities for continued innovation across different domains, including business and management, government, public sector and science and technology. In order to navigate this potential, explore opportunities and mediate challenges, it is essential to integrate humanities and social sciences into the conversation on law, economics,  ethics  and  the  impact  of  AI  and  digital  technology.  Only  together  can we chart a way forward into a beneficial and trustworthy future with our increasingly algorithmic societal systems. The rapidly growing capabilities and increasing presence of AI-based systems in  our  lives  raise  pressing  questions  about  the  impact,  governance,  ethics  and accountability of these technologies around the world (Dwivedi et\u00a0al., 2019). How can decisions be made on when, what for and how AI should be applied? How can the variety of views and requirements of people who use, interact with and are impacted by  these  technologies  be  integrated?  How  do  we  harness  the  potential  of  AI systems, while ensuring that they do not exacerbate existing inequalities and biases, or  even  create  new  ones?  These  questions  cannot  be  answered  from  a  computer science or engineering perspective alone. In fact, we can say that AI is no longer an engineering discipline, but requires a broad involvement of different disciplines and participants.  Here  is  where  education  and  learning  studies  have  an  important  role to play. The learning sciences field is interdisciplinary and encompasses psychology, sociology,  computer  science,  education  and  cognitive  science.  Bringing  learning studies together with AI research and development will increase the understanding of teaching and learning within those who develop AI, which can contribute to better machine-learning techniques and applications. At the same time, such collaborations contribute to the ability of education specialists, teachers and learners to understand and  be  confident  using  AI  (Luckin  and  Cukurova,  2019).  However,  most  current  AI and robotics curricula worldwide provide engineers with a too-narrow task view. The wide impact of AI on society, as argued in Dignum (2020), requires a broadening of engineering education to include: 1.  analysis  of  the  distributed  nature  of  AI  applications  as  these  integrate  socio-technical systems and the complexity of human\u2013agent interaction 2.  reflection  on  the  meaning  and  global  effect  of  the  autonomous,  emergent, decentralized, self-organizing character of distributed learning entities and how they operate 3.  incremental design and development frameworks, and the unforeseen positive and negative influence of individual decisions at a system level, as well as how these impact human rights, democracy and education 4.  the  consequences  of  inclusion  and  diversity  in  design,  and  how  these  inform processes and results 5.  understanding  of  governance  and  normative  issues,  not  only  in  terms  of competences and responsibilities, but also in the case of views on health, safety, risks, explanations and accountability 6.  the underlying societal, legal and economic models of socio-technical systems. Broadening  AI  and  engineering  curricula  is  possibly  also  a  way  to  attract  a  more diverse  student  population.  When  AI  curricula  are  known  to  be  transdisciplinary,  it can be expected that female students, who traditionally choose humanities and social sciences subjects over engineering ones, may be motivated to choose AI. In parallel, humanities and social sciences curricula also need to include subjects on the theory and practice of AI. For example, law curricula need to prepare law experts on how to address legal and regulatory issues around AI.",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                },
                {
                    "id": "62926d75-f02e-4704-9cd8-8b1155dfbb96",
                    "text": "There  is  increasing  recognition  that  AI  should  be  developed  with  a  focus  on  the human consequences as well as the economic benefits. As such, most guidelines and recommendations explicitly consider the need for education and training, in particular to ensure the technical skills needed to drive the role of AI in the digital transformation, even  though  they  are  particularly  short  on  providing  concrete  approaches  for educational transformation.Governance  is  necessary  for  the  reduction  of  incidents,  to  ensure  trust,  and for  the  long-term  stability  of  society  through  the  use  of  well-established  tools  and design  practices.  Well-designed  regulations  do  not  eliminate  innovation,  as  some claim; instead, they can enhance innovation through the development and promotion of  both  socio-legal  and  technical  means  to  enforce  compliance  (Monett  and  Lewis, 2017). Moreover, policy is needed to ensure human responsibility for the development and deployment of intelligent systems, filling the gap that emerges from the increased automation of decision making. The ultimate aim of regulation is to ensure our \u2013 and our societies\u2019 \u2013 well-being in a sustainable world. That is, research, development and use of AI should always be done in a responsible way \u2013 what is often referred to as responsible AI. When developing intelligent systems, besides the obvious necessity to meet legal obligations, societal values and moral considerations must also be taken into account, weighing the respective priorities of values held by different stakeholders in various multicultural contexts. Human responsibility to ensure flourishing and well-being of our societies should always be at the core of any technological development (Floridi et\u00a0al., 2018).According to the Organisation for Economic Co-operation and Development (OECD,  2019:  n.p.):  \u2018Governments  should  take  steps,  including  through  social dialogue, to ensure a fair transition for workers as AI is deployed, such as through training  programmes  along  the  working  life,  support  for  those  affected  by displacement, and access to new opportunities in the labour market.\u2019 The  European  High  Level  Expert  Group  on  AI  devotes  a  whole  chapter  of  its recommendations document to skills and education, encouraging member states to increase digital literacy across Europe, to provide data literacy education to government agencies, to ensure a leading role for Europe in fundamental AI research programmes, retaining  and  attracting  world-class  researchers  and  to  ensure  the  upskilling  and reskilling  of  the  current  workforce  (European  Commission,  n.d.).  This  means  that proficiency in education in STEM subjects can position students to be competitive in the workforce. In the view of the High Level Expert Group: Conscious  and  well-informed  children  and  other  individuals  will  create a  solid  foundation  for  responsible  and  positive  uses  of  AI  systems  and digital  technologies  more  generally,  and  strengthen  their  personal skills  on  cognitive,  social  and  cultural  levels.  This  will  not  only  increase the  available  talent  pool,  but  also  foster  the  relevance  and  quality  of research and innovation of AI systems for society as a whole. (European Commission,\u00a0n.d.)At the same time, as suggested by the G20, ongoing training in the workplace should be reinforced to help workers adapt (Twomey, 2018). It is not only that a human impact review  should  be  part  of  the  AI  development  process  and  that  a  workplace  plan for  managing  disruption  and  transitions  should  be  part  of  the  deployment  process, but  governments  should  also  plan  for  transition  support  as  jobs  disappear  or  are significantly changed.Besides the need for increased attention to AI technology skills, several groups recognize  the  need  for  introducing  ethics  into  STEM  education  (Villani,  2018,  Taebi et\u00a0al., 2019). This is aligned with the overall view that ensuring students are prepared for  the  changing  labour  market  will  be  the  main  challenge  for  education  curricula. Curricula  should  focus  on  the  development  of  those  skills  that  are  likely  to  remain in  demand  (sometimes  referred  to  as  \u2018twenty-first-century  skills\u2019)  and  thus  prioritize teaching critical thinking, problem solving and teamwork across subject areas and at all  education  levels,  from  kindergarten  to  university.  Teaching  students  to  become analytical thinkers, problem solvers and good team members will allow them to remain competitive in the job market, even as the nature of work changes. The role of education is also recognized as a way to build and sustain trust in AI systems, alongside reliability, accountability, processes to monitor and evaluate the integrity of AI systems over time, and the tools and techniques ensuring compliance with norms and standards (Jobin, 2019).",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                },
                {
                    "id": "6354601c-8b66-4c83-b26e-3b927d7596e4",
                    "text": "At  all  levels  and  in  all  domains,  businesses  and  governments  are,  or  will  soon  be, applying AI solutions to a myriad of products and services. It is fundamental that the general public moves from passively adopting or rejecting technology to being in the forefront of the innovation process, demanding and reflecting on the potential results and reach of AI. The success of AI is therefore no longer a matter of financial profit alone, but of how it connects directly to human well-being. Putting human well-being at  the  core  of  development  provides  not  only  a  sure  recipe  for  innovation,  but  also both a realistic goal and a concrete means to measure the impact of AI. The  extent  to  which  various  technologies  are  adopted  and  supported  in educational  institutions  is  often  politically  driven  or  at  least  constrained  (Selwyn, 2020;  Jandric\u00b4,  2017).  While  governments  may  launch  grand-sounding  initiatives such as the \u2018Digital Education Revolution\u2019 (Buchanan, 2011), often the aims are to provide all students with computers and schools with access to networks and digital resources and to equip teachers, students and parents to use digital technologies. While laudable, these aims are not sufficient, and they are certainly not concerned with the use of state-of-the-art technology or new pedagogies. Moreover, teachers often do not have permission, interest, capacity or energy to participate in trials and experiments  with  advanced  technology  applications  in  the  classroom,  even  when the  materials  are  carefully  aligned  to  the  national  curriculum  and  designed  with teachers  to  cover  concepts  and  topics  that  students  find  challenging  (Jacobson et\u00a0al., 2016). Before committing to a future where AI pervades learning, educationalists and technologists need to guide society and governments to understand the potential social  and  ethical  implications  of  this  technology.  Rather  than  worrying  about  AI taking over the world (or at least the classroom), as in the fear of the technological singularity warned of by some (Bostrom, 2016), the main concern should be people\u2019s readiness  to  blindly  accept  the  technology  as  a  given,  which  leads  to  practical problems, including misuse, overreliance, ignorance or underutilization, abuse and use without concern for the consequences (Parasuraman and Riley, 1997). It is also striking  to  see  that  most  guidelines  and  principles  that  have  sprouted  in  the  last couple  of  years,  from  governments,  policy  organizations,  social  agencies  or  tech companies,  are  mostly  lacking  in  concrete  proposals  for  education,  even  though most  recognize  that  education  will  play  an  important  role  in  ensuring  trustworthy and responsible AI.From  an  education  perspective,  a  pressing  question  is  how  to  ensure  the knowledge and skills to develop and deploy AI systems that align with fundamental human principles and values, and with our legal system, and that serve the common good. As industry, research, the public sector and society in general are increasingly experimenting  with,  and  applying,  AI  across  many  different  domains,  governments and  policymakers  are  looking  at  AI  governance,  that  is,  the  means  to  shape  the process  of  decision  making  in  ways  that  ensure  public  safety,  social  stability  and continued innovation. For policy to deliver that goal, it needs to be supported by a solid  knowledge  not  only  of  the  technical  aspects  of  AI,  but  also  of  its  implications for law, ethics, economy and society. This requires a multidisciplinary approach to the study and development of AI. The digital transformation of society is possibly the main challenge of this century. By the end of 2013, those who have grown up in a digital world started to outnumber those who had to adapt to it. However, capacity building to ensure that everybody is able to contribute to the digital ecosystem, and to fully participate in the workforce, is lagging behind, and current education curricula are perhaps not the most suitable to meet the demands of future work.Considering  that,  as  the  media  theorist  Marshall  McLuhan  is  claimed  to  have said, \u2018the tools that we shape will thereafter shape us\u2019, the digital ecosystem will bring about a redefinition of fundamental human values, including our current understanding of work and wealth. In order to ensure the skills needed for resilient and sustainable capacity building for the digital ecosystem, the following aspects must be central in education curricula across the world: \u2022 Collaborate: The digital ecosystem makes possible, and assumes, collaboration across distance, time, cultures and contexts. The world is indeed a village, and all  of  us  are  the  inhabitants  of  this  village.  Skills  are  needed  to  interact,  build relationships and show the self-awareness needed to work effectively with others in person and virtually, across cultures. \u2022 Question: AI systems are great at finding answers and will do this increasingly well. It is up to us to ask the right questions and to critically evaluate results to contribute to responsible implementation of solutions. \u2022 Imagine:  Skills  to  approach  problem-solving  creatively,  using  empathy,  logic and novel thinking, are needed. For this, it is imperative to nurture humanities education and to include humanities subjects in all technology curricula. \u2022 Learn  to  learn:  The  ability  to  adapt  and  pick  up  new  skills  quickly  is  vital  for success, requiring us to continuously learn and grow and to adapt to change. Being  able  to  understand  what  it  is  necessary  to  know,  and  knowing  when  to apply a particular concept, as well as knowing how to do it, are key to continuous success.The  digital  age  is  a  time  for  reinvention  and  creativity.  Capacity  building  must embrace these skills alongside technological expertise. This shows that the traditional separation  between  humanities,  arts,  social  sciences  and  STEM  is  not  suitable  for the needs of the digital age. More than multidisciplinary, future students need to be transdisciplinary \u2013 to be proficient in a variety of intellectual frameworks beyond the disciplinary perspectives. In fact, we have stated that artificial intelligence is not a pure STEM  discipline  (Dignum,  2019,  2020);  it  is  in  essence  transdisciplinary  and  requires a spectrum of capabilities not covered by current education curricula. It is urgent we redesign studies. This will provide a unique opportunity to truly achieve inclusion and diversity across academic fields.",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                },
                {
                    "id": "c0d16918-752f-49f3-8149-d6593d0c1703",
                    "text": "This  work  was  partially  supported  by  the  Wallenberg  AI,  Autonomous  Systems  and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                },
                {
                    "id": "67ddeb92-8ef5-4a39-9e27-f4cd10df4cba",
                    "text": "Virginia Dignum is Professor of Responsible Artificial Intelligence at Ume\u00e5 University, Sweden and Scientific Director of WASP-HS, the Wallenberg Program on Humanities and Society for AI, Autonomous Systems and Software. She is a Fellow of the European Artificial  Intelligence  Association,  member  of  several  policy  advice  groups  including the  European  High  Level  Expert  Group  on  AI,  and  author  of  Responsible  Artificial Intelligence: Developing and using AI in a responsible way (Springer, 2019).",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                },
                {
                    "id": "85d9baa6-50c4-47f2-b65e-8872a17c4927",
                    "text": "The author declares no conflict of interest with this work.",
                    "reference": "[1] Virginia Dignum. 2021. The role and challenges of education for responsible AI. London Review of Education. Retrieved from https://discovery.ucl.ac.uk/id/eprint/10121456/1/lre19010001.pdf"
                }
            ]
        },
        {
            "paper_title": "How different groups prioritize ethical values for responsible AI",
            "authors": "M Jakesch, Z Bu\u00e7inca, S Amershi\u2026",
            "publication_info": "Proceedings of the 2022 \u2026 - dl.acm.org",
            "paper_url": "https://arxiv.org/pdf/2205.07722",
            "chunks": [
                {
                    "id": "fd17d3cb-dc5d-4d53-939b-678addeeaf3f",
                    "text": "",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "9af3d297-3215-4ff3-9c68-b391d11f5b02",
                    "text": "Advances in artificial intelligence (AI) have the potential to benefit people and society, but they also raise ethicalchallenges and concerns about possible adverse impacts [52]. Being prone to errors and biases, AI systems mayharm people [2] for instance by reinforcing stereotypes [7] or by increasing social inequality [21]. While the largerconsequences of AI can be difficult to anticipate [8], systems developed with broader human and societal values inmind stand a better chance of preserving these values [1, 23, 57]. To support the development of socially beneficialAI technologies, several private companies, public sector organizations, and academic groups have published ethicsguidelines with values they consider important for responsible AI [37].These AI ethics guidelines have been found to largely converge on five central values [37]: transparency, fairness,safety, accountability, and privacy. But these values may differ from what a broader and more representative populationwould consider important for the AI technologies they interact with. While prior work has shown that value preferencesdepend on peoples\u2019 backgrounds and personal experiences [14, 36], AI technologies are often developed by relativelyhomogeneous and demographically skewed subsets of the population [13, 32, 42]. Given the lack of reliable data onother groups\u2019 priorities for responsible AI, practitioners may unknowingly encode their own biases and assumptionsinto their concept and operationalization of responsible AI [48, 57].In this work, we present the results of a survey we developed, validated, and fielded to elicit peoples\u2019 value prioritiesfor responsible AI. Drawing on the traditions of empirical ethics [19, 53] and value elicitation research [22, 63], oursurvey asks participants about the perceived importance of a set of 12 responsible AI values both in general and inspecific deployment scenarios. To increase robustness, respondents assessed values from three perspectives: valueselection, contextual assessment of values, and comparative prioritization of values (detailed in \u00a73.2).We administered this survey to three different populations. We analyzed how value priorities of a US census-representative sample (N=743), a crowdworker sample (N=755), and an AI practitioner sample (N=175) vary by de-ployment scenario and individuals\u2019 backgrounds and experiences. We surveyed the value priorities of AI practitionersas they are often the ones making decisions about the AI technologies that are being developed, and compared theirpreferences to those of a more representative sample. We also consulted crowdworkers as they are already involved inproducing data that AI systems are evaluated on to explore the feasibility of involving them in the ethical assessment ofAI systems as well.Our results provide evidence that responsible AI values are perceived and prioritized differently by different groups. AIpractitioners, on average, rated responsible AI values less important than other groups. At the same time, AI practitionersprioritized fairness more often than participants from the US-census representative sample who emphasized safety,privacy, and performance. We also find differences in value priorities along demographic lines. For example, womenand black respondents evaluated responsible AI values as more important than other groups. We observed the mostdisagreement in how people traded-off fairness with performance. Surprisingly, participants reporting past experiencesof discrimination did not prioritize fairness more than others, but liberal-leaning participants prioritized fairness morethan conservative-leaning participants.Our results highlight the need for AI practitioners to contextualize and probe their ethical intuitions and assumptions.The empirical approach to AI ethics explored in this study can help to increase the context sensitivity of the responsible AIdevelopment process. However, as we elaborate in the discussion, opinion research can inform ethical decision-making,but cannot replace sound ethical reasoning.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "93f9ad63-17b9-4cb5-ab1a-462b10366e64",
                    "text": "Our study draws on prior work on responsible AI, value sensitive design [23], empirical ethics [53], value elicitation [22,63], and standpoint theory [36].",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "7c22ef31-1d76-4213-866a-4c13363acbff",
                    "text": "Science and technology studies theorize that computing technologies incorporate a tacit understanding of humannature [70]. Algorithms are described as value-laden artifacts [48] that encode developer assumptions, including ethicaland political values [57]. From this perspective, a product team that decides to maximize the chance that a diseasedetection system will recognize a disease at the cost of increasing false alarms prioritizes certain values over others.Past work has shown that machine learning development and research often narrowly focus on technical values suchas accuracy, efficiency, and generalization [6, 54]. In contrast, proponents of value-sensitive design [23, 24], reflectivedesign [64], and critical technical practice [1] advocate that AI systems should be designed with broader human andsocietal values in mind.What values developers of responsible AI systems should emphasize remains a key question. Some argue these valuesshould be naturally embedded in an organization\u2019s culture [57]. Several organizations have also published guidelinesdescribing what values they believe AI systems should embody. Jobin et al. [37] found these guidelines to convergearound central values, but differ in how they construe these values and concepts. Critics note that reliable methodsto translate values into practice are often missing [51, 57]. Some also argue that statements of high-level values andprinciples are too ambiguous and may gain consensus simply by masking the complexity and contending interpretationsof ethical concepts [69]. For example, people may agree on the importance of fairness, but \u201cfairness\u201d in and by itself haslittle to say about what is fair and why [5].Our study validates and contextualizes value priorities outlined in AI ethics guidelines. To date, there is little empiricaldata on values a broader and more representative public finds important for the AI technologies they interact with. Ourempirical approach to AI ethics probes for possible blind spots in AI practitioners\u2019 and researchers\u2019 assumptions.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "cbaca213-1dc6-43e4-a8f1-646dc0542a4d",
                    "text": "Eliciting people\u2019s values is a central pursuit in the social sciences [22]. Economists explain choices in the marketplacebased on value theory, sociologists seek to understand which values are held by a community and how they change.Psychologists use value elicitation for therapy and counsel, and empirical ethicists enhance the context-sensitivityof their arguments by combining social scientific methods with ethical reasoning [53]. While drawing normativeconclusions from empirical results is difficult, empirical data on ethical preferences can inform decision making [53].Several studies have examined people\u2019s ethical intuitions concerning AI technologies. In the \u201cmoral machine\u201dexperiment, Awad et al. [2] generated a variety of moral dilemmas a self-driving car might find itself in and askparticipants which course of action they recommend. They report significant cross-cultural differences in ethicalpreferences correlated with modern institutions and cultural traits. Hidalgo et al. [31] explored how people judgehumans and machines differently when they make mistakes. They found that people tend to forgive machines more inscenarios with high intentionality. Similarly, Malle et al. [47] compared how people apply moral norms to humans versusrobots. Most related to the empirical study of responsible AI values, Saxena et al. [60] have compared public perceptionsof different fairness paradigms. Similarly, Grgic-Hlaca et al. [28] and Pierson [55] have studied which features peoplefind fair to include in a prediction algorithm. They found substantial disagreement among participants [28], with e.g.,women being less likely to include gender as a feature in a course recommendation algorithm if this might result infemale students seeing fewer recommendations for science courses [55].Going beyond previous work, we develop a responsible AI value survey to explore what values people find mostimportant for responsible AI. Where previous studies have elicited preferences concerning specific technical implemen-tations with convenience samples, we provide a first high-level perspective on a representative public\u2019s priorities forthe AI system they interact with and might be affected by.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "f38dcdf2-f5ce-4192-a8d3-de32b70ed9cf",
                    "text": "Feminist empiricists and standpoint theorists argue that knowledge is achieved from a particular standpoint [71]and that social location systematically influences our experiences and decisions [36]. They hold that homogeneouscommunities are prone to false consensus effects [59] where individuals believe that the collective opinion of their owngroup matches that of the larger population. In homogeneous communities, inaccurate assumptions or biases can be hardto recognize and correct [8, 36]. In communities comprised of individuals with diverse values and experiences, however,how assumptions influence reasoning becomes more visible [36, 45, 58]. Including historically underrepresented groups,in particular, may lead to rigorous critical reflection as their experiences may facilitate the identification of problematicbackground assumptions [36].Demographics and experiences not only affect background assumptions [18], but also shape people\u2019s values andethical preferences [25, 27]. Rather than stemming from overarching belief systems, values often arise through particularsocial practices in a specific context [46]. As such, ethical intuition is contextual and socially situated [14]. For instance,what\u2019s fair to some people may seem unfair to others [43], and some people value privacy and autonomy more thanothers [69]. The population of AI practitioners is demographically skewed [13, 32, 42] with e.g., women and blackpeople being underrepresented [16]. With their specific demographics and experiences, AI practitioners may bringtheir own preferences to what it means for AI to be \u201cresponsible\u201d or \u201cethical\u201d, such as a bias towards deployment [40].Responsible AI technologies developed within homogeneous communities may fail to account for the experiences andneeds of various groups, so it remains crucial to scrutinize who gets to define AI ethics [38].By surveying representative population samples about their priorities for responsible AI, we seek to validate thevalue prioritization in AI ethics frameworks. We explore the social relativity of responsible AI values to provide groundsfor more critical reflection about possibly inaccurate assumptions and false consensus effects.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "33a1ec8c-be3e-470d-a27f-546923bdd30d",
                    "text": "To study how people perceive and prioritize responsible AI values, we combine instruments from value elicitationresearch [22] with the concepts and principles found in AI ethics guidelines [37]. We fielded an iteratively developedonline survey with 743 census-representative participants, 755 crowd workers, and 175 AI practitioners.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "5e270381-2b8f-4be8-a39d-645b979aeaff",
                    "text": "We adapted the Schwartz Value Survey [61, 62] to apply it to responsible AI values. The Schwartz Value Survey hasbeen used to study individual and intercultural differences in general human values in over 60 countries [63]. Basedon an inventory of human values, the Schwartz Value Survey asks respondents to self-report which values are mostimportant to them. Respondents rate the importance of each value on a Likert scale while explanations for each valueare shown.Selecting and explaining responsible AI values. To adapt the Schwartz Value Survey to the study of AI ethics, weconstructed an inventory of responsible AI values. The responsible AI values we chose for our survey are based ona review of published AI ethics guidelines. We drew on work by Jobin et al. [37] finding that AI ethics guidelinescommonly refer to transparency, justice & fairness, non-maleficence, accountability, privacy, beneficence, freedom& autonomy, trust, and dignity. To this list, we added system performance, as it is a central value in AI research anddevelopment [6] that is often used to compare AI models and to make deployment decisions.As responsible AI values are abstract and participants may not easily understand how they apply in the context ofAI technologies [11], we provided additional explanations. To formulate explanations for each value, we drew againon existing AI ethics guidelines, including Microsoft\u2019s responsible AI principles [50], Google\u2019s AI Principles [26], theMontreal Declaration for the Responsible Development of Artificial Intelligence [52], the Deloitte AI ethics guide [15],IBM\u2019s Principles for Trust and Transparency [35], and the EU\u2019s Ethics guidelines for trustworthy AI [67].We tested and iterated on different explanations of responsible AI values in four crowdsourcing pilot studies (N =40,N =80, N =40, N =160). Each pilot asked participants whether they understood an explanation through both Likertscales and open-ended responses. Based on the pilot results, we substituted \u201cnon-maleficence\u201d with \u201csafety\u201d and\u201cbeneficence\u201d with \u201csocial good,\u201d as the former were not well-understood by participants. We also explicitly referredto \u201chuman autonomy\u201d to avoid confusion with autonomous cars and robots. Finally, we did not include \u201ctrust\u201d as itappeared overly general and overlapped with other values such as transparency and accountability.We phrased the explanations in simple, non-technical language, all following the same structure. Each explanationstarts with a sentence describing what a system embodying the value would do, followed by an example of stepsdevelopers might take to realize a value, e.g.: \u201cAn AI system that respects people\u2019s autonomy avoids reducing theiragency. Developers of autonomy-preserving AI systems ensure, as far as possible, that the system provides choices topeople and preserves or increases their control over their lives.\u201d By complementing a general definition with specificoperationalizations of a value, the framing provides a tangible understanding of the value while maintaining a degreeof generality. See Appendix A.2 for a complete list of the explanations we used in our survey.Identifying pairs of possibly conflicting responsible AI values. In addition to assessments of values themselves, weasked participants about their preferences in cases of conflicting values [4]. For example, ensuring fairness might requirecollecting additional sensitive data, potentially diminishing privacy. To identify value conflicts, we searched for mentionsof conflicts in the literature for each pair of values in the responsible AI value inventory. We found prior discussions oftrade-offs between privacy & performance [3, 66], fairness & privacy [3, 20], fairness & performance [12, 56], safety &transparency [10, 33, 49], and autonomy & safety [44]. We combined the value explanations developed above to introducethe conflicts to participants, e.g. \u201cThe developers realize that minimizing the collection of sensitive data (ensuring privacy)may make the system\u2019s predictions less accurate (reducing performance). Should they prioritize privacy or performance?\u201dConstructing hypothetical AI deployment scenarios. We used hypothetical scenarios to make value assessments moretangible and to elicit judgments in specific contexts. We produced four hypothetical deployment settings validatedthrough two pilot studies (N =180, N =160). To design these scenarios, we selected 25 AI systems people may haveencountered in everyday settings starting with a list of general AI use cases [17]. We developed short explanationsof these use cases and asked pilot participants whether they found them understandable and relatable. Based on thepilot results, we further refined the scenarios and kept only the 10 scenarios that were most easily understood bypilot participants. The second pilot then asked participants which scenarios they understood best and whether the AIsystem\u2019s decisions were highly consequential. Based on the responses, we selected two well-understood high-stake andlow-stake scenarios for the study:(a) Medical: An AI system used by a medical clinic to predict whether a patient has a disease (high-stake)(b) Banking: An AI system used by a bank to predict whether an applicant will repay a loan (high-stake)(c) Marketing: An AI system used by a marketing company to match ads to viewers (low-stake)(d) Streaming: An AI system used by a streaming company to recommend movies to users (low-stake)Each scenario states the entity controlling the AI system and the type of data the system is using. It then elaborateswhat predictions are being made and what actions are being taken based on the prediction, e.g.: \u201cA medical clinic usesan AI system that scans patients\u2019 medical records to predict whether a patient has a particular disease. Thousands ofpatients\u2019 treatment plans are automatically adjusted based on the output of this AI system.\u201d The full list of scenarios isincluded in Appendix A.4.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "f5d71fd6-19b4-4087-a8fd-1c8336dbf609",
                    "text": "After providing informed consent, participants received a high-level introduction both covering the general goalsof AI and noting the complex decision-making involved in the AI system development beyond technical challenges(see Appendix A.1). Figure 1 illustrates the subsequent survey steps which combined three value elicitation tasks:(1) value selection\u2014select five responsible AI values (out of the 12) that are deemed most important in general, (2)contextual assessment\u2014evaluate the perceived importance of seven central responsible AI values (transparency, fairness,safety, accountability, privacy, autonomy, and performance) in a specific deployment setting, and (3) comparativeassessment\u2014recommend what product teams should do when values are in conflict.Participants selected the five most important values for AI systems in general, with explanations displayed whena value was hovered over. They then read the first scenario and confirmed their understanding of the deploymentsetting. Overall, participants encountered four scenarios. In scenarios 1 and 2, participants indicated how importantthey thought three responsible AI values were in the given situation on a 5-point Likert scale. In scenario 3, participantsevaluated one more value and then two value conflicts by indicating which value they thought should be prioritizedin the given situation. Finally, they evaluated three value conflicts in the fourth and last scenario. For every rating,participants were given the option to explain their choices.After completing the rating tasks, participants indicated their familiarity with machine learning, user research, andtheir personal experiences with discrimination. We selected these experiential correlates based on the hypothesis thatpersonal experience might inform ethical preferences [14]. For example, user researchers may have learned to empathizewith users, whereas respondents trained in ML may have better insight into the technical constraints of responsible AI.We also asked participants to report their gender identity, age, ethnicity, political views, sector of work, and highest levelof education. Again, these demographic correlates were selected to explore to what extent social location influences theperceived importance of responsible AI values [36]. For all experiential and demographic questions, participants couldchoose not answer.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "02865a8e-0b27-46fd-9364-294dbe46c61c",
                    "text": "To examine how different groups assess responsible AI values, we surveyed three populations:A US census-representative sample (N=743 ) was recruited by Qualtrics to gain insights into how the generalpopulation assesses the importance of responsible AI values. The recruitment process combined a variety of methods tominimize biases and performed stratified random sampling to match the US census along gender, age, race, region, andhousehold income. Participant compensation was handled by Qualtrics.A convenience US-based crowdworker sample (N =755) was recruited via the Clickworker crowdsourcing platform.Participants were US-based and likely previously contributed to the training of AI models by e.g., providing data labels.Each participant received USD 2.8 for a median participation time of 8 minutes. While crowdworkers are not directlyinvolved in the AI development process, their judgments are often a key ingredient to machine learning systems. Weexplored whether their assessments could serve as proxies for the ethical intuition of a more representative population.A sample of AI practitioners (N =175) was recruited through an open call on Twitter (N=156) and internal mailinglists (N=19) at a large tech company. Our call for participation targeted US-based participants whose work is related toAI/ML. We confirmed their background in the survey, but ultimately rely on self-reported expertise. For the internalmailing lists, we specifically targeted teams doing AI/ML related work. Participants could choose to enter a raffle towin one of five $50 gift vouchers after study completion. AI practitioners are a relevant population that makes keydecisions throughout the AI development process. We explore whether their value judgments differ from those of themore general population.We had to work with different types of compensation due to differences in respondent type and recruitment methodacross samples. However, we aimed to provide roughly commensurate compensation across recruitment methods. Thestudy was IRB approved, and we obtained informed consent from all our participants.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "0d645b89-aa11-4d0d-9ff1-88470495c48b",
                    "text": "To counterbalance ordering effects, the arrangement of scenarios, values, and conflict questions was randomized.In addition, the order of response options was randomly flipped per participant. For the conflict questions, we alsorandomized the internal order of the conflict, e.g. fairness vs. performance was inverted to performance vs. fairness. Apop-up window asked participants to slow down whenever they attempted to submit responses in under 3 seconds persurvey page to deter spammers and inattentive participants. The four scenario introductions throughout the surveyserved as attention and comprehension checks for our participants. We removed all participants that had failed morethan one attention check from our analysis to increase response quality, reducing the relevant samples to N =516,N =607, N =140 respectively.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "8beef422-1fdd-4333-8750-71192df8e473",
                    "text": "In Task 1, participants selected five values they deemed most important for AI systems out of an inventory of 12responsible AI values (Figure 2). 76% of respondents from the US-census representative sample selected safety among thetop 5 responsible AI values. Over 60% of participants in this representative panel also selected performance, privacy, andaccountability among the most important values. Respondents from the crowdworker sample selected accountabilityless often, but their preferences were largely consistent with those from the US-census representative sample. AIpractitioners\u2019 preferences were less focused. Compared to the US-census representative sample, practitioners selectedhumanist values such as fairness, inclusiveness, dignity, and solidarity more often and were less likely to select safetyand performance among the most important values.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "6384f3e4-8b0a-4e43-8c9a-d1e3fab2babf",
                    "text": "In Task 2 participants evaluated how important they considered a value in the context of a specific deploymentscenario (Figures 3 and 4). The perceived importance of performance, accountability, fairness, and transparency variedsignificantly across deployment settings. In general responsible AI values were rated as very or extremely important.Compared to both the US-census representative and the crowdworker samples, on average, AI practitioners evaluatedresponsible AI values, and privacy, safety, and performance, in particular, as less important. We also observed significantvariation of perceived importance across deployment settings, with responsible AI values being considered mostimportant in the medical context and least important in the streaming context. A more detailed graph showingresponses by both sample and scenario is included in the Appendix A.6.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "a968b671-65aa-4d0c-9250-0319620f2387",
                    "text": "In Task 3 participants suggested how values should be prioritized when in conflict (Figures 5 and 6). Respondents fromall participant samples agreed on prioritizing safety over autonomy and transparency. Across scenarios, a majorityof respondents agreed on prioritizing privacy over performance or fairness. Most disagreement was observed whenperformance and fairness conflicted: Participants from the US representative sample were almost equally split intheir preferences for fairness versus performance. Crowdworkers were less likely to prioritize performance and AIpractitioners were more likely to prioritize fairness than the US-census representative participants.Across scenarios, respondents prioritized privacy over performance and fairness, and safety over autonomy andtransparency. Again, the performance-fairness trade-off produced most variation: Participants prioritized performancein the medical and streaming scenario, and fairness in the banking and marketing scenario.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "91f3d64f-ac39-47db-9c47-869e3a26acb5",
                    "text": "To explore how demographic and experiential factors correlate with participants\u2019 assessments, we mapped theirresponses to a 5-Likert scale that preserves the direction of the original scale. Treating ordinal scales as interval scalesis controversial, but the scales in our study have a unit of measurement with comparable-size intervals and a zeropoint, so a continuous analysis is meaningful and justifiable [41]. To examine how various demographic, experiential,or contextual factors may explain the variance in respondents\u2019 assessments, we used linear regression to build simplebaseline models that predict their assessments.Marketing systemMedical systemStreaming systemCrowdworker samplePractitioner sampleWomen respondentsGender-diverse resp.Black respondentsHispanic respondentsAsian respondentsAgeEducationPolitical leaningExp. with discriminationFamiliarity with MLFamiliarity with UXConstantObservationsRAdjusted RResidual Std. ErrorF StatisticNote: Privacy-0.0510.005-0.080.005-0.080.04-0.0420.0460.0240.0010.001-0.0200.060-0.027-0.0370.0460.821,2460.0820.0700.2126.84 Safety-0.0240.06-0.09-0.020-0.0570.039-0.0310.0520.042-0.025-0.0001-0.0470.0110.011-0.0070.0430.801,2460.0840.0720.2337.05 Dependent variable:Value importance ratingPerform.-0.140.044-0.120.002-0.0520.05-0.0460.0620.044-0.017-0.001-0.0260.027-0.0570.0050.0530.831,2460.1500.1390.22913.51 Account.-0.0460.030-0.18-0.05-0.100.04-0.0170.0060.034-0.018-0.00040.0090.0230.002-0.0280.0340.821,2460.1300.1180.23811.42 Fairness-0.10-0.031-0.18-0.06-0.0570.050.0860.080.020-0.031-0.0010.0070.0560.004-0.0160.0550.811,2460.1190.1070.24410.33 Transp.-0.080.029-0.16-0.032-0.090.028-0.0100.0190.021-0.057-0.00030.0120.049-0.008-0.0110.0570.771,2460.1110.0990.2429.58 Autonomy-0.0040.10-0.036-0.0410.0050.070.0410.0310.023-0.0070.0010.0130.0060.0050.0090.0430.621,2460.0700.0580.2545.81p<0.05; p<0.01; p<0.001Table 1 shows parameter estimates of linear regression models fitted to predict how important respondents considera value in a specific scenario. The model constant corresponds to a white man from the US-census representativesample evaluating a responsible AI value in the banking scenario. The parameter estimates confirm that the perceivedimportance of values varies significantly across deployment settings. They also confirm that, compared to the US-censusrepresentative sample, AI practitioners evaluated most values as less important. Women and black respondents, onaverage, evaluated most responsible AI values as more important than other groups. Among the experiential correlates,a self-reported liberal political leaning was associated with a higher valuation of privacy. Self-reported experiences withdiscrimination predicted lower perceived importance of performance but were not statistically significantly correlatedwith other responsible AI values. While familiarity with ML did not predict different value priorities, respondentsreporting to be familiar with UX research evaluated most responsible AI values as more important.Table 2 shows parameter estimates predicting participants\u2019 preference in the case of conflicting responsible AI values.Positive coefficients correspond to a preference for the top value. Responses vary significantly by deployment context,Privacy. vs.performance0.164-0.1120.091 Privacy vs.fairness0.3010.1130.209-0.079-0.0600.0550.219-0.006-0.050-0.0330.0010.1040.010-0.1130.008-0.0930.2621,2460.0320.0190.7122.535 0.0480.1140.038-0.128-0.098-0.1600.0240.003-0.126-0.191-0.2050.0730.1240.1001,2460.0560.0430.6794.526Marketing systemMedical systemStreaming systemCrowdworker samplePractitioner sampleWomen respondentsGender-diverse resp.Black respondentsHispanic respondentsAsian respondentsAgeEducationPolitical leaningExp. with discriminationFamiliarity with MLFamiliarity with UXConstantObservationsRAdjusted RResidual Std. ErrorF StatisticNote: Dependent variable:Value preference ratingPerformance vs.fairness0.1130.3780.3420.152-0.0910.057-0.182-0.1090.082-0.113-0.002-0.067-0.2260.023-0.0080.026-0.0571,2460.0800.0680.7016.719 Safety vs.autonomy Safety. vs.transparency0.0670.078-0.1500.058-0.0780.0760.0600.168-0.0170.0330.00020.0450.091-0.1600.0900.0480.1021,2460.0380.0250.6653.009 0.0800.2590.0860.0150.0620.113-0.214-0.029-0.0210.0200.0002-0.063-0.0170.083-0.0780.0160.1591,2460.0310.0180.6882.459p<0.05; p<0.01; p<0.001but only the response to the fairness-performance trade-off varies by sample. Women respondents were more likelyto prioritize safety over transparency than other groups, and black respondents were more likely to prioritize safetyover autonomy. While participants reporting experiences of discrimination were more likely to prioritize fairness overprivacy, they were not more likely to prioritize fairness over performance than other groups. Instead, participantswith liberal political learning were more likely to prioritize fairness over performance and privacy than other groups.Familiarity with ML neither predicted a preference for performance over privacy nor fairness.Some variables were correlated with each other. For example, the practitioner sample contains fewer womenrespondents (r=-0.14, p<0.01) and black respondents (r=-0.11, p<0.01), but more educated (r=0.33, p<0.01) and liberal-leaning (r=0.2, p<0.01) respondents. Similarly, liberal-leaning respondents were younger (r=-0.13, p<0.01) and morelikely to report experiences with ML (r=0.1, p<0.01) and discrimination (r=0.09, p<0.01). However, a correlation analysis(included in the Appendix A.6) suggests that no covariates were highly correlated (r>0.7). The variance inflation factorremained below 1.5 across all covariates, indicating little to no multicollinearity issues [29].5 DISCUSSIONAI practitioners\u2019 value priorities for responsible AI differ from those of the general public. Our results empirically corroboratea commonly raised concern: AI practitioners\u2019 value preferences for responsible AI are not representative of the valuepriorities of the wider US population. Compared to a US-census representative public, AI practitioners evaluatedresponsible AI values as less important in general and emphasized a different set of values.US-census representative and crowdworker respondents agreed on what values they found most important: safety,privacy, and performance. Practitioners, in comparison, were more likely to prioritize fairness, dignity, and inclusiveness.These findings align with prior research finding that different groups have different normative expectations ofhow AI systems should behave in specific situations [2, 28, 31, 55]. Our findings extend prior work by demonstratinghow AI practitioners\u2019 ethical preferences differ from other groups\u2019. We also show that groups not only differ in theirjudgment of specific behaviors and technical details, but may disagree on the importance of the very values at the coreof responsible AI. The disagreement in value priorities highlights the importance of paying attention to who gets to definewhat constitutes \u201cethical\u201d or \u201cresponsible\u201d AI. Responsible AI guidelines [37] may emphasize a different set of valuesdepending on who writes them and who is consulted. We hypothesize that consulting populations outside the Westernworld about their priorities for responsible AI would surface even starker disagreement about the values underlyingresponsible AI [39, 63].What might explain the differences in value priorities between AI practitioners\u2019 and other groups? Our results providelimited insight into plausible drivers of differences in values. First, women and black respondents assessed responsibleAI as more important than other demographic groups. Their relatively low representation in the AI practitioner samplecompared to the representative sample (only 40% and 2.2% compared to 52% and 15% respectively) explains about 15%of the lower importance ratings AI practitioners assigned to values in general. Increasing the representation of e.g.,women and black researchers in AI [13, 32, 42] may thus result in responsible AI values receiving more attention.Another demographic variable that robustly predicted differences in value preferences was respondents\u2019 politicalleaning. Liberal-leaning respondents were 10% more likely to select fairness amongst the most important values thanconservatives, and were 15.5% more likely to prioritize fairness in the fairness-performance trade-off. Compared to therepresentative sample, AI practitioner respondents were substantially more likely to self-identify as liberal-leaning (52%compared to 26%), explaining about 27% of practitioners\u2019 different evaluation of fairness. This result is in line with thebroader research on value differences along ideological lines [9, 68]. It highlights that guidelines for responsible AIneed to navigate a polarized value landscape.Other demographic and experiential variables, however, were less predictive of how our participants assessedresponsible AI values. Respondents reporting experience with discrimination were more likely to prioritize fairnessover privacy, but did not evaluate fairness as more important than other groups. When asked whether developersshould prioritize fairness over performance, participants from minoritized groups and participants reporting experiencewith discrimination were as undecided as other groups. While previous work identified performance as the centralvalue in machine learning research [6], our results do not suggest that AI practitioners or respondents familiar withmachine learning were more likely to value performance. Participants trained in user experience research, however,evaluated responsible AI values more important in general.Can AI practitioners use crowdsourcing to complement their ethical intuitions in the development process? Our findingsemphasize the need for bringing in a diversity of perspectives when decisions are made about the development andoperationalization of responsible AI. Crowdworkers are often the go-to convenience sample, but to what extent couldthey provide a reliable lens into the values that a broader population expect AI systems to adhere to?As in prior research [34], we find that the value priorities of crowdworkers largely align with those of the US-censusrepresentative sample. Our results also show that often a majority of participants agreed on value trade-offs. Forexample, respondents from all samples prioritized privacy over performance across all deployment scenarios. Theagreement raises the question of whether and when product teams could use such results to e.g., justify prioritizingprivacy over performance.Here, consensus alone may not justify practical requirements within specific contexts of use. Rather than providingdefinite answers, the approach developed in this paper provides \u201cvalues levers\u201d [65]: organizational processes that takethe implicit work of value judgments in technology development and transform it into an explicit matter of debate anddocumentation. Empirical data on different groups\u2019 preferences can both inform the development process of responsibleAI and provide opportunities for critical reflection. Rather than prescribing value priorities, responsible AI guidelinescould ask practitioners to justify their choices whenever they go against commonly held value preference.5.1 LimitationsThe quantitative approach to value elicitation explored above has its benefits: It allows consulting large and representativesamples of stakeholders and integrates well with existing crowdwork infrastructures. At the same time, it needs to becomplemented by qualitative, small-n investigations like interviews or focus groups for a comprehensive understandingof value differences across social groups. For example, the current study did not explore how groups understand orinterpret values differently, what other values some groups might have wanted to include, or why it is that e.g. women,on average, rated responsible AI values as more important.The results of this survey also should be interpreted with care. No normative \u201cought\u201d can be derived from a descriptive\u201cis\u201d [53]. We cannot conclude that safety ought to be prioritized over autonomy from the observation that the respondentsin our samples suggested so. Our results aim to increase the context sensitivity of responsible AI decisions, not toprescribe a specific course of action. Empirical ethical research does not replace ethical reasoning but offers perspectivesand critical reflections.Finally, knowledge-dependent tensions arise when contrasting the perspectives of experts and laypeople. One mayargue that non-expert perspectives lack the technical and organizational insight required to evaluate AI systems.However, as we are focusing on ethical rather than technical questions, non-experts have their own valid and legitimateforms of knowledge [30] that experts might not be aware of.6 CONCLUSIONRecently published guidelines for responsible AI seem to converge on a set of central values. However, little is knownabout the values a more representative public would find important for responsible AI. We conducted a survey comparinghow US-representative respondents, crowdworkers, and AI practitioners perceive and prioritize responsible AI values.Our findings show that, compared to the general public, AI practitioners find responsible AI values less important andare likely to focus on a different set of values. Our findings underline the need for more diverse ethical judgement to beincorporated into the AI development process. Crowdworkers, who are already involved in the AI development process,resemble the general public in their value priorities and might provide valuable input.ACKNOWLEDGMENTSWe thank our colleagues from across Microsoft who provided insight and expertise that greatly assisted the research.We are particularly grateful to Su Lin Blodgett, Stephanie Ballard, Michael Madaio, Emery Fine and Kate Crawford fortheir comments on the research framing and survey design.REFERENCESA APPENDIX: SUPPLEMENTARY MATERIALSA.1 Introduction and taskArtificial intelligence (AI) is a set of emerging technologies concerned with building smart systems or machines capableof performing tasks that typically require human intelligence. Besides technical challenges, building AI systems involvescomplex decision-making on what the system should or should not do. In this survey, we will ask you to assess theimportance of ethical principles for four AI systems.A.2 Value Description and Question FramingA.3 Value conflict framingA.4 Application scenario framingA.5 Detailed result graphsPlease refer to Figures 7 and 8.A.6 Covariate correlation analysis",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "d7a13f6b-c1dc-4775-bed9-2f28f2cab077",
                    "text": "",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "0dc2d25b-fe75-4a3a-bde0-3366552cb6d0",
                    "text": "The quantitative approach to value elicitation explored above has its benefits: It allows consulting large and representativesamples of stakeholders and integrates well with existing crowdwork infrastructures. At the same time, it needs to becomplemented by qualitative, small-n investigations like interviews or focus groups for a comprehensive understandingof value differences across social groups. For example, the current study did not explore how groups understand orinterpret values differently, what other values some groups might have wanted to include, or why it is that e.g. women,on average, rated responsible AI values as more important.The results of this survey also should be interpreted with care. No normative \u201cought\u201d can be derived from a descriptive\u201cis\u201d [53]. We cannot conclude that safety ought to be prioritized over autonomy from the observation that the respondentsin our samples suggested so. Our results aim to increase the context sensitivity of responsible AI decisions, not toprescribe a specific course of action. Empirical ethical research does not replace ethical reasoning but offers perspectivesand critical reflections.Finally, knowledge-dependent tensions arise when contrasting the perspectives of experts and laypeople. One mayargue that non-expert perspectives lack the technical and organizational insight required to evaluate AI systems.However, as we are focusing on ethical rather than technical questions, non-experts have their own valid and legitimateforms of knowledge [30] that experts might not be aware of.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                },
                {
                    "id": "89439ba7-ea66-440e-baf3-f119c01dd208",
                    "text": "Recently published guidelines for responsible AI seem to converge on a set of central values. However, little is knownabout the values a more representative public would find important for responsible AI. We conducted a survey comparinghow US-representative respondents, crowdworkers, and AI practitioners perceive and prioritize responsible AI values.Our findings show that, compared to the general public, AI practitioners find responsible AI values less important andare likely to focus on a different set of values. Our findings underline the need for more diverse ethical judgement to beincorporated into the AI development process. Crowdworkers, who are already involved in the AI development process,resemble the general public in their value priorities and might provide valuable input.ACKNOWLEDGMENTSWe thank our colleagues from across Microsoft who provided insight and expertise that greatly assisted the research.We are particularly grateful to Su Lin Blodgett, Stephanie Ballard, Michael Madaio, Emery Fine and Kate Crawford fortheir comments on the research framing and survey design.",
                    "reference": "[1] Julian Jakesch, Zana Bu\u00e7inca, Saleema Amershi. 2022. How different groups prioritize ethical values for responsible AI. arXiv:2205.07722. Retrieved from https://arxiv.org/pdf/2205.07722"
                }
            ]
        },
        {
            "paper_title": "Measurement as governance in and for responsible AI",
            "authors": "AZ Jacobs",
            "publication_info": "arXiv preprint arXiv:2109.05658 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2109.05658",
            "chunks": [
                {
                    "id": "85d76fcb-93cb-4648-be29-213ad8d4acf7",
                    "text": "Measurement of social phenomena is everywhere, unavoidably,in sociotechnical systems. This is not (only) an academic point:Fairness-related harms emerge when there is a mismatch in themeasurement process between the thing we purport to be measur-ing and the thing we actually measure. However, the measurementprocess\u2014where social, cultural, and political values are implicitlyencoded in sociotechnical systems\u2014is almost always obscured. Fur-thermore, this obscured process is where important governancedecisions are encoded: governance about which systems are fair,which individuals belong in which categories, and so on. We canthen use the language of measurement, and the tools of constructvalidity and reliability, to uncover hidden governance decisions.In particular, we highlight two types of construct validity, con-tent validity and consequential validity, that are useful to elicitand characterize the feedback loops between the measurement,social construction, and enforcement of social categories. We thenexplore the constructs of fairness, robustness, and responsibilityin the context of governance in and for responsible AI. Together,these perspectives help us unpack how measurement acts as a hid-den governance process in sociotechnical systems. Understandingmeasurement as governance supports a richer understanding ofthe governance processes already happening in AI\u2014responsible orotherwise\u2014revealing paths to more effective interventions.KEYWORDSmeasurement, algorithmic fairness, governance, responsible AI",
                    "reference": "[1] AZ Jacobs. 2021. Measurement as governance in and for responsible AI. arXiv:2109.05658. Retrieved from https://arxiv.org/pdf/2109.05658"
                },
                {
                    "id": "3c65427e-44ee-41be-8d08-838f3867c47b",
                    "text": "In algorithmic systems, social phenomena are constantly being op-erationalized, from creditworthiness, gender, and race, to employeequality, community health, product quality, and attention, user con-tent preferences, language toxicity, relevance, image descriptions\u2014themselves laden with cultural and social knowledge. In practice,the measurements of these phenomena are constrained by existingpractices, data availability, and other problems of problem formu-lation [61, 62]. The process of creating measures of such phenom-ena is a version of what social scientists call measurement. But inalgorithmic system development and data science, this measure-ment process is almost always implicit\u2014from operationalizing thetheoretical understandings of unobservable theoretical constructs(toxicity, quality, etc.) and connecting them to observable data. Herewe build from the measurement framework put forward by Jacobs and Wallach [44] to show where and how social, cultural, organiza-tional, and political values are encoded in algorithmic systems, andfurthermore how these encodings then shape the social world. Anyattempt at meaningful \u201cresponsible AI\u201d must consider our values(where and when they are encoded, implicitly or otherwise) andfairness-related harms (where and why they emerge). The lens ofmeasurement makes such considerations possible, and moreover,reveals where governance already is playing a role in responsibleAI\u2014and where it could.",
                    "reference": "[1] AZ Jacobs. 2021. Measurement as governance in and for responsible AI. arXiv:2109.05658. Retrieved from https://arxiv.org/pdf/2109.05658"
                },
                {
                    "id": "cd4c86cb-b01d-489b-8e6a-162ec341cb6c",
                    "text": "Here we build directly on the measurement framework put forwardby Jacobs and Wallach [44]. Using this framework, we can under-stand concepts such as employee quality, language toxicity, com-munity health, or social categories, as unobservable theoretical con-structs that are reflected in observable properties of the real world.To connect these observable properties to the constructs of interest,we operationalize a theoretical understanding of the construct and,with a measurement model, show how we connect observable prop-erties to that operationalization. In practice, this process happensconstantly in the design of sociotechnical systems. Sometimes thisprocess of assumptions, operationalization and measurement aremore explicit: proxies refer to a special case of measurement models.Sometimes the distinction between constructs, operationalizations,and measurements are more obscured: consider using inferred de-mographic attributes, predicting clickthrough rate to infer attention,or using other system data exhaust to predict behavior.This process matters beyond academic exercise and pedantrybecause fairness-related harms emerge when there is a mismatchbetween the thing we purport to be measuring and the thing weactually measure [43, 44]. As an example, consider a tool for hir-ing or promotion: we might choose to operationalize \u2018employeequality\u2019 with \u2018past salary.\u2019 We could then choose the measurementmodel of \u2018past salary\u2019 as an individual\u2019s average salary over the lastthree years. Even in this simple example, using this operational-ization of employee quality would reinforce existing gender andracial pay disparities by systematically encoding \u2018quality\u2019 as salary.This is an example of one such fairness-related harm\u2014revealed bya mismatch between the construct of \u2018employee quality\u2019 and itsoperationalization, \u2018past salary.\u2019",
                    "reference": "[1] AZ Jacobs. 2021. Measurement as governance in and for responsible AI. arXiv:2109.05658. Retrieved from https://arxiv.org/pdf/2109.05658"
                },
                {
                    "id": "db85d1f4-8065-41b7-ab36-8bdf628df254",
                    "text": "We have thus far established that measurement is everywhere,whether we acknowledge it or not; that fairness-related harmsemerge from mismatches in the measurement process; and thatbeing explicit about about the measurement process, whether proac-tively or retrospectively, can reveal where those mismatches (andpotential for harms) emerge. A theme from this past work is that theassumptions underlying sociotechnical systems are crucial, and thatframeworks that reveal those assumptions can mitigate and preventharms. Furthermore, assumptions are decisions\u2014and are decisionsthat reflect values\u2014therefore frameworks that reveal those assump-tions reveal implicit decisions in the creation, use, and evaluationof sociotechnical systems. So where is governance in responsibleAI? Let us consider two sites.Governance in responsible AI. First, decisions about what hap-pens within algorithmic systems are governance decisions. Whatdata is used where, to what ends; what is being predicted or decided;what variables are named what and accessible to whom\u2014all of thesedecisions shape what a system can and cannot do. Some of these de-cisions are explicit: for instance, gender is a binary variable; these ad-vertisements should be served to those inferred income groups; onlyusers that have selected one of those two binary gender variableswill be eligible for an account; only users with a paid account canaccess this data; this functionality will be named \u2018Friend\u2019 and thisone, \u2018Like\u2019. The impacts of such decisions, of course, can extend wellbeyond their immediate implementation. And the downstream con-sequences of the decisions\u2014of who is included, of who gets shownwhat content, of how we engage with a system through an API oras a user, of how we interpret functionalities (\u2018Like\u2019)\u2014need not havebeen explicitly engaged with. This is measurement\u2014of gender; ofrelevance of content and of socio-economic status; of a complete orverified identity; of membership; of relationships and interactions.Governance for responsible AI. The second site of governance isdecisions about algorithmic systems. Is the system good at whatit is supposed to do? Does it work the way we expect? (And specif-ically, does it perform well on the training set? Does it generalizeto other settings?) Is it ready for deployment, and for which con-texts? Or even more directly: Is our system fair? Is it robust? Is itresponsible? Taking a step back, where does \u201cfair\u201d or \u201crobust\u201d or \u201cre-sponsible\u201d come from? These are themselves essentially contestedconstructs that are being operationalized [44, 57, 59]. Measurementis governance here too.Thinking tools. What of it then? This integrated perspective\u2014measurement as governance\u2014is a generative one. We have tools\u2014i.e., construct validity and reliability\u2014to unpack the measurementprocess in algorithmic systems and uncover sources of potentialfairness-related harms\u2014i.e., mismatches in the measurement pro-cess [44]. Two types of validity, content and consequential, areparticularly well-suited to explore governance decisions: Contentvalidity elicits substantive understanding of the construct beingoperationalized, while consequential validity reveals the feedbackloop of governance decisions and their impacts as a part of validity.Other types of construct validity and reliability may already be fa-miliar to data scientists and engineers alike, including face validity(do the assumptions seem reasonable?), test\u2013re-test reliability (thisincludes out of sample testing: do we get similar results if we re-runthe system?), and convergent validity (do the measures correlatewith others of the same phenomenon?). These types may be alreadyfamiliar under different names or within different frameworks ofbest practices. Thus an important caveat is that naming which typesof validity are being considered is not necessary to explore them.Asking the questions or interrogating assumptions, or assessing Figure 1: This broader systems-level view\u2014of how measuresare created and used\u2014falls under consequential validity,that is, considering how measurements are used, and whatassumptions were made by whom in the process.validity and reliability, can be done without the specific vocabulary.However, we can use the types of validity as inspiration for whatquestions we ought to ask ourselves and ask of our systems.Finally, understanding measurement as governance serves as areminder that these assumptions will be inherently shaped by thosemaking them [e.g., 1, 12, 23]. The contexts where these assump-tions are made, and by whom, are not necessarily separable fromthe work itself [5, 24, 45, 61, 62, 77]. But we do better to articulatewhat assumptions are being made when, by whom, and to whatdownstream effects, when we consider this broader context (Figure1: \u201cwhen you measure, include the measurer\u201d [52]).",
                    "reference": "[1] AZ Jacobs. 2021. Measurement as governance in and for responsible AI. arXiv:2109.05658. Retrieved from https://arxiv.org/pdf/2109.05658"
                },
                {
                    "id": "33e8e4c3-48ff-4cee-bb6b-e94020754192",
                    "text": "The tools that are available to interrogate assumptions (decisions)built in to sociotechnical systems, i.e., construct validity and relia-bility, can be used to interrogate governance practices. Furthermore,these tools can be used to connect with broader conversations ongovernance in and for AI. Here we explore several of those con-nections. These settings\u2014non-exhaustive and illustrative\u2014point tothe breadth of opportunities for rigorously considering the often-implicit governance decisions built into sociotechnical systems.",
                    "reference": "[1] AZ Jacobs. 2021. Measurement as governance in and for responsible AI. arXiv:2109.05658. Retrieved from https://arxiv.org/pdf/2109.05658"
                },
                {
                    "id": "50bbab04-532f-4207-8c52-b9eda817a017",
                    "text": "Two types of validity are of particular relevance to governance inand for responsible AI: content validity and consequential validity.Content validity. Content validity captures two important placesof mismatches: first, is there a coherent, agreed upon theoreticalunderstanding of the construct of interest? And second, given atheoretical understanding of that construct, does the operational-ization fully match that understanding? Many constructs of interestwill face the first challenge, and also vary across contexts, time, andculture (fairness, for instance [e.g., 17, 41, 59, 71]). Even if there isnot an agreed upon definition, something is still being implemented:content validity then covers the substantive match between the op-erationalization, i.e., what is being measured, and the construct, i.e.,what is intended to be measured. (Why bother with other types ofvalidity then, if this apparently captures everything? Different typesof validity, and the exploration one might pursue while trying to es-tablish them, can reveal previously under- or mis-specified assump-tions [44].) Content validity gets at both governance in responsibleAI\u2014e.g., have we adequately captured \u201ccreditworthiness\u201d or \u201criskto society\u201d?\u2014and governance for responsible AI\u2014e.g., when can wedeclare our system to be \u201cfair\u201d or \u201cresponsible\u201d? This is a significantchallenge\u2014and even more so if that theoretical understanding isunstated or is understood differently by different stakeholders. Forgovernance in AI, Passi and colleagues show how real-world or-ganizations confront both theoretical mismatches and mismatchesbetween operationalization and construct throughout the multi-stakeholder process of making data science systems work [61, 63].For governance for AI, debates about content validity are moreexplicit, both in defining responsible AI systems and in operational-izing legal guidelines for algorithmic systems [e.g., 28, 57, 58, 76, 79].Consequential validity. Consequential validity considers impact.This is not obviously a part of validity, in part because its nameconnotes a normative assessment. The argument that Samuel Mes-sick, a researcher at the Educational Testing Service, put forwardwas that how measures are defined changes how they are used\u2014therefore defining the measure changes the world it was createdfor [44, 53, 54]. That is, consequential validity must be a part ofvalidity, as a measure\u2019s operationalization changes how we under-stand that construct in the first place. More recently, Hand writesthat \u201cmeasurements both reflect structure in the natural world, andimpose structure upon it\u201d [31]. That is, it is clear that there is afeedback loop between what we measure and how we interpret it:so to govern systems well, we must consider how the governancechanges the system itself.The feedback loop inherent in measurement is recognized fromdifferent perspectives across the social sciences. One notion is asCampbell\u2019s Law and its cousin Goodhart\u2019s Law [13, 31]: \u201cwhen ameasure becomes a target it ceases to be a good measure\u201d [78].Messick, and the broader educational community, refer to theevocatively-named washback, where incentives around testing leadto teaching to the test [54]. In economics, these ideas appear throughthe Lucas critique and market devices [40, 49, 50, 60], where modelsof the economy and policies then shape the economy; similar ideasappear in sociology [34]. Philosophical and sociological understand-ings of performativity explore how enacting categories changestheir meaning [29] and how categories and value are socially co-constructed [e.g., 5, 65, 69, 70]. This feedback loop has long been ofinterest in critical infrastructure studies and science and technol-ogy studies [11, 29, 75]. Crucially, consequential validity formallybrings systems-level feedback into our technical understanding ofthe measurement process.",
                    "reference": "[1] AZ Jacobs. 2021. Measurement as governance in and for responsible AI. arXiv:2109.05658. Retrieved from https://arxiv.org/pdf/2109.05658"
                },
                {
                    "id": "19094df8-ff17-4d60-98c8-42ba964fe998",
                    "text": "Rehashing the full breadth of disagreements about how constructssuch as fairness are operationalized (in algorithmic systems specif-ically, and governance systems more broadly) is beyond the scopeof this work. Here we point to some perspectives made availablethrough the lens of construct validity before discussing governancein practice in section 3.3. Fairness. Fairness is an essentially contested construct with di-verse, conflicting, and context-dependent understandings of the the-oretical construct [25, 44, 59]. Even with a given theoretical under-standing of fairness, it is still nontrivial (and sometimes impossible)to operationalize the full theoretical scope of the intended definition[e.g., 3, 35, 56], particularly to implement \u201cthis thing called fairness\u201d[59] in meaningful applied settings (real-world organizations, legalsystems) [39, 61, 71, 79]. These are familiar challenges to the contentvalidity of fairness: does the operationalization match the construct?However, content and consequential validity of fairness in algorith-mic systems provide a wider view. In content and consequence, op-erationalizations of fairness almost always neglect notions of justiceand power, and operationalizations based on static, narrow notionsof fairness will perpetuate existing inequities [27, 36, 37, 46]. Whenencountering a \u201cfair\u201d system, however, different stakeholders mayexpect equitable or equal treatment, justice or dignity [27, 37]. Thelens of consequential validity is key here: A system that is labeled as\u201cfair\u201d under a narrow or static definition may further exacerbate in-equities simply by labeling inequitable outcomes as \u201cfair.\u201d Breakingout of this feedback loop must then be done intentionally [7].Robustness. Robustness\u2014asking how well a system performs,perhaps across contexts, or to do the thing it claims\u2014may seemmore neutral. Is the system good at what it is supposed to do? Doesit work the way we expect? Is it ready for deployment? (And specif-ically, does it perform well on the training set? Does it generalize?)A growing literature has turned to the choices involved in how orga-nizations evaluate these assessments of robustness [63] and, morebroadly, the politics of seemingly technical decisions about modelperformance [16, 30], into the politics of developing larger data setsand models in the name of robustness [4, 8, 19, 64, 77]. Through thelens of consequential validity (i.e., it matters what we call things),implying that robustness only requires technical decisions obscuresthe hidden governance decisions at hand [7, 16, 30].Responsibility. Finally, we can turn to responsibility. Withoutadjudicating different desirable properties of responsible algorith-mic systems, we point to several threads: first, that responsibility isabout harms. Centering harms foregrounds risk and power. Criticalscholars have shown how pivoting to a human rights perspective,away from ethics and fairness, allows us to fundamentally attendto (mitigating) fairness-related harms [7, 9, 15, 28]. Second, naiveattempts towards inclusion create opportunities for further harms[6, 38, 47]. As with robustness, moving towards responsible systemsthrough better auditing benchmarks provides an awkward pathtoward accountability [19, 66, 77]. We now turn to ways that so-called responsible AI systems are governed more broadly, drawingfrom perspectives from measurement and construct validity.",
                    "reference": "[1] AZ Jacobs. 2021. Measurement as governance in and for responsible AI. arXiv:2109.05658. Retrieved from https://arxiv.org/pdf/2109.05658"
                },
                {
                    "id": "e96f2da4-1e59-4fd7-be03-fd1b35c3b523",
                    "text": "The development and critique of governance in and for AI alreadyuses the many of the perspectives discussed here. This is in nosmall part due to the framing of the challenge. If we begin froma focus on algorithmic systems in which measurement processesare mostly implicit, we can then use the perspectives given by mea-surement and construct validity to uncover the hidden governancedecisions in algorithmic systems. However, if we begin from a fo-cus on governance, then the relevance and use of the perspectivesgiven by measurement and construct validity are self-evident: whatdo we want our system to achieve? What values or principles dowe want to enact? How do we know if our system is upholdingthose values? What potential harms may emerge if we fail? (Ofcourse, answering these questions remains a challenge.) Further-more, the potential for harms arising from mismatches betweenthe constructs intended to be operationalized and their operational-ization is also more prominent: in 2016 Mittelstadt and colleagueswrote that the \u201cgaps between the design and operation of algo-rithms and our understanding of their ethical implications can havesevere consequences affecting individuals as well as groups andwhole societies\u201d [58]. As such, efforts to operationalize responsiblegovernance for responsible AI have focused on developing values,principles, and ethics statements (both widely discussed elsewhere[14, 21, 22] and thoughtfully critiqued [7, 57, 76]). While measure-ment perspectives may be more readily available, the challengesto meaningfully operationalize responsible AI are nontrivial inpractice [e.g., 20, 39, 51, 67, 68]. With deep ambiguity involved inconverting values into practice [45, 57], governance for AI needsthe social sciences, interdisciplinary and diverse teams, and broadersystems perspectives [7, 26, 59, 72].An emphasis on measurement can unify existing work on gov-ernance. For instance, governance in the public sector relies onimplicit measurement decisions that then shape policy [48], andgovernance tools like impact assessments operationalize harms andwhat must be done to mitigate them, i.e., impacts and assessmentsthereof are co-constructed [55]. The consequences of governancedecisions play out in other domains as administrative or symbolicviolence\u2014for instance when technical systems classify individualsin a way that misrepresent and exclude trans and nonbinary individ-uals [47, 74] or individuals with disabilities [6, 42]. Consequentialvalidity specifically helps account for how meaning is made, en-coded, and enforced (Alder: \u201cmeasures are more than a creation ofsociety, they create society\u201d [2]). We can then functionally connectour understanding of consequential validity to large and nearbyconversations: for instance, race as and of technology [5, 70], fromthe larger context of critical race theory [18, 32]. Similarly, by shift-ing the focus to harms\u2014rather than, for instance, bias\u2014we have theopportunity to reflect on what critical assumptions are being made;to reflect on what impacts are being addressed (unequal outcomesmay be more or less equitable); to reflect on power (including fromwhere decisions are being enforced, by whom, and who they im-pact); and to reflect on what social phenomena are being displayed\u2014and for which the fixes may not be technical [7, 10, 43, 73].Finally,explicitly connecting measurement and governance makes relevantlessons from past governance successes (and failures) in algorithmicsystems as well as other complex social and organizational systems.",
                    "reference": "[1] AZ Jacobs. 2021. Measurement as governance in and for responsible AI. arXiv:2109.05658. Retrieved from https://arxiv.org/pdf/2109.05658"
                },
                {
                    "id": "afc39781-2dc0-495f-a931-7fe46d095c31",
                    "text": "Developing \u201cresponsible AI\u201d is and will be a social, technical, or-ganizational, and political activity. Angela Carter in Notes from theFront Line writes that \u201cLanguage is power, life and the instrumentof culture, the instrument of domination and liberation.\u201d In algorith-mic systems, it is the often-implicit measurement process where this instrumenting\u2014where governance\u2014happens. By understand-ing measurement as governance, we can bring critical and socialscience perspectives more formally into our conceptions of responsi-ble AI [59, 62, 72, 76], while equipping researchers and practitionerswith better tools and perspectives to develop responsible AI.",
                    "reference": "[1] AZ Jacobs. 2021. Measurement as governance in and for responsible AI. arXiv:2109.05658. Retrieved from https://arxiv.org/pdf/2109.05658"
                }
            ]
        },
        {
            "paper_title": "Toward an understanding of responsible artificial intelligence practices",
            "authors": "Y Wang, M Xiong, H Olya",
            "publication_info": "Proceedings of the 53rd hawaii \u2026 - eprints.whiterose.ac.uk",
            "paper_url": "https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf",
            "chunks": [
                {
                    "id": "f5d5458c-9113-4956-842f-8de4f2b4404e",
                    "text": "expectations Artificial  Intelligence  (AI)  is  influencing  all aspects  of  human  and  business  activities  nowadays. Although  potential  benefits  emerged  from  AI technologies  have  been  widely  discussed  in  many current  literature,  there  is  an  urgently  need  to understand  how  AI  can  be  designed  to  operate in  a  manner  meeting responsibly  and  act stakeholders\u2019  applicable regulations.  We  seek  to  fill  the  gap  by  exploring  the practices  of  responsible  AI  and  identifying  the potential  benefits  when  implementing  responsible  AI practices. In this study, 10 responsible AI cases were selected from different industries to better understand the  use  of  responsible  AI  in  practices.  Four responsible  AI  practices  are  identified,  including governance,  ethically  design  solutions,  risk  control and  training  and  education  and  five  strategies  for firms  who  are  considering  to  adopt  responsible  AI practices are recommended.   and 1. Introduction  Artificial  Intelligence  (AI),  a  set  of  algorithm-based  machine,  is  programmed  to  self-learn  from data and display predictions and intelligent behaviors through  artificial  neural  networks,  automated machine  learning,  robotic  process  automation,  and text  mining  [1].  AI  is  capable  of  responding  to  real-world problems and arriving decisions in real-time or near real-time manner on behalf of human being [2], [3], [4]. For instance, Chatbots, an AI-enabled service robot, developed by Bookings.com provides real-time 24/7  customer  service  with  the  support  of  43 languages  to  answer  travel  related  queries  to  its customers.  With  such  highly  evolved  language processing  capabilities,  Chatbots  can  interact  with them  with  personalized customers  and  provide recommendations.  It  also  enables  Booking.com  to deliver  marketing  automation  thereby  simplify routine works accordingly. AI,  as  a  major  shift  in  the  global  economy,  is influencing  all  aspects  of  human  and  business activities  nowadays.  It  holds  the  promise  to  create  information efficiency  and  effectiveness  by  using  data  generated from  an  explosion  of  digital  touchpoints  [5],  [6].  At the  same  time,  it  comes  with  its  own  concerns relating  to  privacy  concerns,  user  distrust,  data leakages,  transparency,  and  ethical concerns. Such ethical dilemma and concerns, if they are  not  well  addressed  when  developing  AI initiatives,  would  lead  to  the  potential  loss  of credibility  for  products  and  brands  and  hamper  the company reputation in the  marketplaces.  Ethical and societal concerns aroused from AI systems need to be addressed in priority to ensure  effective, ethical, and responsible  use  of  AI  [7].  However,  relatively  little attention  has  been  given  to  understand  responsible approaches  to  the  development,  implementation, management, and governance of AI.   [11].  From responsible  companies Indeed,  corporate  social  responsibility  (CSR)  has become  the  main  preoccupations  of  organizations  in the  global  marketplaces  [8],  used  in  broad  domains including  areas  of  policies,  programs  and  actions while  interacting  with  stakeholders  [9][10].  For instance,  customer  retention  rate  could  be  enhanced as consumers prefer to purchase from and engage in, socially  [8].  Likewise, company  reputation  could  be  built  along  with  CSR activities  the  CSR  perspective, organizations  need  to  embrace  the  goal  of  being socially  responsible  while  bringing  AI  into  the business  mainstream.  However,  according  to  the Cognizant\u2019s report, only about 50% of surveyed 975 executives  across  industries  in  U.S.  and  Europe  had policies  and  procedures  in  place  to  address  ethical concerns  while  designing  AI  applications  [12].  from  AI Although  potential  benefits  emerged technologies  have  been  widely  discussed  in  many current  literatures,  the  sustainable  outcomes  from business  to  the  society  that  AI  presents  is  remained unexplored [6]. Specifically, there is an urgently need to  understand  how  AI  solutions  can  be  designed  to operate  responsibly  and  act  in  a  manner  meeting stakeholder  expectations  and  applicable  regulations [7], [13], [14]. We seek to fill the gap by exploring the practices of  responsible  AI  and  identifying  the  potential responsible  AI implementing benefits  when initiatives. Therefore, this study set out to answer the following research questions. RQ1: What are the practices of responsible AI? RQ2: What benefits and challenges have been brought by implementing responsible AI practices? To answer the above research questions, we hope to  provide  business  practitioners  a  more  current comprehensive  understanding  of  responsible  AI  and both theoretical and practical reference values for the use of AI in a more socially responsible way. In this paper, we begin by providing the historical context of technology  use  of  CSR,  and  then  move  on  to understanding  ethical  challenges  in  AI  and  the development  of  AI  in  responsible  practices.  We conducted  a  multiple  case  study  of  responsible  AI, which  leads  to  the  identification  of  responsible  AI practices  and  the  recommendation  of  responsible  AI strategies.  2. Literature Review  2.1. Technology Use in CSR Corporate  social  responsibility  (CSR)  can  be defined  as  commitments  from  organizations  to  the society  in  improving  societal,  environmental  and economic  well-being  through  different  business practices  [8],  [15].  The  relationship  between  the company\u2019s  social  responsibilities  and  its  financial performance has been documented extensively in the literature [16], [17]. The study from Bernal\u2010Conesa et al.  [18]  has  indicated  that  the  contribution  of  CSR-oriented  strategies  the  overall performance of the organizations. From the empirical perspectives,  this  principle  has  been  incorporated  in marketing communications by many organizations in order  to  enhance  stakeholder  perceptions  and retentions  [19].  Thus,  CSR  is  perceived  to  have increasing  importance  for  increasing  enterprises\u2019 competitiveness.   is  significant  to CSR  domains  within  the  marketing  field  are classified  into  seven  categories,  including  employee relations, human rights, diversity, community issues, corporate  governance,  environmental  issues  and product  issues  [20],  [21].  Consumers  are  evinced  to have  domain-based  pro-company  responses  to  CSR practices  due  to  the  influence  of  moral  foundations theory  (MFT)  either  individual-oriented  or  group-oriented  [8].  Their  reactions  towards  companies  can be  moderated  through  CSR  domains  in  the  case  of CSR  strengths,  therefore,  properly  CSR  activates  in different  aspects  need  to  be  organized  and  lapses  of CSR are required to be solved by companies [8].  As  digital  has  become  a  megatrend  in  the  global economy,  new  technology  gains  great  popularity among different industries, offering new possibilities and bringing benefits in many aspects of human lives [22]. For example, labor force may be replaced by the intelligent  machines  [23].  However,  concept  of  the sustainability has changed as it is confronted with the digital transformation, also known as a technological leap  [24],  leading  to  the  increase  in  the  restraints, from  the  national  laws  and  international  rules,  on companies\u2019  responsibilities  towards  society  and environment  (Bernal-Conesa  et  al.,  2017).  Thus, challenges  could  be  posted  to  organizations  for creating  sustainability  and  responsibility  in  the  long run. Inability to communicate the CSR programmers and  integrate  them  into  strategies  may  lead  to  the failure  from  achieving  full  potentials.  Moreover, criticisms  of  CSR  vary  between  companies  and industries  [20].  Data,  algorithms  and  bots  are  main areas to be explored during the process of sustainable digitalization  [22].  Specifically,  although  having access  to  consumer  data  helps  predict  their  potential moves and create personalized experiences for them, privacy  invasions  and  algorithmic  bias  derived  from the  sophisticated  use  of  consumer  data  cannot  be underestimated  [25].  Hence,  the  performance  of technologies  is  required  to  be  aligned  with  CSR principle  and  enhance  its  implementations  [26].  In practice,  technology  could  identify  the  integration points of CSR initiatives, offering corporate  strategy to increase the overall integrated level. In addition, it the  multi-could dimensional  measurement  on  the  programme performance.  Therefore,  it  is  arguable  that  technical resources  can  be  integrated  with  human  resources, within  or  across  companies,  helping  develop capabilities  to  address  sustainable  concerns  and delivering  responsible  values  to  stakeholders  to obtain sustained benefits [27]. reduce  human  bias  through 2.2. Ethical Challenges in AI AI is no doubt beneficial to society as it helps to harness  empathy  and  creativity  skills  of  human  and leveraging their emotional intelligence [28], [29]. An example  is  that  Siri,  assistant  of  iPhone,  is  able  to recognize user\u2019s requests through voice message, and provide  them  assistance  accordingly.  It  could  lessen the  uncertainty,  spent  on in improve administration  and decision-making process based on the data evidence. In  practice,  the  application  of  AI  varies  as  it  is programmed to use specific data to achieve a certain goal  [30].  Marketers  with  such  data  can  provide additional  benefits  to  target  consumers  in  a  more efficient way [25].   the  efficiency reduce  time the In  recent  years,  the  pace  of  using  consumer  data in the marketing field exceeds the academic scholars\u2019 analytics  [25].  Consequently,  negatively  unforeseen issues  may  come  along  with  initial  programs  and against  its  positive  goals.  In  addition,  the  lack  of transparency  on  algorithms,  in  reality,  has  caught public  attention,  leading  to  the  rise  of  ethical concerns  on  the  use  of  AI  [2].  Ethical  issues  are associated  with  the  emergence  of  machine  learning, as  it  allows  intelligence  system  to  get  access  and learn from numerous datasets, to derive its own rules, its  behaviors  and  produce  cognitive enhance competence  its performances caused ethical reflections, may result in deviating  from  sustained  values  and  presenting  new challenges [28], [29], [32]. For instance, interruptions of systems are of frequent occurrence due to the self-reflection.  Programmers\u2019  biases  might  exist  as  the abilities  of  AI  are  initially  dependent  on  human inputs, therefore, it might be problematic as bias can also be replicated from previous events according  to the algorithm [2]. Thus, it is argued that intelligence systems  are  requiring  moral  reasoning  capabilities while facing certain ethical dilemmas [29]. [31].  The  ways  in  which have system  perspectives, Studies on ethical AI, both from the data and the information  been conducted  recently,  leading  the  mitigation  of  unfair bias.  Reinforcement  learning  (RL)  is  prospected  to prevent  ethical  issues  in  the  process  of  intelligent decision-making [32]. It can learn from interruptions while  using  data,  either  from  humans  or  from environments,  to  avoid  repetitive  problems.  In addition,  formulating  ethical  principles  to  guide  the design  of  AI  system  and  rational  algorithms  are argued  to  be  effective  to  ensure  the  ethics  [33]. Nevertheless,  it  is  not  an  easy  task.  Research  from Robbins  [29]  states  a  lack  of  assistance  from  ethical norms  or  policy  guidelines  to  regulate  AI  developer to achieve a balance between the  effective use of AI and the concerns on ethics in the society. Taddeo and Floridi  [33]  point  out  that  the  formulation  of  ethical principles  depends  on  cultural  contexts  and  the domain of analysis which they could vary. 3. Research Method collections, Our  cases  were  drawn  from  materials  on  current and  past  responsible  AI  projects  from  multiple sources such as practical journals, print publications, case  vendors\u2019, consultants\u2019  or  analysts\u2019  reports.  The  absence  of academic discussion in our case collection about the utilization  of  responsible  AI  is  due  to  the  incipient nature of such in this field.   companies\u2019, and  an (1)  the  case  presents following  case  selection  criteria  were The applied:  actual implementation  of  responsible  AI;  (2)  it  clearly describes  the  practices  of  responsible  AI.  We  were able  to  collect  10  responsible  AI  cases  in  different industry (See Appendix 1). Categorizing by region, 4 cases were collected from Northern America, 6 cases from Europe and UK.  Data  analysis  followed  the  constant  comparison method.  Initially  data  analysis  was  performed concomitantly  with  data  collection,  and  continued with an explicit coding stage and an analytical coding procedure stage [33].  In  the  explicit  coding  stage,  the  analysis  started by  comparing  and  coding  each  statement  extracted from the case materials into categories. This allowed categories  to  emerge  to  fit  in  an  existing  category [33].  Relevant  statements  were  labelled  and  either created  as  a  new  code  and  given  a  definition,  or assigned to the existing codes with memos indicating their relevance and potential properties. Through this process, the statement was broken down into units of meanings.  The  concept  as  a  basic  unit  of  analysis labels  phenomenon  representing  a  practice  of responsible  AI  [35].  After  the  explicit  coding  stage, the  data  were  and categorized  in  terms  of  their  properties,  which initiates the analytical coding stage. conceptualized,  defined During  the  analytical  coding  stage,  the  research team compared the properties and dimensions of the emergent  categories.  In  order  to  constantly  analyze and  compare  the  categories,  the  concept  map  was employed  to  visualize  the  classification  [35].  Four dimensions underlying responsible AI practices were identified.  They  are  described  in  detail  in  the following sections and visualized in Figure 1. 4. Practices of Responsible AI Responsible  AI  is  a  governance  framework  that uses  to  harness,  deploy,  evaluate,  and  monitor  AI machines  to  create  new  opportunities  for  better service  provision.  It  focuses  on  designing  and implementing  ethical,  transparent,  and  accountable AI  solutions  that  help  maintain  individual  trust  and minimize  privacy  invasion.  Responsible  AI  places human  (e.g.,  end-users)  at  the  center  and  meets stakeholder  expectations  and  applicable  regulations and  laws.  Prior  to  designing  and  implementing responsible  AI, organizations need to understand the practices that will help them drive ethics and trust of AI use. The four practices of responsible AI include: (1)  Data  governance;  (2)  Ethically  design  solutions; (3)  Human-centric  surveillance/risk  control;  and  (4) Training  and  Education.  These  practices  are  evident Figure 1. A concept map of responsible AI practices in  the  real-world  cases  of  responsible  AI.  These  are described in turn below. 4.1. Data Governance Governance of responsible AI focuses on building transparency, trust, and explainability.  It  that important Transparency.  the is organizational  use  of  AI  must  be  transparent  to  the stakeholders  by allowing them  fully understand how an  AI  application  processes  their  data  and  arrive  to specific  decisions  [36].  According  to  the  Direct Marketing  Association  (DMA)\u2019s  investigation,  80% of surveyed consumers would be very or moderately comfortable  with  sharing  personal  data  when  they know about how digital data is shared and effectively used  for  marketing  purposes  [37].  Capital  One  is making  the  criteria  system  of  credit  card  transparent by providing a computational decision with complete explanation to their customers when their credit card applications  are  accepted  or  denied  [38].  Likewise, Alder Hey  Children\u2019s Hospital, as one of the largest children\u2019s  hospitals  in  Europe,  has  developed  an  AI featured  digital  App  called  Alder  Play.  Alder  Play has  incorporated  the  cognitive  advances  in  order  to present the enjoyable and informative experiences for its  young  patients.  Young  patients  allow  to  active their  own  avatar  during  their  stay,  receive  awards  when completing treatments, and get access to further guidelines and contents accordingly [39]. Alder Play enables  healthcare  professionals  to  have  access  to medical records of patients who are eligible for NHS treatment. Patients and their families would be able to obtain  their  medical  records  online.  This  could largely  the  clinical transparency processes,  thereby  enhancing  the  quality  of  health services and strengthening the patient engagement. improve  in Trust building. Trusted AI is built through high-quality  data  and  consent  to  use  [12].  AI  with  high-quality  data  could  mitigate  biased  and  inaccurate results generated. To ensure the quality and reliability of  data,  where  the  data  sources  come  from,  the limitation of data, and data rules to sharpen data error detection  should  be  identified  when  developing  AI algorithms  and  systems.  For  example,  PwC  has employed H2O.ai to build a revolutionary bot named GL.ai,  which  uses  AI  algorithms  to  effective  track operational data and transactions and correct errors to and maintain interactions for their business customers.  accurate purchase  histories What makes AI workable is its access to personal information  [36].  However,  widespread  access  to personal  (e.g.,  consumer-generated content,  online  transactional  data,  and  browsing  and clicking  data)  has  brought  negative  impacts  to information to individual,  business,  and  society  [25],  [40].  The availability  of  consumer  data  gives  rise  to  serious concerns  where  consumers  suffer  from  privacy invasion,  fraud,  information  leakage,  and  identity theft,  and  on  the  other  hand,  companies  cannot the collect  consumer  data  effectively  due consumers\u2019 distrust. These trends have led to a focus on data protection and transparency of data use by the regulators  in  many  countries  such  as  General  Data Protection  Regulation  (GDPR)  formulated  by  the European  Union  and  Act  on  the  Protection  of Personal  in  Japan.  These regulations  aim  to  protect  all  individuals\u2019  rights regarding privacy and personal data and give control to  individuals  over  their  personal  data.  With  these regulations  came  is  crucial  for into  force, companies to institutionalize the practice of obtaining consent  statement  or  permission  from  users  and reduce  ambiguity  of  data  use  and  make  the  logic behind  effective clear automation communication with users [12]. Information  through (APPI)  it  the and  describe Explainability.  Providing  meaningful  and personalized explanations about the results generated by  AI  models  could  reduce  uncertainty  and  build trust  with  users  [12].  To  develop  explainable  AI, Supplier\u2019s  Declaration  of  Conformity  (SDoC) proposed by IBM  suggests that effective  AI systems should  be  able  to  interpret  algorithm  outputs  via examples  properly  testing methodology [41]. For example, PwC has released its Responsible  AI  Toolkit  to  guide  companies  to accountably  harness  the  power  of  AI  and  provide them  with  personalized  advisory  services.  Likewise, Alder  Hey  Children\u2019s  NHS  Foundation  Trust  in Liverpool, UK has driven the intelligent use of digital techniques  based  on  big  sets  of  patient  data.  Alder Hey\u2019s AI systems powered by IBM Watson cognitive analytics  enable  healthcare  professions  to  interact with  young  patients  and  deliver  them  with personalized  health  services,  thereby  improving  the quality and experience of care and securing the sound health  services  [39].  AI-enabled  personalized  health services  have  improved  patient  experiences  in  terms of  familiarization,  distraction  and  reward  [42]. Specifically,  before  patients  arrive,  360-degree  tours of  hospital  environments  and  introductive  videos  of blood  test  and  x-ray  check  are  available  for  them  to explore  the  hospital  conditions  and  familiarize  with potential  treatment  experiences.  Parents  could  speak to  a  virtual  assistant  called  Ask  Oli  to  inquire  about the  progress  of  their  children\u2019s  health  checks  and treatments.  Questions  are  assured  to  be  answered  in real  time.  Additionally,  Alder  Hey  offers  young patients  with  character-based  stickers  activated  by using augmented reality (AR).   4.2. Ethically Design solutions Ethical  concerns  should  be  minimized  in designing  AI  solutions  in  three  ways.  First,  design engineers  need  to  be  aware  of  possible  ethical challenges  such  as  artificial  stupidity,  racist  robots, data and cyber security when developing AI systems. To prevent  these  ethical  concerns,  AI  system  allows for  human  inspection  of  the  functionality  of  the algorithms and systems [7]. For example, Google has pointed  out  that  concerns  on  ethical,  environmental and societal challenges while applying AI technology need  to  be  addressed  across  all  sectors  of  society [43]. User-centered AI systems are designed based on Google\u2019s  concept  of  general  best  practices  for software  systems.  As  acting  a  leading  role  in  the development  of  AI,  Google  has  invested  in  AI to research  and  announced  guidance  principles manage its research  fields and product  development, thereby  influencing  its  business  decisions  in  a  more ethical  way  [43].  Assessment  of  responsible  AI applications  could  be  made  via  these  objectives, leading  to  the  obligation  for  Google  to  form  a \u201cresponsible  innovation  team\u201d  with  experts  from  a range  of  disciplines  to  initially  examine  its  ethical level,  and  select  a  council  of  senior  executives  to make decisions for more complicated issues [44][45]. In  addition,  an  external  advisory  group  is  organized with  Google\u2019s  AI solution developers from a variety of  disciplines  to  avoid  unethical  AI  practices  and complement its internal governance [44]. system Second,  a  should responsible  AI themselves  be  able  to  make  socially  significant decisions  by  a  set  of  ethical  algorithms  in  order  to reduce  the  risk  of  unethical  behaviors  [14].  Lessons could  be  learnt  from  a  ridesharing  platform,  for instance,  the  unethical  AI  algorithm  potentially creates unfairness on the distribution of drivers\u2019 task assignments  and  pricing  practices.  This  algorithm exists  like  a  \u201cblack  box\u201d  and  helps  its  drivers  evade local transport regulators. Third, a prerequisite for implementing responsible AI  successfully  is  to  develop  ethical  mindset  and culture  for  organizations  and  employees.  This  is critical  for  reducing  any  risks  when  applying  AI. H&M Group, for instance, has developed a checklist, along with 30 questions to guide all ongoing and new AI  projects  to  ensure  that  AI  applications  are  used results, with governance,  collaboration,  respecting privacy,  focused,  and  security.  Such  a  practice  help H&M  to  ensure  every  AI  solutions  they  develop  are subject  to  the  comprehensive  assessment  of  risks  in its use.   transparency,  beneficial reliability, fairness, 4.3. Training and education  a and  employees  with Building  training  programs  is  another  crucial responsible AI practices. Such programs are to equip managers  deeper understanding  of  ethical  use  of  AI  and  data.  IEEE\u2019s Initiative  for  Ethical  Considerations  in  Artificial Intelligence  Systems   is  a  program  designed  to promote  ethical  and  responsible  AI  and  ensure  AI architects  and  solutions  developers  are  educated  and trained to prioritize ethical considerations of AI [36].   This  program  suggests  that  organizations  should provide training courses for ethical use of AI in areas such  as  methods  to  guide  ethical  design,  and  safety and beneficence of artificial  general intelligence and artificial  superintelligence  to  those  employees  who will  play  a  critical  support  role  of  responsible  AI. Mentoring,  cross-functional  team-based  training  and self-study  are  also  beneficial  training  approaches  to help  employees  develop  the  ethical  AI  mindset  and culture. Google  has  provided  a  series  of  advanced technical  knowledge  online  for  people  to  master technical  skills.  One  suggested  path  is  related  to Machine  Learning  (ML)  techniques,  a  subset  of  AI which could be applied to the datasets generated from the  real  world.  To  be  specific,  Machine  Learning Crash  Course  (MLCC)  is  designed  by  Google engineers  with  the  help  from  university  computer science  faculties,  offering  resources  with  insights  of data  science  and  innovative  ML  approaches  for  the supplement  of  study  by  self-learning.  It  has  featured with  lessons  including  video  lectures,  actual  case studies  and  practical  exercises.  For  example,  a technical module on fairness in 11 language versions has been added to the MLCC by Google, in order to train its staff around the world and help them mitigate bias  [45].  Additionally,  material  rewards  from Kaggle  Machine  Learning  Competitions  could  be given  to  those  who  learn  new  skills  with  ML challenges.  Moreover,  in Technology  Practice\u201d  project  has  been  developed  at the  Markkula  Center  for  Applied  Ethics  at  Santa Clara University [45]. It offers assistance for Google users  to  identify  multifaceted  ethical  issues  during their  daily  work.  Besides,  Resource  Library  from Google  to  create individual pathway.  training  of  \u201cEthics to  be  accessed is  available Cloud AutoML has been introduced to design the own  model  by  using  Google\u2019s  techniques  such  as \u201clearning2learn\u201d  and  \u201ctransfer  learning\u201d  [46].  This   could  increase  the  productive  level  for  less-skilled users. The Google Cloud AI Solution provides either prepackaged solutions or personalized model to serve organizations\u2019  needs  across  industries.  Moreover,  it has  shared  experiences  to  improve  AI  practices, partnered  with  professionals  to  apply  projects  with positive  and  worked  with stakeholders to promote thoughtful leadership in this area  [43].  Therefore,  it  could  guarantee  a  long-term its development  of  AI implication.  technology  as  well  as societal  effects, In  addition,  PwC  has  published  the  articles  and white  papers  to  demonstrate  their  responsible  AI experiences  [47].  \u201cAI:  Sizing  the  prize\u201d  from  PwC aims  to  estimate  the  percentage  of  the  increase  in GDP to be contributed to AI in various regions [48]. From  a  recent  PwC  analysis  report  on  the  financial services  sector,  concerns  related  to  augmentation, automation  has  been  addressed,  and  corresponded advice on the way to adapt AI in the future has been provided. PwC advises exploring AI solutions within explanatory  and  operational  areas,  which  could  help using  budget  and  resources  in  a  more  ethical  and societal  way  [48].  In  addition,  PwC  has  worked  on leveraging  AI  fulfil  client  demands  and expectations,  thereby  sharing  its  own  experiences  to help  customers  to  employ  the  power  of  AI  in  the same  way  [49].  As  AI  cannot  learn  without  human intervention,  consequently,  it  is  vital  to  train  both intelligence machines and staff to acquire appropriate data  [50].  Efforts  from  staff  across  the  whole  PwC global  network  has  accelerated  the  PwC\u2019s  approach to the AI. It is proved that the advantages of aligning AI innovation with core strategic objectives outweigh operating initiatives in isolation [50]. to Another  example,  reported  by  Audi  AG,  is  that the \u201cBeyond AI Initiative\u201d is created to address social acceptance  barriers  of  autonomous  driving  and  the future  of  work  by  educating  development  engineers, scientists and other stakeholders.  4.4. Human-centric surveillance/risk control Successful responsible AI requires a series of risk control  mechanisms  at  the  design,  implementation, and  evaluation  stages.  Several  risks  should  be  taken into  consideration  when  developing  responsible  AI for  organizations  that  includes  security  risks  (cyber intrusion  risks,  privacy  risks,  and  open  source software risk), economic risks (e.g., job displacement risks), and performance risks (e.g., risk of errors and bias and risk of black box, and risk of explainability).  To  minimize  these  AI  risks,  the  first  step  is  to formulate  the  rules  of  risk  controls,  with  clearly focused  goals,  execution  procedures,  metrics,  and performance  measures. In other words, a strong data protocol  should  be  defined  that  provides  clear guidelines to proactively identify AI risks that enable organizations  to  harness  data  effectively  from  the time it is acquired, stored, analyzed, and finally used.  Second, organizations should review the data they gather  internally  and  externally  and  realize  their potential  risks.  AI  comes  from  self-learning  through human designed algorithms. It is imperative to ensure the creditability of data so that AI can learn from the right  patterns  and  act  according  to  their  input.  Once the  potential  risk  of  these  data  has  been  managed, managers  can  make  better  decisions,  thereby minimizing cost and complexity.  Finally,  a  responsible  AI  system  should  consider the economic risks such as job displacement, liability, and  reputation  risks.  It  is  widely  acknowledged  that future  trend  of  AI  will  utilize  AI  approaches  to augment and complement human cognitive skills, and focus  on  human-AI  machine  interaction  and collaboration to bring together the best of each [51].  5. Formulating Responsible AI Strategies Lessons learnt from our selected case studies, we suggest  the  following  five  strategies  might  provide useful  guideline  for  to  develop responsible AI initiative in their organizations. those  seeking 5.1.  Emergence  of  Chief  Responsible  AI Officers (CRaiO)  Firms increasingly expect that the deployment of AI is aligned with their goals and values of CSR.  AI not  only  enable  firms  to  explore  sharper  customer insights,  but  also  become  a  powerful  strategic resource to facilitate positive business reputation and brand  recognition  if  it  is  used  in  an  ethical  and responsible  manner.  However,  only  25%  of  around 250 surveyed companies  have considered the ethical implications of AI before investing in it according to the  PwC\u2019s  investigation  [52].  This  shows  that  the responsible AI practices in most cases are immature. CRaiO  roles  should  emerge  to  in  response  to  this need.  We  define  the  CRaiO  as  a  role  in  charge  of developing  a  responsible  AI  roadmap  and  policy  in conjunction  with  internal  and  external  stakeholders to  make  use  of  trusted  AI,  integrating  the  oeuvre  of responsible AI to the projects across functional units, and  cultivating  an  inclusive  responsible  AI  culture across  organizational  and  functional  boundaries. Creating  a  CRaiO  may  require  intensively  cross-functional collaborations and organizational changes. A careful assessment on organizational resources and taken.  Alternatively,  as capabilities  should  be suggested  by  EY  [53],  AI  ethics  multi-disciplinary   advisory  board  can  be  established  to  provide  advice and guidance to the Board of Directors.   5.2.  Balancing sustainability of AI use  economic  and  social the  reputation company\u2019s AI  for  sustainability  has  attracted  academic  and practical  attentions  in  recent  years,  particularly discussions  on  how  can  AI  techniques  be  applied  to find  a  balance  between  economic  and  social sustainable impact for businesses has been excited in diverse  disciplines.  When  applying  AI,  its  societal impact  on  well-being  of  humans  and  environment should  be  seriously  considered.  If  firms  develop  AI algorisms with controversial impact on human rights, privacy, and employment, it may lead to the potential loss  of  credibility  for  products  and  brands,  and hamper  the marketplaces.  Thus,  the  ultimate  goal  of  responsible AI is to strike a balance between satisfying customer needs  with  less  ethical  concerns  and  dilemmas,  and attaining  long-term  profitability  for  businesses  and services.  Ecological  modernization  theory  (EMT) argues  the  ecological  outcomes  could  be  maximized through  achieving  a  balance  between  economic growth  and  social  sustainability  [54].  In  this  sense, firms should develop their AI solutions by taking the co-creation of economic and social sustainability into consideration.  Specifically,  firms  need  to  establish policies  on  ethical  governance  considering  socially preferable  approaches,  address  ethical  issues  both  in the  initial  design  and  post-launch  stage  of  AI systems,  and  place  AI  ethics  as  part  of  the  CSR strategy.  in 5.3.  Transparent  and  customer-centric  data policy  require There is no strategy with AI without a good data quality  management.  However,  with  the  data protection regulations such as GDPR came into force, to  obtain  consent  statement  or firms permission from consumers if they  want to use their information.  These  regulations  have  been  a  double-edged sword for firms, potentially acting as a barrier the to  behavioural communications  and  other  promotions  plans  of marketers.  On  the  other  hand,  with  appropriate  data policy,  it  will  improve  consumers\u2019  confidence  in sharing the data with firms for AI use [56].  targeting,  personalisation  of Furthermore,  penalties  for  the  GDPR  non-compliance is about ranges from \u20ac10-20 million or 2-4% annual global turnover, which is a hefty fine and challenge  for  small  and  medium  retailers  [55]. Although the GDPR is an EU act, but it has a global acts  as  to communicate with EU citizens must comply with the international  marketers  that  plan regulations.  Thus,  persuading  customers  to  share information through transparent and customer-centric data  policy  may  turn  these  regulations  from  a  threat to  an  opportunity  and  may  improve  their  trust towards AI .  5.4.  Creating  socially  responsible  initiatives with AI   customers\u2019 Responsible  AI  is  not  just  about  designing  AI  to operate  ethically  and  responsibly,  what  do  matter  is how  AI  can  be  leveraged  to  advance  socially responsible initiatives [57]. For instance, Quantcast, a leading  AI  company  who  specializes  in  AI-driven marketing,  optimizes  advertising campaigns  real-time through  using  AI-driven insights.  Meanwhile,  they  rely  on  real-time  data  and machine  learning  capability  to  help  their  customers ensure  brand  safety  and  prevent  consumers  in  the markets  information dissemination.  H&M  utilizes  AI  to  ensure  customer centricity  (approaches  such  as  fitting  consumers\u2019 physical  dimensions  with  their  preferred  style  and incorporating  multiple  data  sources  for  dynamic analysis),  as  a  result  of  cutting  environmental  waste and cost caused by high purchase return rates. These socially  responsible  initiatives  with  AI  contribute  to increased trust and sustainability among consumers.  fraud from  fake and 5.5.  Carrot  and  stick  mechanism  to  regulate AI usage Carrot  (reward/incentive)  and  stick  (punishment) mechanism  has  been  widely  applied  to  regulate  IT usage  [58].  It  is  important  to  understand  what mechanisms  can  trigger  employees\u2019  ethical  AI behavior  or  impede  the  misuse  of  AI.  Floridi  et  al. [59]  have  designed  a  series  of  actionable  plans  to financially  incentivize  ethical  use  of  AI  at  the organizational  level.  First,  firms  should  encourage cross-disciplinary  and  debate  on cooperation legal  aspects  of  AI.  For technological,  social, example,  H&M  has  created  an  Ethical  AI  Debate Club  where  cross-functional  employees  and  their customers and AI researchers can meet for debates on ethical  concerns  and  dilemmas  arise  in  the  fashion industry.  Second,  developing  an  inclusive  triadic configuration  to  capture  the  complex  interactions among ethics, innovation, and policy in confluence, it will  help  firms  to  ensure  AI  has  ethics  as  a  core facilitating consideration  and  policy socially  positive  [59].  Moreover, innovation punishment plays a  key role in affecting employees\u2019 ethical  AI  behavior.  Firms  should  develop  a monitoring,  auditing  and  punishing  mechanism  to redress  for  a  wrong  caused  by  AI  usage  and  to moderately punish unethical AI behaviors. is  guided  6. Conclusion to lead  inevitable  disruptive As being maturing rapidly, AI holds an incredibly power which has created new opportunities for social good.  However,  the  scalability  of  machine  learning might  impacts, consequently,  concerns  may  be  aroused  while misusing  AI. In practice,  only  few companies across industries  have  incorporated  AI  with  a  series  of in  a  manner  consistent  with  ethical practices considerations,  public organizational expectations and societal norms. Attention is urgently needed  for  research  to  formulate  responsible  AI strategies  that  will  enable  firms  to  move  forward  to leverage AI most efficiently and ethically. values, Although  our  study  identifies  responsible  AI practices  which  is  not  only  contributing  to  the disciplinary field of  AI and ethics,  but also provides practical  recommendations  for  practitioners,  it  is subject  to  the  limitation  of  data  source  but  at  the same  time  formulating  new  directions  for  future research  if  primary  data  can  be  collected.  First,  the adoption of responsible AI is still in its infancy. Case materials  used  in  this  study  mainly  came  from companies\u2019  and  consultants\u2019  reports.  The  absence  of academic  works  may  result  in  a  potential  bias,  as companies  usually  publicize  their  success  stories [60].  Further  validation  could  be  undertaken  by collecting  primary  data  from  consumers,  C-level executives,  AI  software  companies,  third  party organizations  and  policy  makers  to  fully  explore responsible  AI  practices  individual, organisational, industrial, and societal levels.  the at understanding Second,  as  we  found  trust  plays  a  vital  role  in implementing  AI,  consumers\u2019 cognitive  appraisals,  emotional  states,  and  behavior responses  toward  irresponsible  use  of  AI  enables practitioners  to  avoid  negative  consequences.  The different  scenario  of  irresponsible  use  of  AI  (e.g., ineffective  marketing  message,  identity  theft,  and invasion  of  privacy)  can  be  examined  through  the surveys and field experiments.  7. References Appendix 1",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "2a85d0da-e973-4aa3-bb64-c03caca55f0c",
                    "text": "expectations Artificial  Intelligence  (AI)  is  influencing  all aspects  of  human  and  business  activities  nowadays. Although  potential  benefits  emerged  from  AI technologies  have  been  widely  discussed  in  many current  literature,  there  is  an  urgently  need  to understand  how  AI  can  be  designed  to  operate in  a  manner  meeting responsibly  and  act stakeholders\u2019  applicable regulations.  We  seek  to  fill  the  gap  by  exploring  the practices  of  responsible  AI  and  identifying  the potential  benefits  when  implementing  responsible  AI practices. In this study, 10 responsible AI cases were selected from different industries to better understand the  use  of  responsible  AI  in  practices.  Four responsible  AI  practices  are  identified,  including governance,  ethically  design  solutions,  risk  control and  training  and  education  and  five  strategies  for firms  who  are  considering  to  adopt  responsible  AI practices are recommended.   and",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "da3817ba-45da-47bc-a02c-f0cc4a8a187d",
                    "text": "Artificial  Intelligence  (AI),  a  set  of  algorithm-based  machine,  is  programmed  to  self-learn  from data and display predictions and intelligent behaviors through  artificial  neural  networks,  automated machine  learning,  robotic  process  automation,  and text  mining  [1].  AI  is  capable  of  responding  to  real-world problems and arriving decisions in real-time or near real-time manner on behalf of human being [2], [3], [4]. For instance, Chatbots, an AI-enabled service robot, developed by Bookings.com provides real-time 24/7  customer  service  with  the  support  of  43 languages  to  answer  travel  related  queries  to  its customers.  With  such  highly  evolved  language processing  capabilities,  Chatbots  can  interact  with them  with  personalized customers  and  provide recommendations.  It  also  enables  Booking.com  to deliver  marketing  automation  thereby  simplify routine works accordingly. AI,  as  a  major  shift  in  the  global  economy,  is influencing  all  aspects  of  human  and  business activities  nowadays.  It  holds  the  promise  to  create  information efficiency  and  effectiveness  by  using  data  generated from  an  explosion  of  digital  touchpoints  [5],  [6].  At the  same  time,  it  comes  with  its  own  concerns relating  to  privacy  concerns,  user  distrust,  data leakages,  transparency,  and  ethical concerns. Such ethical dilemma and concerns, if they are  not  well  addressed  when  developing  AI initiatives,  would  lead  to  the  potential  loss  of credibility  for  products  and  brands  and  hamper  the company reputation in the  marketplaces.  Ethical and societal concerns aroused from AI systems need to be addressed in priority to ensure  effective, ethical, and responsible  use  of  AI  [7].  However,  relatively  little attention  has  been  given  to  understand  responsible approaches  to  the  development,  implementation, management, and governance of AI.   [11].  From responsible  companies Indeed,  corporate  social  responsibility  (CSR)  has become  the  main  preoccupations  of  organizations  in the  global  marketplaces  [8],  used  in  broad  domains including  areas  of  policies,  programs  and  actions while  interacting  with  stakeholders  [9][10].  For instance,  customer  retention  rate  could  be  enhanced as consumers prefer to purchase from and engage in, socially  [8].  Likewise, company  reputation  could  be  built  along  with  CSR activities  the  CSR  perspective, organizations  need  to  embrace  the  goal  of  being socially  responsible  while  bringing  AI  into  the business  mainstream.  However,  according  to  the Cognizant\u2019s report, only about 50% of surveyed 975 executives  across  industries  in  U.S.  and  Europe  had policies  and  procedures  in  place  to  address  ethical concerns  while  designing  AI  applications  [12].  from  AI Although  potential  benefits  emerged technologies  have  been  widely  discussed  in  many current  literatures,  the  sustainable  outcomes  from business  to  the  society  that  AI  presents  is  remained unexplored [6]. Specifically, there is an urgently need to  understand  how  AI  solutions  can  be  designed  to operate  responsibly  and  act  in  a  manner  meeting stakeholder  expectations  and  applicable  regulations [7], [13], [14]. We seek to fill the gap by exploring the practices of  responsible  AI  and  identifying  the  potential responsible  AI implementing benefits  when initiatives. Therefore, this study set out to answer the following research questions. RQ1: What are the practices of responsible AI? RQ2: What benefits and challenges have been brought by implementing responsible AI practices? To answer the above research questions, we hope to  provide  business  practitioners  a  more  current comprehensive  understanding  of  responsible  AI  and both theoretical and practical reference values for the use of AI in a more socially responsible way. In this paper, we begin by providing the historical context of technology  use  of  CSR,  and  then  move  on  to understanding  ethical  challenges  in  AI  and  the development  of  AI  in  responsible  practices.  We conducted  a  multiple  case  study  of  responsible  AI, which  leads  to  the  identification  of  responsible  AI practices  and  the  recommendation  of  responsible  AI strategies.",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "09c38bbe-3130-44db-ba59-41f7cab4bd59",
                    "text": "",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "2c74b4df-61fe-4ddc-83c7-989081d1e94b",
                    "text": "Corporate  social  responsibility  (CSR)  can  be defined  as  commitments  from  organizations  to  the society  in  improving  societal,  environmental  and economic  well-being  through  different  business practices  [8],  [15].  The  relationship  between  the company\u2019s  social  responsibilities  and  its  financial performance has been documented extensively in the literature [16], [17]. The study from Bernal\u2010Conesa et al.  [18]  has  indicated  that  the  contribution  of  CSR-oriented  strategies  the  overall performance of the organizations. From the empirical perspectives,  this  principle  has  been  incorporated  in marketing communications by many organizations in order  to  enhance  stakeholder  perceptions  and retentions  [19].  Thus,  CSR  is  perceived  to  have increasing  importance  for  increasing  enterprises\u2019 competitiveness.   is  significant  to CSR  domains  within  the  marketing  field  are classified  into  seven  categories,  including  employee relations, human rights, diversity, community issues, corporate  governance,  environmental  issues  and product  issues  [20],  [21].  Consumers  are  evinced  to have  domain-based  pro-company  responses  to  CSR practices  due  to  the  influence  of  moral  foundations theory  (MFT)  either  individual-oriented  or  group-oriented  [8].  Their  reactions  towards  companies  can be  moderated  through  CSR  domains  in  the  case  of CSR  strengths,  therefore,  properly  CSR  activates  in different  aspects  need  to  be  organized  and  lapses  of CSR are required to be solved by companies [8].  As  digital  has  become  a  megatrend  in  the  global economy,  new  technology  gains  great  popularity among different industries, offering new possibilities and bringing benefits in many aspects of human lives [22]. For example, labor force may be replaced by the intelligent  machines  [23].  However,  concept  of  the sustainability has changed as it is confronted with the digital transformation, also known as a technological leap  [24],  leading  to  the  increase  in  the  restraints, from  the  national  laws  and  international  rules,  on companies\u2019  responsibilities  towards  society  and environment  (Bernal-Conesa  et  al.,  2017).  Thus, challenges  could  be  posted  to  organizations  for creating  sustainability  and  responsibility  in  the  long run. Inability to communicate the CSR programmers and  integrate  them  into  strategies  may  lead  to  the failure  from  achieving  full  potentials.  Moreover, criticisms  of  CSR  vary  between  companies  and industries  [20].  Data,  algorithms  and  bots  are  main areas to be explored during the process of sustainable digitalization  [22].  Specifically,  although  having access  to  consumer  data  helps  predict  their  potential moves and create personalized experiences for them, privacy  invasions  and  algorithmic  bias  derived  from the  sophisticated  use  of  consumer  data  cannot  be underestimated  [25].  Hence,  the  performance  of technologies  is  required  to  be  aligned  with  CSR principle  and  enhance  its  implementations  [26].  In practice,  technology  could  identify  the  integration points of CSR initiatives, offering corporate  strategy to increase the overall integrated level. In addition, it the  multi-could dimensional  measurement  on  the  programme performance.  Therefore,  it  is  arguable  that  technical resources  can  be  integrated  with  human  resources, within  or  across  companies,  helping  develop capabilities  to  address  sustainable  concerns  and delivering  responsible  values  to  stakeholders  to obtain sustained benefits [27]. reduce  human  bias  through",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "702d0530-1ab0-4573-88ef-dffbd7122193",
                    "text": "AI is no doubt beneficial to society as it helps to harness  empathy  and  creativity  skills  of  human  and leveraging their emotional intelligence [28], [29]. An example  is  that  Siri,  assistant  of  iPhone,  is  able  to recognize user\u2019s requests through voice message, and provide  them  assistance  accordingly.  It  could  lessen the  uncertainty,  spent  on in improve administration  and decision-making process based on the data evidence. In  practice,  the  application  of  AI  varies  as  it  is programmed to use specific data to achieve a certain goal  [30].  Marketers  with  such  data  can  provide additional  benefits  to  target  consumers  in  a  more efficient way [25].   the  efficiency reduce  time the In  recent  years,  the  pace  of  using  consumer  data in the marketing field exceeds the academic scholars\u2019 analytics  [25].  Consequently,  negatively  unforeseen issues  may  come  along  with  initial  programs  and against  its  positive  goals.  In  addition,  the  lack  of transparency  on  algorithms,  in  reality,  has  caught public  attention,  leading  to  the  rise  of  ethical concerns  on  the  use  of  AI  [2].  Ethical  issues  are associated  with  the  emergence  of  machine  learning, as  it  allows  intelligence  system  to  get  access  and learn from numerous datasets, to derive its own rules, its  behaviors  and  produce  cognitive enhance competence  its performances caused ethical reflections, may result in deviating  from  sustained  values  and  presenting  new challenges [28], [29], [32]. For instance, interruptions of systems are of frequent occurrence due to the self-reflection.  Programmers\u2019  biases  might  exist  as  the abilities  of  AI  are  initially  dependent  on  human inputs, therefore, it might be problematic as bias can also be replicated from previous events according  to the algorithm [2]. Thus, it is argued that intelligence systems  are  requiring  moral  reasoning  capabilities while facing certain ethical dilemmas [29]. [31].  The  ways  in  which have system  perspectives, Studies on ethical AI, both from the data and the information  been conducted  recently,  leading  the  mitigation  of  unfair bias.  Reinforcement  learning  (RL)  is  prospected  to prevent  ethical  issues  in  the  process  of  intelligent decision-making [32]. It can learn from interruptions while  using  data,  either  from  humans  or  from environments,  to  avoid  repetitive  problems.  In addition,  formulating  ethical  principles  to  guide  the design  of  AI  system  and  rational  algorithms  are argued  to  be  effective  to  ensure  the  ethics  [33]. Nevertheless,  it  is  not  an  easy  task.  Research  from Robbins  [29]  states  a  lack  of  assistance  from  ethical norms  or  policy  guidelines  to  regulate  AI  developer to achieve a balance between the  effective use of AI and the concerns on ethics in the society. Taddeo and Floridi  [33]  point  out  that  the  formulation  of  ethical principles  depends  on  cultural  contexts  and  the domain of analysis which they could vary.",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "ae7c9c6a-7f11-4b2c-832b-5fce5ed20748",
                    "text": "collections, Our  cases  were  drawn  from  materials  on  current and  past  responsible  AI  projects  from  multiple sources such as practical journals, print publications, case  vendors\u2019, consultants\u2019  or  analysts\u2019  reports.  The  absence  of academic discussion in our case collection about the utilization  of  responsible  AI  is  due  to  the  incipient nature of such in this field.   companies\u2019, and  an (1)  the  case  presents following  case  selection  criteria  were The applied:  actual implementation  of  responsible  AI;  (2)  it  clearly describes  the  practices  of  responsible  AI.  We  were able  to  collect  10  responsible  AI  cases  in  different industry (See Appendix 1). Categorizing by region, 4 cases were collected from Northern America, 6 cases from Europe and UK.  Data  analysis  followed  the  constant  comparison method.  Initially  data  analysis  was  performed concomitantly  with  data  collection,  and  continued with an explicit coding stage and an analytical coding procedure stage [33].  In  the  explicit  coding  stage,  the  analysis  started by  comparing  and  coding  each  statement  extracted from the case materials into categories. This allowed categories  to  emerge  to  fit  in  an  existing  category [33].  Relevant  statements  were  labelled  and  either created  as  a  new  code  and  given  a  definition,  or assigned to the existing codes with memos indicating their relevance and potential properties. Through this process, the statement was broken down into units of meanings.  The  concept  as  a  basic  unit  of  analysis labels  phenomenon  representing  a  practice  of responsible  AI  [35].  After  the  explicit  coding  stage, the  data  were  and categorized  in  terms  of  their  properties,  which initiates the analytical coding stage. conceptualized,  defined During  the  analytical  coding  stage,  the  research team compared the properties and dimensions of the emergent  categories.  In  order  to  constantly  analyze and  compare  the  categories,  the  concept  map  was employed  to  visualize  the  classification  [35].  Four dimensions underlying responsible AI practices were identified.  They  are  described  in  detail  in  the following sections and visualized in Figure 1.",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "eba0af3f-2312-4f5c-afc4-64a204bf8d84",
                    "text": "Responsible  AI  is  a  governance  framework  that uses  to  harness,  deploy,  evaluate,  and  monitor  AI machines  to  create  new  opportunities  for  better service  provision.  It  focuses  on  designing  and implementing  ethical,  transparent,  and  accountable AI  solutions  that  help  maintain  individual  trust  and minimize  privacy  invasion.  Responsible  AI  places human  (e.g.,  end-users)  at  the  center  and  meets stakeholder  expectations  and  applicable  regulations and  laws.  Prior  to  designing  and  implementing responsible  AI, organizations need to understand the practices that will help them drive ethics and trust of AI use. The four practices of responsible AI include: (1)  Data  governance;  (2)  Ethically  design  solutions; (3)  Human-centric  surveillance/risk  control;  and  (4) Training  and  Education.  These  practices  are  evident Figure 1. A concept map of responsible AI practices in  the  real-world  cases  of  responsible  AI.  These  are described in turn below.",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "74866b28-7278-48e4-93f5-96dc7ca6f4b8",
                    "text": "Governance of responsible AI focuses on building transparency, trust, and explainability.  It  that important Transparency.  the is organizational  use  of  AI  must  be  transparent  to  the stakeholders  by allowing them  fully understand how an  AI  application  processes  their  data  and  arrive  to specific  decisions  [36].  According  to  the  Direct Marketing  Association  (DMA)\u2019s  investigation,  80% of surveyed consumers would be very or moderately comfortable  with  sharing  personal  data  when  they know about how digital data is shared and effectively used  for  marketing  purposes  [37].  Capital  One  is making  the  criteria  system  of  credit  card  transparent by providing a computational decision with complete explanation to their customers when their credit card applications  are  accepted  or  denied  [38].  Likewise, Alder Hey  Children\u2019s Hospital, as one of the largest children\u2019s  hospitals  in  Europe,  has  developed  an  AI featured  digital  App  called  Alder  Play.  Alder  Play has  incorporated  the  cognitive  advances  in  order  to present the enjoyable and informative experiences for its  young  patients.  Young  patients  allow  to  active their  own  avatar  during  their  stay,  receive  awards  when completing treatments, and get access to further guidelines and contents accordingly [39]. Alder Play enables  healthcare  professionals  to  have  access  to medical records of patients who are eligible for NHS treatment. Patients and their families would be able to obtain  their  medical  records  online.  This  could largely  the  clinical transparency processes,  thereby  enhancing  the  quality  of  health services and strengthening the patient engagement. improve  in Trust building. Trusted AI is built through high-quality  data  and  consent  to  use  [12].  AI  with  high-quality  data  could  mitigate  biased  and  inaccurate results generated. To ensure the quality and reliability of  data,  where  the  data  sources  come  from,  the limitation of data, and data rules to sharpen data error detection  should  be  identified  when  developing  AI algorithms  and  systems.  For  example,  PwC  has employed H2O.ai to build a revolutionary bot named GL.ai,  which  uses  AI  algorithms  to  effective  track operational data and transactions and correct errors to and maintain interactions for their business customers.  accurate purchase  histories What makes AI workable is its access to personal information  [36].  However,  widespread  access  to personal  (e.g.,  consumer-generated content,  online  transactional  data,  and  browsing  and clicking  data)  has  brought  negative  impacts  to information to individual,  business,  and  society  [25],  [40].  The availability  of  consumer  data  gives  rise  to  serious concerns  where  consumers  suffer  from  privacy invasion,  fraud,  information  leakage,  and  identity theft,  and  on  the  other  hand,  companies  cannot the collect  consumer  data  effectively  due consumers\u2019 distrust. These trends have led to a focus on data protection and transparency of data use by the regulators  in  many  countries  such  as  General  Data Protection  Regulation  (GDPR)  formulated  by  the European  Union  and  Act  on  the  Protection  of Personal  in  Japan.  These regulations  aim  to  protect  all  individuals\u2019  rights regarding privacy and personal data and give control to  individuals  over  their  personal  data.  With  these regulations  came  is  crucial  for into  force, companies to institutionalize the practice of obtaining consent  statement  or  permission  from  users  and reduce  ambiguity  of  data  use  and  make  the  logic behind  effective clear automation communication with users [12]. Information  through (APPI)  it  the and  describe Explainability.  Providing  meaningful  and personalized explanations about the results generated by  AI  models  could  reduce  uncertainty  and  build trust  with  users  [12].  To  develop  explainable  AI, Supplier\u2019s  Declaration  of  Conformity  (SDoC) proposed by IBM  suggests that effective  AI systems should  be  able  to  interpret  algorithm  outputs  via examples  properly  testing methodology [41]. For example, PwC has released its Responsible  AI  Toolkit  to  guide  companies  to accountably  harness  the  power  of  AI  and  provide them  with  personalized  advisory  services.  Likewise, Alder  Hey  Children\u2019s  NHS  Foundation  Trust  in Liverpool, UK has driven the intelligent use of digital techniques  based  on  big  sets  of  patient  data.  Alder Hey\u2019s AI systems powered by IBM Watson cognitive analytics  enable  healthcare  professions  to  interact with  young  patients  and  deliver  them  with personalized  health  services,  thereby  improving  the quality and experience of care and securing the sound health  services  [39].  AI-enabled  personalized  health services  have  improved  patient  experiences  in  terms of  familiarization,  distraction  and  reward  [42]. Specifically,  before  patients  arrive,  360-degree  tours of  hospital  environments  and  introductive  videos  of blood  test  and  x-ray  check  are  available  for  them  to explore  the  hospital  conditions  and  familiarize  with potential  treatment  experiences.  Parents  could  speak to  a  virtual  assistant  called  Ask  Oli  to  inquire  about the  progress  of  their  children\u2019s  health  checks  and treatments.  Questions  are  assured  to  be  answered  in real  time.  Additionally,  Alder  Hey  offers  young patients  with  character-based  stickers  activated  by using augmented reality (AR).",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "780512d6-98c8-45e2-b672-9aa742a5c8c7",
                    "text": "Ethical  concerns  should  be  minimized  in designing  AI  solutions  in  three  ways.  First,  design engineers  need  to  be  aware  of  possible  ethical challenges  such  as  artificial  stupidity,  racist  robots, data and cyber security when developing AI systems. To prevent  these  ethical  concerns,  AI  system  allows for  human  inspection  of  the  functionality  of  the algorithms and systems [7]. For example, Google has pointed  out  that  concerns  on  ethical,  environmental and societal challenges while applying AI technology need  to  be  addressed  across  all  sectors  of  society [43]. User-centered AI systems are designed based on Google\u2019s  concept  of  general  best  practices  for software  systems.  As  acting  a  leading  role  in  the development  of  AI,  Google  has  invested  in  AI to research  and  announced  guidance  principles manage its research  fields and product  development, thereby  influencing  its  business  decisions  in  a  more ethical  way  [43].  Assessment  of  responsible  AI applications  could  be  made  via  these  objectives, leading  to  the  obligation  for  Google  to  form  a \u201cresponsible  innovation  team\u201d  with  experts  from  a range  of  disciplines  to  initially  examine  its  ethical level,  and  select  a  council  of  senior  executives  to make decisions for more complicated issues [44][45]. In  addition,  an  external  advisory  group  is  organized with  Google\u2019s  AI solution developers from a variety of  disciplines  to  avoid  unethical  AI  practices  and complement its internal governance [44]. system Second,  a  should responsible  AI themselves  be  able  to  make  socially  significant decisions  by  a  set  of  ethical  algorithms  in  order  to reduce  the  risk  of  unethical  behaviors  [14].  Lessons could  be  learnt  from  a  ridesharing  platform,  for instance,  the  unethical  AI  algorithm  potentially creates unfairness on the distribution of drivers\u2019 task assignments  and  pricing  practices.  This  algorithm exists  like  a  \u201cblack  box\u201d  and  helps  its  drivers  evade local transport regulators. Third, a prerequisite for implementing responsible AI  successfully  is  to  develop  ethical  mindset  and culture  for  organizations  and  employees.  This  is critical  for  reducing  any  risks  when  applying  AI. H&M Group, for instance, has developed a checklist, along with 30 questions to guide all ongoing and new AI  projects  to  ensure  that  AI  applications  are  used results, with governance,  collaboration,  respecting privacy,  focused,  and  security.  Such  a  practice  help H&M  to  ensure  every  AI  solutions  they  develop  are subject  to  the  comprehensive  assessment  of  risks  in its use.   transparency,  beneficial reliability, fairness,",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "028bc089-c74a-4811-a339-1c4ad0d9649f",
                    "text": "a and  employees  with Building  training  programs  is  another  crucial responsible AI practices. Such programs are to equip managers  deeper understanding  of  ethical  use  of  AI  and  data.  IEEE\u2019s Initiative  for  Ethical  Considerations  in  Artificial Intelligence  Systems   is  a  program  designed  to promote  ethical  and  responsible  AI  and  ensure  AI architects  and  solutions  developers  are  educated  and trained to prioritize ethical considerations of AI [36].   This  program  suggests  that  organizations  should provide training courses for ethical use of AI in areas such  as  methods  to  guide  ethical  design,  and  safety and beneficence of artificial  general intelligence and artificial  superintelligence  to  those  employees  who will  play  a  critical  support  role  of  responsible  AI. Mentoring,  cross-functional  team-based  training  and self-study  are  also  beneficial  training  approaches  to help  employees  develop  the  ethical  AI  mindset  and culture. Google  has  provided  a  series  of  advanced technical  knowledge  online  for  people  to  master technical  skills.  One  suggested  path  is  related  to Machine  Learning  (ML)  techniques,  a  subset  of  AI which could be applied to the datasets generated from the  real  world.  To  be  specific,  Machine  Learning Crash  Course  (MLCC)  is  designed  by  Google engineers  with  the  help  from  university  computer science  faculties,  offering  resources  with  insights  of data  science  and  innovative  ML  approaches  for  the supplement  of  study  by  self-learning.  It  has  featured with  lessons  including  video  lectures,  actual  case studies  and  practical  exercises.  For  example,  a technical module on fairness in 11 language versions has been added to the MLCC by Google, in order to train its staff around the world and help them mitigate bias  [45].  Additionally,  material  rewards  from Kaggle  Machine  Learning  Competitions  could  be given  to  those  who  learn  new  skills  with  ML challenges.  Moreover,  in Technology  Practice\u201d  project  has  been  developed  at the  Markkula  Center  for  Applied  Ethics  at  Santa Clara University [45]. It offers assistance for Google users  to  identify  multifaceted  ethical  issues  during their  daily  work.  Besides,  Resource  Library  from Google  to  create individual pathway.  training  of  \u201cEthics to  be  accessed is  available Cloud AutoML has been introduced to design the own  model  by  using  Google\u2019s  techniques  such  as \u201clearning2learn\u201d  and  \u201ctransfer  learning\u201d  [46].  This   could  increase  the  productive  level  for  less-skilled users. The Google Cloud AI Solution provides either prepackaged solutions or personalized model to serve organizations\u2019  needs  across  industries.  Moreover,  it has  shared  experiences  to  improve  AI  practices, partnered  with  professionals  to  apply  projects  with positive  and  worked  with stakeholders to promote thoughtful leadership in this area  [43].  Therefore,  it  could  guarantee  a  long-term its development  of  AI implication.  technology  as  well  as societal  effects, In  addition,  PwC  has  published  the  articles  and white  papers  to  demonstrate  their  responsible  AI experiences  [47].  \u201cAI:  Sizing  the  prize\u201d  from  PwC aims  to  estimate  the  percentage  of  the  increase  in GDP to be contributed to AI in various regions [48]. From  a  recent  PwC  analysis  report  on  the  financial services  sector,  concerns  related  to  augmentation, automation  has  been  addressed,  and  corresponded advice on the way to adapt AI in the future has been provided. PwC advises exploring AI solutions within explanatory  and  operational  areas,  which  could  help using  budget  and  resources  in  a  more  ethical  and societal  way  [48].  In  addition,  PwC  has  worked  on leveraging  AI  fulfil  client  demands  and expectations,  thereby  sharing  its  own  experiences  to help  customers  to  employ  the  power  of  AI  in  the same  way  [49].  As  AI  cannot  learn  without  human intervention,  consequently,  it  is  vital  to  train  both intelligence machines and staff to acquire appropriate data  [50].  Efforts  from  staff  across  the  whole  PwC global  network  has  accelerated  the  PwC\u2019s  approach to the AI. It is proved that the advantages of aligning AI innovation with core strategic objectives outweigh operating initiatives in isolation [50]. to Another  example,  reported  by  Audi  AG,  is  that the \u201cBeyond AI Initiative\u201d is created to address social acceptance  barriers  of  autonomous  driving  and  the future  of  work  by  educating  development  engineers, scientists and other stakeholders.",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "06eb6447-6756-4d22-bdd6-a50f1619151b",
                    "text": "Successful responsible AI requires a series of risk control  mechanisms  at  the  design,  implementation, and  evaluation  stages.  Several  risks  should  be  taken into  consideration  when  developing  responsible  AI for  organizations  that  includes  security  risks  (cyber intrusion  risks,  privacy  risks,  and  open  source software risk), economic risks (e.g., job displacement risks), and performance risks (e.g., risk of errors and bias and risk of black box, and risk of explainability).  To  minimize  these  AI  risks,  the  first  step  is  to formulate  the  rules  of  risk  controls,  with  clearly focused  goals,  execution  procedures,  metrics,  and performance  measures. In other words, a strong data protocol  should  be  defined  that  provides  clear guidelines to proactively identify AI risks that enable organizations  to  harness  data  effectively  from  the time it is acquired, stored, analyzed, and finally used.  Second, organizations should review the data they gather  internally  and  externally  and  realize  their potential  risks.  AI  comes  from  self-learning  through human designed algorithms. It is imperative to ensure the creditability of data so that AI can learn from the right  patterns  and  act  according  to  their  input.  Once the  potential  risk  of  these  data  has  been  managed, managers  can  make  better  decisions,  thereby minimizing cost and complexity.  Finally,  a  responsible  AI  system  should  consider the economic risks such as job displacement, liability, and  reputation  risks.  It  is  widely  acknowledged  that future  trend  of  AI  will  utilize  AI  approaches  to augment and complement human cognitive skills, and focus  on  human-AI  machine  interaction  and collaboration to bring together the best of each [51].",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "ff88db68-4387-4449-97ff-2592956ac91f",
                    "text": "Lessons learnt from our selected case studies, we suggest  the  following  five  strategies  might  provide useful  guideline  for  to  develop responsible AI initiative in their organizations. those  seeking",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "d67856ec-c27a-4d36-9c01-d32a6364b13d",
                    "text": "",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "40687af7-fe0d-4abc-90ec-c0e8a4705ad9",
                    "text": "Firms increasingly expect that the deployment of AI is aligned with their goals and values of CSR.  AI not  only  enable  firms  to  explore  sharper  customer insights,  but  also  become  a  powerful  strategic resource to facilitate positive business reputation and brand  recognition  if  it  is  used  in  an  ethical  and responsible  manner.  However,  only  25%  of  around 250 surveyed companies  have considered the ethical implications of AI before investing in it according to the  PwC\u2019s  investigation  [52].  This  shows  that  the responsible AI practices in most cases are immature. CRaiO  roles  should  emerge  to  in  response  to  this need.  We  define  the  CRaiO  as  a  role  in  charge  of developing  a  responsible  AI  roadmap  and  policy  in conjunction  with  internal  and  external  stakeholders to  make  use  of  trusted  AI,  integrating  the  oeuvre  of responsible AI to the projects across functional units, and  cultivating  an  inclusive  responsible  AI  culture across  organizational  and  functional  boundaries. Creating  a  CRaiO  may  require  intensively  cross-functional collaborations and organizational changes. A careful assessment on organizational resources and taken.  Alternatively,  as capabilities  should  be suggested  by  EY  [53],  AI  ethics  multi-disciplinary   advisory  board  can  be  established  to  provide  advice and guidance to the Board of Directors.",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "dfda89e8-c2da-4a59-a0d9-d7e85184de3e",
                    "text": "",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "5320cf58-a637-40a9-a644-759e915a404e",
                    "text": "",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "0fa188f4-1352-4b7e-a52c-a4a64fe9f1ca",
                    "text": "the  reputation company\u2019s AI  for  sustainability  has  attracted  academic  and practical  attentions  in  recent  years,  particularly discussions  on  how  can  AI  techniques  be  applied  to find  a  balance  between  economic  and  social sustainable impact for businesses has been excited in diverse  disciplines.  When  applying  AI,  its  societal impact  on  well-being  of  humans  and  environment should  be  seriously  considered.  If  firms  develop  AI algorisms with controversial impact on human rights, privacy, and employment, it may lead to the potential loss  of  credibility  for  products  and  brands,  and hamper  the marketplaces.  Thus,  the  ultimate  goal  of  responsible AI is to strike a balance between satisfying customer needs  with  less  ethical  concerns  and  dilemmas,  and attaining  long-term  profitability  for  businesses  and services.  Ecological  modernization  theory  (EMT) argues  the  ecological  outcomes  could  be  maximized through  achieving  a  balance  between  economic growth  and  social  sustainability  [54].  In  this  sense, firms should develop their AI solutions by taking the co-creation of economic and social sustainability into consideration.  Specifically,  firms  need  to  establish policies  on  ethical  governance  considering  socially preferable  approaches,  address  ethical  issues  both  in the  initial  design  and  post-launch  stage  of  AI systems,  and  place  AI  ethics  as  part  of  the  CSR strategy.  in",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "24085325-1446-4b75-908e-05c5f88c6515",
                    "text": "",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "f6578b7f-c398-4466-a4b4-dcc13893e160",
                    "text": "guidelines  to  regulate  AI  developer to achieve a balance between the  effective use of AI and the concerns on ethics in the society. Taddeo and Floridi  [33]  point  out  that  the  formulation  of  ethical principles  depends  on  cultural  contexts  and  the domain of analysis which they could vary. 3. Research Method collections, Our  cases  were  drawn  from  materials  on  current and  past  responsible  AI  projects  from  multiple sources such as practical journals, print publications, case  vendors\u2019, consultants\u2019  or  analysts\u2019  reports.  The  absence  of academic discussion in our case collection about the utilization  of  responsible  AI  is  due  to  the  incipient nature of such in this field.   companies\u2019, and  an (1)  the  case  presents following  case  selection  criteria  were The applied:  actual implementation  of  responsible  AI;  (2)  it  clearly describes  the  practices  of  responsible  AI.  We  were able  to  collect  10  responsible  AI  cases  in  different industry (See Appendix 1). Categorizing by region, 4 cases were collected from Northern America, 6 cases from Europe and UK.  Data  analysis  followed  the  constant  comparison method.  Initially  data  analysis  was  performed concomitantly  with  data  collection,  and  continued with an explicit coding stage and an analytical coding procedure stage [33].  In  the  explicit  coding  stage,  the  analysis  started by  comparing  and  coding  each  statement  extracted from the case materials into categories. This allowed categories  to  emerge  to  fit  in  an  existing  category [33].  Relevant  statements  were  labelled  and  either created  as  a  new  code  and  given  a  definition,  or assigned to the existing codes with memos indicating their relevance and potential properties. Through this process, the statement was broken down into units of meanings.  The  concept  as  a  basic  unit  of  analysis labels  phenomenon  representing  a  practice  of responsible  AI  [35].  After  the  explicit  coding  stage, the  data  were  and categorized  in  terms  of  their  properties,  which initiates the analytical coding stage. conceptualized,  defined During  the  analytical  coding  stage,  the  research team compared the properties and dimensions of the emergent  categories.  In  order  to  constantly  analyze and  compare  the  categories,  the  concept  map  was employed  to  visualize  the  classification  [35].  Four dimensions underlying responsible AI practices were identified.  They  are  described  in  detail  in  the following sections and visualized in Figure 1. 4. Practices of Responsible AI Responsible  AI  is  a  governance  framework  that uses  to  harness,  deploy,  evaluate,  and  monitor  AI machines  to  create  new  opportunities  for  better service  provision.  It  focuses  on  designing  and implementing  ethical,  transparent,  and  accountable AI  solutions  that  help  maintain  individual  trust  and minimize  privacy  invasion.  Responsible  AI  places human  (e.g.,  end-users)  at  the  center  and  meets stakeholder  expectations  and  applicable  regulations and  laws.  Prior  to  designing  and  implementing responsible  AI, organizations need to understand the practices that will help them drive ethics and trust of AI use. The four practices of responsible AI include: (1)  Data  governance;  (2)  Ethically  design  solutions; (3)  Human-centric  surveillance/risk  control;  and  (4) Training  and  Education.  These  practices  are  evident Figure 1. A concept map of responsible AI practices in  the  real-world  cases  of  responsible  AI.  These  are described in turn below. 4.1. Data Governance Governance of responsible AI focuses on building transparency, trust, and explainability.  It  that important Transparency.  the is organizational  use  of  AI  must  be  transparent  to  the stakeholders  by allowing them  fully understand how an  AI  application  processes  their  data  and  arrive  to specific  decisions  [36].  According  to  the  Direct Marketing  Association  (DMA)\u2019s  investigation,  80% of surveyed consumers would be very or moderately comfortable  with  sharing  personal  data  when  they know about how digital data is shared and effectively used  for  marketing  purposes  [37].  Capital  One  is making  the  criteria  system  of  credit  card  transparent by providing a computational decision with complete explanation to their customers when their credit card applications  are  accepted  or  denied  [38].  Likewise, Alder Hey  Children\u2019s Hospital, as one of the largest children\u2019s  hospitals  in  Europe,  has  developed  an  AI featured  digital  App  called  Alder  Play.  Alder  Play has  incorporated  the  cognitive  advances  in  order  to present the enjoyable and informative experiences for its  young  patients.  Young  patients  allow  to  active their  own  avatar  during  their  stay,  receive  awards  when completing treatments, and get access to further guidelines and contents accordingly [39]. Alder Play enables  healthcare  professionals  to  have  access  to medical records of patients who are eligible for NHS treatment. Patients and their families would be able to obtain  their  medical  records  online.  This  could largely  the  clinical transparency processes,  thereby  enhancing  the  quality  of  health services and strengthening the patient engagement. improve  in Trust building. Trusted AI is built through high-quality  data  and  consent  to  use  [12].  AI  with  high-quality  data  could  mitigate  biased  and  inaccurate results generated. To ensure the quality and reliability of  data,  where  the  data  sources  come  from,  the limitation of data, and data rules to sharpen data error detection  should  be  identified  when  developing  AI algorithms  and  systems.  For  example,  PwC  has employed H2O.ai to build a revolutionary bot named GL.ai,  which  uses  AI  algorithms  to  effective  track operational data and transactions and correct errors to and maintain interactions for their business customers.  accurate purchase  histories What makes AI workable is its access to personal information  [36].  However,  widespread  access  to personal  (e.g.,  consumer-generated content,  online  transactional  data,  and  browsing  and clicking  data)  has  brought  negative  impacts  to information to individual,  business,  and  society  [25],  [40].  The availability  of  consumer  data  gives  rise  to  serious concerns  where  consumers  suffer  from  privacy invasion,  fraud,  information  leakage,  and  identity theft,  and  on  the  other  hand,  companies  cannot the collect  consumer  data  effectively  due consumers\u2019 distrust. These trends have led to a focus on data protection and transparency of data use by the regulators  in  many  countries  such  as  General  Data Protection  Regulation  (GDPR)  formulated  by  the European  Union  and  Act  on  the  Protection  of Personal  in  Japan.  These regulations  aim  to  protect  all  individuals\u2019  rights regarding privacy and personal data and give control to  individuals  over  their  personal  data.  With  these regulations  came  is  crucial  for into  force, companies to institutionalize the practice of obtaining consent  statement  or  permission  from  users  and reduce  ambiguity  of  data  use  and  make  the  logic behind  effective clear automation communication with users [12]. Information  through (APPI)  it  the and  describe Explainability.  Providing  meaningful  and personalized explanations about the results generated by  AI  models  could  reduce  uncertainty  and  build trust  with  users  [12].  To  develop  explainable  AI, Supplier\u2019s  Declaration  of  Conformity  (SDoC) proposed by IBM  suggests that effective  AI systems should  be  able  to  interpret  algorithm  outputs  via examples  properly  testing methodology [41]. For example, PwC has released its Responsible  AI  Toolkit  to  guide  companies  to accountably  harness  the  power  of  AI  and  provide them  with  personalized  advisory  services.  Likewise, Alder  Hey  Children\u2019s  NHS  Foundation  Trust  in Liverpool, UK has driven the intelligent use of digital techniques  based  on  big  sets  of  patient  data.  Alder Hey\u2019s AI systems powered by IBM Watson cognitive analytics  enable  healthcare  professions  to  interact with  young  patients  and  deliver  them  with personalized  health  services,  thereby  improving  the quality and experience of care and securing the sound health  services  [39].  AI-enabled  personalized  health services  have  improved  patient  experiences  in  terms of  familiarization,  distraction  and  reward  [42]. Specifically,  before  patients  arrive,  360-degree  tours of  hospital  environments  and  introductive  videos  of blood  test  and  x-ray  check  are  available  for  them  to explore  the  hospital  conditions  and  familiarize  with potential  treatment  experiences.  Parents  could  speak to  a  virtual  assistant  called  Ask  Oli  to  inquire  about the  progress  of  their  children\u2019s  health  checks  and treatments.  Questions  are  assured  to  be  answered  in real  time.  Additionally,  Alder  Hey  offers  young patients  with  character-based  stickers  activated  by using augmented reality (AR).   4.2. Ethically Design solutions Ethical  concerns  should  be  minimized  in designing  AI  solutions  in  three  ways.  First,  design engineers  need  to  be  aware  of  possible  ethical challenges  such  as  artificial  stupidity,  racist  robots, data and cyber security when developing AI systems. To prevent  these  ethical  concerns,  AI  system  allows for  human  inspection  of  the  functionality  of  the algorithms and systems [7]. For example, Google has pointed  out  that  concerns  on  ethical,  environmental and societal challenges while applying AI technology need  to  be  addressed  across  all  sectors  of  society [43]. User-centered AI systems are designed based on Google\u2019s  concept  of  general  best  practices  for software  systems.  As  acting  a  leading  role  in  the development  of  AI,  Google  has  invested  in  AI to research  and  announced  guidance  principles manage its research  fields and product  development, thereby  influencing  its  business  decisions  in  a  more ethical  way  [43].  Assessment  of  responsible  AI applications  could  be  made  via  these  objectives, leading  to  the  obligation  for  Google  to  form  a \u201cresponsible  innovation  team\u201d  with  experts  from  a range  of  disciplines  to  initially  examine  its  ethical level,  and  select  a  council  of  senior  executives  to make decisions for more complicated issues [44][45]. In  addition,  an  external  advisory  group  is  organized with  Google\u2019s  AI solution developers from a variety of  disciplines  to  avoid  unethical  AI  practices  and complement its internal governance [44]. system Second,  a  should responsible  AI themselves  be  able  to  make  socially  significant decisions  by  a  set  of  ethical  algorithms  in  order  to reduce  the  risk  of  unethical  behaviors  [14].  Lessons could  be  learnt  from  a  ridesharing  platform,  for instance,  the  unethical  AI  algorithm  potentially creates unfairness on the distribution of drivers\u2019 task assignments  and  pricing  practices.  This  algorithm exists  like  a  \u201cblack  box\u201d  and  helps  its  drivers  evade local transport regulators. Third, a prerequisite for implementing responsible AI  successfully  is  to  develop  ethical  mindset  and culture  for  organizations  and  employees.  This  is critical  for  reducing  any  risks  when  applying  AI. H&M Group, for instance, has developed a checklist, along with 30 questions to guide all ongoing and new AI  projects  to  ensure  that  AI  applications  are  used results, with governance,  collaboration,  respecting privacy,  focused,  and  security.  Such  a  practice  help H&M  to  ensure  every  AI  solutions  they  develop  are subject  to  the  comprehensive  assessment  of  risks  in its use.   transparency,  beneficial reliability, fairness, 4.3. Training and education  a and  employees  with Building  training  programs  is  another  crucial responsible AI practices. Such programs are to equip managers  deeper understanding  of  ethical  use  of  AI  and  data.  IEEE\u2019s Initiative  for  Ethical  Considerations  in  Artificial Intelligence  Systems   is  a  program  designed  to promote  ethical  and  responsible  AI  and  ensure  AI architects  and  solutions  developers  are  educated  and trained to prioritize ethical considerations of AI [36].   This  program  suggests  that  organizations  should provide training courses for ethical use of AI in areas such  as  methods  to  guide  ethical  design,  and  safety and beneficence of artificial  general intelligence and artificial  superintelligence  to  those  employees  who will  play  a  critical  support  role  of  responsible  AI. Mentoring,  cross-functional  team-based  training  and self-study  are  also  beneficial  training  approaches  to help  employees  develop  the  ethical  AI  mindset  and culture. Google  has  provided  a  series  of  advanced technical  knowledge  online  for  people  to  master technical  skills.  One  suggested  path  is  related  to Machine  Learning  (ML)  techniques,  a  subset  of  AI which could be applied to the datasets generated from the  real  world.  To  be  specific,  Machine  Learning Crash  Course  (MLCC)  is  designed  by  Google engineers  with  the  help  from  university  computer science  faculties,  offering  resources  with  insights  of data  science  and  innovative  ML  approaches  for  the supplement  of  study  by  self-learning.  It  has  featured with  lessons  including  video  lectures,  actual  case studies  and  practical  exercises.  For  example,  a technical module on fairness in 11 language versions has been added to the MLCC by Google, in order to train its staff around the world and help them mitigate bias  [45].  Additionally,  material  rewards  from Kaggle  Machine  Learning  Competitions  could  be given  to  those  who  learn  new  skills  with  ML challenges.  Moreover,  in Technology  Practice\u201d  project  has  been  developed  at the  Markkula  Center  for  Applied  Ethics  at  Santa Clara University [45]. It offers assistance for Google users  to  identify  multifaceted  ethical  issues  during their  daily  work.  Besides,  Resource  Library  from Google  to  create individual pathway.  training  of  \u201cEthics to  be  accessed is  available Cloud AutoML has been introduced to design the own  model  by  using  Google\u2019s  techniques  such  as \u201clearning2learn\u201d  and  \u201ctransfer  learning\u201d  [46].  This   could  increase  the  productive  level  for  less-skilled users. The Google Cloud AI Solution provides either prepackaged solutions or personalized model to serve organizations\u2019  needs  across  industries.  Moreover,  it has  shared  experiences  to  improve  AI  practices, partnered  with  professionals  to  apply  projects  with positive  and  worked  with stakeholders to promote thoughtful leadership in this area  [43].  Therefore,  it  could  guarantee  a  long-term its development  of  AI implication.  technology  as  well  as societal  effects, In  addition,  PwC  has  published  the  articles  and white  papers  to  demonstrate  their  responsible  AI experiences  [47].  \u201cAI:  Sizing  the  prize\u201d  from  PwC aims  to  estimate  the  percentage  of  the  increase  in GDP to be contributed to AI in various regions [48]. From  a  recent  PwC  analysis  report  on  the  financial services  sector,  concerns  related  to  augmentation, automation  has  been  addressed,  and  corresponded advice on the way to adapt AI in the future has been provided. PwC advises exploring AI solutions within explanatory  and  operational  areas,  which  could  help using  budget  and  resources  in  a  more  ethical  and societal  way  [48].  In  addition,  PwC  has  worked  on leveraging  AI  fulfil  client  demands  and expectations,  thereby  sharing  its  own  experiences  to help  customers  to  employ  the  power  of  AI  in  the same  way  [49].  As  AI  cannot  learn  without  human intervention,  consequently,  it  is  vital  to  train  both intelligence machines and staff to acquire appropriate data  [50].  Efforts  from  staff  across  the  whole  PwC global  network  has  accelerated  the  PwC\u2019s  approach to the AI. It is proved that the advantages of aligning AI innovation with core strategic objectives outweigh operating initiatives in isolation [50]. to Another  example,  reported  by  Audi  AG,  is  that the \u201cBeyond AI Initiative\u201d is created to address social acceptance  barriers  of  autonomous  driving  and  the future  of  work  by  educating  development  engineers, scientists and other stakeholders.  4.4. Human-centric surveillance/risk control Successful responsible AI requires a series of risk control  mechanisms  at  the  design,  implementation, and  evaluation  stages.  Several  risks  should  be  taken into  consideration  when  developing  responsible  AI for  organizations  that  includes  security  risks  (cyber intrusion  risks,  privacy  risks,  and  open  source software risk), economic risks (e.g., job displacement risks), and performance risks (e.g., risk of errors and bias and risk of black box, and risk of explainability).  To  minimize  these  AI  risks,  the  first  step  is  to formulate  the  rules  of  risk  controls,  with  clearly focused  goals,  execution  procedures,  metrics,  and performance  measures. In other words, a strong data protocol  should  be  defined  that  provides  clear guidelines to proactively identify AI risks that enable organizations  to  harness  data  effectively  from  the time it is acquired, stored, analyzed, and finally used.  Second, organizations should review the data they gather  internally  and  externally  and  realize  their potential  risks.  AI  comes  from  self-learning  through human designed algorithms. It is imperative to ensure the creditability of data so that AI can learn from the right  patterns  and  act  according  to  their  input.  Once the  potential  risk  of  these  data  has  been  managed, managers  can  make  better  decisions,  thereby minimizing cost and complexity.  Finally,  a  responsible  AI  system  should  consider the economic risks such as job displacement, liability, and  reputation  risks.  It  is  widely  acknowledged  that future  trend  of  AI  will  utilize  AI  approaches  to augment and complement human cognitive skills, and focus  on  human-AI  machine  interaction  and collaboration to bring together the best of each [51].  5. Formulating Responsible AI Strategies Lessons learnt from our selected case studies, we suggest  the  following  five  strategies  might  provide useful  guideline  for  to  develop responsible AI initiative in their organizations. those  seeking 5.1.  Emergence  of  Chief  Responsible  AI Officers (CRaiO)  Firms increasingly expect that the deployment of AI is aligned with their goals and values of CSR.  AI not  only  enable  firms  to  explore  sharper  customer insights,  but  also  become  a  powerful  strategic resource to facilitate positive business reputation and brand  recognition  if  it  is  used  in  an  ethical  and responsible  manner.  However,  only  25%  of  around 250 surveyed companies  have considered the ethical implications of AI before investing in it according to the  PwC\u2019s  investigation  [52].  This  shows  that  the responsible AI practices in most cases are immature. CRaiO  roles  should  emerge  to  in  response  to  this need.  We  define  the  CRaiO  as  a  role  in  charge  of developing  a  responsible  AI  roadmap  and  policy  in conjunction  with  internal  and  external  stakeholders to  make  use  of  trusted  AI,  integrating  the  oeuvre  of responsible AI to the projects across functional units, and  cultivating  an  inclusive  responsible  AI  culture across  organizational  and  functional  boundaries. Creating  a  CRaiO  may  require  intensively  cross-functional collaborations and organizational changes. A careful assessment on organizational resources and taken.  Alternatively,  as capabilities  should  be suggested  by  EY  [53],  AI  ethics  multi-disciplinary   advisory  board  can  be  established  to  provide  advice and guidance to the Board of Directors.   5.2.  Balancing sustainability of AI use  economic  and  social the  reputation company\u2019s AI  for  sustainability  has  attracted  academic  and practical  attentions  in  recent  years,  particularly discussions  on  how  can  AI  techniques  be  applied  to find  a  balance  between  economic  and  social sustainable impact for businesses has been excited in diverse  disciplines.  When  applying  AI,  its  societal impact  on  well-being  of  humans  and  environment should  be  seriously  considered.  If  firms  develop  AI algorisms with controversial impact on human rights, privacy, and employment, it may lead to the potential loss  of  credibility  for  products  and  brands,  and hamper  the marketplaces.  Thus,  the  ultimate  goal  of  responsible AI is to strike a balance between satisfying customer needs  with  less  ethical  concerns  and  dilemmas,  and attaining  long-term  profitability  for  businesses  and services.  Ecological  modernization  theory  (EMT) argues  the  ecological  outcomes  could  be  maximized through  achieving  a  balance  between  economic growth  and  social  sustainability  [54].  In  this  sense, firms should develop their AI solutions by taking the co-creation of economic and social sustainability into consideration.  Specifically,  firms  need  to  establish policies  on  ethical  governance  considering  socially preferable  approaches,  address  ethical  issues  both  in the  initial  design  and  post-launch  stage  of  AI systems,  and  place  AI  ethics  as  part  of  the  CSR strategy.  in 5.3.  Transparent  and  customer-centric  data policy  require There is no strategy with AI without a good data quality  management.  However,  with  the  data protection regulations such as GDPR came into force, to  obtain  consent  statement  or firms permission from consumers if they  want to use their information.  These  regulations  have  been  a  double-edged sword for firms, potentially acting as a barrier the to  behavioural communications  and  other  promotions  plans  of marketers.  On  the  other  hand,  with  appropriate  data policy,  it  will  improve  consumers\u2019  confidence  in sharing the data with firms for AI use [56].  targeting,  personalisation  of Furthermore,  penalties  for  the  GDPR  non-compliance is about ranges from \u20ac10-20 million or 2-4% annual global turnover, which is a hefty fine and challenge  for  small  and  medium  retailers  [55]. Although the GDPR is an EU act, but it has a global acts  as  to communicate with EU citizens must comply with the international  marketers  that  plan regulations.  Thus,  persuading  customers  to  share information through transparent and customer-centric data  policy  may  turn  these  regulations  from  a  threat to  an  opportunity  and  may  improve  their  trust towards AI .",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "22abae67-5a6c-4467-bda1-d3ed65370354",
                    "text": "",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "38b78821-8796-479f-a162-2c8803f629d0",
                    "text": "without a good data quality  management.  However,  with  the  data protection regulations such as GDPR came into force, to  obtain  consent  statement  or firms permission from consumers if they  want to use their information.  These  regulations  have  been  a  double-edged sword for firms, potentially acting as a barrier the to  behavioural communications  and  other  promotions  plans  of marketers.  On  the  other  hand,  with  appropriate  data policy,  it  will  improve  consumers\u2019  confidence  in sharing the data with firms for AI use [56].  targeting,  personalisation  of Furthermore,  penalties  for  the  GDPR  non-compliance is about ranges from \u20ac10-20 million or 2-4% annual global turnover, which is a hefty fine and challenge  for  small  and  medium  retailers  [55]. Although the GDPR is an EU act, but it has a global acts  as  to communicate with EU citizens must comply with the international  marketers  that  plan regulations.  Thus,  persuading  customers  to  share information through transparent and customer-centric data  policy  may  turn  these  regulations  from  a  threat to  an  opportunity  and  may  improve  their  trust towards AI .  5.4.  Creating  socially  responsible  initiatives with AI   customers\u2019 Responsible  AI  is  not  just  about  designing  AI  to operate  ethically  and  responsibly,  what  do  matter  is how  AI  can  be  leveraged  to  advance  socially responsible initiatives [57]. For instance, Quantcast, a leading  AI  company  who  specializes  in  AI-driven marketing,  optimizes  advertising campaigns  real-time through  using  AI-driven insights.  Meanwhile,  they  rely  on  real-time  data  and machine  learning  capability  to  help  their  customers ensure  brand  safety  and  prevent  consumers  in  the markets  information dissemination.  H&M  utilizes  AI  to  ensure  customer centricity  (approaches  such  as  fitting  consumers\u2019 physical  dimensions  with  their  preferred  style  and incorporating  multiple  data  sources  for  dynamic analysis),  as  a  result  of  cutting  environmental  waste and cost caused by high purchase return rates. These socially  responsible  initiatives  with  AI  contribute  to increased trust and sustainability among consumers.  fraud from  fake and",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "128f9a40-7ab5-480a-b29b-f719e83b0822",
                    "text": "",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "41ed3567-fcb7-4d21-b127-abea44d1b063",
                    "text": "Carrot  (reward/incentive)  and  stick  (punishment) mechanism  has  been  widely  applied  to  regulate  IT usage  [58].  It  is  important  to  understand  what mechanisms  can  trigger  employees\u2019  ethical  AI behavior  or  impede  the  misuse  of  AI.  Floridi  et  al. [59]  have  designed  a  series  of  actionable  plans  to financially  incentivize  ethical  use  of  AI  at  the organizational  level.  First,  firms  should  encourage cross-disciplinary  and  debate  on cooperation legal  aspects  of  AI.  For technological,  social, example,  H&M  has  created  an  Ethical  AI  Debate Club  where  cross-functional  employees  and  their customers and AI researchers can meet for debates on ethical  concerns  and  dilemmas  arise  in  the  fashion industry.  Second,  developing  an  inclusive  triadic configuration  to  capture  the  complex  interactions among ethics, innovation, and policy in confluence, it will  help  firms  to  ensure  AI  has  ethics  as  a  core facilitating consideration  and  policy socially  positive  [59].  Moreover, innovation punishment plays a  key role in affecting employees\u2019 ethical  AI  behavior.  Firms  should  develop  a monitoring,  auditing  and  punishing  mechanism  to redress  for  a  wrong  caused  by  AI  usage  and  to moderately punish unethical AI behaviors. is  guided",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "9e43e05e-94ed-4fec-98c8-431137824c66",
                    "text": "to lead  inevitable  disruptive As being maturing rapidly, AI holds an incredibly power which has created new opportunities for social good.  However,  the  scalability  of  machine  learning might  impacts, consequently,  concerns  may  be  aroused  while misusing  AI. In practice,  only  few companies across industries  have  incorporated  AI  with  a  series  of in  a  manner  consistent  with  ethical practices considerations,  public organizational expectations and societal norms. Attention is urgently needed  for  research  to  formulate  responsible  AI strategies  that  will  enable  firms  to  move  forward  to leverage AI most efficiently and ethically. values, Although  our  study  identifies  responsible  AI practices  which  is  not  only  contributing  to  the disciplinary field of  AI and ethics,  but also provides practical  recommendations  for  practitioners,  it  is subject  to  the  limitation  of  data  source  but  at  the same  time  formulating  new  directions  for  future research  if  primary  data  can  be  collected.  First,  the adoption of responsible AI is still in its infancy. Case materials  used  in  this  study  mainly  came  from companies\u2019  and  consultants\u2019  reports.  The  absence  of academic  works  may  result  in  a  potential  bias,  as companies  usually  publicize  their  success  stories [60].  Further  validation  could  be  undertaken  by collecting  primary  data  from  consumers,  C-level executives,  AI  software  companies,  third  party organizations  and  policy  makers  to  fully  explore responsible  AI  practices  individual, organisational, industrial, and societal levels.  the at understanding Second,  as  we  found  trust  plays  a  vital  role  in implementing  AI,  consumers\u2019 cognitive  appraisals,  emotional  states,  and  behavior responses  toward  irresponsible  use  of  AI  enables practitioners  to  avoid  negative  consequences.  The different  scenario  of  irresponsible  use  of  AI  (e.g., ineffective  marketing  message,  identity  theft,  and invasion  of  privacy)  can  be  examined  through  the surveys and field experiments.",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "5b93a886-e4e5-4c35-a2ca-bd1d2ebd43c4",
                    "text": "",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                },
                {
                    "id": "a8e82441-78c9-422d-9846-f9d9b6a4726a",
                    "text": "). Categorizing by region, 4 cases were collected from Northern America, 6 cases from Europe and UK.  Data  analysis  followed  the  constant  comparison method.  Initially  data  analysis  was  performed concomitantly  with  data  collection,  and  continued with an explicit coding stage and an analytical coding procedure stage [33].  In  the  explicit  coding  stage,  the  analysis  started by  comparing  and  coding  each  statement  extracted from the case materials into categories. This allowed categories  to  emerge  to  fit  in  an  existing  category [33].  Relevant  statements  were  labelled  and  either created  as  a  new  code  and  given  a  definition,  or assigned to the existing codes with memos indicating their relevance and potential properties. Through this process, the statement was broken down into units of meanings.  The  concept  as  a  basic  unit  of  analysis labels  phenomenon  representing  a  practice  of responsible  AI  [35].  After  the  explicit  coding  stage, the  data  were  and categorized  in  terms  of  their  properties,  which initiates the analytical coding stage. conceptualized,  defined During  the  analytical  coding  stage,  the  research team compared the properties and dimensions of the emergent  categories.  In  order  to  constantly  analyze and  compare  the  categories,  the  concept  map  was employed  to  visualize  the  classification  [35].  Four dimensions underlying responsible AI practices were identified.  They  are  described  in  detail  in  the following sections and visualized in Figure 1. 4. Practices of Responsible AI Responsible  AI  is  a  governance  framework  that uses  to  harness,  deploy,  evaluate,  and  monitor  AI machines  to  create  new  opportunities  for  better service  provision.  It  focuses  on  designing  and implementing  ethical,  transparent,  and  accountable AI  solutions  that  help  maintain  individual  trust  and minimize  privacy  invasion.  Responsible  AI  places human  (e.g.,  end-users)  at  the  center  and  meets stakeholder  expectations  and  applicable  regulations and  laws.  Prior  to  designing  and  implementing responsible  AI, organizations need to understand the practices that will help them drive ethics and trust of AI use. The four practices of responsible AI include: (1)  Data  governance;  (2)  Ethically  design  solutions; (3)  Human-centric  surveillance/risk  control;  and  (4) Training  and  Education.  These  practices  are  evident Figure 1. A concept map of responsible AI practices in  the  real-world  cases  of  responsible  AI.  These  are described in turn below. 4.1. Data Governance Governance of responsible AI focuses on building transparency, trust, and explainability.  It  that important Transparency.  the is organizational  use  of  AI  must  be  transparent  to  the stakeholders  by allowing them  fully understand how an  AI  application  processes  their  data  and  arrive  to specific  decisions  [36].  According  to  the  Direct Marketing  Association  (DMA)\u2019s  investigation,  80% of surveyed consumers would be very or moderately comfortable  with  sharing  personal  data  when  they know about how digital data is shared and effectively used  for  marketing  purposes  [37].  Capital  One  is making  the  criteria  system  of  credit  card  transparent by providing a computational decision with complete explanation to their customers when their credit card applications  are  accepted  or  denied  [38].  Likewise, Alder Hey  Children\u2019s Hospital, as one of the largest children\u2019s  hospitals  in  Europe,  has  developed  an  AI featured  digital  App  called  Alder  Play.  Alder  Play has  incorporated  the  cognitive  advances  in  order  to present the enjoyable and informative experiences for its  young  patients.  Young  patients  allow  to  active their  own  avatar  during  their  stay,  receive  awards  when completing treatments, and get access to further guidelines and contents accordingly [39]. Alder Play enables  healthcare  professionals  to  have  access  to medical records of patients who are eligible for NHS treatment. Patients and their families would be able to obtain  their  medical  records  online.  This  could largely  the  clinical transparency processes,  thereby  enhancing  the  quality  of  health services and strengthening the patient engagement. improve  in Trust building. Trusted AI is built through high-quality  data  and  consent  to  use  [12].  AI  with  high-quality  data  could  mitigate  biased  and  inaccurate results generated. To ensure the quality and reliability of  data,  where  the  data  sources  come  from,  the limitation of data, and data rules to sharpen data error detection  should  be  identified  when  developing  AI algorithms  and  systems.  For  example,  PwC  has employed H2O.ai to build a revolutionary bot named GL.ai,  which  uses  AI  algorithms  to  effective  track operational data and transactions and correct errors to and maintain interactions for their business customers.  accurate purchase  histories What makes AI workable is its access to personal information  [36].  However,  widespread  access  to personal  (e.g.,  consumer-generated content,  online  transactional  data,  and  browsing  and clicking  data)  has  brought  negative  impacts  to information to individual,  business,  and  society  [25],  [40].  The availability  of  consumer  data  gives  rise  to  serious concerns  where  consumers  suffer  from  privacy invasion,  fraud,  information  leakage,  and  identity theft,  and  on  the  other  hand,  companies  cannot the collect  consumer  data  effectively  due consumers\u2019 distrust. These trends have led to a focus on data protection and transparency of data use by the regulators  in  many  countries  such  as  General  Data Protection  Regulation  (GDPR)  formulated  by  the European  Union  and  Act  on  the  Protection  of Personal  in  Japan.  These regulations  aim  to  protect  all  individuals\u2019  rights regarding privacy and personal data and give control to  individuals  over  their  personal  data.  With  these regulations  came  is  crucial  for into  force, companies to institutionalize the practice of obtaining consent  statement  or  permission  from  users  and reduce  ambiguity  of  data  use  and  make  the  logic behind  effective clear automation communication with users [12]. Information  through (APPI)  it  the and  describe Explainability.  Providing  meaningful  and personalized explanations about the results generated by  AI  models  could  reduce  uncertainty  and  build trust  with  users  [12].  To  develop  explainable  AI, Supplier\u2019s  Declaration  of  Conformity  (SDoC) proposed by IBM  suggests that effective  AI systems should  be  able  to  interpret  algorithm  outputs  via examples  properly  testing methodology [41]. For example, PwC has released its Responsible  AI  Toolkit  to  guide  companies  to accountably  harness  the  power  of  AI  and  provide them  with  personalized  advisory  services.  Likewise, Alder  Hey  Children\u2019s  NHS  Foundation  Trust  in Liverpool, UK has driven the intelligent use of digital techniques  based  on  big  sets  of  patient  data.  Alder Hey\u2019s AI systems powered by IBM Watson cognitive analytics  enable  healthcare  professions  to  interact with  young  patients  and  deliver  them  with personalized  health  services,  thereby  improving  the quality and experience of care and securing the sound health  services  [39].  AI-enabled  personalized  health services  have  improved  patient  experiences  in  terms of  familiarization,  distraction  and  reward  [42]. Specifically,  before  patients  arrive,  360-degree  tours of  hospital  environments  and  introductive  videos  of blood  test  and  x-ray  check  are  available  for  them  to explore  the  hospital  conditions  and  familiarize  with potential  treatment  experiences.  Parents  could  speak to  a  virtual  assistant  called  Ask  Oli  to  inquire  about the  progress  of  their  children\u2019s  health  checks  and treatments.  Questions  are  assured  to  be  answered  in real  time.  Additionally,  Alder  Hey  offers  young patients  with  character-based  stickers  activated  by using augmented reality (AR).   4.2. Ethically Design solutions Ethical  concerns  should  be  minimized  in designing  AI  solutions  in  three  ways.  First,  design engineers  need  to  be  aware  of  possible  ethical challenges  such  as  artificial  stupidity,  racist  robots, data and cyber security when developing AI systems. To prevent  these  ethical  concerns,  AI  system  allows for  human  inspection  of  the  functionality  of  the algorithms and systems [7]. For example, Google has pointed  out  that  concerns  on  ethical,  environmental and societal challenges while applying AI technology need  to  be  addressed  across  all  sectors  of  society [43]. User-centered AI systems are designed based on Google\u2019s  concept  of  general  best  practices  for software  systems.  As  acting  a  leading  role  in  the development  of  AI,  Google  has  invested  in  AI to research  and  announced  guidance  principles manage its research  fields and product  development, thereby  influencing  its  business  decisions  in  a  more ethical  way  [43].  Assessment  of  responsible  AI applications  could  be  made  via  these  objectives, leading  to  the  obligation  for  Google  to  form  a \u201cresponsible  innovation  team\u201d  with  experts  from  a range  of  disciplines  to  initially  examine  its  ethical level,  and  select  a  council  of  senior  executives  to make decisions for more complicated issues [44][45]. In  addition,  an  external  advisory  group  is  organized with  Google\u2019s  AI solution developers from a variety of  disciplines  to  avoid  unethical  AI  practices  and complement its internal governance [44]. system Second,  a  should responsible  AI themselves  be  able  to  make  socially  significant decisions  by  a  set  of  ethical  algorithms  in  order  to reduce  the  risk  of  unethical  behaviors  [14].  Lessons could  be  learnt  from  a  ridesharing  platform,  for instance,  the  unethical  AI  algorithm  potentially creates unfairness on the distribution of drivers\u2019 task assignments  and  pricing  practices.  This  algorithm exists  like  a  \u201cblack  box\u201d  and  helps  its  drivers  evade local transport regulators. Third, a prerequisite for implementing responsible AI  successfully  is  to  develop  ethical  mindset  and culture  for  organizations  and  employees.  This  is critical  for  reducing  any  risks  when  applying  AI. H&M Group, for instance, has developed a checklist, along with 30 questions to guide all ongoing and new AI  projects  to  ensure  that  AI  applications  are  used results, with governance,  collaboration,  respecting privacy,  focused,  and  security.  Such  a  practice  help H&M  to  ensure  every  AI  solutions  they  develop  are subject  to  the  comprehensive  assessment  of  risks  in its use.   transparency,  beneficial reliability, fairness, 4.3. Training and education  a and  employees  with Building  training  programs  is  another  crucial responsible AI practices. Such programs are to equip managers  deeper understanding  of  ethical  use  of  AI  and  data.  IEEE\u2019s Initiative  for  Ethical  Considerations  in  Artificial Intelligence  Systems   is  a  program  designed  to promote  ethical  and  responsible  AI  and  ensure  AI architects  and  solutions  developers  are  educated  and trained to prioritize ethical considerations of AI [36].   This  program  suggests  that  organizations  should provide training courses for ethical use of AI in areas such  as  methods  to  guide  ethical  design,  and  safety and beneficence of artificial  general intelligence and artificial  superintelligence  to  those  employees  who will  play  a  critical  support  role  of  responsible  AI. Mentoring,  cross-functional  team-based  training  and self-study  are  also  beneficial  training  approaches  to help  employees  develop  the  ethical  AI  mindset  and culture. Google  has  provided  a  series  of  advanced technical  knowledge  online  for  people  to  master technical  skills.  One  suggested  path  is  related  to Machine  Learning  (ML)  techniques,  a  subset  of  AI which could be applied to the datasets generated from the  real  world.  To  be  specific,  Machine  Learning Crash  Course  (MLCC)  is  designed  by  Google engineers  with  the  help  from  university  computer science  faculties,  offering  resources  with  insights  of data  science  and  innovative  ML  approaches  for  the supplement  of  study  by  self-learning.  It  has  featured with  lessons  including  video  lectures,  actual  case studies  and  practical  exercises.  For  example,  a technical module on fairness in 11 language versions has been added to the MLCC by Google, in order to train its staff around the world and help them mitigate bias  [45].  Additionally,  material  rewards  from Kaggle  Machine  Learning  Competitions  could  be given  to  those  who  learn  new  skills  with  ML challenges.  Moreover,  in Technology  Practice\u201d  project  has  been  developed  at the  Markkula  Center  for  Applied  Ethics  at  Santa Clara University [45]. It offers assistance for Google users  to  identify  multifaceted  ethical  issues  during their  daily  work.  Besides,  Resource  Library  from Google  to  create individual pathway.  training  of  \u201cEthics to  be  accessed is  available Cloud AutoML has been introduced to design the own  model  by  using  Google\u2019s  techniques  such  as \u201clearning2learn\u201d  and  \u201ctransfer  learning\u201d  [46].  This   could  increase  the  productive  level  for  less-skilled users. The Google Cloud AI Solution provides either prepackaged solutions or personalized model to serve organizations\u2019  needs  across  industries.  Moreover,  it has  shared  experiences  to  improve  AI  practices, partnered  with  professionals  to  apply  projects  with positive  and  worked  with stakeholders to promote thoughtful leadership in this area  [43].  Therefore,  it  could  guarantee  a  long-term its development  of  AI implication.  technology  as  well  as societal  effects, In  addition,  PwC  has  published  the  articles  and white  papers  to  demonstrate  their  responsible  AI experiences  [47].  \u201cAI:  Sizing  the  prize\u201d  from  PwC aims  to  estimate  the  percentage  of  the  increase  in GDP to be contributed to AI in various regions [48]. From  a  recent  PwC  analysis  report  on  the  financial services  sector,  concerns  related  to  augmentation, automation  has  been  addressed,  and  corresponded advice on the way to adapt AI in the future has been provided. PwC advises exploring AI solutions within explanatory  and  operational  areas,  which  could  help using  budget  and  resources  in  a  more  ethical  and societal  way  [48].  In  addition,  PwC  has  worked  on leveraging  AI  fulfil  client  demands  and expectations,  thereby  sharing  its  own  experiences  to help  customers  to  employ  the  power  of  AI  in  the same  way  [49].  As  AI  cannot  learn  without  human intervention,  consequently,  it  is  vital  to  train  both intelligence machines and staff to acquire appropriate data  [50].  Efforts  from  staff  across  the  whole  PwC global  network  has  accelerated  the  PwC\u2019s  approach to the AI. It is proved that the advantages of aligning AI innovation with core strategic objectives outweigh operating initiatives in isolation [50]. to Another  example,  reported  by  Audi  AG,  is  that the \u201cBeyond AI Initiative\u201d is created to address social acceptance  barriers  of  autonomous  driving  and  the future  of  work  by  educating  development  engineers, scientists and other stakeholders.  4.4. Human-centric surveillance/risk control Successful responsible AI requires a series of risk control  mechanisms  at  the  design,  implementation, and  evaluation  stages.  Several  risks  should  be  taken into  consideration  when  developing  responsible  AI for  organizations  that  includes  security  risks  (cyber intrusion  risks,  privacy  risks,  and  open  source software risk), economic risks (e.g., job displacement risks), and performance risks (e.g., risk of errors and bias and risk of black box, and risk of explainability).  To  minimize  these  AI  risks,  the  first  step  is  to formulate  the  rules  of  risk  controls,  with  clearly focused  goals,  execution  procedures,  metrics,  and performance  measures. In other words, a strong data protocol  should  be  defined  that  provides  clear guidelines to proactively identify AI risks that enable organizations  to  harness  data  effectively  from  the time it is acquired, stored, analyzed, and finally used.  Second, organizations should review the data they gather  internally  and  externally  and  realize  their potential  risks.  AI  comes  from  self-learning  through human designed algorithms. It is imperative to ensure the creditability of data so that AI can learn from the right  patterns  and  act  according  to  their  input.  Once the  potential  risk  of  these  data  has  been  managed, managers  can  make  better  decisions,  thereby minimizing cost and complexity.  Finally,  a  responsible  AI  system  should  consider the economic risks such as job displacement, liability, and  reputation  risks.  It  is  widely  acknowledged  that future  trend  of  AI  will  utilize  AI  approaches  to augment and complement human cognitive skills, and focus  on  human-AI  machine  interaction  and collaboration to bring together the best of each [51].  5. Formulating Responsible AI Strategies Lessons learnt from our selected case studies, we suggest  the  following  five  strategies  might  provide useful  guideline  for  to  develop responsible AI initiative in their organizations. those  seeking 5.1.  Emergence  of  Chief  Responsible  AI Officers (CRaiO)  Firms increasingly expect that the deployment of AI is aligned with their goals and values of CSR.  AI not  only  enable  firms  to  explore  sharper  customer insights,  but  also  become  a  powerful  strategic resource to facilitate positive business reputation and brand  recognition  if  it  is  used  in  an  ethical  and responsible  manner.  However,  only  25%  of  around 250 surveyed companies  have considered the ethical implications of AI before investing in it according to the  PwC\u2019s  investigation  [52].  This  shows  that  the responsible AI practices in most cases are immature. CRaiO  roles  should  emerge  to  in  response  to  this need.  We  define  the  CRaiO  as  a  role  in  charge  of developing  a  responsible  AI  roadmap  and  policy  in conjunction  with  internal  and  external  stakeholders to  make  use  of  trusted  AI,  integrating  the  oeuvre  of responsible AI to the projects across functional units, and  cultivating  an  inclusive  responsible  AI  culture across  organizational  and  functional  boundaries. Creating  a  CRaiO  may  require  intensively  cross-functional collaborations and organizational changes. A careful assessment on organizational resources and taken.  Alternatively,  as capabilities  should  be suggested  by  EY  [53],  AI  ethics  multi-disciplinary   advisory  board  can  be  established  to  provide  advice and guidance to the Board of Directors.   5.2.  Balancing sustainability of AI use  economic  and  social the  reputation company\u2019s AI  for  sustainability  has  attracted  academic  and practical  attentions  in  recent  years,  particularly discussions  on  how  can  AI  techniques  be  applied  to find  a  balance  between  economic  and  social sustainable impact for businesses has been excited in diverse  disciplines.  When  applying  AI,  its  societal impact  on  well-being  of  humans  and  environment should  be  seriously  considered.  If  firms  develop  AI algorisms with controversial impact on human rights, privacy, and employment, it may lead to the potential loss  of  credibility  for  products  and  brands,  and hamper  the marketplaces.  Thus,  the  ultimate  goal  of  responsible AI is to strike a balance between satisfying customer needs  with  less  ethical  concerns  and  dilemmas,  and attaining  long-term  profitability  for  businesses  and services.  Ecological  modernization  theory  (EMT) argues  the  ecological  outcomes  could  be  maximized through  achieving  a  balance  between  economic growth  and  social  sustainability  [54].  In  this  sense, firms should develop their AI solutions by taking the co-creation of economic and social sustainability into consideration.  Specifically,  firms  need  to  establish policies  on  ethical  governance  considering  socially preferable  approaches,  address  ethical  issues  both  in the  initial  design  and  post-launch  stage  of  AI systems,  and  place  AI  ethics  as  part  of  the  CSR strategy.  in 5.3.  Transparent  and  customer-centric  data policy  require There is no strategy with AI without a good data quality  management.  However,  with  the  data protection regulations such as GDPR came into force, to  obtain  consent  statement  or firms permission from consumers if they  want to use their information.  These  regulations  have  been  a  double-edged sword for firms, potentially acting as a barrier the to  behavioural communications  and  other  promotions  plans  of marketers.  On  the  other  hand,  with  appropriate  data policy,  it  will  improve  consumers\u2019  confidence  in sharing the data with firms for AI use [56].  targeting,  personalisation  of Furthermore,  penalties  for  the  GDPR  non-compliance is about ranges from \u20ac10-20 million or 2-4% annual global turnover, which is a hefty fine and challenge  for  small  and  medium  retailers  [55]. Although the GDPR is an EU act, but it has a global acts  as  to communicate with EU citizens must comply with the international  marketers  that  plan regulations.  Thus,  persuading  customers  to  share information through transparent and customer-centric data  policy  may  turn  these  regulations  from  a  threat to  an  opportunity  and  may  improve  their  trust towards AI .  5.4.  Creating  socially  responsible  initiatives with AI   customers\u2019 Responsible  AI  is  not  just  about  designing  AI  to operate  ethically  and  responsibly,  what  do  matter  is how  AI  can  be  leveraged  to  advance  socially responsible initiatives [57]. For instance, Quantcast, a leading  AI  company  who  specializes  in  AI-driven marketing,  optimizes  advertising campaigns  real-time through  using  AI-driven insights.  Meanwhile,  they  rely  on  real-time  data  and machine  learning  capability  to  help  their  customers ensure  brand  safety  and  prevent  consumers  in  the markets  information dissemination.  H&M  utilizes  AI  to  ensure  customer centricity  (approaches  such  as  fitting  consumers\u2019 physical  dimensions  with  their  preferred  style  and incorporating  multiple  data  sources  for  dynamic analysis),  as  a  result  of  cutting  environmental  waste and cost caused by high purchase return rates. These socially  responsible  initiatives  with  AI  contribute  to increased trust and sustainability among consumers.  fraud from  fake and 5.5.  Carrot  and  stick  mechanism  to  regulate AI usage Carrot  (reward/incentive)  and  stick  (punishment) mechanism  has  been  widely  applied  to  regulate  IT usage  [58].  It  is  important  to  understand  what mechanisms  can  trigger  employees\u2019  ethical  AI behavior  or  impede  the  misuse  of  AI.  Floridi  et  al. [59]  have  designed  a  series  of  actionable  plans  to financially  incentivize  ethical  use  of  AI  at  the organizational  level.  First,  firms  should  encourage cross-disciplinary  and  debate  on cooperation legal  aspects  of  AI.  For technological,  social, example,  H&M  has  created  an  Ethical  AI  Debate Club  where  cross-functional  employees  and  their customers and AI researchers can meet for debates on ethical  concerns  and  dilemmas  arise  in  the  fashion industry.  Second,  developing  an  inclusive  triadic configuration  to  capture  the  complex  interactions among ethics, innovation, and policy in confluence, it will  help  firms  to  ensure  AI  has  ethics  as  a  core facilitating consideration  and  policy socially  positive  [59].  Moreover, innovation punishment plays a  key role in affecting employees\u2019 ethical  AI  behavior.  Firms  should  develop  a monitoring,  auditing  and  punishing  mechanism  to redress  for  a  wrong  caused  by  AI  usage  and  to moderately punish unethical AI behaviors. is  guided  6. Conclusion to lead  inevitable  disruptive As being maturing rapidly, AI holds an incredibly power which has created new opportunities for social good.  However,  the  scalability  of  machine  learning might  impacts, consequently,  concerns  may  be  aroused  while misusing  AI. In practice,  only  few companies across industries  have  incorporated  AI  with  a  series  of in  a  manner  consistent  with  ethical practices considerations,  public organizational expectations and societal norms. Attention is urgently needed  for  research  to  formulate  responsible  AI strategies  that  will  enable  firms  to  move  forward  to leverage AI most efficiently and ethically. values, Although  our  study  identifies  responsible  AI practices  which  is  not  only  contributing  to  the disciplinary field of  AI and ethics,  but also provides practical  recommendations  for  practitioners,  it  is subject  to  the  limitation  of  data  source  but  at  the same  time  formulating  new  directions  for  future research  if  primary  data  can  be  collected.  First,  the adoption of responsible AI is still in its infancy. Case materials  used  in  this  study  mainly  came  from companies\u2019  and  consultants\u2019  reports.  The  absence  of academic  works  may  result  in  a  potential  bias,  as companies  usually  publicize  their  success  stories [60].  Further  validation  could  be  undertaken  by collecting  primary  data  from  consumers,  C-level executives,  AI  software  companies,  third  party organizations  and  policy  makers  to  fully  explore responsible  AI  practices  individual, organisational, industrial, and societal levels.  the at understanding Second,  as  we  found  trust  plays  a  vital  role  in implementing  AI,  consumers\u2019 cognitive  appraisals,  emotional  states,  and  behavior responses  toward  irresponsible  use  of  AI  enables practitioners  to  avoid  negative  consequences.  The different  scenario  of  irresponsible  use  of  AI  (e.g., ineffective  marketing  message,  identity  theft,  and invasion  of  privacy)  can  be  examined  through  the surveys and field experiments.  7. References Appendix 1",
                    "reference": "[1] Yixin Wang, Minyuan Xiong, and Hamed Olya. 2020. Toward an understanding of responsible artificial intelligence practices. In Proceedings of the 53rd Hawaii International Conference on System Sciences (HICSS-53). Retrieved from https://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf"
                }
            ]
        },
        {
            "paper_title": "The role of cooperation in responsible AI development",
            "authors": "A Askell, M Brundage, G Hadfield",
            "publication_info": "arXiv preprint arXiv:1907.04534 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/1907.04534.pdf?source=post_page---------------------------",
            "chunks": [
                {
                    "id": "4d39bb20-d7e6-4816-85c8-540b4e6d2bb4",
                    "text": "In this paper, we argue that competitive pressures could incentivize AI companiesto underinvest in ensuring their systems are safe, secure, and have a positive socialimpact. Ensuring that AI systems are developed responsibly may therefore requirepreventing and solving collective action problems between companies. We note thatthere are several key factors that improve the prospects for cooperation in collectiveaction problems. We use this to identify strategies to improve the prospects for indus-try cooperation on the responsible development of AI.IntroductionMachine learning (ML) is used to develop increasingly capable systems targeted at tasks like voicerecognition, fraud detection, and the automation of vehicles. These systems are sometimes referred to asnarrow arti\ufb01cial intelligence (AI) systems. Some companies are also using machine learning techniquesto try to develop more general systems that can learn effectively across a variety of domains rather thanin a single target domain. Although there is a great deal of uncertainty about the development path offuture AI systems\u2014whether they will remain specialized or grow increasingly general, for example\u2014many agree that if the current rate of progress in these domains continues then it is likely that advancedarti\ufb01cial intelligence systems will have an increasingly large impact on society.This paper focuses on the private development of AI systems that could have signi\ufb01cant expected so-cial or economic impact, and the incentives AI companies have to develop these systems responsibly.Responsible development involves ensuring that AI systems are safe, secure, and socially bene\ufb01cial.In most industries, private companies have incentives to invest in developing their products responsibly.These include market incentives, liability laws, and regulation. We argue that AI companies have thesame incentives to develop AI systems responsibly, although they appear to be weaker than they are inother industries. Competition between AI companies could decrease the incentives of each company todevelop responsibly by increasing their incentives to develop faster. As a result, if AI companies wouldprefer to develop AI systems with risk levels that are closer to what is socially optimal\u2014as we believemany do\u2014responsible AI development can be seen as a collective action problem.We identify \ufb01ve key factors that make it more likely that companies will be able to overcome this collec-tive action problem and cooperate\u2014develop AI responsibly with the understanding that others will dolikewise. These factors are: high trust between developers (High Trust), high shared gains from mutualcooperation (Shared Upside), limited exposure to potential losses in the event of unreciprocated cooper-ation (Low Exposure), limited gains from not reciprocating the cooperation of others (Low Advantage),and high shared losses from mutual defection (Shared Downside).Using these \ufb01ve factors, we identify four strategies that AI companies and other relevant parties coulduse to increase the prospects for cooperation around responsible AI development. These include correct-ing harmful misconceptions about AI development, collaborating on shared research and engineeringchallenges, opening up more aspects of AI development to appropriate oversight, and incentivizinggreater adherence to ethical and safety standards. This list is not intended to be exhaustive, but to showthat it is possible to take useful steps towards more responsible AI development.The paper is composed of three sections. In section 1, we outline responsible AI development and itsassociated costs and bene\ufb01ts. In section 2, we show that competitive pressures can generate incentivesfor AI companies to invest less in responsible development than they would in the absence of compe-tition, and outline the \ufb01ve factors that can help solve such collective action problems. In section 3, weoutline the strategies that can help companies realize the gains from cooperation. We close with somequestions for further research.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "11450705-18fd-47b6-a7c1-5e71e12491f1",
                    "text": "AI systems have the ability to harm or create value for the companies that develop them, the people thatuse them, and members of the public who are affected by their use. In order to have high expected valuefor users and society, AI systems must be safe\u2014they must reliably work as intended\u2014and secure\u2014they must have limited potential for misuse or subversion. AI systems should also not introduce whatZwetsloot and Dafoe (2019) call \u201cstructural risks\u201d, which involve shaping the broader environment insubtle but harmful ways. The greater the harm that can result from safety failures, misuse, or structuralrisks, the more important it is that the system is safe and bene\ufb01cial in a wide range of possible conditions(Dunn, 2003). This requires the responsible development of AI.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "6f9e63a5-fc34-462e-bf84-5c1cf074a370",
                    "text": "AI systems are increasingly used to accomplish a wide range of tasks, some of which are critical to users\u2019health and wellbeing. As the range of such tasks grows, the potential for accidents and misuse alsogrows, raising serious safety and security concerns (Amodei, Olah, et al., 2016; Brundage, Avin, et al.,2018). Harmful scenarios associated with insuf\ufb01ciently cautious AI development have already surfacedwith, for example, biases learned from large datasets distorting decisions in credit markets and thecriminal justice system, facial recognition technologies disrupting established expectations of privacyand autonomy, and auto-pilot functions in some automobiles causing new types of driving risk (whilereducing others). Longer term, larger scale scenarios include dangers such as inadvertent escalation ofmilitary con\ufb02ict involving autonomous weapon systems or widespread job displacement.Responsible AI development involves taking steps to ensure that AI systems have an acceptably low riskof harming their users or society and, ideally, to increase their likelihood of being socially bene\ufb01cial.This involves testing the safety and security of systems during development, evaluating the potentialsocial impact of the systems prior to release, being willing to abandon research projects that fail to meeta high bar of safety, and being willing to delay the release of a system until it has been established thatit does not pose a risk to consumers or the public. Responsible AI development comes in degrees butit will be useful to treat it as a binary concept for the purposes of this paper. We will say that an AIsystem has been developed responsibly if the risks of it causing harms are at levels most people wouldconsider tolerable, taking into account their severity, and that the amount of evidence grounding theserisk estimates would also be considered acceptable.Responsible AI development involves work on safety, security, and the structural risks associated withAI systems. Work on the safety of AI aims to mitigate accident risks (Amodei, Olah, et al., 2016) andensure that AI systems function as intended (Ortega, Maini, et al., 2018) and behave in ways that peoplewant (Irving et al., 2018). Work on the security of AI aims to prevent AI systems from being attacked,2co-opted, or misused by bad actors (Brundage, Avin, et al., 2018). Work evaluating the structural im-pact of AI aims to identify and mitigate both the immediate and long term structural risks that AI systemspose to society: risks that don\u2019t quite \ufb01t under narrow de\ufb01nitions of accident and misuse. These includejoblesness, military con\ufb02ict, and threats to political and social institutions.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "641262f4-3334-4533-9117-6dc51d6db13b",
                    "text": "It is likely that responsible development will come at some cost to companies, and this cost may not berecouped in the long-term via increased sales or the avoidance of litigation. In order to build AI systemsresponsibly, companies will likely need to invest resources into data collection and curation, systemtesting, research into the possible social impacts of their system, and, in some cases, technical researchto guarantee that the system is reliably safe. In general, the safer that a company wants a product to be,the more constraints there are on the kind of product the company can build and the more resources itwill need to invest in research and testing during and after its development.If the additional resources invested in ensuring that an AI system is safe and bene\ufb01cial could have beenput towards developing an AI system with fewer constraints more quickly, we should expect responsibleAI development to require more time and money than incautious AI development. This means thatresponsible development is particularly costly to companies if the value of being the \ufb01rst to developand deploy a given type of AI system is high (even if the \ufb01rst system developed and deployed is notdemonstrably safe and bene\ufb01cial).There are generally several advantages that are conferred on the \ufb01rst company to develop a given tech-nology (Lieberman and Montgomery, 1988). If innovations can be patented or kept secret, the companycan gain a larger share of the market by continuing to produce a superior product and by creating switch-ing costs for users. Being a \ufb01rst-mover also allows the company to acquire scarce resources ahead ofcompetitors. If hardware, data, or research talent become scarce, for example, then gaining access tothem early confers an advantage. And if late movers are not able to catch up quickly then \ufb01rst-moveradvantages will be greater. In the context of AI development, having a lead in the development ofa certain class of AI systems could confer a \ufb01rst mover advantage. This effect would be especiallypronounced in the case of discontinuous changes in AI capabilities , but such a discontinuity is notnecessary in order for a \ufb01rst mover advantage to occur.Responsible development may therefore be costly both in terms of immediate resources required, andin the potential loss of a \ufb01rst-mover advantage. Other potential costs of responsible AI developmentinclude performance costs and a loss of revenue from not building certain lucrative AI systems on thegrounds of safety, security, or impact evaluation. An example of a performance cost is imposing a limiton the speed that self-driving vehicles can travel in order to make them safer. An example of revenueloss is refusing to build a certain kind of facial recognition system because it may undermine basic civilliberties (Smith, 2018b).AI companies may not strongly value being the \ufb01rst to develop a particular AI system because \ufb01rst-mover advantages do not always exist. Indeed, there are often advantages to entering a market afterthe front-runner. These include being able to free-ride on the R&D of the front-runner, to act on moreinformation about the relevant market, to act under more regulatory certainty, and having more \ufb02ex-ible assets and structures that let a company respond more effectively to changes in the environment(Gilbert and Birnbaum-More, 1996). These can outweigh the advantages of being the \ufb01rst to enter thatsame market. And it has been argued that late mover advantages often do outweigh \ufb01rst mover advan-tages (Markides and Geroski, 2004; Querbes and Frenken, 2017). We therefore acknowledge that theassumption that there will be a \ufb01rst-mover advantage in AI development may not be true. If a \ufb01rst-moveradvantage in AI is weak or non-existent then companies are less likely to engage in a race to the bottomon safety since speed is of lower value. Instead of offering predictions, this paper should be thought ofas an analysis of more pessimistic scenarios that involve at least a moderate \ufb01rst mover advantage.3Much of the discussion of AI development races assumes that they have a de\ufb01nitive endpoint. Althoughsome have hypothesized that if AI progress is discontinuous or suf\ufb01ciently rapid then it could essen-tially have a de\ufb01nitive endpoint, the case for this remains speculative. It is therefore important tonote that AI development may take the form of a perpetual R&D race: a race to stay technologicallyahead of competitors rather than a race to reach some particular technological endpoint (Aoki, 1991;Breitmoser et al., 2010). If this is the case then AI companies would still have an incentive to speedup development in order to stay ahead of others, especially if the gap between companies was small.The present analysis is applicable to perpetual races in which there is at least a moderate \ufb01rst moveradvantage, several companies are competing to stay ahead, and leadership is not yet entrenched.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "4d839271-a659-418d-90fb-ecb934ef366a",
                    "text": "In the law and economics literature on product safety, it is generally accepted that market forces createincentives for companies to invest in making their products safe (Oi et al., 1973). Suppose that compa-nies have accurate information about how safe the products they are developing are and that consumershave access to accurate information about how safe a company\u2019s product is, either prior to releaseor by observing the harms caused by a product after it is released (Ben-Shahar, 1998; Chen and Hua,2017). If consumers have a preference for safer products and respond rationally to this preference,they will not buy products that are insuf\ufb01ciently safe, or will pay less for them than for safer alternatives(Polinsky and Shavell, 2009). Releasing unsafe products will also result in a costly loss of reputationfor companies (Daughety and Reinganum, 1995). Finally, releasing unsafe products could result inburdensome regulation of the industry or in litigation costs. Therefore companies that are concernedabout a suf\ufb01ciently long time-horizon involving repeated interaction with customers, regulators, andother stakeholders that incentivize safety should internalize the value of responsible development.Market forces alone may not always incentivize companies to invest the appropriate amount into en-suring their products are safe. If consumers cannot get access to information about the safety of aproduct\u2014how likely safety failures are or how costly they are\u2014then companies have an incentive tounder-invest in safety. And if companies have inaccurate information about the safety of the productsthey are developing, they will not invest in safety to the degree demanded by consumers. Finally, poorcorporate governance can result in suboptimal decisions about risk (Cai et al., 2010). Product liabilitylaw and safety regulation are intended to correct such market failures by providing consumers with infor-mation about products, incentivizing companies to invest more in safety, and compensating consumersthat are harmed by product safety failures (Hylton, 2012; Landes and Posner, 1985).We may expect companies to under-invest in safety if the costs to consumers don\u2019t result in commen-surate costs for the company; either via a reduction in revenue, reputation loss, \ufb01nes from regulators,or successful litigation by consumers. Safety failures can also affect those who do not consume theproduct, however. Consider a 2018 recall of over 8,000 Volkswagen vehicles potentially affectedby a brake caliper issue that could result in increased stopping distances or loss of vehicle control(Consumer Reports, 2018). A safety failure resulting from this could harm not only the vehicle\u2019s occu-pants but also pedestrians and other drivers. Harms that safety failures in\ufb02ict on non-consumers arenegative externalities, and bene\ufb01ts that safer products produce for non-consumers are positive external-ities. We should anticipate companies under-investing in reducing negative externalities and increasing4positive externalities relative to their social value, since the costs and bene\ufb01ts this produces for societydon\u2019t result in commensurate costs and bene\ufb01ts for the company (Dahlman, 1979).To give a concrete example, consider facial recognition technology. Microsoft have argued that thistechnology could be used in ways that many would consider harmful: to violate individuals\u2019 privacyor suppress their political speech, for example (Smith, 2018a,b). Even if companies would prefer tobuild facial recognition systems that cannot be misused, either to avoid causing harm or to avoid thereputation costs of this harm, the cost of developing safeguards may not outweigh their bene\ufb01ts ifcompanies cannot be held liable for these harms and there is no regulation preventing misuse. For thisreason, Microsoft has called for regulation that would require that companies invest in measures thatreduce the risks from facial recognition technology, and that could also mitigate potential misuse of thetechnology by commercial entities or by governments (Smith, 2018a).The discussion thus far treats companies as though they were motivated only by pro\ufb01t, i.e. they onlycare about things like reputation and product safety insofar as they are a means to make more pro\ufb01t oravoid losses. This view is common in the literature on corporate social responsibility (Campbell, 2007;Devinney, 2009) but it is clearly an abstraction. Companies are run by, invested in, and composed ofhumans that care about the impact their products will have on the world and on other people. Employeesat technology companies have already shown that they care a great deal about the social implications ofthe systems they are building (Minsberg, 2019).The things that motivate AI companies other than pro\ufb01ts, such as bene\ufb01ting people rather than harmingthem, will generally push even more in favor of responsible development: they will rarely push against it.Assuming that companies are motivated solely by pro\ufb01t therefore lets us analyze a kind of \u2018worst casescenario\u2019 for responsible development. We will therefore often treat companies as though they weredriven solely by pro\ufb01t, even though we do not \ufb01nd this plausible. It is important that the reader bearthis in mind, since treating companies as pro\ufb01t-driven entities can be self-ful\ufb01lling, and can thereforecontribute to the very problems we are attempting to solve.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "aeb38423-6d6c-4d70-b1f5-26e03b3a9106",
                    "text": "If markets are functioning well and companies and consumers have perfect information about the ex-pected harm of a product, companies should invest the socially optimal amount into product safety(Daughety et al., 2018). In real-world scenarios in which markets may not function perfectly and in-formation asymmetries exist, incentives for companies to invest suf\ufb01ciently in product safety typicallycome from three sources: market forces, liability law, and industry or government regulation. Thesethree sources of incentives may not provide strong enough incentives for AI companies to engage inresponsible AI development, however. We will brie\ufb02y survey some reasons for this.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "9ac1b0ee-81e4-4997-97cc-dd824be31e43",
                    "text": "Consumers of AI systems include individuals, private companies, and public institutions. Althoughdifferent consumers will have access to different levels of information about AI systems, informationabout the expected harm of AI systems is likely to be quite limited on average. As cutting-edge AIsystems become more complex, it will be dif\ufb01cult for consumers not involved in the development ofthose systems to get accurate information about how safe the systems are. Consumers cannot directlyevaluate the safety of aviation software, for example, and will face similar dif\ufb01culties when it comesto directly evaluating the safety of complex machine learning models. This is compounded by the factthat it is notoriously dif\ufb01cult to explain the decisions made by neural networks (Doshi-Velez and Kim,2017; Olah et al., 2018). If consumers cannot assess how risky a given AI system is, they cannot adjusttheir willingness to pay for it accordingly. They are also less able to identify and exert pressure on AIcompanies that are investing too little in safety (Anton et al., 2004).Consumers could get information about how safe an AI system is by tracking safety failures after itsrelease, but such a \u2018wait and see\u2019 strategy could leave both consumers and the public vulnerable toharmful safety failures. This is of particular concern if those safety failures could be irreversible orcatastrophic. And the probability of irreversible or catastrophic safety failures is likely to increase as AI5systems become more capable and general, since more advanced systems are more likely to be reliedupon across a wider range of domains and in domains where failures are more harmful.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "1b1de91e-ee7d-44e7-b2e0-e8dfb6d0eaeb",
                    "text": "Measuring the safety, security, and social impact of AI systems may turn out to be extremely dif\ufb01culteven for those who understand the technical details of the system. Neural networks are dif\ufb01cult tointerpret and as such, failures may be dif\ufb01cult to predict. If AI companies are over-con\ufb01dent that theirsystem is not risky, they may under-invest in important risk-reducing measures during development orrelease a system that causes unintended harm.If regulators cannot assess how risky a given AI system is, they may be overly stringent or overly liberalwhen using regulatory controls (Shavell, 1984). The ability to get accurate information about AI systemstherefore seems to be crucial for ex ante safety measures.Our current capacities to identify and measure the expected harms of particular AI systems are extremelylimited. We still do not fully understand the decisions made by complex machine learning models(Olah et al., 2018; Hohman et al., 2018) and the high-dimensionality of the inputs to AI systems makesit such that exhaustive enumeration of all possible inputs and outputs is typically infeasible. Theremay therefore be little consensus about whether a particular system is likely to be unsafe, unsecure, orsocially harmful at present. Given this, it is likely that additional capacity will need to be invested bycompanies or regulators or both in order to decrease these information asymmetries.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "2214fc86-aad2-4eb9-bbc1-820e305e8a93",
                    "text": "Harms caused by AI systems are likely to affect third parties. Biases in algorithmic pre-trial risk assess-ment are more likely to harm those accused of crimes than those that purchase the tools, those thatbene\ufb01t from AI automation may be quite distinct from the people who are displaced by automation,and a major AI disaster\u2014such as an AI system with a faulty reward function being integrated intoa critical system\u2014could affect a large portion of society that is distinct from the AI company and itsconsumers. AI also has the potential to be a general purpose technology\u2014a technology that radicallyaffects many sectors of the economy\u2014and if this is the case we should expect its impact to be systemic(Brynjolfsson et al., 2018; Cockburn et al., 2018).The harms from AI systems may also be dif\ufb01cult to internalize. For example, the social harms thatresult from an increased use of AI systems\u2014such as reduced trust in online sources\u2014could be complexand diffuse, and it may be dif\ufb01cult to hold any one company strictly liable for them. If the harm issuf\ufb01ciently large, it may also be too large for a company or insurer to cover all losses (see note 15).Finally, AI systems could create negative externalities for future generations that are not in a position topenalize companies or prevent them from occurring (Lazear, 1983). We should expect AI companies tounder-invest in measures that could prevent these kinds of negative externalities.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "f35f614f-9cfd-49ac-b335-f7f84ef36c40",
                    "text": "There is currently little in the way of AI-targeted regulation, including government regulation, industryself-regulation, international standards, and clarity on how existing laws will be applied to AI (see note613). Well-designed regulatory mechanisms can incentivize companies to invest appropriate resources insafety, security, and impact evaluation when market failures or coordination failures have weakened theother incentives to do so. Poorly-designed regulation can be harmful rather than helpful, however. Suchregulation can discourage innovation (Heyes, 2009) and even increase risks to the public (Latin, 1988).AI regulation seems particularly tricky to get right, as it would require a detailed understanding of thetechnology on the part of regulators. The fact that private AI companies can generally relocate easilyalso means that any attempt to regulate AI nationally could result in international regulatory competitionrather than an increase in responsible development. Regulation that is reactive and slow may also beinsuf\ufb01cient to deal with the challenges raised by AI systems. AI systems can operate much faster thanhumans, which can lead to what Johnson et al. (2013) call \u2018ultrafast extreme events\u2019 (UEEs) such as\ufb02ash crashes caused by algorithmic trading.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "4fc04993-15ea-4b2d-9622-70f71bbe85de",
                    "text": "Some have hypothesized that progress in AI development will be discontinuous (see note 7). On thisview, there are some types of AI systems\u2014typically advanced \u2018general\u2019 AI systems that are capableof learning effectively across a wide variety of domains\u2014that, if developed, would represent a suddenshift from everything that came before them, and could produce the equivalent of many years of priorprogress on some relevant metric. If AI progress is discontinuous then developing an AI system thatconstitutes a sudden leap forward could give a company a large advantage over others, since the nextbest system would be years behind it in terms of prior progress in the \ufb01eld. Consider the advantagethat a company today would gain if they managed to develop something over a decade ahead of currentsystems used for cyber offense and defense, for example.If progress in AI development is discontinuous then market forces and liability law may do little to en-courage safe development. The value of developing a system that gives a company a huge advantage\u2014that could be used to undermine competition or seize resources, for example\u2014would be largely divorcedfrom the process of getting market feedback. And a company can only be held liable for accidents ifthese accidents are not catastrophic and the existing legal framework can both keep up with the rapidityof technological progress and enforce judgments against companies. Therefore if AI progress is discon-tinuous, ex ante safety measures like industry self-regulation or international oversight may be moreeffective than ex post safety measures like market response and liability.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "281e06dd-9ed6-4a22-9b64-f23356e74459",
                    "text": "Incentives to develop safe products generally come from the market, liability laws, and regulation(Rubin, 2011), as well as factors that motivate AI companies beside pro\ufb01ts, such as a general desireto avoid doing harm. For AI companies, the pro\ufb01t motive to develop AI responsibly is likely to comefrom the additional revenue generated by AI systems that are more valuable to consumers, the avoidance7of reputational harm from safety failures, the avoidance of widespread harms caused by AI systems (seenote 20), and the avoidance of tort litigation or regulatory penalties.A key factor that can in\ufb02uence the cost-bene\ufb01t ratio of responsible AI development that we have notdiscussed, however, is the competitive environment in which the AI systems in question are beingdeveloped. In the next section we will explore the impact that competition between AI companies canhave on the incentives that each company has to invest or fail to invest in responsible development.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "b046bdb0-d5a8-4306-a375-6e564bd9dfc0",
                    "text": "We have argued that safer, more secure, and more socially valuable AI systems will tend to have a highermarket value, be less likely to cause costly accidents that the company is held liable for, and so on. Thismeans that if a company is guaranteed to be the \ufb01rst to develop a system of this type, we can expectthat they will invest resources to ensure that their system is safe, secure, and socially bene\ufb01cial to theextent that this is incentivized by regulators, liability law, and market forces. This means the more thatpositive and negative externalities of AI systems have been internalized via these mechanisms, the morethat companies can expect to invest in responsible development.In this section we will argue that, even with these incentives in place, competitive pressures can causeAI companies to invest less in responsible development than they otherwise would. Responsible AIdevelopment can therefore take the form of a collective action problem. We then identify and discuss \ufb01vekey factors that improve the prospects for cooperation between AI companies that could \ufb01nd themselvesin a collective action problem over responsible development.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "afee97f5-863d-4e6f-999a-58e97911a4bb",
                    "text": "To see how the competitive environment could affect investment in responsible development, supposethat several AI companies are working on a similar type of system. If there is a large degree of substi-tutability between the inputs of different aspects of development, we should not expect AI companiesto invest in responsible development beyond the point at which the expected marginal return is lowerthan the expected marginal return from investing in other areas of development. Suppose each companyplaces less value on coming second than on coming \ufb01rst, less value in coming third than in coming sec-ond, and so on. These companies will likely engage in a technological race: a competition to develop atechnology in which the largest reward goes to the \ufb01rst company (Grossman and Shapiro, 1985). Theresulting dynamics may be similar to those we would expect to see in patent races between \ufb01rms.There are various strategies companies could use in a \u201cwinner takes more\u201d race: they could try todevelop and maintain a strong technical lead or they could try to to maintain a close position behindthe technical leader, for example. For now, we will assume that the best strategy involves trying todevelop and maintain a strong technical lead throughout the race.Since speed is more valuable when racing against others, we should expect investment into responsibledevelopment to be lower when companies are racing against each other. Armstrong et al. (2016)point out that in an AI development race, responsible development could be prey to a \u201crace to thebottom\u201d dynamic. Consider what happens if one company decides to increase their development speedby decreasing their investment in safety, security, and impact evaluation. This increases their expectedranking in the race and decreases the expected ranking of others in the race. A decrease in expected8ranking gives competing AI companies an incentive to decrease their own investment in these areas inorder to maintain or increase their expected ranking in the race.We might ask why racing to the bottom on product safety is not ubiquitous in other industries in whichdecreasing time-to-market is valuable, such as in the pharmaceutical industry. The most plausibleexplanation of this difference is that the cost of safety failures has been internalized to a greater extent inmore established industries via external regulation, self-regulation, liability, and market forces. Thesemechanisms can jointly raise the \u201cbottom\u201d on product safety to a level that is generally consideredacceptable by regulators and consumers.In a race to the bottom on safety, competing AI companies could reduce their investment in responsibledevelopment to the point that winning the technology race\u2014successfully developing the system theyare racing to develop before others\u2014is barely of net positive value for the winner even after all the\ufb01rst-mover advantages, including positive reputational effects, the ability to capture resources like data,hardware and talent, and creating switching costs for consumers, have been taken into account.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "c64b4dac-c218-478b-942d-3ff717a71dc5",
                    "text": "The race to the bottom on safety described above is a collective action problem: a situation in whichall agents would be better off if they could all cooperate with one another, but each agent believes it isin their interest to defect rather than cooperate. As Heckathorn (1989, p. 78) states, \u201cthe inclinationsof individuals (that is, each actor\u2019s preferences regarding his or her own behavior) are in con\ufb02ict withregulatory interests (that is, each actor\u2019s preferences regarding the behavior of others). The collectiveaction problem arises when a group possesses a common interest, or faces a common fate.\u201dIn a race to the bottom on safety, it is in each company\u2019s interest to reduce their investment in responsibledevelopment in order to increase development speed. If all companies do this, however, there is a singleequilibrium: one in which much or all of the value that could have been be gained with coordinationis destroyed. If each company defects, they will have a similar position in the race to the one that theywould have had if they had all successfully coordinated, but they will be developing systems that aremore risky than the ones they would have developed if they had all managed to successfully coordinate.In other words, the situation in which they \ufb01nd themselves is strictly worse than the situation in whichcoordination was successful.Collective action problems between companies can have positive effects on consumers and the public.A price war is a collective action problem between companies with mostly positive effect on consumers,for example, as it results in lower prices. Antitrust law exists to maintain competition between com-panies that has a positive effect on consumers and to prevent collusion between companies that has anegative effect on consumers (e.g. price \ufb01xing). 9When there are negative effects from production that are not captured by the incentives facing producers(i.e. negative externalities), however, competition does not lead to the socially optimal outcome. If thisoutcome is also bad for the producers, it is a collective action problem for producers.A race to the bottom on safety falls into this category if it results in AI systems with safety levels belowwhat is socially optimal and below what AI companies would prefer. Pollution by companies is anotherexample of a collective action problem between companies that has a negative effect on the public(L\u00e9v\u00eaque, 1999).Before discussing strategies for cooperation such as self-regulation in more depth, however, it willbe useful to understand the incentives that AI companies have to abide by norms that involve mutualinvestment in responsible AI development. This will be the focus of the remainder of this section.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "4d10b346-883d-4e85-a141-a6c37a30ca0d",
                    "text": "In an AI development race, companies \u201ccooperate\u201d if they maintain some acceptable level of investmentin responsible development and they \u201cdefect\u201d if they fail to maintain this level of investment, therebyacting in their own interest (hypothetically) and against the collective interest. Encouraging companiesto cooperate should therefore not be confused with encouraging them to stop competing. Companiesagreeing not to compete across the investment in safety dimension does not imply that they will ceaseto compete across the R&D dimension. Competitive dynamics that contain cooperative elements aresometimes referred to as a \u201ccoopetition\u201d.If companies have incentives to prevent or mitigate collective action problems that have negative effectson consumers or the public then we should expect the companies themselves (and not just third partieslike government regulators) to take steps to solve them. And companies often do attempt to cooperateto prevent or solve collective action problems of this sort. One example of a mechanism used to this endis industry self-regulation. (Gunningham and Rees, 1997). Examples of self-regulation include Re-sponsible Care: a self-regulation program in the US chemicals industry (Gamper-Rabindran and Finger,2013), and the Institute of Nuclear Power Operations (INPO): an industry organization that conducts in-spections and facilitates the sharing of best practices in the nuclear power industry (Davis and Wolfram,2012; Hausman, 2014).In order to identify features that affect the degree to which it is in a company\u2019s interest to cooperate onresponsible development, it will be helpful to highlight features that increase incentives to cooperate incollective action problems generally. To do this, consider the payoff matrix of a cooperate-defect gamein which two agents (AI companies) can cooperate (develop responsibly) or defect (fail to developresponsibly). Here the \ufb01rst letter in each pair represent the expected payoff for Agent 1, and the secondletter in each pair represents the payoff for Agent 2.Agent 1 CooperateDefect Agent 2Cooperate Defectb , bd , da , ac , cTable 1: A Normal Form Cooperate-Defect Game10Let p be the probability that Agent 1 assigns to Agent 2 cooperating and let q be the probability thatAgent 2 assigns to Agent 1 cooperating. We assume it is rational for Agent 1 to cooperate if the expectedvalue of cooperation (the likelihood Agent 2 will cooperate times a plus the likelihood Agent 2 willdefect times b ) is greater than the expected value of defection (the likelihood Agent 2 will cooperatetimes c plus the likelihood Agent 2 will defect times d ). We assume the same is true of Agent 2.This lets us identify \ufb01ve highly interrelated factors that increase an agent\u2019s incentive to cooperate. Thesefactors are as follows, where expected values are relative to the agent\u2019s beliefs:(1) High Trust: being more con\ufb01dent that others will cooperate (p, q)(2) Shared Upside: assigning a higher expected value to mutual cooperation (a , a )(3) Low Exposure: assigning a lower expected cost to unreciprocated cooperation (b , c )(4) Low Advantage: assigning a lower expected value to not reciprocating cooperation (c , b )(5) Shared Downside assigning a lower expected value to mutual defection (d , d )The last four factors each refer to the expected value of an action conditional on the behavior of the otheragent, such as cooperating and having your cooperation reciprocated. Note that the expected value of anaction depends on how good the agent perceives the outcome to be and how likely the agent perceivesit to be. This means that an agent could be in a \u2018low exposure\u2019 scenario if she considers unreciprocatedcooperation to be not very valuable or not very likely or both. We can provide agents with evidenceabout the likelihood and value of each outcome by changing the world in some perceptible way, e.g.by offering a reward for responsible development, or by giving them evidence about the way the worldalready is, e.g. by correcting false beliefs.It is useful to separate the degree of trust (factor 1) from incentives (factors 2-5) in order to discuss itsrole in cooperation, but trust is not independent of incentives or vice versa. If one agent comes to trustan agent more, this increases the expected value of the outcomes that involve cooperation. The sameis true in reverse: if the expected value of the outcomes that involve cooperation increase, it is morelikely that the other agent will cooperate. In other words, increasing trust can increase incentives tocooperate, and increasing incentives to cooperate can increase trust between agents.This means that if a company can provide information about itself that increases the probability theother assigns to it cooperating, this will increase the degrees of trust between the companies and makeit more likely each company\u2019s trust threshold will be met. Two important facts follow from this. First,information that companies provide about their intentions and actions\u2014how transparent they are\u2014canplay an important role in whether other companies will cooperate with them. Second, trust is preyto virtuous and vicious cycles. If one company demonstrably increases its trust in another, the othercompany should increase its trust in return. But if one company demonstrably decreases its trust inanother, the other company should decrease its trust in return. .A real world race to the bottom on safety would unfold over many interactions. The factors identi\ufb01edhere also increase the prospect of cooperation in sequential games, however. And iterated collective11action problems are generally easier to solve than one-shot collective action problems because, in iter-ated collective action problems, players have an incentive (and opportunity) to cooperate early in thegame in order to establish trust and avoid retaliation. Using one-shot games to illustrate our points istherefore more likely to skew us towards undue pessimism about our ability to solve races to the bottomrather than undue optimism.One shortcoming of our analysis, however, is that it appeals to an overly simpli\ufb01ed conception of co-operation and defection. For example, we assume that the options available to agents can be dividedinto \u2018cooperation\u2019 and \u2018defection\u2019. In reality, cooperation will come in varying degrees\u2014companiescan invest different amounts in responsible development, for example\u2014and it would be better to talkabout the degree of cooperation that we can expect between agents. We also assume that companieswill make an intentional decision to coooperate or defect over time. In reality, companies could fail toforesee the consequences of investing very little into areas like safety, and may therefore defect withoutintending to. Third, we assume that both companies perfectly understand the actions and assertions ofthe other. In reality, it may not be clear whether a company is living up to an agreement to developAI responsibly. If agreements are not clear then there may not be a bright line between defection andnon-defection that companies can respond to (Chassang, 2010; Gibbons and Henderson, 2012). A morecomplete analysis of collective action problems in AI development should build a more realistic modelof what cooperating and defecting during AI development would look like.We have argued that in order to \u201csolve\u201d a collective action problem, we can try to transform it into asituation in which mutual cooperation is rational. If we can transform it into a situation in which agentshave lower minimum trust thresholds (generally determined by the payoff matrix) and greater trust ofeach other\u2014greater con\ufb01dence that if they cooperate, others will reciprocate (Kydd, 2007, p. 9)\u2014thenGiven this, we should expect \u201clowerwe should expect a higher degree of mutual cooperation.con\ufb02ict\u201d collective action problems\u2014problems in which agents have stronger incentives to cooperate\u2014to be easier to solve than \u201chigher con\ufb02ict\u201d collective action problems\u2014problems in which agents haveweaker incentives to cooperate.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "75f24344-eb3b-4093-beee-c31cf74268c6",
                    "text": "Whether an AI development race will result in a collective action problem and, if so, how bad it willbe are both open questions. But there are many features of an AI development race that affect boththe likelihood and severity of collective action problems. For example, having close frontrunners wouldlikely worsen a collective action problem\u2014would reduce the tractability of resolving it\u2014because thisincreases the expected value frontrunners will assign to not reciprocating the cooperation of others(low advantage) and therefore increases the probability they assign to not having their own cooperation12reciprocated (high trust). Similarly, a misaligned perception of the risks associated with different AIsystems could worsen a collective action problem if it causes less cautious companies to assign a lowercost to not reciprocating cooperation (low advantage), which could increase the probability that cautiouscompanies assign to having their cooperation unreciprocated by less cautious companies (high trust) andincrease the expected harm that cautious companies expect to arise from incautious companies gettingahead this way (low exposure).Features that affect the likelihood and severity of a collective action problem for responsible develop-ment can be used to decrease its likelihood and severity if they are are features that we can control.For example, fundamental distrust between companies is likely to worsen a collective action problembecause companies are less likely to expect that their cooperation will be reciprocated (high trust). Build-ing trust between AI companies can therefore decrease the severity of collective action problems. AnAI race development in which the expected value of winning is much greater than the expected valueof losing is also likely to have a worse collective action problem (low exposure and low advantage).If close frontrunners worsen collective action problems, AI companies may agree to take steps to avoidengaging in a harmful race to the bottom on safety. For example, citing concerns about race dynamics,OpenAI (2018) have stated that \u201cif a value-aligned, safety-conscious project comes close to buildingAGI before we do, we commit to stop competing with and start assisting this project.\u201dThe mechanisms to incentivize investment in product safety outlined in the previous section\u2014marketforces, regulation, and liability\u2014all operate to prevent collective action problems for product safety.Consumers often pay less for products that are unsafe (low advantage and shared downside) and morefor safe products (shared upside and low exposure). Government regulation either removes the optionto underinvest in safety or increases the cost of underinvesting in safety via sanctions and \ufb01nes (lowadvantage and shared downside). And the possibility of being held liable for harms caused by unsafeproducts decreases the expected value of underinvesting in safety to get ahead (low advantage and shareddownside).Market forces, regulation, and liability are all mechanisms operating outside of the AI industry thataffect the incentives that AI companies have to develop responsibly. But if responsible AI developmentis a collective action problem then each AI company expects to bene\ufb01t from being in a better equilibriumand therefore has an incentive to ensure that the AI industry itself collectively coordinates to maintainsome acceptable level of responsible development. Companies should be willing to invest in cooperativemechanisms to the degree that these mechanisms increase the likelihood that they will be able to capturethe cooperation surplus: the additional expected value that cooperation would generate for them.This means that industry-led mechanisms like greater self-regulation could also be developed to incen-tivize responsible AI development.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "5f965799-0e94-49dd-83cd-0228ba967aa3",
                    "text": "In this section we argued that responsible AI development may take the form of a collective actionproblem. We also identi\ufb01ed \ufb01ve factors that generally increase the likelihood of mutual cooperationand can help solve such collective action problems. In the next section we will translate this into moreconcrete suggestions for increasing cooperation on safety between AI companies.13",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "f55c7a68-ae0c-483a-833a-2482986d2c18",
                    "text": "In the previous section, we argued \ufb01ve factors make it more likely that AI companies will cooperateif they are faced with a collective action problem: (1) being more con\ufb01dent that others will cooperate,(2) assigning a higher expected value to mutual cooperation, (3) assigning a lower expected cost tounreciprocated cooperation, (4) assigning a lower expected value to not reciprocating cooperation, (5)assigning a lower expected value to mutual defection.These \ufb01ve factors give high-level direction regarding how to ensure that the fruits of cooperation in AIare realized. However, it is not always obvious what these \ufb01ve factors mean in the real world, so thereis a need for translating these factors into tangible policy strategies that various actors can implement inorder to improve cooperation prospects.It is impossible to prescribe such strategies fully in advance, because we lack information about thefuture which would be needed in order to make informed future decisions, and because a particularpolicy proposal could be effective if well-implemented but counterproductive if poorly executed. How-ever, while detailed, long-term policy prescriptions would be premature today, there are several coarse-grained strategies that seem robustly desirable even if some of the low-level details require research,dialogue, and passage of time before they can be clari\ufb01ed.We believe that the four strategies we identify in this section are robustly desirable in the sense that theyall have substantial bene\ufb01ts with respect to at least one of the factors above, and are unlikely to be veryharmful with respect to the others.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "c09c7d19-2e0d-4b14-9c16-171f74c6f14d",
                    "text": "As noted in prior sections, there are multiple competing conceptions of AI development. In cases wherepeople are demonstrably uninformed about key aspects of AI development, it is likely bene\ufb01cial tocorrect them, and more generally for stakeholders to make nuanced public statements consistent withthe spirit of AI development that involves cooperation on norms of responsible development.Some misconceptions that should be corrected in order to improve prospects for such cooperationinclude incorrect beliefs that safety and security risks can be safely ignored (Brundage, Avin, et al.,2018; Amodei, Olah, et al., 2016; Ortega, Maini, et al., 2018), an unwarranted focus on relative gainsand losses instead of absolute gains and losses (shared upside, low exposure, low advantage, shareddownside), and mistaken belief in interests being more misaligned than they are (low exposure and lowadvantage), In addition to correcting speci\ufb01c misconceptions, there is also likely value in proactivelyinforming people about the case for cooperating on responsible development generally.For example, recent years have seen substantial effort by researchers and activists to highlight the biasesbeing learned by deployed AI systems in critical societal domains such as criminal justice and in widelyused technological platforms such as recommender systems. This work has highlighted the risks ofincautious development to a large and growing swathe of the AI community. Similarly, concerns havebeen raised about both the bias, ef\ufb01cacy, and other properties of medical AI systems, as well as self-driving vehicles and other emerging technologies. Analyzing and communicating these sorts of risksis critical for generating interest in cooperation among a suf\ufb01ciently wide range of actors, as well as inidentifying appropriate norms around research, publication, and deployment given the safety risks andthe ways of mitigating them that have been identi\ufb01ed.In many cases, common knowledge that multiple parties share a concern or interest can be critical forthe initiation of cooperation, and a misconception that parties lack such a shared concern or interestcould be damaging to cooperation on issues like safety. Avoiding such misunderstanding may be partic-ularly important in the case of international cooperation on responsible AI development across distinctcountries with different languages and cultural frames of reference.Propagating accurate information about existing beliefs can also be valuable, as it allows multiple par-ties to stabilize their expectations. For example, the Asilomar AI Principles (Future of Life Institute,2017) commit the many signatories to arms race avoidance, and various statements of principles beforeand after this have similarly committed many actors to various (admittedly still abstract) cooperativestatements and actions. Expanding the breadth and depth of such dialogue, especially across culturaland language boundaries, will be critical in fostering understanding of the large gains from mutual14responsible development (shared upside) and the large losses from mutual irresponsible development(shared downside), and in establishing common knowledge that such understanding exists (high trust).It is possible to create positive spirals of trust, in which an increase in one party\u2019s trust causes the trustedparty to increase their trust in turn. We can also stumble into negative trust spirals, however, in which aloss of trust leads to further distrust between parties. It is therefore also important to avoid feeding intounnecessarily adversarial rhetoric about AI development, lest it become self-ful\ufb01lling (Kreps, 2019).",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "1edb775a-dba3-46d9-9e18-de1d8481f0a4",
                    "text": "On a range of possible research challenges\u2014from basic AI research to applied AI projects to AI safetyand security research\u2014it can be bene\ufb01cial for multiple parties to actively pool resources and ideas, pro-vided this can be done in a way that is procompetive and compliant with antitrust laws (FTC/DoJ, 2000),does not raise security concerns for the companies participating, and so on.Joint research can provide value for cooperation via useful technical insights (such as solutions to safetyproblems; low exposure and low advantage), stabilizing expectations regarding who is working on whatvia public information about joint investments as well as interpersonal dialogue (versus work beingmore shrouded in secrecy; high trust and shared downside), concretizing the joint upsides of AI (e.g.AI for good collaborations; shared upside), and facilitating more societally bene\ufb01cial publication anddeployment decisions by various actors (e.g. via collaborative analysis of the risks of speci\ufb01c systems;shared upside). Note that we refer speci\ufb01cally here to active and explicit research collaboration, ofwhich some already occurs, alongside a much greater amount of implicit collaboration on AI researchthat already exists due to the high degree of openness in the AI research community.Active and explicit research collaboration in AI, especially across institutional and national borders, iscurrently fairly limited in quantity, scale, and scope. This is for a range of reasons. In order to maintainlegitimate academic and industrial competition, researchers or their managers may be averse to publish-ing certain research outputs early or at all. And research ideas, results, datasets, and code can be hard todisentangle from proprietary product plans and technical infrastructure. Furthermore, safety or securityconsiderations can in some cases make the joint analysis of a particular system more challenging thanit would otherwise be (Radford, Wu, et al., 2019). There are also linguistic and logistical barriers tocollaborating across long distances and across different cultures and languages.While we acknowledge that such challenges exist, we advocate a more thorough mapping of possiblecollaborations across organizational and national borders, with particular attention to research and en-gineering challenges whose solutions might be of wide utility. Areas to consider might include jointresearch into the formal veri\ufb01cation of AI systems\u2019 capabilities and other aspects of AI safety andsecurity with wide application; various applied \u201cAI for good\u201d projects whose results might have wide-ranging and largely positive applications (e.g. in domains like sustainability and health); coordinatingon the use of particular benchmarks; joint creation and sharing of datasets that aid in safety research; andjoint development of countermeasures against global AI-related threats such as the misuse of syntheticmedia generation online.",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "396a5526-e048-4baf-81b5-b076de420f04",
                    "text": "Openness about one\u2019s beliefs, actions, and plans is critical to establishing trust generally. In the caseof AI development, those building and deploying AI systems need to provide information about theirdevelopment process so that users can make informed decisions. Likewise, governments need to be ableto appropriately oversee safety-critical AI systems, and (in the absence of relevant regulation) companiesneed to be able to provide information to one another that shows they are following appropriate norms.The general appeal of openness for cooperation-related reasons does not imply that all aspects of AIdevelopment should always be open, and as AI systems become more capable, it will be increasingly15important to decide responsibly what should and shouldn\u2019t be made open (Brundage, Avin, et al., 2018;Bostrom, 2017a; Krakovna, 2016). Full transparency is problematic as an ideal to strive for, in thatit is neither necessary nor suf\ufb01cient for achieving accountability in all cases (Desai and Kroll, 2017;Ananny and Crawford, 2018). Further, some information about AI development cannot or should notbe shared for reasons of safety, security, ethics, or law. For example, AI developers might legitimatelybe wary of releasing code that is intimately tied to proprietary infrastructure, and should certainly bewary of releasing private data as well as AI systems that are easily amenable to misuse.Given that full openness is rarely called for, but that some openness is required for building trust, thereis a need for continuing effort to implement existing modes of trust-building in AI, as well as to dis-cover new ones. Different mechanisms for achieving openness regarding how AI systems are developedand operated include, e.g., publicizing decision-making principles and processes, explaining publica-tion/release decisions, sharing accessible information about how particular AI systems and broad classesof AI systems work, allowing external visitors to the lab, and opening up individual AI systems to de-tailed scrutiny (e.g. via bug bounties or open sourcing).Such openness is critical in allowing reputation to play its stabilizing role in cooperation. Indeed, someactors have explicitly pointed to the challenges of monitoring the development and use of lethal au-tonomous weapons as as a reason not to agree to strict rules, suggesting that the inability to track others\u2019behavior reliably could be a bottleneck on some forms of mutually bene\ufb01cial cooperation (e.g. jointrestraints on weapons development). In cases such as this, a richer set of tools for opening up actors tocritical scrutiny and feedback (while managing the associated risks) would be useful, and we encouragecontinued exploration of approaches such as those mentioned above as well as others in order to widenthe range of cooperative actions available to AI developers.In combination, the appropriate application of transparency mechanisms such as these should reduce theseverity of concerns about others behaving irresponsibly (low exposure), reduce the temptation to defectin partially competitive situations (low advantage), and increase con\ufb01dence that others\u2019 statements abouttheir behavior are accurate (high trust). Openness is a particularly powerful strategy, and applicable to awider range of cooperation problems, if it can be gradually ratcheted up in an iterative fashion betweenparties, as opposed to happening all at once. This gradual approach can reduce the temptation to defectat any particular stage (low advantage) and increase con\ufb01dence in others cooperating (shared downside).",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "fe4f0ed7-e1cf-4276-aeb2-fc07073dc48e",
                    "text": "Cooperative actors might want to introduce additional incentives (reward and/or punishment) related toresponsible AI development beyond those that exist today, or would exist by default in the future. E.g.such actors might strongly value compliance with certain norms intrinsically, and prefer that those whocomply with appropriate norms be rewarded; or one might want to deliberately bring about an incentivefor oneself to act in a certain way, as a commitment mechanism; one might also want to use incentivesas a complement to other governance tools such as monitoring of behavior and direct regulation; andone might want to generally in\ufb02uence the incentives of many actors in a particular direction, and supportpolicies that bring this about.There are several categories of incentives that one might want to consider in this context. Creatingincentives for key actors to act cooperatively, if done effectively, would help with all \ufb01ve factors simul-taneously. Potential incentives include:\u2022 Social incentives (e.g. valorizing or criticizing certain behaviors related to AI development)can in\ufb02uence different companies\u2019 perceptions of risks and opportunities\u2022 Economic incentives (induced by governments, philanthropists, industry, or consumer behav-ior) can increase the share of high-value AI systems in particular markets or more generally,and increase attention to particular norms\u2022 Legal incentives (i.e. proscribing certain forms of AI development with \ufb01nancial or greaterpenalties) could sharply reduce temptation by some actors to defect in certain ways.\u2022 Domain-speci\ufb01c incentives of particular relevance to AI (e.g. early access to the latest genera-tion of computing power) could be used to encourage certain forms of behavior.16As argued in each case above, these strategies are robustly desirable from the perspective of enablingcooperation, but our articulation of them leaves many questions unanswered. In particular, sharpeningthese recommendations and adapting them over time will require technical and social scienti\ufb01c research,creative institutional design, and bold policy experimentation, e.g. via regulatory markets as discussedin Had\ufb01eld and Clark (2019).",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                },
                {
                    "id": "7dd4b386-94a2-42c9-9b63-a4fb5235f9ee",
                    "text": "In this paper we have argued that competition between AI companies could create a collective actionproblem for responsible AI development. We have identi\ufb01ed \ufb01ve key factors that make it more likelythat companies will cooperate on responsible development: high trust, shared upside, low exposure, lowadvantage, and shared downside. We have shown that these \ufb01ve factors can help us to identify strategiesto help AI companies develop responsibly and thereby realize the gains from cooperation. This also hasimportant positive externalities for consumers and the general public.If our analysis is on the right track then it is best thought of as the beginning of a program of research,rather than the last word on the subject. Much work needs to be done to identify whether collectiveaction problems for responsible AI development will occur if we vary who is developing AI, how manyentities are developing AI, what systems they are developing, and so on. More work must also be doneto identify and evaluate strategies that can prevent or mitigate these kinds of collective action problemsacross a wide range of possible scenarios.The possible future research directions on this issue are broad and we do not aim to provide a compre-hensive list of them here, but examples of potentially fruitful research questions include:1. How might the competitive dynamics of industry development of AI differ from government-led or government-supported AI development?2. What is the proper role of legal institutions, governments, and standardization bodies in re-solving collective action problems between companies, particularly if those collective actionproblems can arise between companies internationally?3. What further strategies can be discovered or constructed to help prevent collective action prob-lems for responsible AI development from forming, and to help solve such problems if they doarise? What lessons can we draw from history or from contemporary industries?4. How might competitive dynamics be affected by particular technical developments, or expec-tations of such developments?As we noted at the outset, there is substantial uncertainty about the nature and pace of developments inAI. If the impact of AI systems on society is likely to increase, however, then greater attention must bepaid to ensuring that the systems being developed and released are safe, secure, and socially bene\ufb01cial.In this paper we argued that existing incentives to develop AI responsibly may be weaker than is ideal,and that this may be compounded by competitive pressure between companies, leading to a collectiveaction problem on the responsible development of AI.That such collective action problems will arise or that they will be maintained if they do arise is far froma foregone conclusion, however. Finding ways of preventing and solving these problems may requirenew ways of building trust in novel technological contexts, and in some cases to assume some risk inthe expectation that others will reciprocate in turn. While intellectually and politically challenging, wethink such efforts are integral to realizing the positive-sum potential of AI.AcknowledgmentsWe are grateful to Michael Page, Jack Clark, Larissa Schiavo, Carl Shulman, Luke Muehlhauser, Ge-offrey Irving, Sarah Kreps, Paul Scharre, Michael Horowitz, Robert Trager, Tamay Besiroglu, HelenToner, Cullen O\u2019Keefe, Rebecca Crootof, Ben Gar\ufb01nkel, Adam Gleave, Jasmine Wang, and TobyShevlane for valuable feedback on earlier versions of this paper.17",
                    "reference": "[1] Amanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The role of cooperation in responsible AI development. arXiv:1907.04534. Retrieved from https://arxiv.org/pdf/1907.04534.pdf"
                }
            ]
        },
        {
            "paper_title": "Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices",
            "authors": "B Rakova, J Yang, H Cramer\u2026",
            "publication_info": "Proceedings of the ACM on \u2026 - dl.acm.org",
            "paper_url": "https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew",
            "chunks": [
                {
                    "id": "b2886c9f-e1f5-4780-b310-7d045c57a462",
                    "text": "While the academic discussion of algorithmic bias has an over 20-year long history [12], we havenow reached a transitional phase in which this debate has taken a practical turn. The growingawareness of algorithmic bias and the need to responsibly build and deploy Arti\ufb01cial Intelligence(AI) have led increasing numbers of practitioners to focus their work and careers on translatingthese calls to action within their domains [13, 20]. New AI and machine learning (ML) responsibil-ity or fairness roles and teams are being announced, product and API interventions are being pre-sented, and the \ufb01rst public successes \u2014and lessons learned \u2014are being disseminated [3]. However,practitioners still face considerable challenges in attempting to turn theoretical understanding ofpotential inequities into concrete action [13, 18]. 7Gaps exist between what academic research prioritizes, and what practitioners need. The latterincludes developing organizational tactics and stakeholder management [8, 13] rather than techni-cal methods alone. Beyond the need for domain-speci\ufb01c translation, methods, and technical tools,responsible AI initiatives also require operationalization within \u2014or around \u2014existing corporatestructures and organizational change. Industry professionals, who are increasingly tasked withdeveloping accountable and responsible AI processes, need to grapple with inherent dualities intheir role [22] as both agents for change based on their own values and/or their o\ufb03cial role, butalso as workers with careers in an organization with potentially misaligned incentives that maynot reward or welcome change [20]. Most commonly, practitioners have to navigate the interplayof their organizational structures and algorithmic responsibility e\ufb00orts with relatively little guid-ance. As Orlikowski points out, whether designing, appropriating, modifying, or even resistingtechnology, human agents are in\ufb02uenced by the properties of their organizational context [27].This also means that some organizations can be di\ufb00erentially successful at implementing organi-zational changes. Individuals\u2019 strategies must adapt to the organizational context and follow whatis seen as successful and e\ufb00ective behavior within that setting. Myerson for example describesthe concept of \"tempered radicals.\" These are employees who slowly but surely create corporatechange by pushing organizations through persistent small steps. Advocating for socially respon-sible business practices became part of tempered radicals\u2019 role over time. These employees createboth individual and collective action, relying on their own perceived legitimacy, in\ufb02uence andsupport built within their organizational context [23].Interestingly, the tension between academic research and industry practice is visible in researchcommunities such as FAccT, AIES, and CSCW, where people answering calls to action with practi-cal methods are sometimes met with explicit discomfort or disapproval from practitioners workingwithin large corporate contexts. Vice versa, practitioners a\ufb00ecting concrete change in practice mayhave achieved such results in ways that may not \ufb01t external research community expectations ornorms. Within the discourse on unintended consequences of ML-driven system, we have seen bothsuccesses and very public failures \u2014even within the same corporation [3] \u2014making it imperativeto understand such dynamics.This paper builds on the prior literature in both organizational change and algorithmic respon-sibility in practice to better understand how these still relatively early e\ufb00orts are taking shapewithin organizations. We know that attention to the potential negative impacts of machine learn-ing is growing within organizations, but how to leverage this growing attention to e\ufb00ectively drivechange in the AI industry remains an open question. To this end, we present a study involving 26semi-structured interviews with professionals in roles that involve concrete projects related toinvestigating responsible AI concerns or \"fair-ML\" (fairness-aware machine learning [31]) in prac-tice. We intend this to refer not only to fairness-related projects but also more broadly projectsrelated to the work on responsible AI and accountability of ML products and services given thehigh degree of overlap in goals, research, and people working on these topics.Using the data from the semi-structured qualitative interviews to compare across organizations,we describe prevalent, emergent, and aspirational future states of organizational structure andpractices in the responsible AI \ufb01eld, based on how often respondents identi\ufb01ed the practice dur-ing the interview and whether the practice is currently existing or is a desired future change. Weinvestigate practitioners\u2019 perceptions of their own role, the role of the organizational structuresin their context, and how those structures interact with adopting responsible AI practices. Basedon those answers, we identify four major questions that organizations must now adapt to answeras responsible AI initiatives scale. Furthermore, we describe how respondents perceived transi-tions occurring within their current contexts, focusing on organizational barriers and enablersfor change. Finally, we present the outcome of a workshop where attendees re\ufb02ected upon earlyinsights of this study through a structured design activity.The main contribution of our work is the qualitative analysis of semi-structured interviewsabout the responsible AI work practices of practitioners in industry. We found that most commonly,practitioners have to grapple with lack of accountability, ill-informed performance trade-o\ufb00s andmisalignment of incentives within decision-making structures that are only reactive to externalpressure. Emerging practices that are not yet widespread include the use of organization-levelframeworks and metrics, structural support, and proactive evaluation and mitigation of issues asthey arise. For the future, interviewees aspired to have organizations invest in anticipating andavoiding harms from their products, rede\ufb01ne results to include societal impact, integrate respon-sible AI practices throughout all parts of the organization, and align decision-making at all levelswith an organization\u2019s mission and values. Preliminary \ufb01ndings were shared at an interactiveworkshop during a large machine learning conference, which yielded organizational level recom-mendations to (1) create veto ability across levels, (2) coordinate internal and external pressures,(3) build robust communication channels between and within levels of an organization, and (4)design initiatives that account for the interdependent nature of the responsible AI work practiceswe have heretofore discussed.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "91e73f0c-9c58-4d20-b265-dbd2764d65cc",
                    "text": "An almost overwhelming collection of principles and guidelines have been published to addressthe ethics and potential negative impact of machine learning. Mittelstadt et al. [26] discuss oversixty sets of ethical guidelines, Zeng et al. [34] provide a taxonomy of 74 sets of principles, whileJobin et al. \ufb01nd 84 di\ufb00erent sets of principles [15]. Even if there is relative, high-level agreementbetween most of these abstract guidelines [15, 34], how they are translated into practice in eachcontext remains very unclear [26]. Insight is available from how companies changed their practicesin domains such as privacy and compliance in response to legislative directives [1]. The activedebate on how requirements in the EU\u2019s GDPR are to be interpreted [17, 21], however, illustrate thechallenges of turning yet nascent external guidance into concrete requirements. Kra\ufb00t et al. [18]point out that even between experts, there is a disconnect between policymakers and researchers\u2019de\ufb01nitions of such foundational terms as \u2018AI\u2019. This makes the application of abstract guidelineseven more challenging and raises the concern that focus may be put on future, similarly abstracttechnologies rather than current, already pressing problems.The diverse breadth of application domains for machine learning suggests that requirements forapplying guidelines in practice should be steered by the speci\ufb01c elements of the technologies used,speci\ufb01c usage contexts, and relevant local norms [26]. Practitioners encounter a host of challengeswhen trying to perform such work in practice [13]. Organizing and getting stakeholders on boardare necessary to be able to drive change [8]. This includes dealing with imperfection, and realizingthat tensions and dilemmas may occur when \"doing the right thing\" does not have an obvious andwidely agreed upon answer [7, 11]. It can be hard to foresee all potential consequences of systemswhile building them, and it can be equally di\ufb03cult to identify how to overcome unwanted side-e\ufb00ects, or even why they occur technically [29]. A fundamental challenge is that such assessmentshould not simply be about technical, statistical disparities, but rather active engagement to over-come the lack of guidance decision-makers have on what constitute \"just\" outcomes in non-idealpractice [11]. Additional challenges include organizational pressures for growth, common softwaredevelopment approaches such as agile working that focus on rapid releases of minimal viable prod-ucts, and incentives that motivate a focus on revenue within corporate environments [3, 13, 20].Taking inspiration from other industries where auditing processes are standard practice still meansthat auditing procedures have to be adjusted to product and organizational contexts, and requirede\ufb01ning the goal of the audit in context [29]. This means that wider organizational change isnecessary to translate calls to action into actual process and decision-making.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "1f74ebf8-ffdc-40d4-bcab-1bdee84d8682",
                    "text": "Current challenges faced by responsible AI e\ufb00orts can be compared to a wide selection of related\ufb01ndings in domains such as legal compliance [32] where questions arise regarding whether com-pliance processes actually lead to more ethical behavior [19], diversity and inclusion in corporateenvironments [2, 16], and corporate privacy practices [1]. All of these domains appear to havegone through a process that is mirrored in current algorithmic responsibility discussions: publica-tion of high-level principles and values by a variety of actors, the creation of dedicated roles withinorganizations, and urgent questions about overcoming challenges and achieving \"actual\" results inpractice and how to avoid investing in processes that are costly but do not deliver beyond cosmeticimpact.As Weaver et al. pointed out in 1999 [33], in an analysis of the Fortune 1000 ethics practices,success relies not only on centralized principles, but also their di\ufb00usion into managerial practicesin the wider organization. Interestingly, while external e\ufb00orts can e\ufb00ectively put reputational andlegislative pressure on companies, internal processes and audits are just as important, and theyall interact. Internally, this is apparent in the process of legitimization of the work on \u2018temperedradicals\u2019 in Myerson\u2019s work [23] as described in the introduction, and these radicals\u2019 internal jour-ney. External forces can help in more or less productive ways in that process. As discussed byBamberger and Mulligan [1], for corporate privacy e\ufb00orts in particular, both external and internalforces are necessary for work on corporate responsibility to be e\ufb00ective. Internally, they suggestfocusing on getting onto board-level agendas to ensure attention and resourcing, having a speci\ufb01cboundary-spanning privacy professional to lead adoption of work practices, and ensuring \u2018man-agerialization\u2019 of privacy practices by increasing expertise within business units and integrationwithin existing practices. Externally, they suggest that creating positive ambiguity by keeping leg-islation broad can push more accountability onto \ufb01rms for their speci\ufb01c domains, which can createcommunities and promote sharing around privacy failures. They found that ambiguity in externalprivacy discussions could foster reliance on internal professionals\u2019 judgements, and thus createdautonomy and power for those professionals identi\ufb01ed as leading in privacy protection. Thus,they illustrate how ambiguity \u2014rather than a fully de\ufb01ned list of requirements \u2014can actually helppromote more re\ufb02ection and ensure that e\ufb00orts go beyond compliance.A similar internal/external dynamic is visible within the algorithmic responsibility community.For example, in the Gender Shades project, Buolamwini and Gebru [6] presented not only an ex-ternal audit of facial recognition APIs, but also reactions from the companies whose services wereaudited to illustrate more and less e\ufb00ective responses. Such external audits can result in momen-tum inside of companies to respond to external critique, and in selected cases, to make concretechanges to their products. Internal e\ufb00orts in turn have access to more data, ensure that auditingcan be completed before public releases, develop processes for companies, and allow companiesto take responsibility for their impact [29]. Successes are beginning to emerge, and have rangedfrom positive changes on policy and process resulting from corporate activism, tooling built forclients or internal purposes, to direct product \"\ufb01xes\" in response to external critique [3]. For ex-ample, Raji et al. [29] present an extensive algorithmic auditing framework developed by a smallteam within the larger corporate context of Google. They o\ufb00er general methods such as data andmodel documentation [25] and also tools such as metrics to enable auditing in speci\ufb01c contextslike image search [24]. Implementing these methods and tools then requires corporate processesto provide the resources for such auditing and to ensure that results of audits impact decisionswithin the larger organizational structure.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "d8c7ba01-fae4-45d3-8e75-83ed773051a5",
                    "text": "To situate our work in this broader context, we willbrie\ufb02y examine di\ufb00erent perspectives on organizational structures. First, it is worthwhile to revisitwhat organizational theorist Wanda Orlikowski [27] called the duality of technology in organiza-tions. Orlikowski discusses how people in organizations create and recreate meaning, power andnorms. Orlikowski\u2019s \u2018structurational\u2019 model of technology comprises of these human agents, thetechnology that mediates their task execution, and the properties of organizations. The latter insti-tutional properties range from internal business strategies, control mechanisms, ideology, culture,division of labor and procedures, communication patterns, as well as outside pressures such as gov-ernmental regulation, competition and professional norms, and wider socio-economic conditions.People\u2019s actions are then enabled and constrained by these structures, which are themselves theproduct of previous human actions. This perspective was augmented by Orlikowski [28] to includea practice orientation; repeated interactions with technologies within the speci\ufb01c circumstancesalso enact and form structures.Similarly, Dawson provides an extensive review of perspectives in studies on organizationalchange [9] and discusses the \u2018process turn\u2019, where organizations are seen as ever-changing ratherthan in discrete states; what may appear as stable routines may in actuality be \ufb02uid. Dawsonemphasizes the socially constructed process, and the subjective lived experiences: actors\u2019 collabo-rative e\ufb00orts in organizations unfold over time and dialogue between them shapes interpretationsof changes. Such dynamics are also present in what organizational theorist Richard Scott [30] sum-marized as the rational, natural, and open perspective on organizations. \u2018Rational\u2019 organizationswere seen as \u2018the machine\u2019, best suited to industries such as assembly line manufacturing wheretasks are speci\ufb01ed by pre-designed work\ufb02ow processes. The \u2018natural\u2019 organization signi\ufb01ed a shiftin organizational ideology. No longer were people seen as mere appendages to the machines, butrather as crucial learners in relationship with machines. The metaphor is that of the organizationas an \u2019organism\u2019 with a strong interior vs. exterior boundary, and needs to \u2018survive\u2019. Similar to anorganism, the organisation grows, learns, and develops. As a consequence of the survival ideology,the exterior environment can be seen as a threat against which the organism must adapt to survive.Scott however describes how the notion of \u2018environment as threat\u2019 was replaced by the realizationthat environmental features are the conditions for survival. The central insight emerging from\u2018open\u2019 systems thinking is that all organizations are incomplete and depend on exchanges withother systems. The metaphor became that of an \u2018ecology\u2019. Open systems are characterized by (1)interdependent \ufb02ows of information and (2) interdependent activities, performed by (3) a shiftingcoalition of participants by way of (4) linking actors, resources and institutions, in order to (5) solveproblems in (6) complex environments. For responsible AI e\ufb00orts to succeed then, organizationsmust successfully navigate the changes necessary within \u2018open\u2019 systems.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "3a75e38f-22e6-4aca-af14-8a31222d871f",
                    "text": "The described \u2018ecologies\u2019,particularly in \u2018open\u2019 systems, contain formal and informal meta-organizational structures, whichhave been studied in other contexts and are of increasing growing importance to the \ufb01eld of re-sponsible AI. Organizations often interact with each other through standards bodies, communities,processes, and partnerships. These meta-processes can have as goals (1) producing responses toproposed regulations, standards, and best practices, (2) fostering idea exchange between silos, and(3) self-regulation. Organizations participate in multi-stakeholder initiatives to achieve a numberof their own goals, including advocating for business interests, keeping up to date on industrytrends, and having a voice in shaping standards or regulations that they will then be subjected to.Berkowitz [4] discusses the shift towards governance in sustainability contexts, and the keyrole that meta-organizations can have in facilitating meta-governance of corporate responsibilitybeyond simply complying with legislation. She identi\ufb01es six capabilities needed for sustainable in-novations: (1) anticipation of changes and negative impacts of innovation, (2) resilience to changes,(3) re\ufb02exivity, (4) responsiveness to external pressures and changing circumstances, (5) inclusionof stakeholders beyond immediate decision makers, and (6) comprehensive accountability mech-anisms. Meta-organizations can promote inter-organizational learning and building of these sixcapabilities.Similarly, within the \ufb01eld of AI, multi-stakeholder organizations, standards, and self-organizedprojects have been created in recent years to acknowledge the need for interdisciplinary expertiseto grapple with the wide reaching impacts of AI on people. Many AI researchers have been vocalproponents of expanding the number of perspectives consulted and represented, including stake-holders such as policymakers, civil society, academics from other departments, impacted users,and impacted nonusers. Reconciling perspectives from diverse stakeholders presents its own setof challenges that change depending on the structure of the organization. Participatory action of-fers relevant frameworks for characterizing options for decision making in multistakeholder con-texts. Decision making can be centralized within a formal organization with stakeholders beinginformed, consulted, involved, collaboration, or else stakeholders can self-organize informally toachieve the same levels of participation. The structures present at a meta organizational level willdi\ufb00er and enable the application of di\ufb00erent group-level decision making processes. For example,ad hoc groups of researchers have self-organized to create unconference events and write multi-stakeholder reports, including reports with large groups of authors (e.g. [10]) based originally ondiscussions within workshops held under Chatham house rules, while others have created newformal organizations, conferences such as AIES or FAccT, or research institutes.In a similar manner to Berkowitz [4], we focus here on the \"how\" of achieving more adoptionof responsible AI work practices in industry. We further investigate how practitioners experiencethese changes within the context of di\ufb00erent organizational structures, and what they see as theshifts that drive or hinder their work within their organizations.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "ebee4311-bbd6-46dc-9030-83a00349cf23",
                    "text": "Our motivation for this work was to identify enablers that could shift organizational change to-wards adopting responsible AI practices. Responsible AI research has in\ufb02uenced organizationalpractices in recent years, with individuals and groups within companies increasingly tasked withimplementing research into action, whether formally or informally. Our research applies theoriesand frameworks of organizational structure and change management to characterize the grow-ing practice of applied responsible AI. To better understand the implications of organizationalstructure on day-to-day responsible AI work and outcomes, we interviewed practitioners who areactively involved in these initiatives by themselves or within a larger team.We conducted 26 semi-structured interviews with people based in 4 continents from 19 organiza-tions. Except for two 30 minute interviews, all other interviews lasted between 60 and 90 minutes.Participants were given a choice of whether to allow researchers to record the interview for notetaking purposes. A total of 11 interviews were recorded. In cases where the interview was notrecorded, we relied on writing down the respondents\u2019 answers to the questions during the courseof the interview. In several cases, participants requested to additionally validate any written notesand make necessary clari\ufb01cations before their use in the study to ensure that their anonymity wasnot compromised.Role Respondents Responsible AI / fair-ML workstream framingAI Strategy R16, R25EngineeringHuman ResourcesLegal R1, R14, R19,R21R12, R13R8, R20, R26Marketing and Sales R10, R24ML Research R17, R22, R23Policy R4, R5, R6,R18Productment Manage- R2, R3, R7, R9,R11, R15 thought leadership; strategic planning; building exter-nal relationships; working with operating models; proac-tively \ufb01guring out the pain points and creating solutions;fairness evaluations; internal checklists; implementingnew capabilities; data science;assessment innovation; talent innovation research;policy; legal counseling; investigating legal issues andquestions; responsible AI; privacy; ethical and gover-nance guidelines; comprehensive pillars; digital ethics;algorithmic accountability; understand and explain whatan algorithm does; fairness auditing and explainability interms of bias;algorithmic audits; explainability; social impact of AI; so-ciotechnical systems; educational e\ufb00orts; fairness;distribution of bene\ufb01ts from AI; norms; communica-tion ability and navigating external expecations; fair-ness; mitigation of risk;reconsidering the ML lifecycle; interpretability; in\ufb02u-enced by broader industry trends; practical needs; re-sponsible AI; ethics of technology; ethics review; audit-ing; bias assessment;Sampling technique. Participants were recruited through a convenience sampling technique3.0.1through snowball sampling from participants who recommended other interviewees. Three re-cruiting criteria were used to \ufb01nd interviewees: (1) did they work closely with product, policy,and/or legal teams, (2) did the outputs of their work have a direct impact on ML products andservices, and (3) were some aspects of their work related to the \ufb01eld of responsible AI. We \ufb01l-tered out individuals whose roles were solely research, although interviewees may also be activecontributors to responsible AI research in addition to their existing work stream.Through the ongoing conversations we had with practitioners before as well as after conduct-ing the qualitative interviews, we aimed to establish a substantial level of trust and transparency,which we felt was necessary given the sensitive nature of the topics discussed. This allowed formore open, nuanced, and in-depth discussions where practitioners felt that there is a shared under-standing between interviewers and interviewees about the often unvoiced challenges in responsi-ble AI work.We intentionally sought to interview as diverse a group of practitioners as possible to captureperspectives from a broad range of organizational contexts. In Table 1 we summarize the functionalroles of the interviewees who participated in the project and how they describe their responsibleAI work. Participants came from a wide variety of functions, including AI Strategy, Engineering,Human Resources, Legal, Marketing and Sales, Machine Learning Research, Policy, and ProductManagement. Among the 26 participants, ten had educational background in Social Science, eightin Computer Science, seven in Law and Policy, and one practitioner had a degree in Economics.The majority of respondents were geographically located in the US (21 out of 26), two participantswere in the UK, and the rest of the respondents were based in Australia, Denmark, and Japan. Theaverage length that interviewees have been with their organization is 5 years and 5 months, wheremore than one third of the practitioners have been with their company for more than \ufb01ve years(9 people) and 2 people spent decades with their organization. Lastly in terms of organizationalsectors, 11 practitioners worked in business-to-business organizations, 2 in business-to-consumer,and 13 in organizations which were both business-to-business and business-to-consumer.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "fe53ca05-fd19-4141-87f4-75984e23826f",
                    "text": "The script and questions for the semi-structured interviews were re-viewed by an industrial-organizational psychologist and responsible AI practitioners within threedi\ufb00erent organizations. Questions were grouped into di\ufb00erent sections, exploring the current stateof responsible AI work, the evolution of the work over time, how the work is situated within theorganization, how responsibility and accountability for the work are distributed, performance re-view processes and incentives, and what desired aspirational future structures and processes wouldenable more e\ufb00ective work. The semi-structured nature of the interview provided standard ques-tions that were asked of all participants while allowing interviewers the \ufb02exibility to follow upon interesting insights as they arose during interviews. The full set of questions can be found inAppendix A.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "325711ec-75de-4b29-a688-977160d566a1",
                    "text": "To analyze the interview data, we utilized a standard methodology from contex-tual design - interpretation session and a\ufb03nity diagramming [14]. Through a bottom up a\ufb03nitydiagramming approach, we iteratively assigned codes to various concepts and themes shared bythe interviewees. We iteratively grouped these codes into successive higher level themes and stud-ied the relationships between them.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "d4c2e348-1bde-406d-8a5a-a6f8255f4026",
                    "text": "In addition to semi-structured interviews, we organized a workshop at a ma-chine learning conference attended by a highly self-selected group of people interested in respon-sible AI from industry, academia, government, and civil society. The \ufb01rst half of the workshopwas a presentation of preliminary insights from the literature review and results sections of thispaper. We then conducted an interactive design exercise where participants were organized into13 groups of between 4 and 6 individuals per group. Each group was given a scenario descrip-tion of an AI organization that exempli\ufb01ed the prevalent work practices discussed in the Results:Interviews section below. The facilitators guided groups through a whiteboard discussion of thefollowing questions:\u2022 What are examples of emerging responsible AI work practices in the context of the scenario?\u2022 What are examples of structures or processes in the prevalent organizational structure whichare outside of the scope of responsible AI work but which act to protect and enable emergingfair-ML practices?\u2022 What are examples of outlier practices outside of the prevalent practices in the scenario?\u2022 What connections exist between these practices and organizational structures?\u2022 What practices or organizational structures could enable positive self-reinforcing outcomesthrough making the connections stronger?The workshop activity was designed to allow participants to (1) gain a deeper understanding ofthe responsible AI challenges by connecting study \ufb01ndings to their own experiences, (2) collabora-tively explore what organizational structures could enable the hypothetical organization develop-ing AI products and services to resolve them through, and (3) map interdependencies and feedbackloops that exist between practices to identify potentially e\ufb00ective recommendations to address thechallenges of implementing responsible AI initiatives.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "c2d5ac3a-44c6-4506-ad36-a70d55d0e7cd",
                    "text": "We start with a high level overview of our \ufb01ndings followed by a discussion of the key themesthat emerged from the conducted interviews.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "8c2d573f-3a5c-4d05-8b71-1f01ebd91ba7",
                    "text": "About a quarter of the participants had initiated their responsible AI work in their current orga-nization within the past year (7 out of 26) compared to 73% (19 out of 26) worked on e\ufb00orts thathad started more than an year ago. More than half of the interviewees worked on their initiativesas individuals and not as part of a team (14 out of 26). About 40% of the respondents reportedthat they volunteer time outside of their o\ufb03cial job function to do their work on responsible AIinitiatives (11 out of 26) while the remaining 15 out of 26 participants had o\ufb03cial roles related toresponsible AI. Among the 15 interview participants with o\ufb03cial roles related to responsible AI,8 individuals were externally hired into their current role, while 7 transitioned into it from otherroles within their organization. Interviewees who changed the focus of their existing roles or tran-sitioned into responsible AI-related role were most commonly previously in project managementroles (4 out of 7), then research (2 out of 7), then legal (1 out of 7). The majority of participantswho had o\ufb03cial responsible AI-related roles reported bene\ufb01ting from an organizational structurethat allowed them to craft their own role in a very dynamic and context-speci\ufb01c way.Since the beginning of our conversations, we noticed that practitioners used di\ufb00erent languagein the way they described their work and how it relates to responsible AI. We observed common-alities in the way practitioners from each function framed their responsible AI work (see Table 1).For example, while project managers described their work in terms of product life-cycles and in-dustry trends, legal practitioners discussed the responsible AI aspects of their role in terms ofcomprehensive pillars and ethical governance guidelines.We note that a few interviewees described going through stress-related challenges in relationto their responsible AI work. During some of the interviews, we saw a noticeable tone change inthe interviewees\u2019 voice when discussing questions related to ethical tensions, accountability, riskculture, and others. Furthermore, some respondents have left their organizations between whenwe conducted the interviews in late 2019 and when we submitted this paper in October 2020. Whilewe acknowledge the nascent state of responsible AI functions, these observations could point toopportunities for further study.There were various common perspectives that we heard practitioners express repeatedly. Wesaw the need for a multi-faceted thematic analysis which encompasses three intuitive clusters ofdata: (1) currently dominant or prevalent practices, (2) emerging practices, and (3) aspirationalfuture context for responsible AI work practices in industry:\u2022 The prevalent practices comprise what we saw most commonly in the data.\u2022 The set of emerging practices includes practices which are shared among practitioners butless common than prevalent practices.\u2022 The aspirational future consists of the ideas and perspectives practitioners shared whenexplicitly asked about what they envision for the ideal future state of their work withintheir organizational context.Within the thematic analysis (see Table 2), we found four related but distinct key questions thatevery organization must have processes and structures to support answering:\u2022 When and how do we act?\u2022 How do we measure success?\u2022 What are the internal structures we rely on?\u2022 How do we resolve tensions?Prevalent practices Emerging practices Aspirational futureWhen andhow do weact?How do wemeasure suc-cess?What are theinternal struc-tures we relyon? doresolveHowwetensions? ReactiveOrganizations actonly when pushedby external forces(e.g. media, regula-tory pressure)Performancetrade-o\ufb00sOrg-level conversa-tions about responsi-ble AI dominated byill-informed perfor-mance trade-o\ufb00sLack ofaccountabilityResponsible AI workfalls thethroughcracks due to roleuncertaintyFragmentedMisalignment be-tween individual andteam incentives andorg-level missionstatements Proactive actOrganizations ad-proactively todress potentialresponsible AI issuesProvenance metricsOrg-levelframeworks andprocesses are imple-mented to evaluateAIresponsibleprojectsStructural supportSca\ufb00olding to sup-port responsible AIwork begins to beerected on top of ex-isting internal struc-turesRigidOverly rigid organi-zational incentivesdemotivate address-ing ethical tensionsin responsible AIwork Anticipatory de-Organizations thatployedallow for anticipating riskshaveframeworksConcrete resultsConcepts of results are rede-\ufb01ned to include societal im-pact through data-informede\ufb00ortsIntegratedResponsible AI work is inte-grated throughout all busi-ness processes related toproduct teamsAlignedEthical tensions in work areresolved in accordance withorg-level mission and valuesAs organizations seek to scale responsible AI practices, they will have to transition from the preva-lent or emerging practices of answering these questions to the structures and processes of the as-pirational future. It is important to note that not all emerging practices we found in the data willnecessarily lead to the aspirational future. In what follows, we provide details about practitioners\u2019personal perspectives and experiences within the individual themes and questions.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "a8d5d427-b55b-4299-9733-0ad10c523c26",
                    "text": "One transition we identi\ufb01ed in the data is how organizations choose when and how to act. Thisincludes questions of who chooses to prioritize what information within which decision-makingprocesses. We found that many organizations behave reactively, fewer are now proactive, andrespondents aspire for their organizations to become anticipatory in the future.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "66f7f522-29e3-435f-9982-aa2d4ff957bb",
                    "text": "Most commonly, interviewees described responsible AI work intheir organizations as reactive. The most prevalent incentives for action were catastrophic mediaattention and decreasing media tolerance for the status quo. Many participants reported that re-sponsible AI work can be perceived as a \"taboo topic\" in their organizations. Raising awarenesswas a challenge for one interviewee who shared: \"It was an organizational challenge for us, it\u2019shard as when something is so new - we run into \u2019Whose job is this?\u2019\" when they bring up topicsabout algorithmic fairness or inequity in harm at work. We found that the uncertainty and unwill-ingness to engage in a deeper understanding of responsible AI issues may lead to unproductivediscussions or outright dismissal of important but often unvoiced concerns. responsible AI work isoften not compensated, as in the case of the 40% of respondents volunteering their time to work onresponsible AI initiatives, or is perceived as ambiguous or too complicated for the organization\u2019scurrent level of resources. In response to the question about how interviewees are recognized fortheir work, one interviewee shared: \"many of the people volunteering with our team had trouble\ufb01guring out how to put this work in the context of their performance evaluation.\" In several cases,the formation of a full-time team to conduct responsible AI work was only catalyzed by the resultsfrom volunteer-led investigations of potential bias issues within models that were en route to de-ployment. The volunteers for these investigations went far beyond their existing role descriptions,sometimes risking their own career progression, to take on additional uncompensated labor toprevent negative outcomes for the company. This highlights the reactive nature of organizationalsupport for responsible AI work in prevalent practice. Legal compliance was another factor thatparticipants said could motivate organizational action. Beyond legal concerns, some practitionersreported that being able to use reputational risk as leverage to increase investment in responsi-ble AI work, bringing hypothetical questions like \"What if ProPublica found out about ...?\" intodecision-making meetings. Participant responses in this section illustrate how a reactive organiza-tional stance towards responsible AI work shifts the labor and cost of identifying and addressingissues onto the individual worker.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "d6f86821-1408-4816-9e4d-f9352cb4d958",
                    "text": "In emerging practices on how and when to act, a few organiza-tions have implemented proactive responsible AI evaluation and review processes for their MLsystems, with the work and accountability often distributed across several teams. For example,some respondents reported support and oversight from legal teams. In a few cases, intervieweesspoke with enthusiasm about the growing number of both internal and external educational ini-tiatives. This included onboarding and upskilling employees through internal responsible AI cur-ricula to educate employees about responsible AI-related issues and risks as well as externallyfacing materials to educate consumers and customers. Respondents referred to these e\ufb00orts as anorganization-level proactive investment to set up the organization to better address future respon-sible AI issues. Furthermore, a few participants shared about the availability or their involvementin preparing externally facing materials to educate their organization\u2019s customers or potential cus-tomers about responsible AI considerations in practice. A small number of interviewees reportedthat their work on responsible AI is acknowledged and explicitly part of their compensated role,in contrast to the volunteers in the prevalent practices theme, which is another organization-leveldi\ufb00erence between prevalent and emerging practices.On the other hand, emerging practices still show how individuals rather than organizationalprocesses or structures remain the engine of proactive practices. In a few cases, proactive cham-pions organizing grassroots actions and internal advocacy with leadership have made responsibleAI a company-wide priority, which then sometimes made it easier for people to get resourcingfor responsible AI initiatives and to establish proactive organization-wide processes. Some partic-ipants reported leveraging existing internal communication channels to organize responsible AIdiscussions. One participant even captured screenshots of problematic algorithmic outcomes andcirculated them among key internal stakeholders to build support for responsible AI work. Similarto prevalent practices, these individuals are tasked with the labor of using existing organizationalstructures to build organizational support for their responsible AI work in addition to doing theirresponsible AI work. The di\ufb00erence in emerging organizational practice is how these individualsare \ufb01nding more success in instilling a proactive, rather than reactive, mindset for approachingalgorithmic responsibility.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "63e65576-2da7-4132-9add-b587a0191209",
                    "text": "In an ideal future, many interviewees envisioned organi-zational frameworks that encourage an anticipatory approach. In the future state, an individualwanting to engage with algorithmic responsibility issues would not necessarily need to do the or-ganizational labor of changing structures as in the prevalent and emerging practices, but rather besupported by organization-wide resources and processes to focus their e\ufb00orts directly on respon-sible AI work. In this aspirational future, respondents envisioned technical tools to enable large-scale implementation of responsible AI evaluations both internally and externally: well-integratedtechnical tools would assess algorithmic models developed by product teams and feed seamlesslyinto organization-wide evaluation processes that identify and address risks of pending ML systemsbefore they go live in products, while externally, customers using the algorithmic models in di\ufb00er-ent contexts have oversight through explicit assessments, which feed information about identi\ufb01edrisks back to the organization. Their organizations would utilize clear and transparent communi-cation strategies to explain the process and results of these evaluations both internally within theentire organization and externally with customers and other stakeholders. One practitioner ques-tioned if their team should even engage with customers who do not agree to deploy an assessmentframework ex-ante, suggesting a new baseline expectation for customers to also play their role infaster feedback loops for identifying and mitigating risk. Respondents reported that in the ideal fu-ture, product managers would have an easier way to understand responsible AI concerns relevantto their products without needing to regularly read large numbers of research papers, which couldbe supported by organization-level teams, tools, and/or education to synthesize and disseminaterelevant knowledge. Several participants expressed that the traditional engineering mindset wouldneed to become better aligned with the dynamic nature of responsible AI issues which cannot be\ufb01xed in prede\ufb01ned quantitative metrics. Anticipatory responsible AI frameworks could allow or-ganizations to respond to the responsible AI challenges in ways which uphold organizational codeof ethics and society\u2019s values at large.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "5943548e-82fe-4d4c-b518-3308d48ae299",
                    "text": "Another transition we saw our respondents navigating in their work and organizations is howorganizations measure success. Many responsible AI initiatives are relatively newer and aim tomeasure the societal impact of technology, which is a departure from traditional business metricslike revenue or pro\ufb01tability. Learning organizations need to make an active change to better ac-count for this shift. Respondents reported that many challenges in their prevalent work practicesarise from the inability to adequately use existing metrics to account for the goals of responsible AIwork, while emerging practices aim to begin rewarding success that falls outside of pre-existingnarrow de\ufb01nitions. In an aspirational future, organizations value responsible AI work and pro-cesses re\ufb02ect that at every level.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "3b37b804-e4bc-4add-b91a-eea2bfc33305",
                    "text": "The majority of respondents reported that one of the biggest chal-lenges for their responsible AI work is the lack of metrics that adequately capture its true impact.The majority of respondents also expressed at least some degree of di\ufb03culty in communicatingthe impact of their work. Combined, this hinders them from fully illustrating the importance ofresponsible AI work for the organization\u2019s success, which in turn keeps them from being able toreceive adequate credit and compensation for their true impact. The challenges of measuring theimpact of responsible AI is a deeply researched topic in the \ufb01eld of fairness, accountability, andtransparency of ML. Through our interview questions, we have tried to further disentangle the per-spectives on this challenge in industry. For example, some industry practitioners reported that theuse of inappropriate and misleading metrics is a bigger threat than the lack of metrics. Respondentsshared that academic metrics are very di\ufb00erent than industry metrics, which include benchmarksand other key performance indicators tracked by product teams, such as metrics related to cus-tomer retention and development (click rate, time spent using a product, etc.) Project managersreported trying to implement academic metrics in order to both leverage academic research andfacilitate a collaboration between research and product teams within their organization. One ofthe interviewees shared that in their personal perspective, \"industry-speci\ufb01c product-related prob-lems may not have su\ufb03cient research merit or more speci\ufb01cally an ability for the researcher topublish, sometimes because of privacy reasons data used in the research experiments may not al-low researchers to be recognized for their work.\u201d This may be due to the nature of the problem ordue to privacy reasons. Since data used in the research experiments may not allow researchers tobe recognized for their work, this may ultimately discourage them from investigating real worldresponsible AI issues. Practitioners embedded in product teams explained that they often need todistill what they do into standard metrics such as number of clicks, user acquisition, or churn rate,which may not apply to their work. Most commonly, interviewees reported being measured ondelivering work that generates revenue. They spoke at length about the di\ufb03culties of measuringresponsible AI impact in terms of impact on the business \"bottom line.\u201d In some cases, practition-ers framed their impact in terms of pro\ufb01tability by arguing that mitigating responsible AI risksprior to launch is much cheaper than waiting for and \ufb01xing problems that arise after launch wherereal-world harm and reputational risk come into play. Again, the prevalent work practices revealindividuals working on responsible AI taking on the extra labor of trying to translate their workinto ill-\ufb01tting terms and metrics that are not designed to measure or motivate success on respon-sible AI outcomes.The majority of respondents expressed at least some degree of di\ufb03culty in communicating theimpact of their work. The metrics-related challenges they described included: (1) product teamsoften have short-term development timelines and thus do not consider metrics that aim to encom-pass long-term outcomes; (2) time pressure within fast-paced development cycles leads individualsto focus on short-term and easier to measure goals; (3) qualitative work is not prioritized becauseit requires skills that are often not present within engineering teams; (4) leadership teams mayhave an expectation for \"magic,\" such as \ufb01nding easy to implement solutions, which in reality maynot exist or work; (5) organizations do not measure leadership qualities and (6) do not reward thevisionary leaders who proactively address the responsible AI issues that arise; (7) performanceevaluation processes do not account for responsible AI work, making it di\ufb03cult to impossible forpractitioners to be rewarded or recognized for their responsible AI contributions.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "dbfafd18-3d76-409f-88cb-fe8458246d48",
                    "text": "A few interviewees reported that their organizations have imple-mented metrics frameworks and processes in order to evaluate responsible AI risks in products andservices. Practitioners talked enthusiastically about how their organizations have moved beyondethics washing [5] in order to accommodate diverse and long-term goals aligned with algorithmicresponsibility and harm mitigation, the goals of a responsible AI practice. Interviewees identi\ufb01edthe following enablers for this shift in organizational culture: (1) rewarding a broad range of ef-forts focused on internal education; (2) rewarding risk-taking for the public good; (3) followingup on potential issues with internal investigations; (4) creating organizational mechanisms thatenable cross-functional collaboration. These emerging organizational enablers begin to set orga-nizational sca\ufb00olding of a work environment that supports individuals working on responsibleAI as they seek to change how their organization assigns value to work to better align with thesocietally-focused outcomes.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "561ec22b-8815-4c86-a683-b35cd9a156c3",
                    "text": "In an aspirational future where responsible AI work is ef-fective and fully supported by organizational structures, interviewees reported that their organi-zations would measure success very di\ufb00erently than today\u2019s prevalent practices: (1) their orga-nizations would have a tangible strategy incorporate responsible AI practices or issues into thekey performance indicators of product teams; (2) teams would employ a data-driven approachto manage ethical challenges and ethical decisions in product development; (3) employee perfor-mance evaluation processes would be rede\ufb01ned to encompass qualitative work; (4) organizationalprocesses would enable practitioners to collaborate more closely with marginalized communities,while taking into account legal and other socio-technical considerations; (5) what is researchedin academic institutions would be more aligned with what is needed in practice; (6) collaborationmechanisms would be broadly utilized. Speci\ufb01cally, participants discussed two kinds of mecha-nisms to enable collaboration: (1) working with external groups and experts in the \ufb01eld to de\ufb01nebenchmarks prior to deployment, and (2) working with external groups to continuously monitorperformance from multiple perspectives after deployment.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "4eb6e4a1-192d-4573-bb2b-0bdc13d147ad",
                    "text": "In order for individuals to better enable responsible AI work, they need to reexamine the proper-ties of their organizations. This involves leveraging what Orlikowski called the \"structurational\"model of technology in a speci\ufb01c applied context [28]. In the prevalent practices, organizations donot have internal structures to ensure accountability for responsible AI work, which can then beneglected due to role uncertainty without consequences. Distributed accountability on top of ex-isting structures was reported in emerging practices, while in the aspirational future, responsibleAI work would become integrated into all product-related processes to ensure accountability.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "4d867993-bec3-49df-86d0-937d7d825df3",
                    "text": "Most commonly, participants reported ambiguity and uncertaintyabout role de\ufb01nitions and responsibilities within responsible AI work at their organization, some-times due to how rapidly the work is evolving. Multiple practitioners expressed that their responsi-ble AI related concerns were heard on account of their seniority in their team and organization. Inresponse to \"Do you have autonomy to make impactful decisions?\", one data science practitionerwho was volunteering time with the responsible AI team shared, \"More senior people are mak-ing the decisions. I saw ethical concerns but there was di\ufb03culty in communicating between mymanagers and the [responsible AI] team. People weren\u2019t open for scrutinization.\" This illustratesthe fragility of the prevalent practice since accountability relies on the individual\u2019s own resources,interests, and situational power rather than scalable and systemic organizational structures andprocesses that would ensure the desired outcomes. Several interviewees talked about the lack ofaccountability across di\ufb00erent parts of their organization, naming reputational risk as the biggestincentive their leadership sees for responsible AI work, again tying accountability to individualincentives to take responsibility rather than ensuring accountability through organization-wideprocesses and policies.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "9f4ab759-ca18-40ea-bafd-9a7bd951d4ca",
                    "text": "Interviewees shared these emerging organizational structures asenablers for responsible AI work: (1) \ufb02exibility to craft their roles dynamically in response to inter-nal and external factors; (2) distributed accountability across organizational structures and amongteams working across the entire product life cycle; (3) accountability integrated into work\ufb02ows;(4) processes to hold teams accountable for what they committed to; (5) escalation of responsibleAI issues to management; (6) responsible AI research groups that contribute to spreading internalawareness of issues and potential solutions; (7) internal review boards that oversee responsibleAI topics; (8) publication and release norms that are consistently and widely followed; (9) cross-functional responsible AI roles that work across product groups, are embedded in product groups,and/or collaborate closely with legal or policy teams. Participants also reported being increasinglycognizant of external drivers for change, such as cities and governments participating to createcenters of excellence, for example, New York\u2019s Capital District AI Center of Excellence. As before,these emerging structures begin to shift the locus of responsibility for managing organizationalchange away from the individual who seeks to do responsible AI work, which is not necessar-ily the same as organizational change management work, and onto organizational processes andstructures that can distribute that labor in an appropriate manner.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "c21dd762-3d9c-44c5-8fb8-c2b682fd4b25",
                    "text": "In the future, interviewees envisioned internal organiza-tional structures that would enable responsible AI responsibilities to be integrated throughout allbusiness processes related to the work of product teams. One practitioner suggested that whilea product is being developed, there could be a parallel development of product-speci\ufb01c artefactsthat assess and mitigate potential responsible AI issues. The majority of interviewees imaginedthat responsible AI reviews and reports would be required prior to release of new features. NewML operations roles would be created as part of responsible AI audit teams. Currently, this workfalls within ML engineering, but respondents identi\ufb01ed the need for new organizational structuresthat would ensure that responsible AI concerns are being addressed while allowing ML engineersto be creative and experiment. For example, one practitioner suggested that a responsible AI op-erations role could act as a safeguard and ensure that continuous responsible AI assessments arebeing executed once a system is deployed. Some interviewees described the need for organizationalstructures that enable external critical scrutiny. Scale could be achieved through partnership-basedand multistakeholder frameworks. In the future, public shaming of high-stakes AI failures wouldprovide motivation towards building shared industry benchmarks, and structures would exist toallow organizations to share benchmark data with each other. External or internal stakeholderswould need to call out high impact failure use cases to enable industry-wide learning from indi-vidual mistakes. Industry-wide standards could be employed to facilitate distributed accountabil-ity and sharing of data, guidelines, and best practices. Of note is that in the aspirational future,organizational structures and processes incorporate external parties and perspectives, providingorganizations better channels to understand their societal impact.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "28bb993a-cdce-46a6-9c03-4a7137e620fc",
                    "text": "Lastly, responsible AI work brings new types of tensions that organizations may not yet haveprocesses to resolve, especially related to the questions of ethics and unintended consequences ofsocio-technical systems like AI. This requires organizations to update their prevalent practices intheir transitions to better enable responsible AI work. Resolving tensions requires organizations tochoose what to prioritize in a situation where there\u2019s a need for trade-o\ufb00s. The practices describedbelow show the di\ufb00erent approaches that organizations are taking in prevalent practices, emergingpractices, and in the aspirational future.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "768d932b-97cd-4b67-bc0a-ae0276dbdcff",
                    "text": "The majority of respondents reported that they see misalignmentbetween individual, team, and organizational level incentives and mission statements within theirorganization. Often, individuals reported doing ad hoc work based on their own values and per-sonal assessment of relative importance. Similarly, the spread of information relies on individualrelationships. Practitioners reported relying on their personal relationships and ability to navigatemultiple levels of obscured organizational structures to drive responsible AI work. Related to thequestion about \"What are the ethical tensions that you/your team faces?\u201d, one of the intervieweesshared, \"We often work on prototypes for speci\ufb01c geographic units which are not meant to bescaled, it\u2019s really meant not to be scaled. We need to step in and make that clear. Also sometimespeople state the model is complete, we need a disclaimer that we\u2019re still updating and validatingit, it is work in progress.\" Many of the interviewees had to navigate tensions related to scale andexpectations on a daily basis. Like in the other transitions, this highlights a prevalent practice ofrelying on individuals to decide how to resolve tensions rather than organizational processes thatwould support individuals in evaluating tensions in alignment with the organization\u2019s mission orvalues. This creates additional labor and uncertainty for individuals doing responsible AI work inorganizations exhibiting prevalent practices.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "2d9a38ed-2e22-4421-8a34-8ba72fb09e8d",
                    "text": "One of the biggest challenges practitioners reported was that asresponsible AI ethical tensions are identi\ufb01ed, overly rigid organizational incentives may demoti-vate addressing them, compounded by organizational inertia which sustains those rigid incentives.In this case, although the organizational structures in the emerging work practice shift labor awayfrom individuals onto organization-wide processes, the processes themselves are not su\ufb03cientlyaligned with the ultimate goals of responsible AI. Therefore, this makes the transition from preva-lent to emerging practice one that steers the organization away from, rather than towards, theaspirational future where organizations resolve tensions in a way that encourages responsible AIwork. Respondents described that in this situation, research and product teams struggle to justifyresearch agendas related to responsible AI. This was caused by competing priorities that may alignbetter with existing incentives and metrics for success, which as reported in Section 4.3: How dowe measure success?, do not adequately account for the impact of responsible AI initiatives.Interviewees identi\ufb01ed several factors that limit an organization\u2019s ability to resolve tensions ina manner that enables, instead of hinders, responsible AI work: (1) incentives that reward complex-ity whether or not it is needed - individuals are rewarded for complex technical solutions; (2) lackof clarity around expectations and internal or external consequences; (3) impact of responsibleAI work being perceived as di\ufb00use and hard to identify; (4) lack of adequate support and com-munication structures - whether interviewees were able to address responsible AI tensions oftendepended on their network of high trust relationships within the organization; (5) lack of data forsensitive attributes, which can make it impossible to evaluate certain responsible AI concerns.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "c825132e-1675-471c-9adb-c8308f5d3dbc",
                    "text": "When asked about their vision for the future of their re-sponsible AI initiatives, several respondents wanted responsible AI tensions to be addressed inbetter alignment with organization-level values and mission statements. They imagined that or-ganizational leadership would understand, support, and engage deeply with responsible AI con-cerns, which would be contextualized within their organizational context. Responsible AI wouldbe prioritized as part of the high-level organizational mission and then translated into actionablegoals down at the individual levels through established processes. Respondents wanted the spreadof information to go through well-established channels so that people know where to look andhow to share information. With communication and prioritization processes in place, \ufb01nding asolution or best practice in one team or department would lead to rapid scaling via existing or-ganizational protocols and internal infrastructure for communications, training, and compliance,in contrast to the current prevalent situation that respondents described. Respondents wanted or-ganizational culture to be transformed to enable (1) releasing the fear of being scrutinized as aroadblock for allowing external critical review and (2) distributing accountability for responsibleAI concerns across di\ufb00erent organizational functions. In the future state, every single person in theorganization would understand risk, teams would have a collective understanding of risk, whileorganizational leadership would talk about risk publicly, admit when failures happen, and takeresponsibility for broader socioeconomic and socio-cultural implications.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "ed46bb32-c86b-4a74-a4a0-d314035dbbff",
                    "text": "As described in the Section 3: Study and Methods, after the interviews with practitioners werecompleted, a workshop was held at a responsible AI oriented venue [anonymized for review].Each of the four key organizational questions we identi\ufb01ed in Section 4: Results: Interviews needsto be considered within the unique socio-technical context of speci\ufb01c teams and organizations - (1)When and how do we act? (2) How do we measure success? (3) What are the internal structures werely on? and (4) How do we resolve tensions? However, the literature and interview \ufb01ndings suggestthat there are likely similar steps or tactics that could lead to positive outcomes. The workshopactivity allowed groups to create landscapes of practices based on their own experiences and thenilluminate connections and feedback loops between di\ufb00erent practices. Participants were givena simple scenario describing the prevalent work practices and organizational structure of an AIproduct company in industry as described in Section 3: Study and Methods. They then engagedin identifying enablers and tensions elucidating current barriers and pointing the way towardspossible solutions. The following themes emerged in the insights participants shared during theactivity:",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "774af293-f1da-4390-91c7-405f02182020",
                    "text": "Multiple groups mentioned that beforeconsidering how the fairness or societal implications of an AI system can be addressed, it is cru-cial to ask whether an AI system is appropriate in the \ufb01rst place. It may not be due to risks ofharm, or the problem may not need an AI solution. Crucially, if the answer is negative, then workmust stop. They recommended designing a veto power that is available to people and committeesacross many di\ufb00erent levels, from individual employees via whistleblower protections, to internalmultidisciplinary oversight committees to external investors and board members. The most im-portant design feature is that the decision to cease further development is respected and cannotbe overruled by other considerations.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "51900919-e015-43e2-aae0-0488131fd641",
                    "text": "The dif-ferent and synergistic roles of internal and external pressure was another theme across multiplegroups\u2019 discussions. Internal evaluation processes have more access to information and may pro-vide higher levels of transparency, while external processes can leverage more stakeholders andincrease momentum by building coalitions. External groups may be able to apply pressure morefreely than internal employees that may worry about repercussions for speaking up.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "e7d51dc2-b82b-4500-be70-23d202559657",
                    "text": "Fundamentally organizations aregroups of people, and creating opportunities for di\ufb00erent sets of people to exchange perspectiveswas another key enabler identi\ufb01ed by multiple groups. One group recommended a regular townhall for employees to be able to provide input into organization-wide values in a semi-public forum.Sequencing these actions will not be easy because they are highly interdependent. Many of the5.0.4groups identi\ufb01ed latent implementation challenges because the discussed organizational enablerswork best in tandem. For example, whistleblower protections for employees and a culture thatsupports their creation would be crucial to ensure that people feel safe speaking candidly at townhalls.It is interesting to observe that workshop discussion groups identi\ufb01ed organization-level struc-tures and processes that support and amplify individual e\ufb00orts as one of the key enablers forresponsible AI work. Additionally, these themes are shared as a starting point to spark experimen-tation. Further pooling of results from trying these recommendations would accelerate learningand progress for all towards achieving positive societal outcomes through scaling responsible AIpractices.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                },
                {
                    "id": "a5a74cfd-e69f-4d8c-bfe8-66b54dffa858",
                    "text": "As ML systems become more pervasive, there is growing interest and attention in protecting peo-ple from harms while also equitably distributing the bene\ufb01ts from these systems. This has ledresearchers to focus on algorithmic accountability and transparency as intermediary goals on thepath to better outcomes. However, corporate responsibility and organizational change are not newthemes. The processes we see elucidated by Orlikowski [28] and Myserson[23] also apply to re-sponsible AI. Myserson described how tempered radicals forge collective action through clarifyingissues and creating movements, with a focus on internal culture and actively soliciting support us-ing small but persistent steps. Our interviews and workshop discussions echo similar processes.The results suggest that what individuals working on responsible AI need is for the organizationalstructures around them to adapt in order to support rather than hinder their work. This can happenas a product of their own advocacy, demonstrated early successes, and/or from leadership proac-tively steering into these transitions. As Myerson points out, tempered radicals should be awareof new opportunities or threats during their work to elevate social responsibility to an internalcorporate priority, and frame their work so it appeals to organizational interests. The resultingtensions in how labor and responsibility are distributed between individuals compared to support-ing processes or structures were also prevalent in our \ufb01ndings.In order to succeed, practitioners have to map out a route from prevalent work practices to theiraspirational future state goals. Along the way, they need to leverage existing practices to buildmomentum for emerging work practices that can lead them there. Similarly, it is essential thatpractitioners are able to identify and avoid creating emerging work practices that work againsttheir desired long-term outcomes. They need to have a clear enough view of what the aspirationalfuture should be, while adjusting to changing circumstances. This means both maintaining align-ment with the existing organizational state, while maintaining a long-term goal orientation. Ourinterviews and workshop discussion identi\ufb01ed the resulting tensions in getting to that aspirationalstate.Throughout the four key organizational questions in which transitions are necessary to accom-modate responsible AI work, we saw that prevalent practices can place the burden of responsibilityand labor squarely on individuals to identify issues and try to change outcomes within existingstructures.This means pushing for changes in those structures and processes, as their goals may be an-tithetical to what is currently supported by the organizational structure. Thus, individuals whowant to bring responsible AI issues into their work must do their own jobs, do the responsible AIwork if that is not their o\ufb03cial job, do the di\ufb03cult work of redesigning organizational structuresaround them to accommodate the responsible AI work, and on top of it all, do the change manage-ment to get those new organizational practices to be adopted. As a result, incentives may appearmisaligned between individuals and their organizational context. This can make it challenging tocreate adequate support, which should come from communicating the (sometimes small) steps thattogether make the larger organizational successes. This can invoke feelings of a lack of clarity onexpectations and impact. As summarized in Table 2, our participants had to decide when and howto act, how to reframe success, orient themselves within internal structures, and resolve tensionsbetween incentives.Navigating these questions in organizations exhibiting prevalent practices requires skills thatare not necessarily part of the regular conversation at academic venues. Perhaps then, rather thanfocusing on technical complexity, or on calls to (ideal) action alone, we should as a research com-munity also prioritize providing researchers and practitioners with the tools and organizationalinsight to ensure that they have clear strategies to face this challenge. Researchers who maketransitions to industry need to communicate the impact of their work in ways that builds themsupport within organizations, and legitimacy along the way. They need to have the skills and toolsto navigate internal structures and tensions. This requires training, mentorship and sponsorshipmuch beyond technical or research skills. The most e\ufb00ective ways for a particular organizationmay not always be perfectly aligned with research community norms; perhaps there lies anothertension. Rather than having individuals \ufb01nd out the organizational work required on their own,and encounter pitfalls anew as individuals, we can provide support as an insights community, butonly when we take this less public work very seriously as a core \ufb01eld of inquiry, as well as edu-cation. Perhaps then, this could help us as a wider community to move towards an \"open\" system,as described in other organizational settings by Scott [30], linking di\ufb00erent actors, resources andinstitutions, and solving complex problems, in similarly complex environments.We observed that organizations exhibiting the emerging practices were beginning to implementnew structures and processes or adapt existing ones, although some emerging structures like rigidorganizational incentives within high-inertia contexts can hinder rather than support responsibleAI work. The remaining emerging work practices did better enable responsible AI work, often byreducing the labor burden on individuals especially to identify what organizational processes andpolicies are necessary to support their responsible AI work and to change manage the transitionsof adopting those new work practices. This frees up time that individuals can reclaim to focus onthe responsible AI work itself.In the aspirational future, organizational structures and processes would fully provide mecha-nisms for monitoring and adapting system-level practices to incorporate and address emergentethical concerns, so individuals who care about algorithmic responsibility issues can easily devotetheir time and labor to making progress on the speci\ufb01c issues within their functions. The internaladvocacy and change management work would be full-time roles given to people with the skills,training, and desire to focus on that work, who could also o\ufb00er expertise and mentorship to otherindividuals as they band together to create system-level change inside and beyond their organi-zations. Individuals working on responsible AI would then be free to focus on their speci\ufb01c job,rather than on changing the job environment to make it possible to do their job.The impact of ML systems on people cannot be changed without considering the people whobuild them and the organizational structure and culture of the human systems within which theyoperate. A qualitative methodological approach has allowed us to build rich context around thepeople and organizations building and deploying ML technology in industry. We have utilized thisqualitative approach here in order to investigate the organizational tensions that practitioners needto navigate in practice. We describe existing enablers and barriers for the uptake of responsibleAI practices and map a transition towards an aspirational future that practitioners describe fortheir work. In line with earlier organizational research, we emphasize such transitions are not tobe seen as linear movements from one \ufb01xed state to another, rather they represent persistent stepsand coalition building within the ever-changing nature of organizational contexts themselves.",
                    "reference": "[1] Beena Ammanath Rakova, Jieyu Jay Yang, Henriette Cramer, and Rumman Chowdhury. 2021. Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices. arXiv:2006.12358. Retrieved from https://arxiv.org/pdf/2006.12358.pdf?utm_source=morning_brew"
                }
            ]
        },
        {
            "paper_title": "Designing responsible ai: Adaptations of ux practice to meet responsible ai challenges",
            "authors": "Q Wang, M Madaio, S Kane, S Kapania\u2026",
            "publication_info": "Proceedings of the \u2026 - dl.acm.org",
            "paper_url": "https://dl.acm.org/doi/pdf/10.1145/3544548.3581278",
            "chunks": [
                {
                    "id": "615e158e-6fff-4d21-9cbf-389bdae6ab71",
                    "text": "Technology companies continue to invest in eforts to incorporate responsibility in their Artifcial Intelligence (AI) advancements, while eforts to audit and regulate AI systems expand. This shift towards Responsible AI (RAI) in the tech industry necessitates new practices and adaptations to roles\u2014undertaken by a variety of prac-titioners in more or less formal positions, many of whom focus on the user-centered aspects of AI. To better understand practices at the intersection of user experience (UX) and RAI, we conducted an interview study with industrial UX practitioners and RAI subject matter experts, both of whom are actively involved in addressing RAI concerns throughout the early design and development of new AI-based prototypes, demos, and products, at a large technology company. Many of the specifc practices and their associated chal-lenges have yet to be surfaced in the literature, and distilling them ofers a critical view into how practitioners\u2019 roles are adapting to meet present-day RAI challenges. We present and discuss three emerging practices in which RAI is being enacted and reifed in UX practitioners\u2019 everyday work. We conclude by arguing that the emerging practices, goals, and types of expertise that surfaced in our study point to an evolution in praxis, with associated challenges that suggest important areas for further research in HCI. CCS CONCEPTS \u2022 Human-centered computing \u2192 Empirical studies in HCI;KEYWORDS responsible AI; industry practice; UX; interview  INTRODUCTION  1  Technology companies continue to invest in eforts to work toward responsible design and development of AI, responding to increased demand to account for and mitigate the social risks posed by AI technology. As responsible AI (RAI ) eforts become more estab-lished as organizational practices [84], an increasing number of individuals and groups within both the private and public sector are responding to RAI-related research and product needs. Much of the literature focusing on RAI eforts is devoted to un-derstanding and improving machine learning (ML) models, through improvements to data annotation practices [9, 28, 31, 71], model evaluations [32, 56, 88], data and model documentation [36, 47, 83], or building developer-facing tools to help engineers interpret mod-els post-hoc [3, 6, 7, 10, 68, 107]. However, in contrast with a de-velopment paradigm in which the design of ML models occurs in parallel with their use in products, in many cases, ML models are in-creasingly used to power user-facing AI applications developed by wholly distinct product teams\u2014in some cases, by product teams at other organizations [15, 58]. Thus, RAI eforts in industry continue after models leave the research lab.Before deploying AI applications in deployment contexts where they could cause far-reaching societal consequences, many prac-titioners are undertaking RAI work, in more or less visible, and more or less formal, positions. Although prior RAI research in CHI and CSCW has focused on opportunities to intervene in AI devel-opment practices [e.g., 53, 69, 70, 84], it has (with few exceptions) focused on the work practices of data scientists and ML engineers in the model development process, rather than how a wider set of practitioners are involved in applying existing models for novel AI applications. Among this wider set of practitioners applying the models, many focus on user-facing and user-centered aspects of technology (e.g., interaction designers, user experience (UX) designers, or UX researchers). Yet, the specifc practices of such user-centered practitioners involved in RAI have yet to be identifed or formalized, and distilling them ofers a critical view into how they are adapting to meet present-day challenges in RAI. In this paper, we explore an emerging set of RAI practices car-ried out by user-centered practitioners when applying large models during early stages and refnement of AI application design. To understand these emerging user-centered RAI practices, we con-ducted interviews with UX practitioners, who are often involved in early-stage AI application ideation, design, prototyping, and evalu-ation, and with RAI subject matter experts (whom we refer to as \u201cRAI experts\u201d throughout the paper), who often perform evalua-tion and ofer consultation about responsibility in AI projects and products or product features, at a U.S. site of a large technology company that has a large AI function. These two sets of practition-ers were actively involved in addressing RAI concerns, formally by RAI experts and informally by UX practitioners, both early in design and refnement of new, AI-based prototypes, demos, and product features. We aim to identify and understand the emerging RAI work carried out by UX practitioners in their design processes, by investigating these practices in light of the RAI work conducted by RAI experts in their formal job capacity. We thus seek to answer two research questions: RQ 1:  How do UX practitioners currently incorporate and examine RAI considerations during early stages and refnement of AI application design given their organizational context? RQ 2:  What challenges do UX practitioners encounter in their cur-rent RAI practices during early stages and refnement of AI application design given their organizational context? We  conducted  a  refexive  thematic  analysis  of  interviews  with  participants  to  make  the  following  research  contributions:  (1)   An  identifcation  of  three  key  emerging  practices  that  UX  practitioners  are  developing  to  meet  evolving  RAI  needs:  a)  building  and  reinforcing  an  RAI  lens,  b)  responsible  prototyp-ing,  and  c)  responsible  evaluation  of  AI  applications.  (2)   A  refection  on  the  hidden  RAI  work  UX  practitioners  are  carrying  out  and  ways  to  support  this  evolution  in  UX  praxis  moving  forward.  (3)   A  discussion  of  the  implications  of  UX  practitioners\u2019  current  challenges  in  designing  with  ML  models  and  the  need  to  reconfgure  the  role  of  the  user  when  designing  RAI.  We frst situate our study with respect to prior work on RAI work practices in industry contexts, existing research on values and ethics in UX practice, and prior literature on designing and prototyping with ML models. After introducing our study and data analysis process, we identify three emerging RAI practices developed and carried out by UX practitioners in order to adapt existing UX prac-tices to meet evolving RAI challenges. These practices\u2014building and reinforcing an RAI lens, responsible prototyping, and responsi-ble evaluation of AI applications\u2014are not linear, but are embedded  throughout work practices in iterative ways. Within each emerging practice, we situate the RAI work of UX practitioners\u2019 with respect to the RAI work carried out by RAI experts in their formal job capac-ity, and highlight strategies and techniques that UX practitioners adopted, to sensitize people to RAI concerns, to communicate RAI issues with the teams, to consider potential consequences of users\u2019 mental models of AI systems (e.g., over-reliance on AI [cf. 80]), and to surface and mitigate potential RAI issues through an emerging AI prototyping technique called prompt programming. Finally, we present practices that place RAI concerns in direct conversation with traditional user evaluation approaches. We refect on these fndings and what they mean for the evolution of UX praxis in the Discussion, and conclude by highlighting opportunities for further HCI research to support the work of designing responsible AI. 2   RELATED  WORK  2.1   Responsible  AI  Practices  Among  Industry  Practitioners  A growing body of work in HCI examines the work practices of industry practitioners as they address RAI issues during the design and development process [53, 67, 69, 70, 84]\u2014in contrast with prior work that has studied data annotation [9, 28, 31, 71] and model evaluation [32, 56, 88], or developed resources for data and model documentation [36, 47, 83] or post-hoc interpretation of model performance [3, 6, 7, 10, 68, 107]. Prior work has found that AI prac-titioners seek to uncover fairness issues prior to deployment [53], yet many RAI approaches or mitigations are reactionary, initiated as a result of public relations issues, media attention, or customer complaints [53, 69, 84].  intervening Practitioners  have  emphasized  in  training datasets  [31,  32,  53],  as  well  as  algorithmic  mitigations  [2],  as critical in working toward RAI. However, AI practitioners fnd it challenging to represent diverse demographic groups in datasets and  in  fairness  assessments,  given  that  practitioners  tend  to draw  on  their  (often  homogenous  [106])  personal  experiences and perspectives when it comes to RAI [30, 53, 69]. Madaio et al. [2022] also note that the heuristics practitioners used to determine priorities  in  assessing  fairness  issues  (e.g.,  perceived  fairness severity, perceived brand impact, etc.) could compound existing inequities of AI systems [69]. Given these challenges, practitioners call  for  more  guidance  and  support  (e.g.,  practices,  tools,  and other resources) in working toward the design, development, and deployment of RAI [53, 69, 84]. To  fulfll  practitioners\u2019  needs  for  RAI  support,  a  plethora  of toolkits, guidelines, and other resources have been developed [e.g. 1, 4, 7, 10, 54, 68, 72]. Recent work has explored the sociotechnical practice of how RAI toolkits are integrated into practitioners\u2019 work-fows [30, 70, 73, 114]. For example, Deng et al. [2022] point out that these toolkits are often less prescriptive than required\u2014containing guidance on what to do (e.g., engage stakeholders), but not how to carry out those recommendations [30]. In addition, many RAI toolk-its are also designed to primarily support specifc types of technical work from technical practitioners to address RAI issues, rendering themselves less accessible to contributors with varying types of expertise [30, 114]. Relying on toolkits may also enact a form of techno-solutionism, by promoting or incentivizing solely technical solutions to the sociotechnical work of responsible AI [70, 90, 114]. Existing literature has also highlighted the role of organizational factors in RAI practices [53, 69, 70, 84], including the lack of organi-zational incentives to address RAI issues (or the disincentives to this work) [53, 70, 84]. Coupled with the lack of clarity over roles and responsibilities in RAI work [72, 84], addressing RAI issues during AI development often relies on individuals who are able to dedi-cate time to developing and promoting RAI processes\u2014processes that may or may not be formally adopted within the rest of the organization [70, 84]. Identifying, assessing, and mitigating RAI issues in datasets, algorithms, and model behavior often require practitioners (and their organizations) to invest signifcant time and resources, yet AI practitioners often face time and resource constraints in carrying out their RAI practices (including incen-tives to ship products on fast-paced timelines) [30, 53, 69, 84, 112]. Many scholars have called for changes in organizational structures and processes, to support practitioners\u2019 RAI practices instead of hindering them [53, 69, 70, 72, 84]. 2.2   Values  and  Ethics  in  UX  Practice  Although recent work has focused on AI practitioners\u2019 work prac-tices for RAI, substantial prior research has identifed the ways that values are instantiated in technology design more broadly [40, 57, 79, 103, 110], as well as through ongoing practices of technol-ogy use, appropriation, maintenance, and repair [e.g., 55]. As such, various methods, tools, and theoretical frameworks [e.g., 5, 24, 42\u2013 44, 92, 115] have been developed to support designers (including UX practitioners) in surfacing relevant values throughout the de-sign process and bringing those values to bear on design decisions (what JafariNaimi et al. [2015] refer to as the \u201cidentify/apply\u201d logic of values in design) [57]. For instance, Value-Sensitive Design (VSD) is a framework in-tended to support designers in understanding the role of specifc values in the design of technology, including how they might be promoted or undermined through a given system [42, 44]. In addi-tion, numerous other methods and tools have been developed to support designers\u2019 \u201cvalues work\u201d in technology design [112], in-cluding resources to identify relevant values and surface potential harms of technology, such as Envisioning Cards [43], Judgment Call [5], Timelines [115] and more (see Chivukula et al. [2021] for a review). Although VSD and related methods are not the focus of our work in this paper, they represent a key aspect of the training and resources that may inform how UX practitioners address values as part of the work involved in addressing RAI. 2.2.1  Situated  Work  Practices  of  UX  Practitioners.  As  part  of  a broader turn to practice within HCI scholarship [66], recent re-search has focused not only on developing methods and tools for designers to use, but has also explored the situated work practices of UX practitioners in engaging with values as part of their every-day work [e.g., 23, 25, 50, 112, 113]. For instance, Chivukula et al.  [2021] have identifed a set of \u201cidentity claims\u201d that UX practition-ers articulate for the various roles they take on as part of doing values work in their practice, including learner and educator (i.e., learning and teaching others on their team about ethics), translator (i.e., taking resources about values in one domain and translat-ing them to their own work), and advocate/activist, among others. Situating individual UX practitioners within larger social and orga-nizational contexts, Gray and Chivukula [2019] describe various factors that mediate the relationship between designers\u2019 individual ethical awareness and action, including the role of organizational practices [25, 50]. This body of scholarship has identifed the social and political work that UX practitioners engage in, in addition to their technical work; including rhetorical work to convince leadership of the value of UX (and of the importance of values work in UX) [e.g., 87]\u2014an issue of critical importance given the relatively lower status of UX and design compared with software engineering roles in large tech-nology companies [112]. Given the role that organizational factors play in mediating UX practitioners\u2019 values work in practice, Wong [2021] has identifed tactics for \u201csoft resistance\u201d that UX practi-tioners engage in to create space to address values in their work. In part, this involves making values visible and relevant to others in their organization (similar to the advocate or activist identity described by Chivukula et al. [2021]), as well as working to change organizational norms and practices from within, by tactically ex-panding the defnition of who is considered to be the \u201cuser\u201d [cf. 116], through leveraging (and trying to change) organizational goals and priorities (e.g., Objectives and Key Results, or Key Performance Indicators), or more broadly leveraging corporate logics to make a business case for values in UX [112]. However, as Wong [2021] point out, each of these tactics are par-tial and embody contradictions, in that they are trying to challenge, contest, or change organizational practices, while leveraging those same logics and discourses. Finally, as Wong [2021] identify, the values work of UX practitioners often involves \u201cwork outside of the technology design process,\u201d work that often involves substantial emotional labor [cf. 96], and which may not be valued as part of their everyday work [113]. Relatively little literature, however, has explored the values work that UX practitioners engage in as part of designing and developing AI applications, or how that work is aligned with the work practices of RAI more generally. 2.3   Designing  and  Prototyping  with  Machine  Learning  Models  Prior research has studied UX practitioners\u2019 current practices and challenges when designing and prototyping ML-powered AI appli-cations [38, 117, 119, 120, 122]. Existing work has found that UX practitioners, given their training in HCI and User-Centered De-sign (UCD), commonly leverage traditional HCI and UCD methods and toolkits when designing with AI, yet these are often insuf-fcient [117, 120, 122]. For example, traditional methods such as Wizard-Of-Oz, sketching, paper prototyping, and rapid prototyp-ing can fail to accommodate the non-deterministic behaviors and opaque mechanisms of AI systems [117, 119, 122]. Prior work has highlighted the need for high-fdelity prototypes that can generate tangible, realistic behaviors, instead of toy sce-narios, to elicit user feedback and better assess the potential human and societal impacts of AI applications [54, 119, 122]. Zdanowska and Taylor [2022] note that UX practitioners found that most HCI and UCD methods and tools place too much emphasis on the design and evaluation of the user interface, which is only one of many components when designing with AI [122]. Users\u2019 mental models of the AI system [120, 122] and the feasibility and user acceptance of the design [122] are also key considerations for UX practitioners. When designing and prototyping with AI, UX practitioners also frequently collaborate with technical experts such as engineers and data scientists [82, 97, 118] to gain a deeper understanding of the capabilities and limitations of the models [38, 117, 119]. While high-level abstractions are sufcient for UX practitioners to design AI applications [118, 122], a deeper understanding of ML model func-tionality could help UX practitioners better envision use cases that may not yet exist [38, 119]. However, this collaboration also poses challenges in that UX practitioners and engineers don\u2019t always share a common perspective, language, or workfow [82, 97, 119], leading UX practitioners to take on extra work to bridge disciplinary boundaries through sharing user stories and raw user feedback from user testing video recordings to help engineers understand user needs [98], and sometimes, adapting to and embracing a more data-centric culture to communicate user needs through both qualitative and quantitative metrics [118]. 2.3.1  Prototyping with Prompt Programming.  To meet the increas-ing demand for new UX prototyping and design tools when de-signing with AI, an emerging practice involves prototyping with ML models through prompt programming [20, 27, 60, 121]. Using prompting techniques [27, 60], UX practitioners are able to send natural language prompts to ML models as inputs and interact with the models directly to test-drive their capabilities and limita-tions [60]. Prior work has found that prompt-based prototyping with large language models (LLMs) helps UX practitioners reduce their reliance on engineers and developers to understand model ca-pabilities, speed up the prototyping process to test out initial ideas and \u201cfail fast\u201d [cf. 117], and better communicate with collaborators using prototypes as boundary objects [60]. Prompt programming is usually conducted with pre-trained, large-scale models such as LLMs [27, 60] and text-to-image gen-eration models [124], which are prone to generate outputs that may perpetuate social stereotypes, toxicity, discrimination, and exclusionary norms [15, 33, 48, 105]. Researchers have been ex-ploring ways to evaluate LLMs and other generative models prior to putting them into use. Common evaluation methods include manually generating general test cases, or tests targeted at specifc failure modes [59, 86], as well as automatically generating test in-puts using the model itself [46, 81]. However, others have noted that such \u201cbehavioral tests\u201d and use of benchmarks to prompt models to intentionally generate harmful outputs (so they can be prevented) often come with pitfalls that render these methods invalid [14]. As such, there are increasing calls for human-centered approaches from HCI and UX practitioners to support the work of identifying and mitigating RAI issues with LLMs [e.g., 12\u201315]. Yet, insights into  how prompt programming can facilitate or hinder UX practitioners\u2019 work on RAI has not been explored. 3   METHODS  3.1   Recruitment  To  investigate  our  research  questions,  we  conducted  semi-structured interviews with both UX practitioners (n = 15) and sub-ject matter experts in a designated RAI role (n = 8). We recruited participants through snowball sampling at our study site (via direct emails to contacts). Our study site was chosen due to the company\u2019s large AI function, as well as the depth of researcher access that could be achieved to participants\u2019 work practices and teams. All participants were recruited from the same company that the au-thors were employed at during the time of the study, a decision we discuss further in section 3.2, below. Inclusion criteria included UX practitioners who had worked on or were currently working on the design, prototyping, user research, or user evaluation of AI applica-tions (prototypes, demos, product features) that were powered by large-scale models. We specifcally sought out UX practitioners who were either directly or tangentially involved with addressing RAI concerns as part of their work with these applications. For RAI experts, our inclusion criteria included experts in a formal Respon-sible AI role, who had experience evaluating or being consulted about responsibility in AI projects and products or product features. Information about participants\u2019 job roles can be found in Table 1. UX participants had an average of 5.6 years experience at the com-pany (SD=2.3), an average of 11.8 years working in UX (SD=7.3), and an average of 7.8 years working with AI (SD=4.4); RAI expert participants had an average of 2.3 years working at the company (SD=1.6), an average of 2.4 years working in RAI (SD=1.2), and an average of 5.9 years working with AI (SD=2.6). We also asked participants to optionally share their gender identity: among UX participants, seven were women, eight were men, and one was non-binary. Among RAI experts, fve were women, two were men, and one was non-binary. Each participant was compensated through a donation to their charity of choice valuing $40 USD. 3.2   Data  Collection  Our study ran from June through July, 2022. All of the participants worked in the United States, in hybrid or fully remote roles at the time of study; hence, all interviews were conducted virtually through an internal virtual meeting platform. Except for two 30-minute interviews, and one 90-minute interview, all interviews lasted about 60 minutes. All participants provided written consent to participate in the research study before interviews began. Scoping the present work to a specifc technology company as a research site allowed us to take advantage of internal AI resources (described below in 3.2.3) to ground interview discussions. As researchers were also company employees, participants could provide more details  of  their  AI  projects  and  context  for  their  RAI  practices, while upholding confdentiality and IP protection. However, we Table 1: Interview participant information. UX participants worked across diferent AI product and research areas. Listed AI areas are based on the specifc projects/products mentioned in interviews. For RAI expert participants, we list their background. did not limit interview discussions to participants\u2019 current work experiences; many had experience working in multiple settings and companies, and we prompted them to refect more broadly on their experiences over the course of their careers. Given the sensitive nature of RAI topics, we also sought to foster a high level of transparency and trust in our communications with each practitioner, before, during, and after the qualitative interview. For example, we held pre-interview calls and shared research briefs with our participants to answer questions about the research, and shared our data interpretations and fndings back with each partici-pant by email, noting their specifc quotes in the paper and asking for any feedback before submission. To inform our research questions, we frst observed a conversa-tional AI design sprint. We did not include our observations as research data, but instead followed up with four sprint participants to enroll them in our interview study. We also studied any artifacts that were provided in the interview, including sprint artifacts dis-cussed. We discuss each interview protocol below, and they are each provided in Supplementary Materials. We also describe a prompt programming tool called PromptMaker [60] below, which we used as a probe during interviews with both UX practitioners and RAI experts. 3.2.1  UX Practitioner Protocol.  In the interviews with UX practi-tioners, we asked them to broadly describe the types of UX work they do related to AI-based prototypes, demos, and/or product fea-tures, and then to dive into more details by having them walk us through one or two specifc projects they had worked on. We specif-ically focused on how RAI issues surfaced in their work, mitigation strategies or precautions they used to address RAI issues, and how RAI issues might have infuenced the project direction. At the end of the interview, we also asked UX practitioners about their thoughts on how RAI processes for early-stage AI application design could be improved.  3.2.2  RAI Expert Protocol.  In our interviews with RAI experts, we asked participants in designated RAI roles about their experi-ences reviewing, analyzing, evaluating, and/or consulting on AI technologies as part of organizational RAI practice. We then asked the experts to walk us through an AI project that they had been involved with, which drew on their expertise, the RAI issues that were present, and how they worked with the project/product teams to address those RAI concerns. Towards the end of the interview, we also asked the experts to envision possible improvements to current RAI practices and possible processes or tools to support RAI in early-stage AI application design. 3.2.3  PromptMaker As a Probe.  During interviews with all par-ticipants, we introduced and described an LLM prototyping tool called PromptMaker, as described in Jiang et al. [2022]. Prompt-Maker provides a web-based interface to LLMs, enabling users to interactively write and test LLM prompts. PromptMaker also en-ables practitioners to remotely execute a prompt. For example, a basic prompt to translate English into French could be created (see example in footnote), then embedded within a prototype to test out a translation feature. Collectively, PromptMaker\u2019s capabilities enable practitioners to rapidly prototype and test new AI features in hours or days, without requiring signifcant machine learning experience. Many of the interview participants were familiar with Prompt-Maker and seven UX participants indicated in the interviews that they use PromptMaker in their daily job during early-stage AI appli-cation design. In our interviews, we used PromptMaker as a probe to understand how emerging AI design and prototyping tools were shaping UX practitioners\u2019 RAI practices and to understand the po-tential opportunities and challenges new AI design and prototyping tools present for RAI. 3.3   Data  Analysis  All interviews were video-recorded and later transcribed verbatim for data analysis purposes. To analyze the interview data, we drew on Braun and Clarke\u2019s [2019\u20132021] refexive thematic analysis ap-proach [17\u201319]. Refexive Thematic Analysis is a post-positivist approach that emphasizes researchers\u2019 role in knowledge produc-tion, including the philosophical stance and theoretical assumptions that they make in informing their data analysis approach [18]. This difers from other qualitative data analysis approaches such as con-structing codebooks and establishing inter-rater reliability metrics, which might not ofer the fexibility needed for researchers to ac-tively participate in the analytic process in a systematic and rigorous way [18]. A refexive thematic analysis approach also encourages researchers to collaborate and discuss interpretations throughout the process to facilitate the generation of themes. Five  authors  participated  in  the  interview  data  analysis  pro-cess and continuously and collaboratively discussed the codes and themes  throughout.  We  followed  the  analysis  process  outlined in Braun and Clarke [2006]. First, all fve researchers familiarized ourselves with the data by reading through the transcripts and taking notes. We then began generating initial codes and divided the transcripts among the fve researchers\u2014each interview tran-script was read and used in initial code generation by at least two researchers. Each researcher reviewed nine to 11 transcripts at this phase and each generated hundreds of open codes. After initial codes were generated, we frequently met to search, review, discuss, and defne themes based on initial codes. In the early stages of our codes-to-themes process, we generated four domain categories (e.g., early-stage RAI challenges, RAI conceptualizations, RAI strategies and practices, aspirational RAI and improvements) and 53 prelim-inary themes through continuous discussions and iterations. All data was analyzed using shared spreadsheets and text documents. In parallel, two authors reviewed artifacts provided by participants, such as design sprint materials, framework documents, and user study fndings, to understand practices associated with early-stage AI application design. After discussing observations together, they shared them with the entire study team. After  further  review  and  discussion  of  all  observations  and themes, we distilled three emergent RAI practices: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications, each of which comprises two high-level themes, which we present in the Findings section. 3.4   Author  Positionality  Our author team is comprised of researchers with both academic and industry research backgrounds, with varied professional expe-riences that shape our perspectives. All researchers were employees of the company that served as the research site during the research period. One author has experience in an industry UX role at a large technology company. Two authors have experience developing applications that incorporate large-scale ML models. Two authors were born in and are currently, or have previously, lived in APAC countries, and four identify as white Americans. All authors completed the bulk of their research training, and work in, predominantly Western institutions. Five identify as having experience with marginalization in computing, either as a member  of a marginalized group themselves and/or through many years of conducting HCI research with marginalized groups. The authors\u2019 background and experiences infuence our posi-tionality: as HCI researchers trained and working in predominantly Western organizations, we acknowledge that complementary schol-arship related to our research questions is needed, to extend and further the understandings presented in this paper. Our position-ality has also infuenced the subjectivity inherent in framing our research questions, a snowball sampling approach that makes use of our professional networks, the study protocol design, and our data interpretation and analysis. 4   FINDINGS  Through our data analysis, we identifed key emerging practices that UX practitioners carried out to meet evolving RAI needs. These practices are not linear, but are embedded throughout their work in iterative ways. In this section, we present three emerging RAI prac-tices: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications. For each practice, we frst introduce how RAI experts (labeled with \u2018R\u2019 throughout) and UX practitioners (labeled with \u2018U\u2019 throughout) enact and oper-ationalize RAI. We then highlight UX practitioners\u2019 RAI practice in light of the RAI work conducted by RAI experts in their formal job capacity, and present how their practices, goals, challenges, and aspirations for the role of UX in RAI take shape. 4.1   Building  and  Reinforcing  an  RAI  Lens  We begin by highlighting an overarching practice that infuencesour  subsequent  fndings:  the  importance  of  seeing  responsibility  as  a  \u201clens\u201d.  An  RAI  lens  positions  RAI  as  a  refexive,  ongoing,  and  holistic  perspective  that  infuences  practices  and  decisions  throughout  de-sign  and  development,  as  they  arise  in  context.  It  is  thus  not  bound  to  a  specifc  artifact,  protocol,  or  type  of  analysis  of  data  or  model  outputs\u2014though  it  can  incorporate  them.  Instead,  it  represents  an  ongoing,  shared  mindset  that  acknowledges  and  seeks  to  account  for  the  social  position  of  those  who  shape  technology\u2014and  the  company  itself\u2014and  the  intersecting  relationships  between  specifc  design  and  development  choices  and  their  societal  implications.  As  such,  both  RAI  experts  and  UX  practitioners  went  to  great  lengths  to  cultivate  and  reinforce  an  RAI  lens,  not  only  in  their  work,  but  in  their  teams  and  the  broader  culture.  This  practice  was  carried  out  explicitly  by  RAI  experts  who  inhabited  established  and  prominent  RAI  positions  with  focused  RAI  expertise  (e.g.,  ethics,  philosophy,  law)  in  the  company,  and  implicitly  by  UX  practitioners  who  considered  RAI  as  an  emerging  yet  crucial  piece  of  the  UX  of  AI  projects/products.  Below,  we  characterize  the  work  both  groups  of  practitioners  carried  out  to  pursue  this  goal  of  an  RAI  lens:  from  self-education  and  sensitization  to  RAI  issues,  to  actively  communicating  and  working  with  teams  to  surface  and  mitigate  RAI  issues.  Both  UX  practitioners  and  RAI  4.1.1   Sensitizing  to  RAI  Concerns.  experts  were  often  called  upon  by  product  teams  to  address  RAI  issues:  UX  practitioners  given  their  expertise  in  accounting  for  the  impacts  of  technology  on  users;  RAI  experts  given  their  formal  job  positions  and  expertise  in  RAI  and  ethics.  While  RAI  is  not  part  of  UX  practitioners\u2019  formal  training  and  role,  by  accumulating  experience  and  knowledge  of  technologies\u2019  impact  on  users,  RAI  had  already  been  integrated  into  many  UX  practitioners\u2019  day-to-day  practices  and  became  what  UX  practitioners  described  as  a  lens:  \u201cWith  my  lens,  I  make  [RAI  issues]  surface,  so  I\u2019m  not  sure  that  [RAI  issues]  would  just  naturally  bubble  up  to  the  surface.  But  I  think  part  of  my  research  ethos  is  to  look  for  those  gaps  and  those  red  fags.\u201d(U7)  For  RAI  experts,  a  large  part  of  their  job  included  sensitizing  [cf.  16]  teams  across  the  company  to  RAI  concerns  by  ofering  RAI  resources  and  support  such  as  RAI  reviews,  ofce  hours,  consul-tations,  and  RAI  workshops.  When  asked  about  their  processes  of  RAI  review  and  consultations,  many  RAI  experts  mentioned  the  usage  of  RAI  frameworks  and  guidelines,  many  of  which  they  (or  their  colleagues)  had  developed.  RAI  experts  not  only  used  these  RAI  frameworks  and  guidelines  to  sensitize  teams  to  RAI  consider-ations,  but  also  to  help  themselves  be  more  aware  of  potential  RAI  issues  in  their  work.  For  instance,  R6,  R8,  and  their  colleagues  are  creating  an  AI  harms  framework,  outlining  domain-specifc  RAI  considerations,  and  best  practices  to  help  engage  practitioners  with  RAI  issues.  R8  told  us:  \u201cI\u2019ve  found  [the  AI  harms  framework]  to  be  pretty  helpful  for  me  because  even  if  I  do  this  all  the  time,  I  can  still  forget  about  one  of  these  possible  negative  implications  that  could  happen.  But  teams  also  like  it  because  they  tend  to  be  new  to  these  ideas,  so  it\u2019s  helpful  for  them  to  just  be  like,  here  is  the  landscape  of  things  that  could  happen  in  a  pretty  digestible  format.\u201d  These  RAI  frameworks,  guidelines,  and  best  practices  established  by  RAI  experts  were  commonly  referred  to  by  UX  practitioners  during  their  interviews.  Given  that  RAI  was  not  part  of  their  formal  training  or  job  requirement,  to  sensitize  themselves  to  RAI  consid-erations,  UX  practitioners  often  explicitly  and  actively  sought  out  these  internal  RAI  resources  as  well  as  external  literature  to  better  understand  the  issues  and  integrate  them  into  their  work.  However,  a  few  UX  practitioners  also  mentioned  building  frame-works  from  scratch  to  meet  the  specifc  needs  of  an  application  domain.  Similar  to  RAI  experts\u2019  practices,  many  of  the  UX  prac-titioners  in  our  interviews  talked  about  their  eforts  to  compile  their  RAI  knowledge  and  experience  into  actionable  RAI  guidelines  and  best  practices.  Some  UX  practitioners  even  integrated  RAI  into  their  standard  practices  and  design  pipeline:  \u201cOne  of  our  team\u2019s  RAI  standard  practices  is  just  how  we  collect  data,  that  we\u2019re  putting  in  a  process  on  collecting  data  fairly  [...]  [F]olks  have  built  out  a  much  more  rigid  pipeline  for  how  we  collect  that  stuf  [...]  it\u2019s  just  part  of  the  standardized  practice  now.\u201d  (U14).  To  further  reinforce  their  RAI  perspective  during  the  design  pro-cess,  UX  practitioners  implemented  responsibility  lifts,  a  series  of  activities  at  the  beginning  of  the  design  process  to  reinforce  RAI  as  a  lens  to  inspect  and  flter  design  ideas.  U4  talked  about  a  design  sprint  on  ideating  about  AI  products  powered  by  LLMs:  \u201c[RAI]  was  called  out  in  the  brief  of  this  sprint  as  a  whole  to  think  about  responsibility  for  any  ideas  that  we  come  up  with.  And  what  are  the  responsibility  implications  of  those?\u201d  Similarly,  U14  who  organized   the  conversational  AI  design  sprint  and  invited  internal  guest  speak-ers  to  talk  about  RAI  at  the  beginning  of  the  sprint,  explained  his  rationale  during  the  interview:  \u201cEven  just  doing  stuf  like  that  [having  an  RAI  lightning  talk  at  the  beginning  of  the  sprint],  [...]  having  a  space  put  in  to  any  workshop  moving  forward,  or  any  team  doing  this  kind  of  work  with  AI  and  everything,  you  make  the  space  for  someone  to  give  a  presentation  like  that,  where  it  can  at  least  put  those  ideas  in  the  designer,  and  the  prototyper,  and  the  engineers\u2019  brains,  so  it  is  at  least  top  of  mind  [...]  you  make  space  for  responsible  design  and  thinking  when  you\u2019re  in  the  thick  of  it.\u201d  4.1.2   Organizational  Challenges  of  Communicating  about  RAI  with  Teams.   As  RAI  is  a  rather  new  and  still-emerging  discipline,  those  with  expertise  are  limited,  and  practitioners  with  demonstrated  expertise  are  often  called  upon  to  help  shepherd  projects  through  the  examination  of  RAI  considerations  and  mitigation  strategies.  Many  RAI  experts  and  UX  practitioners  in  our  study  were  members  of  centralized  teams  and  were  often  positioned  to  apply  RAI  exper-tise  horizontally.  As  such,  both  RAI  experts  and  UX  practitioners  described  \u201cdropping  in\u201d  and  then  out  of  specifc  teams  to  conduct  RAI  work  in  particular  phases.  Due  to  this  form  of  centralized  organizational  structure  (rather  than,  for  instance,  embedded  RAI  experts  and  UX  practitioner  with  permanent  roles  on  a  single  AI  team),  many  RAI  experts  and  UX  practitioners  thus  invested  signifcant  time  in  sensitizing  members  of  the  teams  they  were  \u201cdropping  in[to]\u201d  to  RAI  considerations.  Oftentimes,  RAI  experts  and  UX  practitioners  joined  AI  projects  that  were  still  relatively  early  in  development,  but  which  had  already  begun,  making  it  more  difcult  to  reverse  decisions  that  had  already  been  made  or  shape  the  design  direction  in  fundamental  ways,  particularly  as  they  worked  to  understand  and  navigate  the  power  dynamics  among  the  AI  team  with  whom  they  were  working.  Both  RAI  experts  and  UX  practitioners  talked  about  how  the  norms  and  implicit  values  associated  with  the  larger  organizational  culture  incentivized  \u201cmoving  fast\u201d  and  emphasizing  positive  out-comes  of  AI  systems  [cf.  53,  70,  84].  This  remained  a  challenge  even  for  RAI  experts  who  were  in  formal  positions  as  RAI  reviewers  or  ethics  consultants  that  had  more  power,  sometimes  with  blocking  power,  over  project/product  directions  and  releases.  In  their  inter-views,  many  RAI  experts  said  they  mostly  worked  with  teams  that  voluntarily  reached  out  to  them  for  RAI  consultations  and  reviews.  These  teams  tend  to  be  more  open  to  suggestions  and  critiques  that  RAI  experts  brought  up  during  the  process.  Many  RAI  experts  emphasized  teams\u2019  openness  in  discussing  RAI  and  initiatives  in  making  changes  as  paramount  when  working  with  the  teams  on  RAI  issues:  \u201c[RAI  consultations]  is  very  much  conversation-based.  And  we  steer  the  conversations.  But  the  important  thing  is  that  the  interest  has  to  come  from  [the  product/project  teams].  It  has  to  be  that  they  want  to  change  their  beliefs  and  behaviors.  We  can  try  and  impose  it,  of  course.  It\u2019s  always  better  if  this  kind  of  culture  change  comes  internally  rather  than  externally.\u201d  (R3)  However,  this  was  not  always  the  case  for  UX  practitioners  who  had  less  power  to  sway  the  project  directions  over  potential  RAI  concerns. UX practitioners also worked closely with the teams on the design and development of the products on a day-to-day basis and faced the same time constraints alongside the product teams. In the face of these pressures, UX practitioners described using a combination of communication techniques and product team activities to reconcile these incentives for speed with the introduction of new, more intentional RAI processes that encourage refection on a range of potential outcomes and harms of AI systems. Here, UX practitioners emphasized the importance of communi-cating potential RAI issues strategically and presenting themselves in a supportive role, rather than being seen as a \u201cblocker.\u201d U8 told us, \u201cMaking it as blameless as possible is one of the best things we\u2019ve learned. I\u2019m not trying to tell anyone they\u2019re bad, I\u2019m not trying to freak anyone out. I more want to highlight, here\u2019s something that could go wrong. Here are the ways that people could be afected, here\u2019s the ways the business could be afected. You can choose to act on that or not, but I would strongly advise you to do so.\u201d In the face of organizational pressures to ship products rapidly, UX practitioners took on additional hidden work to sensitize their team and organizational leadership to the potential harms of AI systems and the importance of mitigating those harms prior to deployment\u2014crucial labor that was not always recognized as being core to their work by their organization. When not directly negotiating with team members or organi-zational leadership about RAI concerns, UX practitioners would sometimes create activities or documents meant to enable team members to respond to RAI issues. One strategy included showing potential impacts through user testing that surfaced concerns, or by creating artifacts that illustrate potential harms. For example, U12 had difculty getting their team to respond to potential concerns and ran a team activity to create fake newspaper headlines [cf. 115] to illustrate how things could go wrong: \u201cI had put together a deck of like fake headlines of how this could go wrong. [...] I think at the time I think it had a big efect. People backed of the idea, we didn\u2019t have to go any further with it.\u201d This activity and others like it were part of the additional work that UX practitioners took on to sensitize others in the AI teams they worked with (and the company more broadly) to the range of potential harms from AI systems. 4.2   Responsible  Prototyping:  Ideating  and  Building  with  Machine  Learning  Models  During the interviews, both sets of practitioners described how they understood RAI in terms of anticipating and surfacing harms that AI technology could bring to the end-users, society, and the public. They both brought up similar sets of harms in their interviews that they tried to anticipate and surface, which included, but were not limited to: safety, misinformation, inequity and/or culture and iden-tity erasure, stereotyping, over-reliance on AI, anthropomorphism of AI, toxicity and/or ofensiveness, and privacy. However, RAI experts and UX practitioners approached this operational defnition of RAI diferently based on their respective methods and perspectives. In their interviews, RAI experts told us that given their expertise and the nature of their jobs, they were often invited by the teams to explicitly and intentionally look for RAI issues and concerns, often through question-asking and adversarial testing. As R8 described, \u201cI think the vast majority of  people are not thinking adversarial-y. I\u2019m here to think about the worst of the worst. That\u2019s what I was hired to do.\u201d When asked about how they helped the teams to surface harms, R6 said, \u201cIt\u2019s honestly just asking questions. This job is mostly just knowing what questions to ask. It\u2019s just critical thinking, and it\u2019s helping other teams think critically about their projects. We\u2019re always asking, what is the worst-case scenario?\u201d In contrast, while UX practitioners were not trained or required to surface and anticipate potential AI harms in their day-to-day work, we found that UX practitioners were committed to surface and anticipate harms from their unique human-focused perspective and with their unique skills, methods, and tools during their design and prototyping process. In this section, we describe how UX practitioners anticipated potential harms by envisioning how user interface design decisions could infuence the users\u2019 mental model of AI, and how they at-tempted to surface harms by leveraging both traditional and new approaches to design and prototyping. 4.2.1  Considering the Consequences of Users\u2019 Mental Models of AI.  During the interviews, UX practitioners told us that due to the stochastic nature of ML models they typically work with (in particular, generative language models, which often include some stochasticity to aid in producing variety in the model outputs), they were often concerned about how users would perceive AI applications driven by these models. In part, UX practitioners were concerned that users might overestimate the capabilities of the language model or treat the model as if it were human-like (i.e., anthropomorphizing it [80, 104]). UX practitioners felt responsible for appropriately communicating model capabilities to users, both through how they designed the user studies as well as the design of the application interface. For example, U1 discussed the potential misinterpretations of AI technologies such as LLMs: \u201cI think one of the other things about this technology [LLM] that is like, really a misnomer, is that, a lot of people see it as this, like general-purpose chatbot frst and then, \u2018oh, it can do all this other stuf.\u201d\u2019 To mitigate the potential consequences of people\u2019s mental mod-els of AI technology, UX practitioners leveraged diferent design strategies and techniques. During the interviews, UX practitioners mentioned techniques such as putting constraints on user input and model output, to limit model behavior (e.g., preventing it from generating answers to of-topic user questions) in AI applications; designing LLM-driven AI applications that are not chatbots to ex-pand people\u2019s understanding of LLMs; or changing the appearance of the UI of a chatbot application when they discovered users\u2019 expec-tations of an LLM-based chatbot didn\u2019t match the actual language interaction: \u2018So we had to reconfgure how we show these prototypes visually and also how we communicate about what these prototypes are. And that [higher-fdelity UI] was vital to change up. So the team quickly responded and changed the way that it looked, and they changed it from looking like a product to looking more like, terminal, like a very simple, old-school Lo-Fi terminal chatbot.\u201d (U7). Other UX participants brought up the need to build features and functions to allow users to dig deeper into the AI technology\u2019s ca-pability, to improve their understanding of the model. For example, U14 said \u201cProviding the insight for the user on what features and capabilities are possible helps expand the user\u2019s mental model of how they can interact with the system. That all develops trust [built on] knowing what\u2019s possible and what they can use the system for.\u201d During her interview, U10, a UX researcher, also refected on the UX practitioners\u2019 role and obligation in design to help users with low technology literacy understand the AI system and avoid overes-timating the model\u2019s capability or erroneously anthropomorphizing the LLM: \u201c[W]e might have many users who don\u2019t even know what AI or ML is. And so there were defnitely com-ponents that I think [raised] a very big, open question for me, in interviews where I asked people how they thought it worked, and they\u2019d be like, \u2018There\u2019s a person that\u2019s looking at these, and they get back to you really quickly.\u2019 And so, there\u2019s this question about what obli-gation, if any, do we have to correct that misconception, and what potential downstream harm could having that misconception lead to, and what might onboarding to a system like this look like for people who have lower tech literacy? [...] How do you help people understand that this is an AI system and what that means?\u201d 4.2.2  Examining ML Models through \u201cTest-Driving\u201d.  A large part of UX designers\u2019 job during early-stage AI application design, besides representing the user perspective as part of standard UX practice, was to understand the potential harm and capability of the ML model underlying the AI application. To do this, UX participants described how they supplemented traditional UX design and proto-typing methods with novel emerging methods to surface potential RAI issues that traditional methods may not have been able to uncover. Several participants mentioned using traditional UX design meth-ods such as Wizard-of-Oz studies or toy examples to quickly test out their AI design ideas. However, they also pointed out that these traditional design methods are fawed, as their ability to surface real-world RAI issues is limited. The scope, time frame, and re-searcher supervision constraints of typical user studies don\u2019t allow users to bring in situ, authentic personal data, needs, and use cases to bear on model interaction (U3). In part, the organizational factors that we described in section 4.1.2 impact UX practitioners\u2019 ability to conduct more longitudinal studies of the harms of AI systems in a more ecologically valid context. To work around those con-straints, UX practitioners used toy examples they believed to be representative of usage scenarios. However, as described by U10, the use of toy examples during user studies makes it difcult to surface RAI issues based on real-world usage: \u201c[O]ne of the hardest parts of prototyping a tool like this, is that it\u2019s a highly personal experience when you have a health condition, and your level of anxiety might be incredibly high or your level of investment in fnding relevant information may be way higher than you can get in an actual study [using toy examples]. And so, that\u2019s been a big challenge for us, is reading the tea leaves a little bit...\u201d  to interact with the ML models directly. For example, the Prompt-Maker tool we provided in the interview was already used by several UX participants. During prototyping activities that involved model prompting, UX practitioners performed what many referred to as \u201ctest-driving,\u201d which refers to the activity of continuously probing model outputs using diferent natural language prompts (that serve as input to the model) to understand model capabilities and limita-tions, in order to determine ft between a design idea and model capabilities. UX practitioners considered good prompt design to be an \u201cart form,\u201d in that it was difcult to identify reasons why prompt construction led to certain model outputs, making repeatable best practices and rules hard to distill. U5 recounted: \u201cI think it\u2019s [PromptMaker] a great tool to \u2018scratch pad\u2019 with, to move really quickly to just try something new. But generally, prompting is really weird. It\u2019s a kind of weird art form. Little weird things like just having one space key at the end of your prompt can change the complete output of what comes out. That\u2019s really subtle and hard to have somebody to understand what that means. So it\u2019s a useful tool, I love it, it was the most accessible thing I think that we\u2019ve created so far. But it\u2019s not just self-service yet, or intuitive enough on its own.\u201d This lack of consistency and intuitiveness of model prompting made it challenging to come up with a consistent practice to ad-dress the need to test-drive model interaction during early-stage AI application design. During his interview, U3 also raised the issue that prompting could further introduce bias, requiring intentional eforts to avoid \u201ccodifying our own belief systems. I think what hap-pens is the minute you get a new user that starts to ask questions to your system, then we need diferent sets of belief systems. And so if you\u2019re not thoughtful to [that] fact, your prompt might represent your beliefs in a stronger way...\u201d To mitigate this RAI issue during responsible prototyping, U3 said he liked to crowdsource few-shotexamples, and intentionally eliminated prompts that looked too similar: \u201cBecause what I\u2019m really looking for is, I\u2019m looking for few shots that represent a diversity of types of input that might come in. And sometimes I even model, slightly adversarial examples.\u201d However, most UX practitioners currently lack a standard practice to address this, beyond trying to diversify few-shot examples or deferring to user testing, which we discuss below. 4.3   Responsible  Evaluation  of  AI  Applications:  Involving  Users  to  Assess  Responsible  AI  In our interviews, both sets of practitioners explained how they saw inclusion of a diversity of experiences and perspectives as a core dimension of RAI. Almost all RAI experts in our study pointed out the importance of user research and evaluation in ensuring accountability and responsibility, as well as validating and refecting on the social benefts of the AI products during the early-stage design process. However, several RAI experts called for longer-term engagement with users throughout the AI product lifecycle given that traditional user evaluation and testing might not be sufcient in designing Many of the UX participants in our study were able to get access to emerging UX prototyping tools that allowed UX practitioners RAI. For instance, R5 critiqued the short-term nature of user testing and questioned the assumptions in traditional user testing and evaluation processes: \u201cI think at base level, not making research just be like a short-term, one-of thing, but having it be something that you are doing constantly at all diferent parts of this design process. Making sure that [...] you are not just going in with pre-thought of categories where you are like, \u2018Ok, this is how we defne failure. Has this failure happened? No, it hasn\u2019t. Ok, great.\u2019 but kind of like collectively defning some of those terms with the users. I think just having that process be integrated throughout all of these diferent steps would probably be a little bit more helpful.\u201d Other RAI experts in our study more explicitly called out their aspiration of taking more participatory approaches to involve users as well as external stakeholders in the design and evaluation of RAI. For example, R2 said, \u2018I don\u2019t think responsible AI can be achieved without some form of robust participation in a nutshell. For me, respon-sible AI is the extent to which it can be made participatory. Responsible AI is participatory AI.\u201d Through our interviews with both RAI experts and UX practition-ers, we found that while involving users and taking participatory approaches to evaluate RAI remained largely an aspiration from RAI experts, UX practitioners, with their disciplinary perspective towards involving users and other stakeholders in design and eval-uation, were already carrying out this aspiration through involving prospective users of a future AI application in the design process to surface and assess RAI issues. In this section, we outline UX practitioners\u2019 processes and chal-lenges for involving users in the process of evaluating potential RAI issues: from preparing the model prior to involving users, to coping with unexpected model output during user evaluations. 4.3.1  Preventing Harmful Model Output Prior to User Involvement. Due to the inherent stochasticity of generative language models, UX practitioners frequently witnessed model outputs that they considered to be toxic when testing ideas through direct model interaction. Many UX practitioners, while hoping to get users in-volved as early as possible in RAI research to surface and identify potential harms, were also concerned about exposing users to these outputs. U8 told us that he was primarily \u201cconcerned [with] what can I do before this gets to someone who\u2019s not on the team...\u201d To address the uncertainty associated with potentially harm-ful model outputs, before inviting users in to interact with these models, teams generated a wide range of constraint-based input and output suppression techniques that often relied on classifers and flters. U15 described approaches to handling input and out-put that was \u201cinequitable\u201d in terms of ofending particular groups: \u201cOne [technique] is [to] detect if the user[s] themselves are initiating  something that is inequitable, if the model is giving out something that is inequitable, and [another] thing that we did was reduce the scope of what the model can do in terms of its output. The [other] is reduce the scope of what a person can do in terms of its input into the model.\u201d Sometimes, UX practitioners curated resources used to classify and mitigate toxicity: \u201cWe put in place a bunch of flters to make sure that it\u2019s on topic, that it\u2019s not ofensive, that it doesn\u2019t use a set of words or phrases that we\u2019ve specifcally banned.\u201d (U1) However, it was not always possible to prevent harmful model output entirely, in which case practitioners had to come up with other user study safeguards. U14 described these challenges in the context of their experiences in an AI application design sprint, during which they met milestones related to ideating, designing, prototyping, and conducting user testing with the prototypes all within a matter of days: \u201cI  think  we  had  two  and  a  half  days  until  we  were actually testing in front of [external, prospective] users, so it was so fast and rapid, which is one of the pros about it [...] but if something did go wrong, and if [the model] did come up with bad suggestions, or if we did have an answer that went of the rails, we couldn\u2019t just shut it down ... [all we could do] was just make the screen go blank, and hopefully they didn\u2019t see it in that second.\u201d Although developing guardrails and constraints is a common strategy to mitigate and prevent RAI issues with large-scale models (albeit one that may reproduce existing structural inequities in AI [37, 89]), some practitioners pointed out the importance of explor-ing other approaches, so as to not over-limit the types of inputs and topics that users can discuss. As R1 told us, \u201cI think we need to fgure out how to teach the model in a controlled-generation way, to respond more appropriately so you\u2019re not always taking the sledgehammer and just suppressing results.\u201d However, as R8 told us, UX practitioners and product teams also struggle with developing more fexible mitigation approaches for complex algorithmic assemblages, for which they believed a constraint-based mitigation strategy was the best feasible option for mitigating potentially toxic output: \u201c[for some applications] it\u2019s actually many models at once. [...] For [product] purposes, a blocklist at the code level is really the most feasible thing because [a product application team] can\u2019t go back and retrain the model.\u201d 4.3.2  User Evaluation of the AI Application.  Through our inter-views, we found that UX practitioners ascribed specifc purposes and meaning to user evaluation in order to meet RAI challenges in early-stage AI application design. They saw user testing as an avenue for surfacing and identifying levels of comfort with large-scale model interactions and identifying possible RAI concerns through user evaluation of AI application prototypes. Here, the line between user feedback about a potential AI application, and adversarial testing of the model underlying it, has the potential to blur. As U7 told us, \u201cWe have to change our perspective on how we user test these things [AI prototypes] and think about the greater good because if we just focus on \u2018oh the button needs to change\u2019 and \u2018people didn\u2019t like the font size,\u2019 we\u2019re in trouble.\u201d To surface and mitigate RAI issues during user evaluations, UX practitioners often sought to recruit a broad range of users to im-prove diversity in user testing. This often includes, but is not limited to, recruiting for users across diferent demographics, race, gender, tech literacy, age, etc. However, UX practitioners also needed to balance resource and time constraints while trying to diversify user testing eforts. This often leads to the need to prioritize certain RAI issues for user testing. For example, U15 described how he gave suggestions on prioritizing the RAI issues that the team knew the least about, given limited time and resources: \u201cThere are 21 things that you are doing. My suggestion would be to look at these fve ini-tially and redesign because my understanding of the model is that these fve things have not been tested previously. [...] Let\u2019s focus on things that we know the least about.\u201d Many UX practitioners talked about the importance of prepar-ing users for potentially toxic or otherwise harmful outputs that the AI application might generate during user evaluation sessions. Many also described their anxiety around viewing unpredictable model outputs with the users during these sessions. To mitigate this, UX practitioners discussed the importance of setting expec-tations and communicating with users at the beginning of user testing sessions: \u201cYou are showing them to an end user at the same time you\u2019re seeing them yourself. And so there\u2019s a certain amount of anxiety there. So, in the research protocol, it\u2019s really important to be able to set people\u2019s expectations: \u2018This is early technology, if you see some things that are harmful, I want you to be able to talk to me about it. I\u2019ll have some narrative to help you understand what you\u2019re seeing.\u201d\u2019(U3) UX practitioners also pointed out the difculties users could face in providing honest feedback and surfacing RAI issues that were meaningful to them during user study sessions. User stud-ies, especially those in which the researcher and participant have not built a relationship, have always grappled with the potential for social desirability bias, participant conformity to researchers\u2019 expectations, and preferences to avoid taboo topics or embarrass-ing (or personally invasive) social interactions. These concerns are made more salient when evaluating high-fdelity AI applications, given that large models could generate unpredictable and harm-ful outputs. To help mitigate this challenge, UX practitioners also sought to create a safe and encouraging environment for users to feel comfortable discussing RAI issues. U13 described one strategy they used to accomplish this by matching identity characteristics like practitioners\u2019 race and ethnicity with those of users, to help them feel comfortable talking about potential RAI concerns: \u201cI think one of the things [for] responsible AI consid-eration is when you\u2019re doing user research, does the makeup of your team and the people who are inter-viewing those folks, are users comfortable talking to them about fairness issues? We always try to match the race of the moderator to the race or the ethnicity of the participant for psychological safety. Because a lot of times, users don\u2019t really feel comfortable talking about inequity with a person who may not have experiences with systemic inequity. How are you making sure that  you\u2019re conducting not just research with users responsi-bly and ethically, but also you are pairing them with interviewers that could lead to honest feedback.\u201d Next, we refect on our fndings and the ways in which, taken together, they suggest an evolution of UX praxis. We conclude by refecting on opportunities for HCI research to move our feld closer to designing responsible AI. 5   DISCUSSION  In our fndings, we highlight three types of emerging RAI practices carried out by UX practitioners to meet RAI challenges: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications. Specifcally, we identifed strategies that UX practitioners employed to self-educate and communicate with others about potential RAI issues; challenges during proto-typing, such as communicating model capabilities (and limitations) to users; and current approaches to responsible evaluation of AI applications, including preparing both the models and the users for potentially harmful model outputs. In this section, we frst discuss implications of the hidden work of RAI conducted by UX practitioners, highlighting the labor as-sociated with these practices. We then discuss research opportu-nities and ways to support UX practitioners in their RAI practice, particularly for applying large-scale language models to develop applications. Finally, we identify opportunities to reconfgure the role of the user in designing RAI. 5.1   Supporting  the  Hidden  Work  of  RAI  in  UX  Practice  In light of increasing calls for UX practitioners to be involved in the work of identifying and addressing issues of RAI in technology design [e.g., 13\u201315, 53, 69, 70, 84], we highlight the hidden work that UX practitioners are conducting to meet RAI challenges. We call out this hidden work by situating UX practitioners\u2019 emerging RAI practices in the context of the work conducted by RAI experts in their formal RAI roles. We fnd that UX practitioners and RAI experts share similar conceptualizations and aspirations for the human impacts of AI\u2014they both saw RAI as, in large part, antici-pating, surfacing, and mitigating a variety of potential harms for users and other stakeholders who may be impacted by a given AI system. Through our study, we see RAI experts carrying out the work of developing the RAI agenda, guidelines, toolkits, evaluation, and foundational research at an organizational level throughout the entire AI design and development process, while UX practition-ers operate in specifc application areas, contexts, and domains to actively promote, implement, and adapt RAI to ft their day-to-day work in early-stage AI application design. Drawing upon their unique human-centered values and design techniques, UX practitioners are adapting their UX practices, negoti-ating their respective roles in RAI work, and learning and educating others about human-centered values and RAI concerns. However, carrying out these RAI eforts often requires UX practitioners to devote additional time and eforts in their day-to-day work, or what Star and Strauss [1999] have described as \u201cinvisible work,\u201d or akin to what Strauss [1988] has referred to as \u201carticulation work,\u201d or the work required to make other work happen. We fnd that UX practitioners take on a translation role [cf. 23] to adapt high-level RAI guidelines into specifc practices applica-ble for their teams, given that RAI issues are often domain- and technology-specifc [e.g. 30]. Our fndings thus encourage more HCI research eforts to understand how to design RAI guidelines and frameworks that can be easily tailored to ft into existing UX workfows and methods. A good starting point would be to under-stand how UX practitioners adapt and apply existing RAI guidelines and frameworks to ft their workfows and, accordingly, provide design implications on RAI tools, practices, and frameworks for UX practitioners. Oftentimes, UX practitioners also need to take on additional work to learn more about RAI given that RAI and design ethics are often not included in many HCI and UX curricula [108], as well as educating others on their team about UX methods, values, and impacts of AI systems on people through strategic communica-tion. Our fndings suggest that these emerging roles as learners and educators for the UX aspects of RAI may need to be explicitly articulated as part of UX practice in order to be valued by UX prac-titioners\u2019 organizations [cf. 23]. Organizations could also provide UX practitioners with resources and guidance to help them raise RAI concerns, as prior work has proposed for AI practitioners more broadly [70, 84, 114]. UX-specifc resources might thus be designed to support UX practitioners in taking on more of such an advocate or activist role [cf. 23, 112] in working towards more responsible AI design and development processes. This labor that we outlined above not only requires additional eforts and time from UX practitioners, but also takes the form of emotional (or afective) labor, which Wong [2021] has identifed as being a crucial part of designers\u2019 ethics work, despite not being valued as part of the typical technology design process. The hidden work of RAI conducted by UX practitioners is often not recog-nized or valued by their managers or organizations, and therefore constantly at risk of being deprioritized in the face of competing priorities and limited resources [113], or leading others to view UX practitioners as a \u201cblocker\u201d of the design and development process. In our study, we also fnd that in order to surface potential RAI issues in application design, UX practitioners intentionally and repeatedly generate and witness harmful and toxic outputs through model prompting, as well as prepare participants in their studies to view such toxic content. Much like Gray and Suri [2019] identifed for content moderation, the burden of viewing harmful content often falls to those who are least likely to be protected from or compensated appropriately for handling the toxic externalities of large-scale sociotechnical systems. 5.2   Challenges  and  Opportunities  in  Responsibly  Designing  with  Large  Language  Models  In addition to implications for the work of UX practitioners in RAI more broadly, our fndings suggest specifc implications for the design of new tools and methods for UX work in responsible design and evaluation of AI applications powered by LLMs. Existing ethics-focused design methods [24] or frameworks such as value-sensitive design [42] largely do not focus on AI or language models, while  existing AI ethics toolkits are largely designed to support \u201ctechni-cal\u201d work of RAI, rather than UX practices [114]. Indeed, despite its prevalence in the academic literature, none of our participants mentioned using value-sensitive design as an approach to designing RAI. In our study, we fnd that traditional HCI methods such as Wizard-of-Oz studies are not only insufcient to support UX prac-titioners in their ideation and design processes for AI applications powered by LLMs [38, 118, 119, 122], but they are also insufcient in helping UX practitioners identify and evaluate potential RAI issues through real use cases in early-stage AI application design. Our study sheds light on the opportunities and challenges for RAI from the emerging design and prototyping practice of prompt programming with LLMs. In their interviews, while many UX prac-titioners talked about the many advantages that the prompt pro-gramming tool PromptMaker brought to them (e.g., test-driving model capabilities directly, reduced reliance on other experts like engineers, and data scientists [97, 118]), practitioners also pointed out that it is not yet a mature tool for RAI design. One substantial risk of using prompt programming to uncover RAI issues during the design and prototyping of high-fdelity AI prototypes, as pointed out by the UX practitioners in the interviews, is the danger of embedding their belief systems into users\u2019 experi-ence of the AI system via prompting. Similar concerns were also raised regarding the use of prompting or generating \u201cbehavioral tests\u201d [14, 86] to prompt models to generate harmful outputs, which may be limited by the positionality of the practitioner or researcher creating those tests [14, 123]. While some UX practitioners in our study attempted to mitigate this issue by crowdsourcing prompts [e.g., 75, 77, 78], this mitigation strategy may be hindered by a lack of representative groups of participants as well as a disconnect between the abstract model the tests are created for and the downstream application of that model within a particular sociocultural context and use case. As a result, new tools and methods are needed that can ground the often speculative work of anticipating potential harms of language models in their specifc contexts of use, rather than abstracting that social context away [cf. 117]. Substantial  prior  work  has  identifed  the  potential  harms  of technologies built on LLMs [e.g., 8, 15, 105], while at the same time acknowledging the challenge of evaluating and mitigating such harms [14, 100, 123] and calling for HCI and UX researchers and practitioners to contribute a human-centered perspective to identifying and addressing RAI issues in LLMs [12\u201315, 123]. Our fndings suggest that constraint-based input and output suppression techniques such as blocklists are a commonly used strategy to prevent users from encountering toxic output during user testing, in addition to preventing models from generating such output after deployment as well [37, 89]. As prior work has identifed [37, 89], blocklists are crude instru-ments that may lead to harms of erasure [33, 37] if and when they fail to account for the social context of language use [cf. 11, 14, 123]. However, although practitioners acknowledged in their interviews that such suppression techniques are not ideal, it is often the most feasible option they have, given that product teams often deal with cascading RAI issues from upstream models they may not have access to or control over. As such, more research is needed to under-stand how\u2014in development paradigms where pre-trained models are fne-tuned or used in downstream applications [15, 58]\u2014RAI issues may be propagated from the beginning of the model devel-opment to the design process of AI applications. In addition, more research is needed to understand how constraint-based approaches such as classifers and blocklists are developed and used, and by whom, to interrogate the assumptions underlying their design. 5.3   Reconfguring  the  Role  of  the  \u201cUser\u201d  in  RAI  Recognizing  the  limited  perspectives  of  researchers  and practi-tioners involved in AI design, there are increasing calls for more user-centered or participatory approaches to responsible AI [e.g., 4, 29, 35, 52, 64, 91, 102, 111]. In our study, we found that UX prac-titioners involve members of the public, potentially impacted user groups, and domain experts in the design and evaluation of RAI ap-plications; however, this new practice of involving users and other impacted stakeholders in RAI work poses new challenges and re-search questions for the HCI community, including when and how people should be involved in RAI design and evaluation, as well as how to protect people from any potential harms of participating in those processes. Our fndings suggest opportunities to rethink how UX practi-tioners conceptualize and draw on people\u2019s mental models of AI applications during the responsible design and evaluation of AI systems. For instance, prior work on folk theories of algorithms [e.g., 34, 39, 62, 93] suggests that the accuracy of folk theories of how algorithms work may be less critical for UX practitioners than what those folk theories (or mental models) of algorithms reveal about people\u2019s orientations towards algorithmic systems. In our fndings, UX practitioners\u2019 concerns about people anthropomor-phizing LLMs may suggest opportunities (e.g., more seamful design [21]) to reveal the capabilities and limitations of applications based on LLMs. In addition, our fndings suggest the need to reconsider how UX practitioners confgure the role that users and other poten-tially impacted stakeholders play in identifying and mitigating RAI issues. For instance, we see UX practitioners using user testing sessions to conduct adversarial testing of potential model harms; prior literature has suggested crowdsourcing [77] or using \u201cbias bounties\u201d [49] or \u201ccrowd audits\u201d to identify potentially harmful model outputs [35, 91]. However, these approaches are still nascent, and UX education and praxis has not yet developed robust methods, frameworks, and practices for UX practitioners to either lead such eforts themselves or incorporate their results into UX design and evaluation. Moreover, as we fnd in our work, the nature of identi-fying potential RAI harms may lead to unintended consequences for the participants in such studies who may either have to gener-ate ofensive, toxic output themselves, or be exposed to ofensive  language as a result of prompts that the UX practitioners or other participants create. Future research should thus explore ways to protect participants from these toxic externalities of RAI work. Furthermore, despite calls for broader participation, participa-tory design (PD), or community-based design of AI, the current modes for engaging people in responsible AI evaluation may not deliver on the empowering goals of participatory approaches [29]. For instance, relying on users (and other stakeholders) to identify potential harms may inadvertently relegate them to a more consul-tative or extractive mode of engagement, rather than empowering them to have more generative, creative input into RAI design, as suggested by traditions such as participatory design [e.g., 29, 74], aspirations-based design [65, 101], and community-collaborative design approaches [26]. Moreover, the design paradigm of train-ing large-scale AI models and applying them in downstream AI-powered applications poses serious questions for HCI research about how we might develop modes of participation in RAI de-sign and evaluation that empower participants to have meaningful control over the design of AI applications. 6   LIMITATIONS  While our work provides valuable insights into and implications of emerging RAI practices carried out by UX practitioners, our study has limitations. First, all study participants were recruited from one large technology company and their RAI perspectives and prac-tices could be shaped or limited by the organization\u2019s processes and culture around RAI; hence, more research is needed to identify the relevance and applicability of our fndings and implications in other industry contexts\u2014including at smaller technology compa-nies. That almost all participants in our study had prior experience working at diferent technology companies allowed us to draw from their prior work experience during the interviews. Second, our par-ticipants mostly discussed their experiences related to computer vision and language-based ML models, as participants were pri-marily working within these areas. Interviewing participants with expertise in other types of ML models, such as sound-based models, could identify diferent challenges, strategies, or tensions. Third, our study focused on one specifc style of designing and deploying AI systems, i.e., a model-frst trajectory [76], in which ML models were developed before practitioners designed and built AI products around the model. We acknowledge that there are other forms of AI application design and development, such as product-frst, or integrated approaches, and that our fndings might be more or less transferable to these processes. Future studies should replicate our work across organizations of diferent sizes, AI development ap-proaches, and maturity levels [cf. 109], to expand on the practices, challenges, and needs associated with RAI in the UX practices that we identifed through our study. 7   CONCLUSION  This paper reports on interviews with ffteen UX practitioners and eight RAI subject matter experts, to understand and situate the emergent RAI practices of UX practitioners in a large technology company. Through the interviews, we identify three emerging RAI practices conducted by UX practitioners in AI application design: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications. We distill challenges and strategies UX practitioners employed to self-educate and commu-nicate RAI issues with the team, to surface and identify potential harms when designing and prototyping with ML models, and to involve users in RAI application design and evaluation processes. Based on our fndings, we discuss and highlight the hidden work of RAI carried out by UX practitioners. We then outline research opportunities and questions for the HCI community, to increase practitioner support for managing RAI challenges, and to move towards best practices for participatory involvement of impacted stakeholders in RAI-related processes. ACKNOWLEDGMENTS  We thank our study participants. We also thank anonymous review-ers for their valuable feedback on the paper. REFERENCES",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "eefa3e09-8408-4948-9ad7-0d69c27ce412",
                    "text": "Technology companies continue to invest in eforts to incorporate responsibility in their Artifcial Intelligence (AI) advancements, while eforts to audit and regulate AI systems expand. This shift towards Responsible AI (RAI) in the tech industry necessitates new practices and adaptations to roles\u2014undertaken by a variety of prac-titioners in more or less formal positions, many of whom focus on the user-centered aspects of AI. To better understand practices at the intersection of user experience (UX) and RAI, we conducted an interview study with industrial UX practitioners and RAI subject matter experts, both of whom are actively involved in addressing RAI concerns throughout the early design and development of new AI-based prototypes, demos, and products, at a large technology company. Many of the specifc practices and their associated chal-lenges have yet to be surfaced in the literature, and distilling them ofers a critical view into how practitioners\u2019 roles are adapting to meet present-day RAI challenges. We present and discuss three emerging practices in which RAI is being enacted and reifed in UX practitioners\u2019 everyday work. We conclude by arguing that the emerging practices, goals, and types of expertise that surfaced in our study point to an evolution in praxis, with associated challenges that suggest important areas for further research in HCI.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "236cfa07-f7ba-4753-beb5-f0e568ea69f0",
                    "text": "\u2022 Human-centered computing \u2192 Empirical studies in HCI;",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "7a862c69-d4cf-46b6-ad7c-8c2d7f584ca4",
                    "text": "responsible AI; industry practice; UX; interview",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "709dc6df-37cf-4629-bbb4-b8cff4cb34fd",
                    "text": "Technology companies continue to invest in eforts to work toward responsible design and development of AI, responding to increased demand to account for and mitigate the social risks posed by AI technology. As responsible AI (RAI ) eforts become more estab-lished as organizational practices [84], an increasing number of individuals and groups within both the private and public sector are responding to RAI-related research and product needs. Much of the literature focusing on RAI eforts is devoted to un-derstanding and improving machine learning (ML) models, through improvements to data annotation practices [9, 28, 31, 71], model evaluations [32, 56, 88], data and model documentation [36, 47, 83], or building developer-facing tools to help engineers interpret mod-els post-hoc [3, 6, 7, 10, 68, 107]. However, in contrast with a de-velopment paradigm in which the design of ML models occurs in parallel with their use in products, in many cases, ML models are in-creasingly used to power user-facing AI applications developed by wholly distinct product teams\u2014in some cases, by product teams at other organizations [15, 58]. Thus, RAI eforts in industry continue after models leave the research lab.Before deploying AI applications in deployment contexts where they could cause far-reaching societal consequences, many prac-titioners are undertaking RAI work, in more or less visible, and more or less formal, positions. Although prior RAI research in CHI and CSCW has focused on opportunities to intervene in AI devel-opment practices [e.g., 53, 69, 70, 84], it has (with few exceptions) focused on the work practices of data scientists and ML engineers in the model development process, rather than how a wider set of practitioners are involved in applying existing models for novel AI applications. Among this wider set of practitioners applying the models, many focus on user-facing and user-centered aspects of technology (e.g., interaction designers, user experience (UX) designers, or UX researchers). Yet, the specifc practices of such user-centered practitioners involved in RAI have yet to be identifed or formalized, and distilling them ofers a critical view into how they are adapting to meet present-day challenges in RAI. In this paper, we explore an emerging set of RAI practices car-ried out by user-centered practitioners when applying large models during early stages and refnement of AI application design. To understand these emerging user-centered RAI practices, we con-ducted interviews with UX practitioners, who are often involved in early-stage AI application ideation, design, prototyping, and evalu-ation, and with RAI subject matter experts (whom we refer to as \u201cRAI experts\u201d throughout the paper), who often perform evalua-tion and ofer consultation about responsibility in AI projects and products or product features, at a U.S. site of a large technology company that has a large AI function. These two sets of practition-ers were actively involved in addressing RAI concerns, formally by RAI experts and informally by UX practitioners, both early in design and refnement of new, AI-based prototypes, demos, and product features. We aim to identify and understand the emerging RAI work carried out by UX practitioners in their design processes, by investigating these practices in light of the RAI work conducted by RAI experts in their formal job capacity. We thus seek to answer two research questions: RQ 1:  How do UX practitioners currently incorporate and examine RAI considerations during early stages and refnement of AI application design given their organizational context? RQ 2:  What challenges do UX practitioners encounter in their cur-rent RAI practices during early stages and refnement of AI application design given their organizational context? We  conducted  a  refexive  thematic  analysis  of  interviews  with  participants  to  make  the  following  research  contributions:  (1)   An  identifcation  of  three  key  emerging  practices  that  UX  practitioners  are  developing  to  meet  evolving  RAI  needs:  a)  building  and  reinforcing  an  RAI  lens,  b)  responsible  prototyp-ing,  and  c)  responsible  evaluation  of  AI  applications.  (2)   A  refection  on  the  hidden  RAI  work  UX  practitioners  are  carrying  out  and  ways  to  support  this  evolution  in  UX  praxis  moving  forward.  (3)   A  discussion  of  the  implications  of  UX  practitioners\u2019  current  challenges  in  designing  with  ML  models  and  the  need  to  reconfgure  the  role  of  the  user  when  designing  RAI.  We frst situate our study with respect to prior work on RAI work practices in industry contexts, existing research on values and ethics in UX practice, and prior literature on designing and prototyping with ML models. After introducing our study and data analysis process, we identify three emerging RAI practices developed and carried out by UX practitioners in order to adapt existing UX prac-tices to meet evolving RAI challenges. These practices\u2014building and reinforcing an RAI lens, responsible prototyping, and responsi-ble evaluation of AI applications\u2014are not linear, but are embedded  throughout work practices in iterative ways. Within each emerging practice, we situate the RAI work of UX practitioners\u2019 with respect to the RAI work carried out by RAI experts in their formal job capac-ity, and highlight strategies and techniques that UX practitioners adopted, to sensitize people to RAI concerns, to communicate RAI issues with the teams, to consider potential consequences of users\u2019 mental models of AI systems (e.g., over-reliance on AI [cf. 80]), and to surface and mitigate potential RAI issues through an emerging AI prototyping technique called prompt programming. Finally, we present practices that place RAI concerns in direct conversation with traditional user evaluation approaches. We refect on these fndings and what they mean for the evolution of UX praxis in the Discussion, and conclude by highlighting opportunities for further HCI research to support the work of designing responsible AI.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "0ab922e7-abc1-4bbc-be22-a54df35d8900",
                    "text": "",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "058f60a7-fdff-47a9-bb39-f3d34370657f",
                    "text": "",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "5dcbb07e-f62f-45ed-9092-78362787afeb",
                    "text": "A growing body of work in HCI examines the work practices of industry practitioners as they address RAI issues during the design and development process [53, 67, 69, 70, 84]\u2014in contrast with prior work that has studied data annotation [9, 28, 31, 71] and model evaluation [32, 56, 88], or developed resources for data and model documentation [36, 47, 83] or post-hoc interpretation of model performance [3, 6, 7, 10, 68, 107]. Prior work has found that AI prac-titioners seek to uncover fairness issues prior to deployment [53], yet many RAI approaches or mitigations are reactionary, initiated as a result of public relations issues, media attention, or customer complaints [53, 69, 84].  intervening Practitioners  have  emphasized  in  training datasets  [31,  32,  53],  as  well  as  algorithmic  mitigations  [2],  as critical in working toward RAI. However, AI practitioners fnd it challenging to represent diverse demographic groups in datasets and  in  fairness  assessments,  given  that  practitioners  tend  to draw  on  their  (often  homogenous  [106])  personal  experiences and perspectives when it comes to RAI [30, 53, 69]. Madaio et al. [2022] also note that the heuristics practitioners used to determine priorities  in  assessing  fairness  issues  (e.g.,  perceived  fairness severity, perceived brand impact, etc.) could compound existing inequities of AI systems [69]. Given these challenges, practitioners call  for  more  guidance  and  support  (e.g.,  practices,  tools,  and other resources) in working toward the design, development, and deployment of RAI [53, 69, 84]. To  fulfll  practitioners\u2019  needs  for  RAI  support,  a  plethora  of toolkits, guidelines, and other resources have been developed [e.g. 1, 4, 7, 10, 54, 68, 72]. Recent work has explored the sociotechnical practice of how RAI toolkits are integrated into practitioners\u2019 work-fows [30, 70, 73, 114]. For example, Deng et al. [2022] point out that these toolkits are often less prescriptive than required\u2014containing guidance on what to do (e.g., engage stakeholders), but not how to carry out those recommendations [30]. In addition, many RAI toolk-its are also designed to primarily support specifc types of technical work from technical practitioners to address RAI issues, rendering themselves less accessible to contributors with varying types of expertise [30, 114]. Relying on toolkits may also enact a form of techno-solutionism, by promoting or incentivizing solely technical solutions to the sociotechnical work of responsible AI [70, 90, 114]. Existing literature has also highlighted the role of organizational factors in RAI practices [53, 69, 70, 84], including the lack of organi-zational incentives to address RAI issues (or the disincentives to this work) [53, 70, 84]. Coupled with the lack of clarity over roles and responsibilities in RAI work [72, 84], addressing RAI issues during AI development often relies on individuals who are able to dedi-cate time to developing and promoting RAI processes\u2014processes that may or may not be formally adopted within the rest of the organization [70, 84]. Identifying, assessing, and mitigating RAI issues in datasets, algorithms, and model behavior often require practitioners (and their organizations) to invest signifcant time and resources, yet AI practitioners often face time and resource constraints in carrying out their RAI practices (including incen-tives to ship products on fast-paced timelines) [30, 53, 69, 84, 112]. Many scholars have called for changes in organizational structures and processes, to support practitioners\u2019 RAI practices instead of hindering them [53, 69, 70, 72, 84].",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "e8fd8f32-1ed8-4f86-ab83-8c0f347f2a61",
                    "text": "Although recent work has focused on AI practitioners\u2019 work prac-tices for RAI, substantial prior research has identifed the ways that values are instantiated in technology design more broadly [40, 57, 79, 103, 110], as well as through ongoing practices of technol-ogy use, appropriation, maintenance, and repair [e.g., 55]. As such, various methods, tools, and theoretical frameworks [e.g., 5, 24, 42\u2013 44, 92, 115] have been developed to support designers (including UX practitioners) in surfacing relevant values throughout the de-sign process and bringing those values to bear on design decisions (what JafariNaimi et al. [2015] refer to as the \u201cidentify/apply\u201d logic of values in design) [57]. For instance, Value-Sensitive Design (VSD) is a framework in-tended to support designers in understanding the role of specifc values in the design of technology, including how they might be promoted or undermined through a given system [42, 44]. In addi-tion, numerous other methods and tools have been developed to support designers\u2019 \u201cvalues work\u201d in technology design [112], in-cluding resources to identify relevant values and surface potential harms of technology, such as Envisioning Cards [43], Judgment Call [5], Timelines [115] and more (see Chivukula et al. [2021] for a review). Although VSD and related methods are not the focus of our work in this paper, they represent a key aspect of the training and resources that may inform how UX practitioners address values as part of the work involved in addressing RAI. 2.2.1  Situated  Work  Practices  of  UX  Practitioners.  As  part  of  a broader turn to practice within HCI scholarship [66], recent re-search has focused not only on developing methods and tools for designers to use, but has also explored the situated work practices of UX practitioners in engaging with values as part of their every-day work [e.g., 23, 25, 50, 112, 113]. For instance, Chivukula et al.  [2021] have identifed a set of \u201cidentity claims\u201d that UX practition-ers articulate for the various roles they take on as part of doing values work in their practice, including learner and educator (i.e., learning and teaching others on their team about ethics), translator (i.e., taking resources about values in one domain and translat-ing them to their own work), and advocate/activist, among others. Situating individual UX practitioners within larger social and orga-nizational contexts, Gray and Chivukula [2019] describe various factors that mediate the relationship between designers\u2019 individual ethical awareness and action, including the role of organizational practices [25, 50]. This body of scholarship has identifed the social and political work that UX practitioners engage in, in addition to their technical work; including rhetorical work to convince leadership of the value of UX (and of the importance of values work in UX) [e.g., 87]\u2014an issue of critical importance given the relatively lower status of UX and design compared with software engineering roles in large tech-nology companies [112]. Given the role that organizational factors play in mediating UX practitioners\u2019 values work in practice, Wong [2021] has identifed tactics for \u201csoft resistance\u201d that UX practi-tioners engage in to create space to address values in their work. In part, this involves making values visible and relevant to others in their organization (similar to the advocate or activist identity described by Chivukula et al. [2021]), as well as working to change organizational norms and practices from within, by tactically ex-panding the defnition of who is considered to be the \u201cuser\u201d [cf. 116], through leveraging (and trying to change) organizational goals and priorities (e.g., Objectives and Key Results, or Key Performance Indicators), or more broadly leveraging corporate logics to make a business case for values in UX [112]. However, as Wong [2021] point out, each of these tactics are par-tial and embody contradictions, in that they are trying to challenge, contest, or change organizational practices, while leveraging those same logics and discourses. Finally, as Wong [2021] identify, the values work of UX practitioners often involves \u201cwork outside of the technology design process,\u201d work that often involves substantial emotional labor [cf. 96], and which may not be valued as part of their everyday work [113]. Relatively little literature, however, has explored the values work that UX practitioners engage in as part of designing and developing AI applications, or how that work is aligned with the work practices of RAI more generally.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "8c0c68bc-d572-4477-b926-4c67eec69a26",
                    "text": "",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "6470cf4e-7847-46f9-b882-c43300649537",
                    "text": "Prior research has studied UX practitioners\u2019 current practices and challenges when designing and prototyping ML-powered AI appli-cations [38, 117, 119, 120, 122]. Existing work has found that UX practitioners, given their training in HCI and User-Centered De-sign (UCD), commonly leverage traditional HCI and UCD methods and toolkits when designing with AI, yet these are often insuf-fcient [117, 120, 122]. For example, traditional methods such as Wizard-Of-Oz, sketching, paper prototyping, and rapid prototyp-ing can fail to accommodate the non-deterministic behaviors and opaque mechanisms of AI systems [117, 119, 122]. Prior work has highlighted the need for high-fdelity prototypes that can generate tangible, realistic behaviors, instead of toy sce-narios, to elicit user feedback and better assess the potential human and societal impacts of AI applications [54, 119, 122]. Zdanowska and Taylor [2022] note that UX practitioners found that most HCI and UCD methods and tools place too much emphasis on the design and evaluation of the user interface, which is only one of many components when designing with AI [122]. Users\u2019 mental models of the AI system [120, 122] and the feasibility and user acceptance of the design [122] are also key considerations for UX practitioners. When designing and prototyping with AI, UX practitioners also frequently collaborate with technical experts such as engineers and data scientists [82, 97, 118] to gain a deeper understanding of the capabilities and limitations of the models [38, 117, 119]. While high-level abstractions are sufcient for UX practitioners to design AI applications [118, 122], a deeper understanding of ML model func-tionality could help UX practitioners better envision use cases that may not yet exist [38, 119]. However, this collaboration also poses challenges in that UX practitioners and engineers don\u2019t always share a common perspective, language, or workfow [82, 97, 119], leading UX practitioners to take on extra work to bridge disciplinary boundaries through sharing user stories and raw user feedback from user testing video recordings to help engineers understand user needs [98], and sometimes, adapting to and embracing a more data-centric culture to communicate user needs through both qualitative and quantitative metrics [118]. 2.3.1  Prototyping with Prompt Programming.  To meet the increas-ing demand for new UX prototyping and design tools when de-signing with AI, an emerging practice involves prototyping with ML models through prompt programming [20, 27, 60, 121]. Using prompting techniques [27, 60], UX practitioners are able to send natural language prompts to ML models as inputs and interact with the models directly to test-drive their capabilities and limita-tions [60]. Prior work has found that prompt-based prototyping with large language models (LLMs) helps UX practitioners reduce their reliance on engineers and developers to understand model ca-pabilities, speed up the prototyping process to test out initial ideas and \u201cfail fast\u201d [cf. 117], and better communicate with collaborators using prototypes as boundary objects [60]. Prompt programming is usually conducted with pre-trained, large-scale models such as LLMs [27, 60] and text-to-image gen-eration models [124], which are prone to generate outputs that may perpetuate social stereotypes, toxicity, discrimination, and exclusionary norms [15, 33, 48, 105]. Researchers have been ex-ploring ways to evaluate LLMs and other generative models prior to putting them into use. Common evaluation methods include manually generating general test cases, or tests targeted at specifc failure modes [59, 86], as well as automatically generating test in-puts using the model itself [46, 81]. However, others have noted that such \u201cbehavioral tests\u201d and use of benchmarks to prompt models to intentionally generate harmful outputs (so they can be prevented) often come with pitfalls that render these methods invalid [14]. As such, there are increasing calls for human-centered approaches from HCI and UX practitioners to support the work of identifying and mitigating RAI issues with LLMs [e.g., 12\u201315]. Yet, insights into  how prompt programming can facilitate or hinder UX practitioners\u2019 work on RAI has not been explored.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "dbfe7e11-56e4-48a2-a008-a87f40c52981",
                    "text": "",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "35c28c07-5572-4e8b-887f-32d4597d68cf",
                    "text": "To  investigate  our  research  questions,  we  conducted  semi-structured interviews with both UX practitioners (n = 15) and sub-ject matter experts in a designated RAI role (n = 8). We recruited participants through snowball sampling at our study site (via direct emails to contacts). Our study site was chosen due to the company\u2019s large AI function, as well as the depth of researcher access that could be achieved to participants\u2019 work practices and teams. All participants were recruited from the same company that the au-thors were employed at during the time of the study, a decision we discuss further in section 3.2, below. Inclusion criteria included UX practitioners who had worked on or were currently working on the design, prototyping, user research, or user evaluation of AI applica-tions (prototypes, demos, product features) that were powered by large-scale models. We specifcally sought out UX practitioners who were either directly or tangentially involved with addressing RAI concerns as part of their work with these applications. For RAI experts, our inclusion criteria included experts in a formal Respon-sible AI role, who had experience evaluating or being consulted about responsibility in AI projects and products or product features. Information about participants\u2019 job roles can be found in Table 1. UX participants had an average of 5.6 years experience at the com-pany (SD=2.3), an average of 11.8 years working in UX (SD=7.3), and an average of 7.8 years working with AI (SD=4.4); RAI expert participants had an average of 2.3 years working at the company (SD=1.6), an average of 2.4 years working in RAI (SD=1.2), and an average of 5.9 years working with AI (SD=2.6). We also asked participants to optionally share their gender identity: among UX participants, seven were women, eight were men, and one was non-binary. Among RAI experts, fve were women, two were men, and one was non-binary. Each participant was compensated through a donation to their charity of choice valuing $40 USD.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "0ace680b-e1c8-4311-b616-66f73d810b90",
                    "text": "Our study ran from June through July, 2022. All of the participants worked in the United States, in hybrid or fully remote roles at the time of study; hence, all interviews were conducted virtually through an internal virtual meeting platform. Except for two 30-minute interviews, and one 90-minute interview, all interviews lasted about 60 minutes. All participants provided written consent to participate in the research study before interviews began. Scoping the present work to a specifc technology company as a research site allowed us to take advantage of internal AI resources (described below in 3.2.3) to ground interview discussions. As researchers were also company employees, participants could provide more details  of  their  AI  projects  and  context  for  their  RAI  practices, while upholding confdentiality and IP protection. However, we Table 1: Interview participant information. UX participants worked across diferent AI product and research areas. Listed AI areas are based on the specifc projects/products mentioned in interviews. For RAI expert participants, we list their background. did not limit interview discussions to participants\u2019 current work experiences; many had experience working in multiple settings and companies, and we prompted them to refect more broadly on their experiences over the course of their careers. Given the sensitive nature of RAI topics, we also sought to foster a high level of transparency and trust in our communications with each practitioner, before, during, and after the qualitative interview. For example, we held pre-interview calls and shared research briefs with our participants to answer questions about the research, and shared our data interpretations and fndings back with each partici-pant by email, noting their specifc quotes in the paper and asking for any feedback before submission. To inform our research questions, we frst observed a conversa-tional AI design sprint. We did not include our observations as research data, but instead followed up with four sprint participants to enroll them in our interview study. We also studied any artifacts that were provided in the interview, including sprint artifacts dis-cussed. We discuss each interview protocol below, and they are each provided in Supplementary Materials. We also describe a prompt programming tool called PromptMaker [60] below, which we used as a probe during interviews with both UX practitioners and RAI experts. 3.2.1  UX Practitioner Protocol.  In the interviews with UX practi-tioners, we asked them to broadly describe the types of UX work they do related to AI-based prototypes, demos, and/or product fea-tures, and then to dive into more details by having them walk us through one or two specifc projects they had worked on. We specif-ically focused on how RAI issues surfaced in their work, mitigation strategies or precautions they used to address RAI issues, and how RAI issues might have infuenced the project direction. At the end of the interview, we also asked UX practitioners about their thoughts on how RAI processes for early-stage AI application design could be improved.  3.2.2  RAI Expert Protocol.  In our interviews with RAI experts, we asked participants in designated RAI roles about their experi-ences reviewing, analyzing, evaluating, and/or consulting on AI technologies as part of organizational RAI practice. We then asked the experts to walk us through an AI project that they had been involved with, which drew on their expertise, the RAI issues that were present, and how they worked with the project/product teams to address those RAI concerns. Towards the end of the interview, we also asked the experts to envision possible improvements to current RAI practices and possible processes or tools to support RAI in early-stage AI application design. 3.2.3  PromptMaker As a Probe.  During interviews with all par-ticipants, we introduced and described an LLM prototyping tool called PromptMaker, as described in Jiang et al. [2022]. Prompt-Maker provides a web-based interface to LLMs, enabling users to interactively write and test LLM prompts. PromptMaker also en-ables practitioners to remotely execute a prompt. For example, a basic prompt to translate English into French could be created (see example in footnote), then embedded within a prototype to test out a translation feature. Collectively, PromptMaker\u2019s capabilities enable practitioners to rapidly prototype and test new AI features in hours or days, without requiring signifcant machine learning experience. Many of the interview participants were familiar with Prompt-Maker and seven UX participants indicated in the interviews that they use PromptMaker in their daily job during early-stage AI appli-cation design. In our interviews, we used PromptMaker as a probe to understand how emerging AI design and prototyping tools were shaping UX practitioners\u2019 RAI practices and to understand the po-tential opportunities and challenges new AI design and prototyping tools present for RAI.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "e111fb64-abd1-490e-9253-fe9c6b5aaf06",
                    "text": "All interviews were video-recorded and later transcribed verbatim for data analysis purposes. To analyze the interview data, we drew on Braun and Clarke\u2019s [2019\u20132021] refexive thematic analysis ap-proach [17\u201319]. Refexive Thematic Analysis is a post-positivist approach that emphasizes researchers\u2019 role in knowledge produc-tion, including the philosophical stance and theoretical assumptions that they make in informing their data analysis approach [18]. This difers from other qualitative data analysis approaches such as con-structing codebooks and establishing inter-rater reliability metrics, which might not ofer the fexibility needed for researchers to ac-tively participate in the analytic process in a systematic and rigorous way [18]. A refexive thematic analysis approach also encourages researchers to collaborate and discuss interpretations throughout the process to facilitate the generation of themes. Five  authors  participated  in  the  interview  data  analysis  pro-cess and continuously and collaboratively discussed the codes and themes  throughout.  We  followed  the  analysis  process  outlined in Braun and Clarke [2006]. First, all fve researchers familiarized ourselves with the data by reading through the transcripts and taking notes. We then began generating initial codes and divided the transcripts among the fve researchers\u2014each interview tran-script was read and used in initial code generation by at least two researchers. Each researcher reviewed nine to 11 transcripts at this phase and each generated hundreds of open codes. After initial codes were generated, we frequently met to search, review, discuss, and defne themes based on initial codes. In the early stages of our codes-to-themes process, we generated four domain categories (e.g., early-stage RAI challenges, RAI conceptualizations, RAI strategies and practices, aspirational RAI and improvements) and 53 prelim-inary themes through continuous discussions and iterations. All data was analyzed using shared spreadsheets and text documents. In parallel, two authors reviewed artifacts provided by participants, such as design sprint materials, framework documents, and user study fndings, to understand practices associated with early-stage AI application design. After discussing observations together, they shared them with the entire study team. After  further  review  and  discussion  of  all  observations  and themes, we distilled three emergent RAI practices: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications, each of which comprises two high-level themes, which we present in the Findings section.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "45d45811-d6aa-4b6a-8006-fcd1af3bfbbd",
                    "text": "Our author team is comprised of researchers with both academic and industry research backgrounds, with varied professional expe-riences that shape our perspectives. All researchers were employees of the company that served as the research site during the research period. One author has experience in an industry UX role at a large technology company. Two authors have experience developing applications that incorporate large-scale ML models. Two authors were born in and are currently, or have previously, lived in APAC countries, and four identify as white Americans. All authors completed the bulk of their research training, and work in, predominantly Western institutions. Five identify as having experience with marginalization in computing, either as a member  of a marginalized group themselves and/or through many years of conducting HCI research with marginalized groups. The authors\u2019 background and experiences infuence our posi-tionality: as HCI researchers trained and working in predominantly Western organizations, we acknowledge that complementary schol-arship related to our research questions is needed, to extend and further the understandings presented in this paper. Our position-ality has also infuenced the subjectivity inherent in framing our research questions, a snowball sampling approach that makes use of our professional networks, the study protocol design, and our data interpretation and analysis.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "d071477d-f06f-47ab-b93d-8d24ff4ee89c",
                    "text": "Through our data analysis, we identifed key emerging practices that UX practitioners carried out to meet evolving RAI needs. These practices are not linear, but are embedded throughout their work in iterative ways. In this section, we present three emerging RAI prac-tices: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications. For each practice, we frst introduce how RAI experts (labeled with \u2018R\u2019 throughout) and UX practitioners (labeled with \u2018U\u2019 throughout) enact and oper-ationalize RAI. We then highlight UX practitioners\u2019 RAI practice in light of the RAI work conducted by RAI experts in their formal job capacity, and present how their practices, goals, challenges, and aspirations for the role of UX in RAI take shape.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "a9e1cf57-2769-44ef-bff1-5e8741f7dede",
                    "text": "We begin by highlighting an overarching practice that infuencesour  subsequent  fndings:  the  importance  of  seeing  responsibility  as  a  \u201clens\u201d.  An  RAI  lens  positions  RAI  as  a  refexive,  ongoing,  and  holistic  perspective  that  infuences  practices  and  decisions  throughout  de-sign  and  development,  as  they  arise  in  context.  It  is  thus  not  bound  to  a  specifc  artifact,  protocol,  or  type  of  analysis  of  data  or  model  outputs\u2014though  it  can  incorporate  them.  Instead,  it  represents  an  ongoing,  shared  mindset  that  acknowledges  and  seeks  to  account  for  the  social  position  of  those  who  shape  technology\u2014and  the  company  itself\u2014and  the  intersecting  relationships  between  specifc  design  and  development  choices  and  their  societal  implications.  As  such,  both  RAI  experts  and  UX  practitioners  went  to  great  lengths  to  cultivate  and  reinforce  an  RAI  lens,  not  only  in  their  work,  but  in  their  teams  and  the  broader  culture.  This  practice  was  carried  out  explicitly  by  RAI  experts  who  inhabited  established  and  prominent  RAI  positions  with  focused  RAI  expertise  (e.g.,  ethics,  philosophy,  law)  in  the  company,  and  implicitly  by  UX  practitioners  who  considered  RAI  as  an  emerging  yet  crucial  piece  of  the  UX  of  AI  projects/products.  Below,  we  characterize  the  work  both  groups  of  practitioners  carried  out  to  pursue  this  goal  of  an  RAI  lens:  from  self-education  and  sensitization  to  RAI  issues,  to  actively  communicating  and  working  with  teams  to  surface  and  mitigate  RAI  issues.  Both  UX  practitioners  and  RAI  4.1.1   Sensitizing  to  RAI  Concerns.  experts  were  often  called  upon  by  product  teams  to  address  RAI  issues:  UX  practitioners  given  their  expertise  in  accounting  for  the  impacts  of  technology  on  users;  RAI  experts  given  their  formal  job  positions  and  expertise  in  RAI  and  ethics.  While  RAI  is  not  part  of  UX  practitioners\u2019  formal  training  and  role,  by  accumulating  experience  and  knowledge  of  technologies\u2019  impact  on  users,  RAI  had  already  been  integrated  into  many  UX  practitioners\u2019  day-to-day  practices  and  became  what  UX  practitioners  described  as  a  lens:  \u201cWith  my  lens,  I  make  [RAI  issues]  surface,  so  I\u2019m  not  sure  that  [RAI  issues]  would  just  naturally  bubble  up  to  the  surface.  But  I  think  part  of  my  research  ethos  is  to  look  for  those  gaps  and  those  red  fags.\u201d(U7)  For  RAI  experts,  a  large  part  of  their  job  included  sensitizing  [cf.  16]  teams  across  the  company  to  RAI  concerns  by  ofering  RAI  resources  and  support  such  as  RAI  reviews,  ofce  hours,  consul-tations,  and  RAI  workshops.  When  asked  about  their  processes  of  RAI  review  and  consultations,  many  RAI  experts  mentioned  the  usage  of  RAI  frameworks  and  guidelines,  many  of  which  they  (or  their  colleagues)  had  developed.  RAI  experts  not  only  used  these  RAI  frameworks  and  guidelines  to  sensitize  teams  to  RAI  consider-ations,  but  also  to  help  themselves  be  more  aware  of  potential  RAI  issues  in  their  work.  For  instance,  R6,  R8,  and  their  colleagues  are  creating  an  AI  harms  framework,  outlining  domain-specifc  RAI  considerations,  and  best  practices  to  help  engage  practitioners  with  RAI  issues.  R8  told  us:  \u201cI\u2019ve  found  [the  AI  harms  framework]  to  be  pretty  helpful  for  me  because  even  if  I  do  this  all  the  time,  I  can  still  forget  about  one  of  these  possible  negative  implications  that  could  happen.  But  teams  also  like  it  because  they  tend  to  be  new  to  these  ideas,  so  it\u2019s  helpful  for  them  to  just  be  like,  here  is  the  landscape  of  things  that  could  happen  in  a  pretty  digestible  format.\u201d  These  RAI  frameworks,  guidelines,  and  best  practices  established  by  RAI  experts  were  commonly  referred  to  by  UX  practitioners  during  their  interviews.  Given  that  RAI  was  not  part  of  their  formal  training  or  job  requirement,  to  sensitize  themselves  to  RAI  consid-erations,  UX  practitioners  often  explicitly  and  actively  sought  out  these  internal  RAI  resources  as  well  as  external  literature  to  better  understand  the  issues  and  integrate  them  into  their  work.  However,  a  few  UX  practitioners  also  mentioned  building  frame-works  from  scratch  to  meet  the  specifc  needs  of  an  application  domain.  Similar  to  RAI  experts\u2019  practices,  many  of  the  UX  prac-titioners  in  our  interviews  talked  about  their  eforts  to  compile  their  RAI  knowledge  and  experience  into  actionable  RAI  guidelines  and  best  practices.  Some  UX  practitioners  even  integrated  RAI  into  their  standard  practices  and  design  pipeline:  \u201cOne  of  our  team\u2019s  RAI  standard  practices  is  just  how  we  collect  data,  that  we\u2019re  putting  in  a  process  on  collecting  data  fairly  [...]  [F]olks  have  built  out  a  much  more  rigid  pipeline  for  how  we  collect  that  stuf  [...]  it\u2019s  just  part  of  the  standardized  practice  now.\u201d  (U14).  To  further  reinforce  their  RAI  perspective  during  the  design  pro-cess,  UX  practitioners  implemented  responsibility  lifts,  a  series  of  activities  at  the  beginning  of  the  design  process  to  reinforce  RAI  as  a  lens  to  inspect  and  flter  design  ideas.  U4  talked  about  a  design  sprint  on  ideating  about  AI  products  powered  by  LLMs:  \u201c[RAI]  was  called  out  in  the  brief  of  this  sprint  as  a  whole  to  think  about  responsibility  for  any  ideas  that  we  come  up  with.  And  what  are  the  responsibility  implications  of  those?\u201d  Similarly,  U14  who  organized   the  conversational  AI  design  sprint  and  invited  internal  guest  speak-ers  to  talk  about  RAI  at  the  beginning  of  the  sprint,  explained  his  rationale  during  the  interview:  \u201cEven  just  doing  stuf  like  that  [having  an  RAI  lightning  talk  at  the  beginning  of  the  sprint],  [...]  having  a  space  put  in  to  any  workshop  moving  forward,  or  any  team  doing  this  kind  of  work  with  AI  and  everything,  you  make  the  space  for  someone  to  give  a  presentation  like  that,  where  it  can  at  least  put  those  ideas  in  the  designer,  and  the  prototyper,  and  the  engineers\u2019  brains,  so  it  is  at  least  top  of  mind  [...]  you  make  space  for  responsible  design  and  thinking  when  you\u2019re  in  the  thick  of  it.\u201d  4.1.2   Organizational  Challenges  of  Communicating  about  RAI  with  Teams.   As  RAI  is  a  rather  new  and  still-emerging  discipline,  those  with  expertise  are  limited,  and  practitioners  with  demonstrated  expertise  are  often  called  upon  to  help  shepherd  projects  through  the  examination  of  RAI  considerations  and  mitigation  strategies.  Many  RAI  experts  and  UX  practitioners  in  our  study  were  members  of  centralized  teams  and  were  often  positioned  to  apply  RAI  exper-tise  horizontally.  As  such,  both  RAI  experts  and  UX  practitioners  described  \u201cdropping  in\u201d  and  then  out  of  specifc  teams  to  conduct  RAI  work  in  particular  phases.  Due  to  this  form  of  centralized  organizational  structure  (rather  than,  for  instance,  embedded  RAI  experts  and  UX  practitioner  with  permanent  roles  on  a  single  AI  team),  many  RAI  experts  and  UX  practitioners  thus  invested  signifcant  time  in  sensitizing  members  of  the  teams  they  were  \u201cdropping  in[to]\u201d  to  RAI  considerations.  Oftentimes,  RAI  experts  and  UX  practitioners  joined  AI  projects  that  were  still  relatively  early  in  development,  but  which  had  already  begun,  making  it  more  difcult  to  reverse  decisions  that  had  already  been  made  or  shape  the  design  direction  in  fundamental  ways,  particularly  as  they  worked  to  understand  and  navigate  the  power  dynamics  among  the  AI  team  with  whom  they  were  working.  Both  RAI  experts  and  UX  practitioners  talked  about  how  the  norms  and  implicit  values  associated  with  the  larger  organizational  culture  incentivized  \u201cmoving  fast\u201d  and  emphasizing  positive  out-comes  of  AI  systems  [cf.  53,  70,  84].  This  remained  a  challenge  even  for  RAI  experts  who  were  in  formal  positions  as  RAI  reviewers  or  ethics  consultants  that  had  more  power,  sometimes  with  blocking  power,  over  project/product  directions  and  releases.  In  their  inter-views,  many  RAI  experts  said  they  mostly  worked  with  teams  that  voluntarily  reached  out  to  them  for  RAI  consultations  and  reviews.  These  teams  tend  to  be  more  open  to  suggestions  and  critiques  that  RAI  experts  brought  up  during  the  process.  Many  RAI  experts  emphasized  teams\u2019  openness  in  discussing  RAI  and  initiatives  in  making  changes  as  paramount  when  working  with  the  teams  on  RAI  issues:  \u201c[RAI  consultations]  is  very  much  conversation-based.  And  we  steer  the  conversations.  But  the  important  thing  is  that  the  interest  has  to  come  from  [the  product/project  teams].  It  has  to  be  that  they  want  to  change  their  beliefs  and  behaviors.  We  can  try  and  impose  it,  of  course.  It\u2019s  always  better  if  this  kind  of  culture  change  comes  internally  rather  than  externally.\u201d  (R3)  However,  this  was  not  always  the  case  for  UX  practitioners  who  had  less  power  to  sway  the  project  directions  over  potential  RAI  concerns. UX practitioners also worked closely with the teams on the design and development of the products on a day-to-day basis and faced the same time constraints alongside the product teams. In the face of these pressures, UX practitioners described using a combination of communication techniques and product team activities to reconcile these incentives for speed with the introduction of new, more intentional RAI processes that encourage refection on a range of potential outcomes and harms of AI systems. Here, UX practitioners emphasized the importance of communi-cating potential RAI issues strategically and presenting themselves in a supportive role, rather than being seen as a \u201cblocker.\u201d U8 told us, \u201cMaking it as blameless as possible is one of the best things we\u2019ve learned. I\u2019m not trying to tell anyone they\u2019re bad, I\u2019m not trying to freak anyone out. I more want to highlight, here\u2019s something that could go wrong. Here are the ways that people could be afected, here\u2019s the ways the business could be afected. You can choose to act on that or not, but I would strongly advise you to do so.\u201d In the face of organizational pressures to ship products rapidly, UX practitioners took on additional hidden work to sensitize their team and organizational leadership to the potential harms of AI systems and the importance of mitigating those harms prior to deployment\u2014crucial labor that was not always recognized as being core to their work by their organization. When not directly negotiating with team members or organi-zational leadership about RAI concerns, UX practitioners would sometimes create activities or documents meant to enable team members to respond to RAI issues. One strategy included showing potential impacts through user testing that surfaced concerns, or by creating artifacts that illustrate potential harms. For example, U12 had difculty getting their team to respond to potential concerns and ran a team activity to create fake newspaper headlines [cf. 115] to illustrate how things could go wrong: \u201cI had put together a deck of like fake headlines of how this could go wrong. [...] I think at the time I think it had a big efect. People backed of the idea, we didn\u2019t have to go any further with it.\u201d This activity and others like it were part of the additional work that UX practitioners took on to sensitize others in the AI teams they worked with (and the company more broadly) to the range of potential harms from AI systems.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "9bb3e653-926b-47ae-b036-a47ea80d22b5",
                    "text": "",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "5ef09621-e3e0-42ed-9847-6105f3c82826",
                    "text": "During the interviews, both sets of practitioners described how they understood RAI in terms of anticipating and surfacing harms that AI technology could bring to the end-users, society, and the public. They both brought up similar sets of harms in their interviews that they tried to anticipate and surface, which included, but were not limited to: safety, misinformation, inequity and/or culture and iden-tity erasure, stereotyping, over-reliance on AI, anthropomorphism of AI, toxicity and/or ofensiveness, and privacy. However, RAI experts and UX practitioners approached this operational defnition of RAI diferently based on their respective methods and perspectives. In their interviews, RAI experts told us that given their expertise and the nature of their jobs, they were often invited by the teams to explicitly and intentionally look for RAI issues and concerns, often through question-asking and adversarial testing. As R8 described, \u201cI think the vast majority of  people are not thinking adversarial-y. I\u2019m here to think about the worst of the worst. That\u2019s what I was hired to do.\u201d When asked about how they helped the teams to surface harms, R6 said, \u201cIt\u2019s honestly just asking questions. This job is mostly just knowing what questions to ask. It\u2019s just critical thinking, and it\u2019s helping other teams think critically about their projects. We\u2019re always asking, what is the worst-case scenario?\u201d In contrast, while UX practitioners were not trained or required to surface and anticipate potential AI harms in their day-to-day work, we found that UX practitioners were committed to surface and anticipate harms from their unique human-focused perspective and with their unique skills, methods, and tools during their design and prototyping process. In this section, we describe how UX practitioners anticipated potential harms by envisioning how user interface design decisions could infuence the users\u2019 mental model of AI, and how they at-tempted to surface harms by leveraging both traditional and new approaches to design and prototyping. 4.2.1  Considering the Consequences of Users\u2019 Mental Models of AI.  During the interviews, UX practitioners told us that due to the stochastic nature of ML models they typically work with (in particular, generative language models, which often include some stochasticity to aid in producing variety in the model outputs), they were often concerned about how users would perceive AI applications driven by these models. In part, UX practitioners were concerned that users might overestimate the capabilities of the language model or treat the model as if it were human-like (i.e., anthropomorphizing it [80, 104]). UX practitioners felt responsible for appropriately communicating model capabilities to users, both through how they designed the user studies as well as the design of the application interface. For example, U1 discussed the potential misinterpretations of AI technologies such as LLMs: \u201cI think one of the other things about this technology [LLM] that is like, really a misnomer, is that, a lot of people see it as this, like general-purpose chatbot frst and then, \u2018oh, it can do all this other stuf.\u201d\u2019 To mitigate the potential consequences of people\u2019s mental mod-els of AI technology, UX practitioners leveraged diferent design strategies and techniques. During the interviews, UX practitioners mentioned techniques such as putting constraints on user input and model output, to limit model behavior (e.g., preventing it from generating answers to of-topic user questions) in AI applications; designing LLM-driven AI applications that are not chatbots to ex-pand people\u2019s understanding of LLMs; or changing the appearance of the UI of a chatbot application when they discovered users\u2019 expec-tations of an LLM-based chatbot didn\u2019t match the actual language interaction: \u2018So we had to reconfgure how we show these prototypes visually and also how we communicate about what these prototypes are. And that [higher-fdelity UI] was vital to change up. So the team quickly responded and changed the way that it looked, and they changed it from looking like a product to looking more like, terminal, like a very simple, old-school Lo-Fi terminal chatbot.\u201d (U7). Other UX participants brought up the need to build features and functions to allow users to dig deeper into the AI technology\u2019s ca-pability, to improve their understanding of the model. For example, U14 said \u201cProviding the insight for the user on what features and capabilities are possible helps expand the user\u2019s mental model of how they can interact with the system. That all develops trust [built on] knowing what\u2019s possible and what they can use the system for.\u201d During her interview, U10, a UX researcher, also refected on the UX practitioners\u2019 role and obligation in design to help users with low technology literacy understand the AI system and avoid overes-timating the model\u2019s capability or erroneously anthropomorphizing the LLM: \u201c[W]e might have many users who don\u2019t even know what AI or ML is. And so there were defnitely com-ponents that I think [raised] a very big, open question for me, in interviews where I asked people how they thought it worked, and they\u2019d be like, \u2018There\u2019s a person that\u2019s looking at these, and they get back to you really quickly.\u2019 And so, there\u2019s this question about what obli-gation, if any, do we have to correct that misconception, and what potential downstream harm could having that misconception lead to, and what might onboarding to a system like this look like for people who have lower tech literacy? [...] How do you help people understand that this is an AI system and what that means?\u201d 4.2.2  Examining ML Models through \u201cTest-Driving\u201d.  A large part of UX designers\u2019 job during early-stage AI application design, besides representing the user perspective as part of standard UX practice, was to understand the potential harm and capability of the ML model underlying the AI application. To do this, UX participants described how they supplemented traditional UX design and proto-typing methods with novel emerging methods to surface potential RAI issues that traditional methods may not have been able to uncover. Several participants mentioned using traditional UX design meth-ods such as Wizard-of-Oz studies or toy examples to quickly test out their AI design ideas. However, they also pointed out that these traditional design methods are fawed, as their ability to surface real-world RAI issues is limited. The scope, time frame, and re-searcher supervision constraints of typical user studies don\u2019t allow users to bring in situ, authentic personal data, needs, and use cases to bear on model interaction (U3). In part, the organizational factors that we described in section 4.1.2 impact UX practitioners\u2019 ability to conduct more longitudinal studies of the harms of AI systems in a more ecologically valid context. To work around those con-straints, UX practitioners used toy examples they believed to be representative of usage scenarios. However, as described by U10, the use of toy examples during user studies makes it difcult to surface RAI issues based on real-world usage: \u201c[O]ne of the hardest parts of prototyping a tool like this, is that it\u2019s a highly personal experience when you have a health condition, and your level of anxiety might be incredibly high or your level of investment in fnding relevant information may be way higher than you can get in an actual study [using toy examples]. And so, that\u2019s been a big challenge for us, is reading the tea leaves a little bit...\u201d  to interact with the ML models directly. For example, the Prompt-Maker tool we provided in the interview was already used by several UX participants. During prototyping activities that involved model prompting, UX practitioners performed what many referred to as \u201ctest-driving,\u201d which refers to the activity of continuously probing model outputs using diferent natural language prompts (that serve as input to the model) to understand model capabilities and limita-tions, in order to determine ft between a design idea and model capabilities. UX practitioners considered good prompt design to be an \u201cart form,\u201d in that it was difcult to identify reasons why prompt construction led to certain model outputs, making repeatable best practices and rules hard to distill. U5 recounted: \u201cI think it\u2019s [PromptMaker] a great tool to \u2018scratch pad\u2019 with, to move really quickly to just try something new. But generally, prompting is really weird. It\u2019s a kind of weird art form. Little weird things like just having one space key at the end of your prompt can change the complete output of what comes out. That\u2019s really subtle and hard to have somebody to understand what that means. So it\u2019s a useful tool, I love it, it was the most accessible thing I think that we\u2019ve created so far. But it\u2019s not just self-service yet, or intuitive enough on its own.\u201d This lack of consistency and intuitiveness of model prompting made it challenging to come up with a consistent practice to ad-dress the need to test-drive model interaction during early-stage AI application design. During his interview, U3 also raised the issue that prompting could further introduce bias, requiring intentional eforts to avoid \u201ccodifying our own belief systems. I think what hap-pens is the minute you get a new user that starts to ask questions to your system, then we need diferent sets of belief systems. And so if you\u2019re not thoughtful to [that] fact, your prompt might represent your beliefs in a stronger way...\u201d To mitigate this RAI issue during responsible prototyping, U3 said he liked to crowdsource few-shotexamples, and intentionally eliminated prompts that looked too similar: \u201cBecause what I\u2019m really looking for is, I\u2019m looking for few shots that represent a diversity of types of input that might come in. And sometimes I even model, slightly adversarial examples.\u201d However, most UX practitioners currently lack a standard practice to address this, beyond trying to diversify few-shot examples or deferring to user testing, which we discuss below.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "5a54394a-4931-4abb-b0c4-6683ad2f9a08",
                    "text": "",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "bb2776d6-1abb-4d3d-b88a-1e019a608547",
                    "text": "In our interviews, both sets of practitioners explained how they saw inclusion of a diversity of experiences and perspectives as a core dimension of RAI. Almost all RAI experts in our study pointed out the importance of user research and evaluation in ensuring accountability and responsibility, as well as validating and refecting on the social benefts of the AI products during the early-stage design process. However, several RAI experts called for longer-term engagement with users throughout the AI product lifecycle given that traditional user evaluation and testing might not be sufcient in designing Many of the UX participants in our study were able to get access to emerging UX prototyping tools that allowed UX practitioners RAI. For instance, R5 critiqued the short-term nature of user testing and questioned the assumptions in traditional user testing and evaluation processes: \u201cI think at base level, not making research just be like a short-term, one-of thing, but having it be something that you are doing constantly at all diferent parts of this design process. Making sure that [...] you are not just going in with pre-thought of categories where you are like, \u2018Ok, this is how we defne failure. Has this failure happened? No, it hasn\u2019t. Ok, great.\u2019 but kind of like collectively defning some of those terms with the users. I think just having that process be integrated throughout all of these diferent steps would probably be a little bit more helpful.\u201d Other RAI experts in our study more explicitly called out their aspiration of taking more participatory approaches to involve users as well as external stakeholders in the design and evaluation of RAI. For example, R2 said, \u2018I don\u2019t think responsible AI can be achieved without some form of robust participation in a nutshell. For me, respon-sible AI is the extent to which it can be made participatory. Responsible AI is participatory AI.\u201d Through our interviews with both RAI experts and UX practition-ers, we found that while involving users and taking participatory approaches to evaluate RAI remained largely an aspiration from RAI experts, UX practitioners, with their disciplinary perspective towards involving users and other stakeholders in design and eval-uation, were already carrying out this aspiration through involving prospective users of a future AI application in the design process to surface and assess RAI issues. In this section, we outline UX practitioners\u2019 processes and chal-lenges for involving users in the process of evaluating potential RAI issues: from preparing the model prior to involving users, to coping with unexpected model output during user evaluations. 4.3.1  Preventing Harmful Model Output Prior to User Involvement. Due to the inherent stochasticity of generative language models, UX practitioners frequently witnessed model outputs that they considered to be toxic when testing ideas through direct model interaction. Many UX practitioners, while hoping to get users in-volved as early as possible in RAI research to surface and identify potential harms, were also concerned about exposing users to these outputs. U8 told us that he was primarily \u201cconcerned [with] what can I do before this gets to someone who\u2019s not on the team...\u201d To address the uncertainty associated with potentially harm-ful model outputs, before inviting users in to interact with these models, teams generated a wide range of constraint-based input and output suppression techniques that often relied on classifers and flters. U15 described approaches to handling input and out-put that was \u201cinequitable\u201d in terms of ofending particular groups: \u201cOne [technique] is [to] detect if the user[s] themselves are initiating  something that is inequitable, if the model is giving out something that is inequitable, and [another] thing that we did was reduce the scope of what the model can do in terms of its output. The [other] is reduce the scope of what a person can do in terms of its input into the model.\u201d Sometimes, UX practitioners curated resources used to classify and mitigate toxicity: \u201cWe put in place a bunch of flters to make sure that it\u2019s on topic, that it\u2019s not ofensive, that it doesn\u2019t use a set of words or phrases that we\u2019ve specifcally banned.\u201d (U1) However, it was not always possible to prevent harmful model output entirely, in which case practitioners had to come up with other user study safeguards. U14 described these challenges in the context of their experiences in an AI application design sprint, during which they met milestones related to ideating, designing, prototyping, and conducting user testing with the prototypes all within a matter of days: \u201cI  think  we  had  two  and  a  half  days  until  we  were actually testing in front of [external, prospective] users, so it was so fast and rapid, which is one of the pros about it [...] but if something did go wrong, and if [the model] did come up with bad suggestions, or if we did have an answer that went of the rails, we couldn\u2019t just shut it down ... [all we could do] was just make the screen go blank, and hopefully they didn\u2019t see it in that second.\u201d Although developing guardrails and constraints is a common strategy to mitigate and prevent RAI issues with large-scale models (albeit one that may reproduce existing structural inequities in AI [37, 89]), some practitioners pointed out the importance of explor-ing other approaches, so as to not over-limit the types of inputs and topics that users can discuss. As R1 told us, \u201cI think we need to fgure out how to teach the model in a controlled-generation way, to respond more appropriately so you\u2019re not always taking the sledgehammer and just suppressing results.\u201d However, as R8 told us, UX practitioners and product teams also struggle with developing more fexible mitigation approaches for complex algorithmic assemblages, for which they believed a constraint-based mitigation strategy was the best feasible option for mitigating potentially toxic output: \u201c[for some applications] it\u2019s actually many models at once. [...] For [product] purposes, a blocklist at the code level is really the most feasible thing because [a product application team] can\u2019t go back and retrain the model.\u201d 4.3.2  User Evaluation of the AI Application.  Through our inter-views, we found that UX practitioners ascribed specifc purposes and meaning to user evaluation in order to meet RAI challenges in early-stage AI application design. They saw user testing as an avenue for surfacing and identifying levels of comfort with large-scale model interactions and identifying possible RAI concerns through user evaluation of AI application prototypes. Here, the line between user feedback about a potential AI application, and adversarial testing of the model underlying it, has the potential to blur. As U7 told us, \u201cWe have to change our perspective on how we user test these things [AI prototypes] and think about the greater good because if we just focus on \u2018oh the button needs to change\u2019 and \u2018people didn\u2019t like the font size,\u2019 we\u2019re in trouble.\u201d To surface and mitigate RAI issues during user evaluations, UX practitioners often sought to recruit a broad range of users to im-prove diversity in user testing. This often includes, but is not limited to, recruiting for users across diferent demographics, race, gender, tech literacy, age, etc. However, UX practitioners also needed to balance resource and time constraints while trying to diversify user testing eforts. This often leads to the need to prioritize certain RAI issues for user testing. For example, U15 described how he gave suggestions on prioritizing the RAI issues that the team knew the least about, given limited time and resources: \u201cThere are 21 things that you are doing. My suggestion would be to look at these fve ini-tially and redesign because my understanding of the model is that these fve things have not been tested previously. [...] Let\u2019s focus on things that we know the least about.\u201d Many UX practitioners talked about the importance of prepar-ing users for potentially toxic or otherwise harmful outputs that the AI application might generate during user evaluation sessions. Many also described their anxiety around viewing unpredictable model outputs with the users during these sessions. To mitigate this, UX practitioners discussed the importance of setting expec-tations and communicating with users at the beginning of user testing sessions: \u201cYou are showing them to an end user at the same time you\u2019re seeing them yourself. And so there\u2019s a certain amount of anxiety there. So, in the research protocol, it\u2019s really important to be able to set people\u2019s expectations: \u2018This is early technology, if you see some things that are harmful, I want you to be able to talk to me about it. I\u2019ll have some narrative to help you understand what you\u2019re seeing.\u201d\u2019(U3) UX practitioners also pointed out the difculties users could face in providing honest feedback and surfacing RAI issues that were meaningful to them during user study sessions. User stud-ies, especially those in which the researcher and participant have not built a relationship, have always grappled with the potential for social desirability bias, participant conformity to researchers\u2019 expectations, and preferences to avoid taboo topics or embarrass-ing (or personally invasive) social interactions. These concerns are made more salient when evaluating high-fdelity AI applications, given that large models could generate unpredictable and harm-ful outputs. To help mitigate this challenge, UX practitioners also sought to create a safe and encouraging environment for users to feel comfortable discussing RAI issues. U13 described one strategy they used to accomplish this by matching identity characteristics like practitioners\u2019 race and ethnicity with those of users, to help them feel comfortable talking about potential RAI concerns: \u201cI think one of the things [for] responsible AI consid-eration is when you\u2019re doing user research, does the makeup of your team and the people who are inter-viewing those folks, are users comfortable talking to them about fairness issues? We always try to match the race of the moderator to the race or the ethnicity of the participant for psychological safety. Because a lot of times, users don\u2019t really feel comfortable talking about inequity with a person who may not have experiences with systemic inequity. How are you making sure that  you\u2019re conducting not just research with users responsi-bly and ethically, but also you are pairing them with interviewers that could lead to honest feedback.\u201d Next, we refect on our fndings and the ways in which, taken together, they suggest an evolution of UX praxis. We conclude by refecting on opportunities for HCI research to move our feld closer to designing responsible AI.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "af1affff-1f5f-4183-8b8d-99ba9e8c5f8a",
                    "text": "In our fndings, we highlight three types of emerging RAI practices carried out by UX practitioners to meet RAI challenges: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications. Specifcally, we identifed strategies that UX practitioners employed to self-educate and communicate with others about potential RAI issues; challenges during proto-typing, such as communicating model capabilities (and limitations) to users; and current approaches to responsible evaluation of AI applications, including preparing both the models and the users for potentially harmful model outputs. In this section, we frst discuss implications of the hidden work of RAI conducted by UX practitioners, highlighting the labor as-sociated with these practices. We then discuss research opportu-nities and ways to support UX practitioners in their RAI practice, particularly for applying large-scale language models to develop applications. Finally, we identify opportunities to reconfgure the role of the user in designing RAI.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "042eb933-cb4c-47f6-88c7-29f8199ee261",
                    "text": "",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "0fb3ddbc-e44c-426e-9d3a-6f41a51415d2",
                    "text": "to Meet Responsible AI Challenges Michael Madaio Google Research New York, New York, USA madaiom@google.com Qiaosi Wang\u2217Georgia Institute of Technology Atlanta, Georgia, USA qswang@gatech.edu  Shaun Kane Google Research Boulder, Colorado, USA shaunkane@google.com Shivani Kapania Google Research Bangalore, India kapania@google.com  Michael Terry Google Research Cambridge, Massachusetts, USA michaelterry@google.com  Lauren Wilcox Google Research Mountain View, California, USA lwilcox@google.com ABSTRACT  Technology companies continue to invest in eforts to incorporate responsibility in their Artifcial Intelligence (AI) advancements, while eforts to audit and regulate AI systems expand. This shift towards Responsible AI (RAI) in the tech industry necessitates new practices and adaptations to roles\u2014undertaken by a variety of prac-titioners in more or less formal positions, many of whom focus on the user-centered aspects of AI. To better understand practices at the intersection of user experience (UX) and RAI, we conducted an interview study with industrial UX practitioners and RAI subject matter experts, both of whom are actively involved in addressing RAI concerns throughout the early design and development of new AI-based prototypes, demos, and products, at a large technology company. Many of the specifc practices and their associated chal-lenges have yet to be surfaced in the literature, and distilling them ofers a critical view into how practitioners\u2019 roles are adapting to meet present-day RAI challenges. We present and discuss three emerging practices in which RAI is being enacted and reifed in UX practitioners\u2019 everyday work. We conclude by arguing that the emerging practices, goals, and types of expertise that surfaced in our study point to an evolution in praxis, with associated challenges that suggest important areas for further research in HCI. CCS CONCEPTS \u2022 Human-centered computing \u2192 Empirical studies in HCI;KEYWORDS responsible AI; industry practice; UX; interview  INTRODUCTION  1  Technology companies continue to invest in eforts to work toward responsible design and development of AI, responding to increased demand to account for and mitigate the social risks posed by AI technology. As responsible AI (RAI ) eforts become more estab-lished as organizational practices [84], an increasing number of individuals and groups within both the private and public sector are responding to RAI-related research and product needs. Much of the literature focusing on RAI eforts is devoted to un-derstanding and improving machine learning (ML) models, through improvements to data annotation practices [9, 28, 31, 71], model evaluations [32, 56, 88], data and model documentation [36, 47, 83], or building developer-facing tools to help engineers interpret mod-els post-hoc [3, 6, 7, 10, 68, 107]. However, in contrast with a de-velopment paradigm in which the design of ML models occurs in parallel with their use in products, in many cases, ML models are in-creasingly used to power user-facing AI applications developed by wholly distinct product teams\u2014in some cases, by product teams at other organizations [15, 58]. Thus, RAI eforts in industry continue after models leave the research lab.Before deploying AI applications in deployment contexts where they could cause far-reaching societal consequences, many prac-titioners are undertaking RAI work, in more or less visible, and more or less formal, positions. Although prior RAI research in CHI and CSCW has focused on opportunities to intervene in AI devel-opment practices [e.g., 53, 69, 70, 84], it has (with few exceptions) focused on the work practices of data scientists and ML engineers in the model development process, rather than how a wider set of practitioners are involved in applying existing models for novel AI applications. Among this wider set of practitioners applying the models, many focus on user-facing and user-centered aspects of technology (e.g., interaction designers, user experience (UX) designers, or UX researchers). Yet, the specifc practices of such user-centered practitioners involved in RAI have yet to be identifed or formalized, and distilling them ofers a critical view into how they are adapting to meet present-day challenges in RAI. In this paper, we explore an emerging set of RAI practices car-ried out by user-centered practitioners when applying large models during early stages and refnement of AI application design. To understand these emerging user-centered RAI practices, we con-ducted interviews with UX practitioners, who are often involved in early-stage AI application ideation, design, prototyping, and evalu-ation, and with RAI subject matter experts (whom we refer to as \u201cRAI experts\u201d throughout the paper), who often perform evalua-tion and ofer consultation about responsibility in AI projects and products or product features, at a U.S. site of a large technology company that has a large AI function. These two sets of practition-ers were actively involved in addressing RAI concerns, formally by RAI experts and informally by UX practitioners, both early in design and refnement of new, AI-based prototypes, demos, and product features. We aim to identify and understand the emerging RAI work carried out by UX practitioners in their design processes, by investigating these practices in light of the RAI work conducted by RAI experts in their formal job capacity. We thus seek to answer two research questions: RQ 1:  How do UX practitioners currently incorporate and examine RAI considerations during early stages and refnement of AI application design given their organizational context? RQ 2:  What challenges do UX practitioners encounter in their cur-rent RAI practices during early stages and refnement of AI application design given their organizational context? We  conducted  a  refexive  thematic  analysis  of  interviews  with  participants  to  make  the  following  research  contributions:  (1)   An  identifcation  of  three  key  emerging  practices  that  UX  practitioners  are  developing  to  meet  evolving  RAI  needs:  a)  building  and  reinforcing  an  RAI  lens,  b)  responsible  prototyp-ing,  and  c)  responsible  evaluation  of  AI  applications.  (2)   A  refection  on  the  hidden  RAI  work  UX  practitioners  are  carrying  out  and  ways  to  support  this  evolution  in  UX  praxis  moving  forward.  (3)   A  discussion  of  the  implications  of  UX  practitioners\u2019  current  challenges  in  designing  with  ML  models  and  the  need  to  reconfgure  the  role  of  the  user  when  designing  RAI.  We frst situate our study with respect to prior work on RAI work practices in industry contexts, existing research on values and ethics in UX practice, and prior literature on designing and prototyping with ML models. After introducing our study and data analysis process, we identify three emerging RAI practices developed and carried out by UX practitioners in order to adapt existing UX prac-tices to meet evolving RAI challenges. These practices\u2014building and reinforcing an RAI lens, responsible prototyping, and responsi-ble evaluation of AI applications\u2014are not linear, but are embedded  throughout work practices in iterative ways. Within each emerging practice, we situate the RAI work of UX practitioners\u2019 with respect to the RAI work carried out by RAI experts in their formal job capac-ity, and highlight strategies and techniques that UX practitioners adopted, to sensitize people to RAI concerns, to communicate RAI issues with the teams, to consider potential consequences of users\u2019 mental models of AI systems (e.g., over-reliance on AI [cf. 80]), and to surface and mitigate potential RAI issues through an emerging AI prototyping technique called prompt programming. Finally, we present practices that place RAI concerns in direct conversation with traditional user evaluation approaches. We refect on these fndings and what they mean for the evolution of UX praxis in the Discussion, and conclude by highlighting opportunities for further HCI research to support the work of designing responsible AI. 2   RELATED  WORK  2.1   Responsible  AI  Practices  Among  Industry  Practitioners  A growing body of work in HCI examines the work practices of industry practitioners as they address RAI issues during the design and development process [53, 67, 69, 70, 84]\u2014in contrast with prior work that has studied data annotation [9, 28, 31, 71] and model evaluation [32, 56, 88], or developed resources for data and model documentation [36, 47, 83] or post-hoc interpretation of model performance [3, 6, 7, 10, 68, 107]. Prior work has found that AI prac-titioners seek to uncover fairness issues prior to deployment [53], yet many RAI approaches or mitigations are reactionary, initiated as a result of public relations issues, media attention, or customer complaints [53, 69, 84].  intervening Practitioners  have  emphasized  in  training datasets  [31,  32,  53],  as  well  as  algorithmic  mitigations  [2],  as critical in working toward RAI. However, AI practitioners fnd it challenging to represent diverse demographic groups in datasets and  in  fairness  assessments,  given  that  practitioners  tend  to draw  on  their  (often  homogenous  [106])  personal  experiences and perspectives when it comes to RAI [30, 53, 69]. Madaio et al. [2022] also note that the heuristics practitioners used to determine priorities  in  assessing  fairness  issues  (e.g.,  perceived  fairness severity, perceived brand impact, etc.) could compound existing inequities of AI systems [69]. Given these challenges, practitioners call  for  more  guidance  and  support  (e.g.,  practices,  tools,  and other resources) in working toward the design, development, and deployment of RAI [53, 69, 84]. To  fulfll  practitioners\u2019  needs  for  RAI  support,  a  plethora  of toolkits, guidelines, and other resources have been developed [e.g. 1, 4, 7, 10, 54, 68, 72]. Recent work has explored the sociotechnical practice of how RAI toolkits are integrated into practitioners\u2019 work-fows [30, 70, 73, 114]. For example, Deng et al. [2022] point out that these toolkits are often less prescriptive than required\u2014containing guidance on what to do (e.g., engage stakeholders), but not how to carry out those recommendations [30]. In addition, many RAI toolk-its are also designed to primarily support specifc types of technical work from technical practitioners to address RAI issues, rendering themselves less accessible to contributors with varying types of expertise [30, 114]. Relying on toolkits may also enact a form of techno-solutionism, by promoting or incentivizing solely technical solutions to the sociotechnical work of responsible AI [70, 90, 114]. Existing literature has also highlighted the role of organizational factors in RAI practices [53, 69, 70, 84], including the lack of organi-zational incentives to address RAI issues (or the disincentives to this work) [53, 70, 84]. Coupled with the lack of clarity over roles and responsibilities in RAI work [72, 84], addressing RAI issues during AI development often relies on individuals who are able to dedi-cate time to developing and promoting RAI processes\u2014processes that may or may not be formally adopted within the rest of the organization [70, 84]. Identifying, assessing, and mitigating RAI issues in datasets, algorithms, and model behavior often require practitioners (and their organizations) to invest signifcant time and resources, yet AI practitioners often face time and resource constraints in carrying out their RAI practices (including incen-tives to ship products on fast-paced timelines) [30, 53, 69, 84, 112]. Many scholars have called for changes in organizational structures and processes, to support practitioners\u2019 RAI practices instead of hindering them [53, 69, 70, 72, 84]. 2.2   Values  and  Ethics  in  UX  Practice  Although recent work has focused on AI practitioners\u2019 work prac-tices for RAI, substantial prior research has identifed the ways that values are instantiated in technology design more broadly [40, 57, 79, 103, 110], as well as through ongoing practices of technol-ogy use, appropriation, maintenance, and repair [e.g., 55]. As such, various methods, tools, and theoretical frameworks [e.g., 5, 24, 42\u2013 44, 92, 115] have been developed to support designers (including UX practitioners) in surfacing relevant values throughout the de-sign process and bringing those values to bear on design decisions (what JafariNaimi et al. [2015] refer to as the \u201cidentify/apply\u201d logic of values in design) [57]. For instance, Value-Sensitive Design (VSD) is a framework in-tended to support designers in understanding the role of specifc values in the design of technology, including how they might be promoted or undermined through a given system [42, 44]. In addi-tion, numerous other methods and tools have been developed to support designers\u2019 \u201cvalues work\u201d in technology design [112], in-cluding resources to identify relevant values and surface potential harms of technology, such as Envisioning Cards [43], Judgment Call [5], Timelines [115] and more (see Chivukula et al. [2021] for a review). Although VSD and related methods are not the focus of our work in this paper, they represent a key aspect of the training and resources that may inform how UX practitioners address values as part of the work involved in addressing RAI. 2.2.1  Situated  Work  Practices  of  UX  Practitioners.  As  part  of  a broader turn to practice within HCI scholarship [66], recent re-search has focused not only on developing methods and tools for designers to use, but has also explored the situated work practices of UX practitioners in engaging with values as part of their every-day work [e.g., 23, 25, 50, 112, 113]. For instance, Chivukula et al.  [2021] have identifed a set of \u201cidentity claims\u201d that UX practition-ers articulate for the various roles they take on as part of doing values work in their practice, including learner and educator (i.e., learning and teaching others on their team about ethics), translator (i.e., taking resources about values in one domain and translat-ing them to their own work), and advocate/activist, among others. Situating individual UX practitioners within larger social and orga-nizational contexts, Gray and Chivukula [2019] describe various factors that mediate the relationship between designers\u2019 individual ethical awareness and action, including the role of organizational practices [25, 50]. This body of scholarship has identifed the social and political work that UX practitioners engage in, in addition to their technical work; including rhetorical work to convince leadership of the value of UX (and of the importance of values work in UX) [e.g., 87]\u2014an issue of critical importance given the relatively lower status of UX and design compared with software engineering roles in large tech-nology companies [112]. Given the role that organizational factors play in mediating UX practitioners\u2019 values work in practice, Wong [2021] has identifed tactics for \u201csoft resistance\u201d that UX practi-tioners engage in to create space to address values in their work. In part, this involves making values visible and relevant to others in their organization (similar to the advocate or activist identity described by Chivukula et al. [2021]), as well as working to change organizational norms and practices from within, by tactically ex-panding the defnition of who is considered to be the \u201cuser\u201d [cf. 116], through leveraging (and trying to change) organizational goals and priorities (e.g., Objectives and Key Results, or Key Performance Indicators), or more broadly leveraging corporate logics to make a business case for values in UX [112]. However, as Wong [2021] point out, each of these tactics are par-tial and embody contradictions, in that they are trying to challenge, contest, or change organizational practices, while leveraging those same logics and discourses. Finally, as Wong [2021] identify, the values work of UX practitioners often involves \u201cwork outside of the technology design process,\u201d work that often involves substantial emotional labor [cf. 96], and which may not be valued as part of their everyday work [113]. Relatively little literature, however, has explored the values work that UX practitioners engage in as part of designing and developing AI applications, or how that work is aligned with the work practices of RAI more generally. 2.3   Designing  and  Prototyping  with  Machine  Learning  Models  Prior research has studied UX practitioners\u2019 current practices and challenges when designing and prototyping ML-powered AI appli-cations [38, 117, 119, 120, 122]. Existing work has found that UX practitioners, given their training in HCI and User-Centered De-sign (UCD), commonly leverage traditional HCI and UCD methods and toolkits when designing with AI, yet these are often insuf-fcient [117, 120, 122]. For example, traditional methods such as Wizard-Of-Oz, sketching, paper prototyping, and rapid prototyp-ing can fail to accommodate the non-deterministic behaviors and opaque mechanisms of AI systems [117, 119, 122]. Prior work has highlighted the need for high-fdelity prototypes that can generate tangible, realistic behaviors, instead of toy sce-narios, to elicit user feedback and better assess the potential human and societal impacts of AI applications [54, 119, 122]. Zdanowska and Taylor [2022] note that UX practitioners found that most HCI and UCD methods and tools place too much emphasis on the design and evaluation of the user interface, which is only one of many components when designing with AI [122]. Users\u2019 mental models of the AI system [120, 122] and the feasibility and user acceptance of the design [122] are also key considerations for UX practitioners. When designing and prototyping with AI, UX practitioners also frequently collaborate with technical experts such as engineers and data scientists [82, 97, 118] to gain a deeper understanding of the capabilities and limitations of the models [38, 117, 119]. While high-level abstractions are sufcient for UX practitioners to design AI applications [118, 122], a deeper understanding of ML model func-tionality could help UX practitioners better envision use cases that may not yet exist [38, 119]. However, this collaboration also poses challenges in that UX practitioners and engineers don\u2019t always share a common perspective, language, or workfow [82, 97, 119], leading UX practitioners to take on extra work to bridge disciplinary boundaries through sharing user stories and raw user feedback from user testing video recordings to help engineers understand user needs [98], and sometimes, adapting to and embracing a more data-centric culture to communicate user needs through both qualitative and quantitative metrics [118]. 2.3.1  Prototyping with Prompt Programming.  To meet the increas-ing demand for new UX prototyping and design tools when de-signing with AI, an emerging practice involves prototyping with ML models through prompt programming [20, 27, 60, 121]. Using prompting techniques [27, 60], UX practitioners are able to send natural language prompts to ML models as inputs and interact with the models directly to test-drive their capabilities and limita-tions [60]. Prior work has found that prompt-based prototyping with large language models (LLMs) helps UX practitioners reduce their reliance on engineers and developers to understand model ca-pabilities, speed up the prototyping process to test out initial ideas and \u201cfail fast\u201d [cf. 117], and better communicate with collaborators using prototypes as boundary objects [60]. Prompt programming is usually conducted with pre-trained, large-scale models such as LLMs [27, 60] and text-to-image gen-eration models [124], which are prone to generate outputs that may perpetuate social stereotypes, toxicity, discrimination, and exclusionary norms [15, 33, 48, 105]. Researchers have been ex-ploring ways to evaluate LLMs and other generative models prior to putting them into use. Common evaluation methods include manually generating general test cases, or tests targeted at specifc failure modes [59, 86], as well as automatically generating test in-puts using the model itself [46, 81]. However, others have noted that such \u201cbehavioral tests\u201d and use of benchmarks to prompt models to intentionally generate harmful outputs (so they can be prevented) often come with pitfalls that render these methods invalid [14]. As such, there are increasing calls for human-centered approaches from HCI and UX practitioners to support the work of identifying and mitigating RAI issues with LLMs [e.g., 12\u201315]. Yet, insights into  how prompt programming can facilitate or hinder UX practitioners\u2019 work on RAI has not been explored. 3   METHODS  3.1   Recruitment  To  investigate  our  research  questions,  we  conducted  semi-structured interviews with both UX practitioners (n = 15) and sub-ject matter experts in a designated RAI role (n = 8). We recruited participants through snowball sampling at our study site (via direct emails to contacts). Our study site was chosen due to the company\u2019s large AI function, as well as the depth of researcher access that could be achieved to participants\u2019 work practices and teams. All participants were recruited from the same company that the au-thors were employed at during the time of the study, a decision we discuss further in section 3.2, below. Inclusion criteria included UX practitioners who had worked on or were currently working on the design, prototyping, user research, or user evaluation of AI applica-tions (prototypes, demos, product features) that were powered by large-scale models. We specifcally sought out UX practitioners who were either directly or tangentially involved with addressing RAI concerns as part of their work with these applications. For RAI experts, our inclusion criteria included experts in a formal Respon-sible AI role, who had experience evaluating or being consulted about responsibility in AI projects and products or product features. Information about participants\u2019 job roles can be found in Table 1. UX participants had an average of 5.6 years experience at the com-pany (SD=2.3), an average of 11.8 years working in UX (SD=7.3), and an average of 7.8 years working with AI (SD=4.4); RAI expert participants had an average of 2.3 years working at the company (SD=1.6), an average of 2.4 years working in RAI (SD=1.2), and an average of 5.9 years working with AI (SD=2.6). We also asked participants to optionally share their gender identity: among UX participants, seven were women, eight were men, and one was non-binary. Among RAI experts, fve were women, two were men, and one was non-binary. Each participant was compensated through a donation to their charity of choice valuing $40 USD. 3.2   Data  Collection  Our study ran from June through July, 2022. All of the participants worked in the United States, in hybrid or fully remote roles at the time of study; hence, all interviews were conducted virtually through an internal virtual meeting platform. Except for two 30-minute interviews, and one 90-minute interview, all interviews lasted about 60 minutes. All participants provided written consent to participate in the research study before interviews began. Scoping the present work to a specifc technology company as a research site allowed us to take advantage of internal AI resources (described below in 3.2.3) to ground interview discussions. As researchers were also company employees, participants could provide more details  of  their  AI  projects  and  context  for  their  RAI  practices, while upholding confdentiality and IP protection. However, we Table 1: Interview participant information. UX participants worked across diferent AI product and research areas. Listed AI areas are based on the specifc projects/products mentioned in interviews. For RAI expert participants, we list their background. did not limit interview discussions to participants\u2019 current work experiences; many had experience working in multiple settings and companies, and we prompted them to refect more broadly on their experiences over the course of their careers. Given the sensitive nature of RAI topics, we also sought to foster a high level of transparency and trust in our communications with each practitioner, before, during, and after the qualitative interview. For example, we held pre-interview calls and shared research briefs with our participants to answer questions about the research, and shared our data interpretations and fndings back with each partici-pant by email, noting their specifc quotes in the paper and asking for any feedback before submission. To inform our research questions, we frst observed a conversa-tional AI design sprint. We did not include our observations as research data, but instead followed up with four sprint participants to enroll them in our interview study. We also studied any artifacts that were provided in the interview, including sprint artifacts dis-cussed. We discuss each interview protocol below, and they are each provided in Supplementary Materials. We also describe a prompt programming tool called PromptMaker [60] below, which we used as a probe during interviews with both UX practitioners and RAI experts. 3.2.1  UX Practitioner Protocol.  In the interviews with UX practi-tioners, we asked them to broadly describe the types of UX work they do related to AI-based prototypes, demos, and/or product fea-tures, and then to dive into more details by having them walk us through one or two specifc projects they had worked on. We specif-ically focused on how RAI issues surfaced in their work, mitigation strategies or precautions they used to address RAI issues, and how RAI issues might have infuenced the project direction. At the end of the interview, we also asked UX practitioners about their thoughts on how RAI processes for early-stage AI application design could be improved.  3.2.2  RAI Expert Protocol.  In our interviews with RAI experts, we asked participants in designated RAI roles about their experi-ences reviewing, analyzing, evaluating, and/or consulting on AI technologies as part of organizational RAI practice. We then asked the experts to walk us through an AI project that they had been involved with, which drew on their expertise, the RAI issues that were present, and how they worked with the project/product teams to address those RAI concerns. Towards the end of the interview, we also asked the experts to envision possible improvements to current RAI practices and possible processes or tools to support RAI in early-stage AI application design. 3.2.3  PromptMaker As a Probe.  During interviews with all par-ticipants, we introduced and described an LLM prototyping tool called PromptMaker, as described in Jiang et al. [2022]. Prompt-Maker provides a web-based interface to LLMs, enabling users to interactively write and test LLM prompts. PromptMaker also en-ables practitioners to remotely execute a prompt. For example, a basic prompt to translate English into French could be created (see example in footnote), then embedded within a prototype to test out a translation feature. Collectively, PromptMaker\u2019s capabilities enable practitioners to rapidly prototype and test new AI features in hours or days, without requiring signifcant machine learning experience. Many of the interview participants were familiar with Prompt-Maker and seven UX participants indicated in the interviews that they use PromptMaker in their daily job during early-stage AI appli-cation design. In our interviews, we used PromptMaker as a probe to understand how emerging AI design and prototyping tools were shaping UX practitioners\u2019 RAI practices and to understand the po-tential opportunities and challenges new AI design and prototyping tools present for RAI. 3.3   Data  Analysis  All interviews were video-recorded and later transcribed verbatim for data analysis purposes. To analyze the interview data, we drew on Braun and Clarke\u2019s [2019\u20132021] refexive thematic analysis ap-proach [17\u201319]. Refexive Thematic Analysis is a post-positivist approach that emphasizes researchers\u2019 role in knowledge produc-tion, including the philosophical stance and theoretical assumptions that they make in informing their data analysis approach [18]. This difers from other qualitative data analysis approaches such as con-structing codebooks and establishing inter-rater reliability metrics, which might not ofer the fexibility needed for researchers to ac-tively participate in the analytic process in a systematic and rigorous way [18]. A refexive thematic analysis approach also encourages researchers to collaborate and discuss interpretations throughout the process to facilitate the generation of themes. Five  authors  participated  in  the  interview  data  analysis  pro-cess and continuously and collaboratively discussed the codes and themes  throughout.  We  followed  the  analysis  process  outlined in Braun and Clarke [2006]. First, all fve researchers familiarized ourselves with the data by reading through the transcripts and taking notes. We then began generating initial codes and divided the transcripts among the fve researchers\u2014each interview tran-script was read and used in initial code generation by at least two researchers. Each researcher reviewed nine to 11 transcripts at this phase and each generated hundreds of open codes. After initial codes were generated, we frequently met to search, review, discuss, and defne themes based on initial codes. In the early stages of our codes-to-themes process, we generated four domain categories (e.g., early-stage RAI challenges, RAI conceptualizations, RAI strategies and practices, aspirational RAI and improvements) and 53 prelim-inary themes through continuous discussions and iterations. All data was analyzed using shared spreadsheets and text documents. In parallel, two authors reviewed artifacts provided by participants, such as design sprint materials, framework documents, and user study fndings, to understand practices associated with early-stage AI application design. After discussing observations together, they shared them with the entire study team. After  further  review  and  discussion  of  all  observations  and themes, we distilled three emergent RAI practices: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications, each of which comprises two high-level themes, which we present in the Findings section. 3.4   Author  Positionality  Our author team is comprised of researchers with both academic and industry research backgrounds, with varied professional expe-riences that shape our perspectives. All researchers were employees of the company that served as the research site during the research period. One author has experience in an industry UX role at a large technology company. Two authors have experience developing applications that incorporate large-scale ML models. Two authors were born in and are currently, or have previously, lived in APAC countries, and four identify as white Americans. All authors completed the bulk of their research training, and work in, predominantly Western institutions. Five identify as having experience with marginalization in computing, either as a member  of a marginalized group themselves and/or through many years of conducting HCI research with marginalized groups. The authors\u2019 background and experiences infuence our posi-tionality: as HCI researchers trained and working in predominantly Western organizations, we acknowledge that complementary schol-arship related to our research questions is needed, to extend and further the understandings presented in this paper. Our position-ality has also infuenced the subjectivity inherent in framing our research questions, a snowball sampling approach that makes use of our professional networks, the study protocol design, and our data interpretation and analysis. 4   FINDINGS  Through our data analysis, we identifed key emerging practices that UX practitioners carried out to meet evolving RAI needs. These practices are not linear, but are embedded throughout their work in iterative ways. In this section, we present three emerging RAI prac-tices: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications. For each practice, we frst introduce how RAI experts (labeled with \u2018R\u2019 throughout) and UX practitioners (labeled with \u2018U\u2019 throughout) enact and oper-ationalize RAI. We then highlight UX practitioners\u2019 RAI practice in light of the RAI work conducted by RAI experts in their formal job capacity, and present how their practices, goals, challenges, and aspirations for the role of UX in RAI take shape. 4.1   Building  and  Reinforcing  an  RAI  Lens  We begin by highlighting an overarching practice that infuencesour  subsequent  fndings:  the  importance  of  seeing  responsibility  as  a  \u201clens\u201d.  An  RAI  lens  positions  RAI  as  a  refexive,  ongoing,  and  holistic  perspective  that  infuences  practices  and  decisions  throughout  de-sign  and  development,  as  they  arise  in  context.  It  is  thus  not  bound  to  a  specifc  artifact,  protocol,  or  type  of  analysis  of  data  or  model  outputs\u2014though  it  can  incorporate  them.  Instead,  it  represents  an  ongoing,  shared  mindset  that  acknowledges  and  seeks  to  account  for  the  social  position  of  those  who  shape  technology\u2014and  the  company  itself\u2014and  the  intersecting  relationships  between  specifc  design  and  development  choices  and  their  societal  implications.  As  such,  both  RAI  experts  and  UX  practitioners  went  to  great  lengths  to  cultivate  and  reinforce  an  RAI  lens,  not  only  in  their  work,  but  in  their  teams  and  the  broader  culture.  This  practice  was  carried  out  explicitly  by  RAI  experts  who  inhabited  established  and  prominent  RAI  positions  with  focused  RAI  expertise  (e.g.,  ethics,  philosophy,  law)  in  the  company,  and  implicitly  by  UX  practitioners  who  considered  RAI  as  an  emerging  yet  crucial  piece  of  the  UX  of  AI  projects/products.  Below,  we  characterize  the  work  both  groups  of  practitioners  carried  out  to  pursue  this  goal  of  an  RAI  lens:  from  self-education  and  sensitization  to  RAI  issues,  to  actively  communicating  and  working  with  teams  to  surface  and  mitigate  RAI  issues.  Both  UX  practitioners  and  RAI  4.1.1   Sensitizing  to  RAI  Concerns.  experts  were  often  called  upon  by  product  teams  to  address  RAI  issues:  UX  practitioners  given  their  expertise  in  accounting  for  the  impacts  of  technology  on  users;  RAI  experts  given  their  formal  job  positions  and  expertise  in  RAI  and  ethics.  While  RAI  is  not  part  of  UX  practitioners\u2019  formal  training  and  role,  by  accumulating  experience  and  knowledge  of  technologies\u2019  impact  on  users,  RAI  had  already  been  integrated  into  many  UX  practitioners\u2019  day-to-day  practices  and  became  what  UX  practitioners  described  as  a  lens:  \u201cWith  my  lens,  I  make  [RAI  issues]  surface,  so  I\u2019m  not  sure  that  [RAI  issues]  would  just  naturally  bubble  up  to  the  surface.  But  I  think  part  of  my  research  ethos  is  to  look  for  those  gaps  and  those  red  fags.\u201d(U7)  For  RAI  experts,  a  large  part  of  their  job  included  sensitizing  [cf.  16]  teams  across  the  company  to  RAI  concerns  by  ofering  RAI  resources  and  support  such  as  RAI  reviews,  ofce  hours,  consul-tations,  and  RAI  workshops.  When  asked  about  their  processes  of  RAI  review  and  consultations,  many  RAI  experts  mentioned  the  usage  of  RAI  frameworks  and  guidelines,  many  of  which  they  (or  their  colleagues)  had  developed.  RAI  experts  not  only  used  these  RAI  frameworks  and  guidelines  to  sensitize  teams  to  RAI  consider-ations,  but  also  to  help  themselves  be  more  aware  of  potential  RAI  issues  in  their  work.  For  instance,  R6,  R8,  and  their  colleagues  are  creating  an  AI  harms  framework,  outlining  domain-specifc  RAI  considerations,  and  best  practices  to  help  engage  practitioners  with  RAI  issues.  R8  told  us:  \u201cI\u2019ve  found  [the  AI  harms  framework]  to  be  pretty  helpful  for  me  because  even  if  I  do  this  all  the  time,  I  can  still  forget  about  one  of  these  possible  negative  implications  that  could  happen.  But  teams  also  like  it  because  they  tend  to  be  new  to  these  ideas,  so  it\u2019s  helpful  for  them  to  just  be  like,  here  is  the  landscape  of  things  that  could  happen  in  a  pretty  digestible  format.\u201d  These  RAI  frameworks,  guidelines,  and  best  practices  established  by  RAI  experts  were  commonly  referred  to  by  UX  practitioners  during  their  interviews.  Given  that  RAI  was  not  part  of  their  formal  training  or  job  requirement,  to  sensitize  themselves  to  RAI  consid-erations,  UX  practitioners  often  explicitly  and  actively  sought  out  these  internal  RAI  resources  as  well  as  external  literature  to  better  understand  the  issues  and  integrate  them  into  their  work.  However,  a  few  UX  practitioners  also  mentioned  building  frame-works  from  scratch  to  meet  the  specifc  needs  of  an  application  domain.  Similar  to  RAI  experts\u2019  practices,  many  of  the  UX  prac-titioners  in  our  interviews  talked  about  their  eforts  to  compile  their  RAI  knowledge  and  experience  into  actionable  RAI  guidelines  and  best  practices.  Some  UX  practitioners  even  integrated  RAI  into  their  standard  practices  and  design  pipeline:  \u201cOne  of  our  team\u2019s  RAI  standard  practices  is  just  how  we  collect  data,  that  we\u2019re  putting  in  a  process  on  collecting  data  fairly  [...]  [F]olks  have  built  out  a  much  more  rigid  pipeline  for  how  we  collect  that  stuf  [...]  it\u2019s  just  part  of  the  standardized  practice  now.\u201d  (U14).  To  further  reinforce  their  RAI  perspective  during  the  design  pro-cess,  UX  practitioners  implemented  responsibility  lifts,  a  series  of  activities  at  the  beginning  of  the  design  process  to  reinforce  RAI  as  a  lens  to  inspect  and  flter  design  ideas.  U4  talked  about  a  design  sprint  on  ideating  about  AI  products  powered  by  LLMs:  \u201c[RAI]  was  called  out  in  the  brief  of  this  sprint  as  a  whole  to  think  about  responsibility  for  any  ideas  that  we  come  up  with.  And  what  are  the  responsibility  implications  of  those?\u201d  Similarly,  U14  who  organized   the  conversational  AI  design  sprint  and  invited  internal  guest  speak-ers  to  talk  about  RAI  at  the  beginning  of  the  sprint,  explained  his  rationale  during  the  interview:  \u201cEven  just  doing  stuf  like  that  [having  an  RAI  lightning  talk  at  the  beginning  of  the  sprint],  [...]  having  a  space  put  in  to  any  workshop  moving  forward,  or  any  team  doing  this  kind  of  work  with  AI  and  everything,  you  make  the  space  for  someone  to  give  a  presentation  like  that,  where  it  can  at  least  put  those  ideas  in  the  designer,  and  the  prototyper,  and  the  engineers\u2019  brains,  so  it  is  at  least  top  of  mind  [...]  you  make  space  for  responsible  design  and  thinking  when  you\u2019re  in  the  thick  of  it.\u201d  4.1.2   Organizational  Challenges  of  Communicating  about  RAI  with  Teams.   As  RAI  is  a  rather  new  and  still-emerging  discipline,  those  with  expertise  are  limited,  and  practitioners  with  demonstrated  expertise  are  often  called  upon  to  help  shepherd  projects  through  the  examination  of  RAI  considerations  and  mitigation  strategies.  Many  RAI  experts  and  UX  practitioners  in  our  study  were  members  of  centralized  teams  and  were  often  positioned  to  apply  RAI  exper-tise  horizontally.  As  such,  both  RAI  experts  and  UX  practitioners  described  \u201cdropping  in\u201d  and  then  out  of  specifc  teams  to  conduct  RAI  work  in  particular  phases.  Due  to  this  form  of  centralized  organizational  structure  (rather  than,  for  instance,  embedded  RAI  experts  and  UX  practitioner  with  permanent  roles  on  a  single  AI  team),  many  RAI  experts  and  UX  practitioners  thus  invested  signifcant  time  in  sensitizing  members  of  the  teams  they  were  \u201cdropping  in[to]\u201d  to  RAI  considerations.  Oftentimes,  RAI  experts  and  UX  practitioners  joined  AI  projects  that  were  still  relatively  early  in  development,  but  which  had  already  begun,  making  it  more  difcult  to  reverse  decisions  that  had  already  been  made  or  shape  the  design  direction  in  fundamental  ways,  particularly  as  they  worked  to  understand  and  navigate  the  power  dynamics  among  the  AI  team  with  whom  they  were  working.  Both  RAI  experts  and  UX  practitioners  talked  about  how  the  norms  and  implicit  values  associated  with  the  larger  organizational  culture  incentivized  \u201cmoving  fast\u201d  and  emphasizing  positive  out-comes  of  AI  systems  [cf.  53,  70,  84].  This  remained  a  challenge  even  for  RAI  experts  who  were  in  formal  positions  as  RAI  reviewers  or  ethics  consultants  that  had  more  power,  sometimes  with  blocking  power,  over  project/product  directions  and  releases.  In  their  inter-views,  many  RAI  experts  said  they  mostly  worked  with  teams  that  voluntarily  reached  out  to  them  for  RAI  consultations  and  reviews.  These  teams  tend  to  be  more  open  to  suggestions  and  critiques  that  RAI  experts  brought  up  during  the  process.  Many  RAI  experts  emphasized  teams\u2019  openness  in  discussing  RAI  and  initiatives  in  making  changes  as  paramount  when  working  with  the  teams  on  RAI  issues:  \u201c[RAI  consultations]  is  very  much  conversation-based.  And  we  steer  the  conversations.  But  the  important  thing  is  that  the  interest  has  to  come  from  [the  product/project  teams].  It  has  to  be  that  they  want  to  change  their  beliefs  and  behaviors.  We  can  try  and  impose  it,  of  course.  It\u2019s  always  better  if  this  kind  of  culture  change  comes  internally  rather  than  externally.\u201d  (R3)  However,  this  was  not  always  the  case  for  UX  practitioners  who  had  less  power  to  sway  the  project  directions  over  potential  RAI  concerns. UX practitioners also worked closely with the teams on the design and development of the products on a day-to-day basis and faced the same time constraints alongside the product teams. In the face of these pressures, UX practitioners described using a combination of communication techniques and product team activities to reconcile these incentives for speed with the introduction of new, more intentional RAI processes that encourage refection on a range of potential outcomes and harms of AI systems. Here, UX practitioners emphasized the importance of communi-cating potential RAI issues strategically and presenting themselves in a supportive role, rather than being seen as a \u201cblocker.\u201d U8 told us, \u201cMaking it as blameless as possible is one of the best things we\u2019ve learned. I\u2019m not trying to tell anyone they\u2019re bad, I\u2019m not trying to freak anyone out. I more want to highlight, here\u2019s something that could go wrong. Here are the ways that people could be afected, here\u2019s the ways the business could be afected. You can choose to act on that or not, but I would strongly advise you to do so.\u201d In the face of organizational pressures to ship products rapidly, UX practitioners took on additional hidden work to sensitize their team and organizational leadership to the potential harms of AI systems and the importance of mitigating those harms prior to deployment\u2014crucial labor that was not always recognized as being core to their work by their organization. When not directly negotiating with team members or organi-zational leadership about RAI concerns, UX practitioners would sometimes create activities or documents meant to enable team members to respond to RAI issues. One strategy included showing potential impacts through user testing that surfaced concerns, or by creating artifacts that illustrate potential harms. For example, U12 had difculty getting their team to respond to potential concerns and ran a team activity to create fake newspaper headlines [cf. 115] to illustrate how things could go wrong: \u201cI had put together a deck of like fake headlines of how this could go wrong. [...] I think at the time I think it had a big efect. People backed of the idea, we didn\u2019t have to go any further with it.\u201d This activity and others like it were part of the additional work that UX practitioners took on to sensitize others in the AI teams they worked with (and the company more broadly) to the range of potential harms from AI systems. 4.2   Responsible  Prototyping:  Ideating  and  Building  with  Machine  Learning  Models  During the interviews, both sets of practitioners described how they understood RAI in terms of anticipating and surfacing harms that AI technology could bring to the end-users, society, and the public. They both brought up similar sets of harms in their interviews that they tried to anticipate and surface, which included, but were not limited to: safety, misinformation, inequity and/or culture and iden-tity erasure, stereotyping, over-reliance on AI, anthropomorphism of AI, toxicity and/or ofensiveness, and privacy. However, RAI experts and UX practitioners approached this operational defnition of RAI diferently based on their respective methods and perspectives. In their interviews, RAI experts told us that given their expertise and the nature of their jobs, they were often invited by the teams to explicitly and intentionally look for RAI issues and concerns, often through question-asking and adversarial testing. As R8 described, \u201cI think the vast majority of  people are not thinking adversarial-y. I\u2019m here to think about the worst of the worst. That\u2019s what I was hired to do.\u201d When asked about how they helped the teams to surface harms, R6 said, \u201cIt\u2019s honestly just asking questions. This job is mostly just knowing what questions to ask. It\u2019s just critical thinking, and it\u2019s helping other teams think critically about their projects. We\u2019re always asking, what is the worst-case scenario?\u201d In contrast, while UX practitioners were not trained or required to surface and anticipate potential AI harms in their day-to-day work, we found that UX practitioners were committed to surface and anticipate harms from their unique human-focused perspective and with their unique skills, methods, and tools during their design and prototyping process. In this section, we describe how UX practitioners anticipated potential harms by envisioning how user interface design decisions could infuence the users\u2019 mental model of AI, and how they at-tempted to surface harms by leveraging both traditional and new approaches to design and prototyping. 4.2.1  Considering the Consequences of Users\u2019 Mental Models of AI.  During the interviews, UX practitioners told us that due to the stochastic nature of ML models they typically work with (in particular, generative language models, which often include some stochasticity to aid in producing variety in the model outputs), they were often concerned about how users would perceive AI applications driven by these models. In part, UX practitioners were concerned that users might overestimate the capabilities of the language model or treat the model as if it were human-like (i.e., anthropomorphizing it [80, 104]). UX practitioners felt responsible for appropriately communicating model capabilities to users, both through how they designed the user studies as well as the design of the application interface. For example, U1 discussed the potential misinterpretations of AI technologies such as LLMs: \u201cI think one of the other things about this technology [LLM] that is like, really a misnomer, is that, a lot of people see it as this, like general-purpose chatbot frst and then, \u2018oh, it can do all this other stuf.\u201d\u2019 To mitigate the potential consequences of people\u2019s mental mod-els of AI technology, UX practitioners leveraged diferent design strategies and techniques. During the interviews, UX practitioners mentioned techniques such as putting constraints on user input and model output, to limit model behavior (e.g., preventing it from generating answers to of-topic user questions) in AI applications; designing LLM-driven AI applications that are not chatbots to ex-pand people\u2019s understanding of LLMs; or changing the appearance of the UI of a chatbot application when they discovered users\u2019 expec-tations of an LLM-based chatbot didn\u2019t match the actual language interaction: \u2018So we had to reconfgure how we show these prototypes visually and also how we communicate about what these prototypes are. And that [higher-fdelity UI] was vital to change up. So the team quickly responded and changed the way that it looked, and they changed it from looking like a product to looking more like, terminal, like a very simple, old-school Lo-Fi terminal chatbot.\u201d (U7). Other UX participants brought up the need to build features and functions to allow users to dig deeper into the AI technology\u2019s ca-pability, to improve their understanding of the model. For example, U14 said \u201cProviding the insight for the user on what features and capabilities are possible helps expand the user\u2019s mental model of how they can interact with the system. That all develops trust [built on] knowing what\u2019s possible and what they can use the system for.\u201d During her interview, U10, a UX researcher, also refected on the UX practitioners\u2019 role and obligation in design to help users with low technology literacy understand the AI system and avoid overes-timating the model\u2019s capability or erroneously anthropomorphizing the LLM: \u201c[W]e might have many users who don\u2019t even know what AI or ML is. And so there were defnitely com-ponents that I think [raised] a very big, open question for me, in interviews where I asked people how they thought it worked, and they\u2019d be like, \u2018There\u2019s a person that\u2019s looking at these, and they get back to you really quickly.\u2019 And so, there\u2019s this question about what obli-gation, if any, do we have to correct that misconception, and what potential downstream harm could having that misconception lead to, and what might onboarding to a system like this look like for people who have lower tech literacy? [...] How do you help people understand that this is an AI system and what that means?\u201d 4.2.2  Examining ML Models through \u201cTest-Driving\u201d.  A large part of UX designers\u2019 job during early-stage AI application design, besides representing the user perspective as part of standard UX practice, was to understand the potential harm and capability of the ML model underlying the AI application. To do this, UX participants described how they supplemented traditional UX design and proto-typing methods with novel emerging methods to surface potential RAI issues that traditional methods may not have been able to uncover. Several participants mentioned using traditional UX design meth-ods such as Wizard-of-Oz studies or toy examples to quickly test out their AI design ideas. However, they also pointed out that these traditional design methods are fawed, as their ability to surface real-world RAI issues is limited. The scope, time frame, and re-searcher supervision constraints of typical user studies don\u2019t allow users to bring in situ, authentic personal data, needs, and use cases to bear on model interaction (U3). In part, the organizational factors that we described in section 4.1.2 impact UX practitioners\u2019 ability to conduct more longitudinal studies of the harms of AI systems in a more ecologically valid context. To work around those con-straints, UX practitioners used toy examples they believed to be representative of usage scenarios. However, as described by U10, the use of toy examples during user studies makes it difcult to surface RAI issues based on real-world usage: \u201c[O]ne of the hardest parts of prototyping a tool like this, is that it\u2019s a highly personal experience when you have a health condition, and your level of anxiety might be incredibly high or your level of investment in fnding relevant information may be way higher than you can get in an actual study [using toy examples]. And so, that\u2019s been a big challenge for us, is reading the tea leaves a little bit...\u201d  to interact with the ML models directly. For example, the Prompt-Maker tool we provided in the interview was already used by several UX participants. During prototyping activities that involved model prompting, UX practitioners performed what many referred to as \u201ctest-driving,\u201d which refers to the activity of continuously probing model outputs using diferent natural language prompts (that serve as input to the model) to understand model capabilities and limita-tions, in order to determine ft between a design idea and model capabilities. UX practitioners considered good prompt design to be an \u201cart form,\u201d in that it was difcult to identify reasons why prompt construction led to certain model outputs, making repeatable best practices and rules hard to distill. U5 recounted: \u201cI think it\u2019s [PromptMaker] a great tool to \u2018scratch pad\u2019 with, to move really quickly to just try something new. But generally, prompting is really weird. It\u2019s a kind of weird art form. Little weird things like just having one space key at the end of your prompt can change the complete output of what comes out. That\u2019s really subtle and hard to have somebody to understand what that means. So it\u2019s a useful tool, I love it, it was the most accessible thing I think that we\u2019ve created so far. But it\u2019s not just self-service yet, or intuitive enough on its own.\u201d This lack of consistency and intuitiveness of model prompting made it challenging to come up with a consistent practice to ad-dress the need to test-drive model interaction during early-stage AI application design. During his interview, U3 also raised the issue that prompting could further introduce bias, requiring intentional eforts to avoid \u201ccodifying our own belief systems. I think what hap-pens is the minute you get a new user that starts to ask questions to your system, then we need diferent sets of belief systems. And so if you\u2019re not thoughtful to [that] fact, your prompt might represent your beliefs in a stronger way...\u201d To mitigate this RAI issue during responsible prototyping, U3 said he liked to crowdsource few-shotexamples, and intentionally eliminated prompts that looked too similar: \u201cBecause what I\u2019m really looking for is, I\u2019m looking for few shots that represent a diversity of types of input that might come in. And sometimes I even model, slightly adversarial examples.\u201d However, most UX practitioners currently lack a standard practice to address this, beyond trying to diversify few-shot examples or deferring to user testing, which we discuss below. 4.3   Responsible  Evaluation  of  AI  Applications:  Involving  Users  to  Assess  Responsible  AI  In our interviews, both sets of practitioners explained how they saw inclusion of a diversity of experiences and perspectives as a core dimension of RAI. Almost all RAI experts in our study pointed out the importance of user research and evaluation in ensuring accountability and responsibility, as well as validating and refecting on the social benefts of the AI products during the early-stage design process. However, several RAI experts called for longer-term engagement with users throughout the AI product lifecycle given that traditional user evaluation and testing might not be sufcient in designing Many of the UX participants in our study were able to get access to emerging UX prototyping tools that allowed UX practitioners RAI. For instance, R5 critiqued the short-term nature of user testing and questioned the assumptions in traditional user testing and evaluation processes: \u201cI think at base level, not making research just be like a short-term, one-of thing, but having it be something that you are doing constantly at all diferent parts of this design process. Making sure that [...] you are not just going in with pre-thought of categories where you are like, \u2018Ok, this is how we defne failure. Has this failure happened? No, it hasn\u2019t. Ok, great.\u2019 but kind of like collectively defning some of those terms with the users. I think just having that process be integrated throughout all of these diferent steps would probably be a little bit more helpful.\u201d Other RAI experts in our study more explicitly called out their aspiration of taking more participatory approaches to involve users as well as external stakeholders in the design and evaluation of RAI. For example, R2 said, \u2018I don\u2019t think responsible AI can be achieved without some form of robust participation in a nutshell. For me, respon-sible AI is the extent to which it can be made participatory. Responsible AI is participatory AI.\u201d Through our interviews with both RAI experts and UX practition-ers, we found that while involving users and taking participatory approaches to evaluate RAI remained largely an aspiration from RAI experts, UX practitioners, with their disciplinary perspective towards involving users and other stakeholders in design and eval-uation, were already carrying out this aspiration through involving prospective users of a future AI application in the design process to surface and assess RAI issues. In this section, we outline UX practitioners\u2019 processes and chal-lenges for involving users in the process of evaluating potential RAI issues: from preparing the model prior to involving users, to coping with unexpected model output during user evaluations. 4.3.1  Preventing Harmful Model Output Prior to User Involvement. Due to the inherent stochasticity of generative language models, UX practitioners frequently witnessed model outputs that they considered to be toxic when testing ideas through direct model interaction. Many UX practitioners, while hoping to get users in-volved as early as possible in RAI research to surface and identify potential harms, were also concerned about exposing users to these outputs. U8 told us that he was primarily \u201cconcerned [with] what can I do before this gets to someone who\u2019s not on the team...\u201d To address the uncertainty associated with potentially harm-ful model outputs, before inviting users in to interact with these models, teams generated a wide range of constraint-based input and output suppression techniques that often relied on classifers and flters. U15 described approaches to handling input and out-put that was \u201cinequitable\u201d in terms of ofending particular groups: \u201cOne [technique] is [to] detect if the user[s] themselves are initiating  something that is inequitable, if the model is giving out something that is inequitable, and [another] thing that we did was reduce the scope of what the model can do in terms of its output. The [other] is reduce the scope of what a person can do in terms of its input into the model.\u201d Sometimes, UX practitioners curated resources used to classify and mitigate toxicity: \u201cWe put in place a bunch of flters to make sure that it\u2019s on topic, that it\u2019s not ofensive, that it doesn\u2019t use a set of words or phrases that we\u2019ve specifcally banned.\u201d (U1) However, it was not always possible to prevent harmful model output entirely, in which case practitioners had to come up with other user study safeguards. U14 described these challenges in the context of their experiences in an AI application design sprint, during which they met milestones related to ideating, designing, prototyping, and conducting user testing with the prototypes all within a matter of days: \u201cI  think  we  had  two  and  a  half  days  until  we  were actually testing in front of [external, prospective] users, so it was so fast and rapid, which is one of the pros about it [...] but if something did go wrong, and if [the model] did come up with bad suggestions, or if we did have an answer that went of the rails, we couldn\u2019t just shut it down ... [all we could do] was just make the screen go blank, and hopefully they didn\u2019t see it in that second.\u201d Although developing guardrails and constraints is a common strategy to mitigate and prevent RAI issues with large-scale models (albeit one that may reproduce existing structural inequities in AI [37, 89]), some practitioners pointed out the importance of explor-ing other approaches, so as to not over-limit the types of inputs and topics that users can discuss. As R1 told us, \u201cI think we need to fgure out how to teach the model in a controlled-generation way, to respond more appropriately so you\u2019re not always taking the sledgehammer and just suppressing results.\u201d However, as R8 told us, UX practitioners and product teams also struggle with developing more fexible mitigation approaches for complex algorithmic assemblages, for which they believed a constraint-based mitigation strategy was the best feasible option for mitigating potentially toxic output: \u201c[for some applications] it\u2019s actually many models at once. [...] For [product] purposes, a blocklist at the code level is really the most feasible thing because [a product application team] can\u2019t go back and retrain the model.\u201d 4.3.2  User Evaluation of the AI Application.  Through our inter-views, we found that UX practitioners ascribed specifc purposes and meaning to user evaluation in order to meet RAI challenges in early-stage AI application design. They saw user testing as an avenue for surfacing and identifying levels of comfort with large-scale model interactions and identifying possible RAI concerns through user evaluation of AI application prototypes. Here, the line between user feedback about a potential AI application, and adversarial testing of the model underlying it, has the potential to blur. As U7 told us, \u201cWe have to change our perspective on how we user test these things [AI prototypes] and think about the greater good because if we just focus on \u2018oh the button needs to change\u2019 and \u2018people didn\u2019t like the font size,\u2019 we\u2019re in trouble.\u201d To surface and mitigate RAI issues during user evaluations, UX practitioners often sought to recruit a broad range of users to im-prove diversity in user testing. This often includes, but is not limited to, recruiting for users across diferent demographics, race, gender, tech literacy, age, etc. However, UX practitioners also needed to balance resource and time constraints while trying to diversify user testing eforts. This often leads to the need to prioritize certain RAI issues for user testing. For example, U15 described how he gave suggestions on prioritizing the RAI issues that the team knew the least about, given limited time and resources: \u201cThere are 21 things that you are doing. My suggestion would be to look at these fve ini-tially and redesign because my understanding of the model is that these fve things have not been tested previously. [...] Let\u2019s focus on things that we know the least about.\u201d Many UX practitioners talked about the importance of prepar-ing users for potentially toxic or otherwise harmful outputs that the AI application might generate during user evaluation sessions. Many also described their anxiety around viewing unpredictable model outputs with the users during these sessions. To mitigate this, UX practitioners discussed the importance of setting expec-tations and communicating with users at the beginning of user testing sessions: \u201cYou are showing them to an end user at the same time you\u2019re seeing them yourself. And so there\u2019s a certain amount of anxiety there. So, in the research protocol, it\u2019s really important to be able to set people\u2019s expectations: \u2018This is early technology, if you see some things that are harmful, I want you to be able to talk to me about it. I\u2019ll have some narrative to help you understand what you\u2019re seeing.\u201d\u2019(U3) UX practitioners also pointed out the difculties users could face in providing honest feedback and surfacing RAI issues that were meaningful to them during user study sessions. User stud-ies, especially those in which the researcher and participant have not built a relationship, have always grappled with the potential for social desirability bias, participant conformity to researchers\u2019 expectations, and preferences to avoid taboo topics or embarrass-ing (or personally invasive) social interactions. These concerns are made more salient when evaluating high-fdelity AI applications, given that large models could generate unpredictable and harm-ful outputs. To help mitigate this challenge, UX practitioners also sought to create a safe and encouraging environment for users to feel comfortable discussing RAI issues. U13 described one strategy they used to accomplish this by matching identity characteristics like practitioners\u2019 race and ethnicity with those of users, to help them feel comfortable talking about potential RAI concerns: \u201cI think one of the things [for] responsible AI consid-eration is when you\u2019re doing user research, does the makeup of your team and the people who are inter-viewing those folks, are users comfortable talking to them about fairness issues? We always try to match the race of the moderator to the race or the ethnicity of the participant for psychological safety. Because a lot of times, users don\u2019t really feel comfortable talking about inequity with a person who may not have experiences with systemic inequity. How are you making sure that  you\u2019re conducting not just research with users responsi-bly and ethically, but also you are pairing them with interviewers that could lead to honest feedback.\u201d Next, we refect on our fndings and the ways in which, taken together, they suggest an evolution of UX praxis. We conclude by refecting on opportunities for HCI research to move our feld closer to designing responsible AI. 5   DISCUSSION  In our fndings, we highlight three types of emerging RAI practices carried out by UX practitioners to meet RAI challenges: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications. Specifcally, we identifed strategies that UX practitioners employed to self-educate and communicate with others about potential RAI issues; challenges during proto-typing, such as communicating model capabilities (and limitations) to users; and current approaches to responsible evaluation of AI applications, including preparing both the models and the users for potentially harmful model outputs. In this section, we frst discuss implications of the hidden work of RAI conducted by UX practitioners, highlighting the labor as-sociated with these practices. We then discuss research opportu-nities and ways to support UX practitioners in their RAI practice, particularly for applying large-scale language models to develop applications. Finally, we identify opportunities to reconfgure the role of the user in designing RAI. 5.1   Supporting  the  Hidden  Work  of  RAI  in  UX  Practice  In light of increasing calls for UX practitioners to be involved in the work of identifying and addressing issues of RAI in technology design [e.g., 13\u201315, 53, 69, 70, 84], we highlight the hidden work that UX practitioners are conducting to meet RAI challenges. We call out this hidden work by situating UX practitioners\u2019 emerging RAI practices in the context of the work conducted by RAI experts in their formal RAI roles. We fnd that UX practitioners and RAI experts share similar conceptualizations and aspirations for the human impacts of AI\u2014they both saw RAI as, in large part, antici-pating, surfacing, and mitigating a variety of potential harms for users and other stakeholders who may be impacted by a given AI system. Through our study, we see RAI experts carrying out the work of developing the RAI agenda, guidelines, toolkits, evaluation, and foundational research at an organizational level throughout the entire AI design and development process, while UX practition-ers operate in specifc application areas, contexts, and domains to actively promote, implement, and adapt RAI to ft their day-to-day work in early-stage AI application design. Drawing upon their unique human-centered values and design techniques, UX practitioners are adapting their UX practices, negoti-ating their respective roles in RAI work, and learning and educating others about human-centered values and RAI concerns. However, carrying out these RAI eforts often requires UX practitioners to devote additional time and eforts in their day-to-day work, or what Star and Strauss [1999] have described as \u201cinvisible work,\u201d or akin to what Strauss [1988] has referred to as \u201carticulation work,\u201d or the work required to make other work happen. We fnd that UX practitioners take on a translation role [cf. 23] to adapt high-level RAI guidelines into specifc practices applica-ble for their teams, given that RAI issues are often domain- and technology-specifc [e.g. 30]. Our fndings thus encourage more HCI research eforts to understand how to design RAI guidelines and frameworks that can be easily tailored to ft into existing UX workfows and methods. A good starting point would be to under-stand how UX practitioners adapt and apply existing RAI guidelines and frameworks to ft their workfows and, accordingly, provide design implications on RAI tools, practices, and frameworks for UX practitioners. Oftentimes, UX practitioners also need to take on additional work to learn more about RAI given that RAI and design ethics are often not included in many HCI and UX curricula [108], as well as educating others on their team about UX methods, values, and impacts of AI systems on people through strategic communica-tion. Our fndings suggest that these emerging roles as learners and educators for the UX aspects of RAI may need to be explicitly articulated as part of UX practice in order to be valued by UX prac-titioners\u2019 organizations [cf. 23]. Organizations could also provide UX practitioners with resources and guidance to help them raise RAI concerns, as prior work has proposed for AI practitioners more broadly [70, 84, 114]. UX-specifc resources might thus be designed to support UX practitioners in taking on more of such an advocate or activist role [cf. 23, 112] in working towards more responsible AI design and development processes. This labor that we outlined above not only requires additional eforts and time from UX practitioners, but also takes the form of emotional (or afective) labor, which Wong [2021] has identifed as being a crucial part of designers\u2019 ethics work, despite not being valued as part of the typical technology design process. The hidden work of RAI conducted by UX practitioners is often not recog-nized or valued by their managers or organizations, and therefore constantly at risk of being deprioritized in the face of competing priorities and limited resources [113], or leading others to view UX practitioners as a \u201cblocker\u201d of the design and development process. In our study, we also fnd that in order to surface potential RAI issues in application design, UX practitioners intentionally and repeatedly generate and witness harmful and toxic outputs through model prompting, as well as prepare participants in their studies to view such toxic content. Much like Gray and Suri [2019] identifed for content moderation, the burden of viewing harmful content often falls to those who are least likely to be protected from or compensated appropriately for handling the toxic externalities of large-scale sociotechnical systems.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "c959efb1-1a96-40b3-897d-2c8cb0407d28",
                    "text": "",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "b7c92e9b-4f3e-4c00-85a0-fff4a2a50151",
                    "text": "",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "7c60df1a-c02a-4fa4-a2ce-2718868d1d80",
                    "text": "Prior research has studied UX practitioners\u2019 current practices and challenges when designing and prototyping ML-powered AI appli-cations [38, 117, 119, 120, 122]. Existing work has found that UX practitioners, given their training in HCI and User-Centered De-sign (UCD), commonly leverage traditional HCI and UCD methods and toolkits when designing with AI, yet these are often insuf-fcient [117, 120, 122]. For example, traditional methods such as Wizard-Of-Oz, sketching, paper prototyping, and rapid prototyp-ing can fail to accommodate the non-deterministic behaviors and opaque mechanisms of AI systems [117, 119, 122]. Prior work has highlighted the need for high-fdelity prototypes that can generate tangible, realistic behaviors, instead of toy sce-narios, to elicit user feedback and better assess the potential human and societal impacts of AI applications [54, 119, 122]. Zdanowska and Taylor [2022] note that UX practitioners found that most HCI and UCD methods and tools place too much emphasis on the design and evaluation of the user interface, which is only one of many components when designing with AI [122]. Users\u2019 mental models of the AI system [120, 122] and the feasibility and user acceptance of the design [122] are also key considerations for UX practitioners. When designing and prototyping with AI, UX practitioners also frequently collaborate with technical experts such as engineers and data scientists [82, 97, 118] to gain a deeper understanding of the capabilities and limitations of the models [38, 117, 119]. While high-level abstractions are sufcient for UX practitioners to design AI applications [118, 122], a deeper understanding of ML model func-tionality could help UX practitioners better envision use cases that may not yet exist [38, 119]. However, this collaboration also poses challenges in that UX practitioners and engineers don\u2019t always share a common perspective, language, or workfow [82, 97, 119], leading UX practitioners to take on extra work to bridge disciplinary boundaries through sharing user stories and raw user feedback from user testing video recordings to help engineers understand user needs [98], and sometimes, adapting to and embracing a more data-centric culture to communicate user needs through both qualitative and quantitative metrics [118]. 2.3.1  Prototyping with Prompt Programming.  To meet the increas-ing demand for new UX prototyping and design tools when de-signing with AI, an emerging practice involves prototyping with ML models through prompt programming [20, 27, 60, 121]. Using prompting techniques [27, 60], UX practitioners are able to send natural language prompts to ML models as inputs and interact with the models directly to test-drive their capabilities and limita-tions [60]. Prior work has found that prompt-based prototyping with large language models (LLMs) helps UX practitioners reduce their reliance on engineers and developers to understand model ca-pabilities, speed up the prototyping process to test out initial ideas and \u201cfail fast\u201d [cf. 117], and better communicate with collaborators using prototypes as boundary objects [60]. Prompt programming is usually conducted with pre-trained, large-scale models such as LLMs [27, 60] and text-to-image gen-eration models [124], which are prone to generate outputs that may perpetuate social stereotypes, toxicity, discrimination, and exclusionary norms [15, 33, 48, 105]. Researchers have been ex-ploring ways to evaluate LLMs and other generative models prior to putting them into use. Common evaluation methods include manually generating general test cases, or tests targeted at specifc failure modes [59, 86], as well as automatically generating test in-puts using the model itself [46, 81]. However, others have noted that such \u201cbehavioral tests\u201d and use of benchmarks to prompt models to intentionally generate harmful outputs (so they can be prevented) often come with pitfalls that render these methods invalid [14]. As such, there are increasing calls for human-centered approaches from HCI and UX practitioners to support the work of identifying and mitigating RAI issues with LLMs [e.g., 12\u201315]. Yet, insights into  how prompt programming can facilitate or hinder UX practitioners\u2019 work on RAI has not been explored. 3   METHODS  3.1   Recruitment  To  investigate  our  research  questions,  we  conducted  semi-structured interviews with both UX practitioners (n = 15) and sub-ject matter experts in a designated RAI role (n = 8). We recruited participants through snowball sampling at our study site (via direct emails to contacts). Our study site was chosen due to the company\u2019s large AI function, as well as the depth of researcher access that could be achieved to participants\u2019 work practices and teams. All participants were recruited from the same company that the au-thors were employed at during the time of the study, a decision we discuss further in section 3.2, below. Inclusion criteria included UX practitioners who had worked on or were currently working on the design, prototyping, user research, or user evaluation of AI applica-tions (prototypes, demos, product features) that were powered by large-scale models. We specifcally sought out UX practitioners who were either directly or tangentially involved with addressing RAI concerns as part of their work with these applications. For RAI experts, our inclusion criteria included experts in a formal Respon-sible AI role, who had experience evaluating or being consulted about responsibility in AI projects and products or product features. Information about participants\u2019 job roles can be found in Table 1. UX participants had an average of 5.6 years experience at the com-pany (SD=2.3), an average of 11.8 years working in UX (SD=7.3), and an average of 7.8 years working with AI (SD=4.4); RAI expert participants had an average of 2.3 years working at the company (SD=1.6), an average of 2.4 years working in RAI (SD=1.2), and an average of 5.9 years working with AI (SD=2.6). We also asked participants to optionally share their gender identity: among UX participants, seven were women, eight were men, and one was non-binary. Among RAI experts, fve were women, two were men, and one was non-binary. Each participant was compensated through a donation to their charity of choice valuing $40 USD. 3.2   Data  Collection  Our study ran from June through July, 2022. All of the participants worked in the United States, in hybrid or fully remote roles at the time of study; hence, all interviews were conducted virtually through an internal virtual meeting platform. Except for two 30-minute interviews, and one 90-minute interview, all interviews lasted about 60 minutes. All participants provided written consent to participate in the research study before interviews began. Scoping the present work to a specifc technology company as a research site allowed us to take advantage of internal AI resources (described below in 3.2.3) to ground interview discussions. As researchers were also company employees, participants could provide more details  of  their  AI  projects  and  context  for  their  RAI  practices, while upholding confdentiality and IP protection. However, we Table 1: Interview participant information. UX participants worked across diferent AI product and research areas. Listed AI areas are based on the specifc projects/products mentioned in interviews. For RAI expert participants, we list their background. did not limit interview discussions to participants\u2019 current work experiences; many had experience working in multiple settings and companies, and we prompted them to refect more broadly on their experiences over the course of their careers. Given the sensitive nature of RAI topics, we also sought to foster a high level of transparency and trust in our communications with each practitioner, before, during, and after the qualitative interview. For example, we held pre-interview calls and shared research briefs with our participants to answer questions about the research, and shared our data interpretations and fndings back with each partici-pant by email, noting their specifc quotes in the paper and asking for any feedback before submission. To inform our research questions, we frst observed a conversa-tional AI design sprint. We did not include our observations as research data, but instead followed up with four sprint participants to enroll them in our interview study. We also studied any artifacts that were provided in the interview, including sprint artifacts dis-cussed. We discuss each interview protocol below, and they are each provided in Supplementary Materials. We also describe a prompt programming tool called PromptMaker [60] below, which we used as a probe during interviews with both UX practitioners and RAI experts. 3.2.1  UX Practitioner Protocol.  In the interviews with UX practi-tioners, we asked them to broadly describe the types of UX work they do related to AI-based prototypes, demos, and/or product fea-tures, and then to dive into more details by having them walk us through one or two specifc projects they had worked on. We specif-ically focused on how RAI issues surfaced in their work, mitigation strategies or precautions they used to address RAI issues, and how RAI issues might have infuenced the project direction. At the end of the interview, we also asked UX practitioners about their thoughts on how RAI processes for early-stage AI application design could be improved.  3.2.2  RAI Expert Protocol.  In our interviews with RAI experts, we asked participants in designated RAI roles about their experi-ences reviewing, analyzing, evaluating, and/or consulting on AI technologies as part of organizational RAI practice. We then asked the experts to walk us through an AI project that they had been involved with, which drew on their expertise, the RAI issues that were present, and how they worked with the project/product teams to address those RAI concerns. Towards the end of the interview, we also asked the experts to envision possible improvements to current RAI practices and possible processes or tools to support RAI in early-stage AI application design. 3.2.3  PromptMaker As a Probe.  During interviews with all par-ticipants, we introduced and described an LLM prototyping tool called PromptMaker, as described in Jiang et al. [2022]. Prompt-Maker provides a web-based interface to LLMs, enabling users to interactively write and test LLM prompts. PromptMaker also en-ables practitioners to remotely execute a prompt. For example, a basic prompt to translate English into French could be created (see example in footnote), then embedded within a prototype to test out a translation feature. Collectively, PromptMaker\u2019s capabilities enable practitioners to rapidly prototype and test new AI features in hours or days, without requiring signifcant machine learning experience. Many of the interview participants were familiar with Prompt-Maker and seven UX participants indicated in the interviews that they use PromptMaker in their daily job during early-stage AI appli-cation design. In our interviews, we used PromptMaker as a probe to understand how emerging AI design and prototyping tools were shaping UX practitioners\u2019 RAI practices and to understand the po-tential opportunities and challenges new AI design and prototyping tools present for RAI. 3.3   Data  Analysis  All interviews were video-recorded and later transcribed verbatim for data analysis purposes. To analyze the interview data, we drew on Braun and Clarke\u2019s [2019\u20132021] refexive thematic analysis ap-proach [17\u201319]. Refexive Thematic Analysis is a post-positivist approach that emphasizes researchers\u2019 role in knowledge produc-tion, including the philosophical stance and theoretical assumptions that they make in informing their data analysis approach [18]. This difers from other qualitative data analysis approaches such as con-structing codebooks and establishing inter-rater reliability metrics, which might not ofer the fexibility needed for researchers to ac-tively participate in the analytic process in a systematic and rigorous way [18]. A refexive thematic analysis approach also encourages researchers to collaborate and discuss interpretations throughout the process to facilitate the generation of themes. Five  authors  participated  in  the  interview  data  analysis  pro-cess and continuously and collaboratively discussed the codes and themes  throughout.  We  followed  the  analysis  process  outlined in Braun and Clarke [2006]. First, all fve researchers familiarized ourselves with the data by reading through the transcripts and taking notes. We then began generating initial codes and divided the transcripts among the fve researchers\u2014each interview tran-script was read and used in initial code generation by at least two researchers. Each researcher reviewed nine to 11 transcripts at this phase and each generated hundreds of open codes. After initial codes were generated, we frequently met to search, review, discuss, and defne themes based on initial codes. In the early stages of our codes-to-themes process, we generated four domain categories (e.g., early-stage RAI challenges, RAI conceptualizations, RAI strategies and practices, aspirational RAI and improvements) and 53 prelim-inary themes through continuous discussions and iterations. All data was analyzed using shared spreadsheets and text documents. In parallel, two authors reviewed artifacts provided by participants, such as design sprint materials, framework documents, and user study fndings, to understand practices associated with early-stage AI application design. After discussing observations together, they shared them with the entire study team. After  further  review  and  discussion  of  all  observations  and themes, we distilled three emergent RAI practices: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications, each of which comprises two high-level themes, which we present in the Findings section. 3.4   Author  Positionality  Our author team is comprised of researchers with both academic and industry research backgrounds, with varied professional expe-riences that shape our perspectives. All researchers were employees of the company that served as the research site during the research period. One author has experience in an industry UX role at a large technology company. Two authors have experience developing applications that incorporate large-scale ML models. Two authors were born in and are currently, or have previously, lived in APAC countries, and four identify as white Americans. All authors completed the bulk of their research training, and work in, predominantly Western institutions. Five identify as having experience with marginalization in computing, either as a member  of a marginalized group themselves and/or through many years of conducting HCI research with marginalized groups. The authors\u2019 background and experiences infuence our posi-tionality: as HCI researchers trained and working in predominantly Western organizations, we acknowledge that complementary schol-arship related to our research questions is needed, to extend and further the understandings presented in this paper. Our position-ality has also infuenced the subjectivity inherent in framing our research questions, a snowball sampling approach that makes use of our professional networks, the study protocol design, and our data interpretation and analysis. 4   FINDINGS  Through our data analysis, we identifed key emerging practices that UX practitioners carried out to meet evolving RAI needs. These practices are not linear, but are embedded throughout their work in iterative ways. In this section, we present three emerging RAI prac-tices: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications. For each practice, we frst introduce how RAI experts (labeled with \u2018R\u2019 throughout) and UX practitioners (labeled with \u2018U\u2019 throughout) enact and oper-ationalize RAI. We then highlight UX practitioners\u2019 RAI practice in light of the RAI work conducted by RAI experts in their formal job capacity, and present how their practices, goals, challenges, and aspirations for the role of UX in RAI take shape. 4.1   Building  and  Reinforcing  an  RAI  Lens  We begin by highlighting an overarching practice that infuencesour  subsequent  fndings:  the  importance  of  seeing  responsibility  as  a  \u201clens\u201d.  An  RAI  lens  positions  RAI  as  a  refexive,  ongoing,  and  holistic  perspective  that  infuences  practices  and  decisions  throughout  de-sign  and  development,  as  they  arise  in  context.  It  is  thus  not  bound  to  a  specifc  artifact,  protocol,  or  type  of  analysis  of  data  or  model  outputs\u2014though  it  can  incorporate  them.  Instead,  it  represents  an  ongoing,  shared  mindset  that  acknowledges  and  seeks  to  account  for  the  social  position  of  those  who  shape  technology\u2014and  the  company  itself\u2014and  the  intersecting  relationships  between  specifc  design  and  development  choices  and  their  societal  implications.  As  such,  both  RAI  experts  and  UX  practitioners  went  to  great  lengths  to  cultivate  and  reinforce  an  RAI  lens,  not  only  in  their  work,  but  in  their  teams  and  the  broader  culture.  This  practice  was  carried  out  explicitly  by  RAI  experts  who  inhabited  established  and  prominent  RAI  positions  with  focused  RAI  expertise  (e.g.,  ethics,  philosophy,  law)  in  the  company,  and  implicitly  by  UX  practitioners  who  considered  RAI  as  an  emerging  yet  crucial  piece  of  the  UX  of  AI  projects/products.  Below,  we  characterize  the  work  both  groups  of  practitioners  carried  out  to  pursue  this  goal  of  an  RAI  lens:  from  self-education  and  sensitization  to  RAI  issues,  to  actively  communicating  and  working  with  teams  to  surface  and  mitigate  RAI  issues.  Both  UX  practitioners  and  RAI  4.1.1   Sensitizing  to  RAI  Concerns.  experts  were  often  called  upon  by  product  teams  to  address  RAI  issues:  UX  practitioners  given  their  expertise  in  accounting  for  the  impacts  of  technology  on  users;  RAI  experts  given  their  formal  job  positions  and  expertise  in  RAI  and  ethics.  While  RAI  is  not  part  of  UX  practitioners\u2019  formal  training  and  role,  by  accumulating  experience  and  knowledge  of  technologies\u2019  impact  on  users,  RAI  had  already  been  integrated  into  many  UX  practitioners\u2019  day-to-day  practices  and  became  what  UX  practitioners  described  as  a  lens:  \u201cWith  my  lens,  I  make  [RAI  issues]  surface,  so  I\u2019m  not  sure  that  [RAI  issues]  would  just  naturally  bubble  up  to  the  surface.  But  I  think  part  of  my  research  ethos  is  to  look  for  those  gaps  and  those  red  fags.\u201d(U7)  For  RAI  experts,  a  large  part  of  their  job  included  sensitizing  [cf.  16]  teams  across  the  company  to  RAI  concerns  by  ofering  RAI  resources  and  support  such  as  RAI  reviews,  ofce  hours,  consul-tations,  and  RAI  workshops.  When  asked  about  their  processes  of  RAI  review  and  consultations,  many  RAI  experts  mentioned  the  usage  of  RAI  frameworks  and  guidelines,  many  of  which  they  (or  their  colleagues)  had  developed.  RAI  experts  not  only  used  these  RAI  frameworks  and  guidelines  to  sensitize  teams  to  RAI  consider-ations,  but  also  to  help  themselves  be  more  aware  of  potential  RAI  issues  in  their  work.  For  instance,  R6,  R8,  and  their  colleagues  are  creating  an  AI  harms  framework,  outlining  domain-specifc  RAI  considerations,  and  best  practices  to  help  engage  practitioners  with  RAI  issues.  R8  told  us:  \u201cI\u2019ve  found  [the  AI  harms  framework]  to  be  pretty  helpful  for  me  because  even  if  I  do  this  all  the  time,  I  can  still  forget  about  one  of  these  possible  negative  implications  that  could  happen.  But  teams  also  like  it  because  they  tend  to  be  new  to  these  ideas,  so  it\u2019s  helpful  for  them  to  just  be  like,  here  is  the  landscape  of  things  that  could  happen  in  a  pretty  digestible  format.\u201d  These  RAI  frameworks,  guidelines,  and  best  practices  established  by  RAI  experts  were  commonly  referred  to  by  UX  practitioners  during  their  interviews.  Given  that  RAI  was  not  part  of  their  formal  training  or  job  requirement,  to  sensitize  themselves  to  RAI  consid-erations,  UX  practitioners  often  explicitly  and  actively  sought  out  these  internal  RAI  resources  as  well  as  external  literature  to  better  understand  the  issues  and  integrate  them  into  their  work.  However,  a  few  UX  practitioners  also  mentioned  building  frame-works  from  scratch  to  meet  the  specifc  needs  of  an  application  domain.  Similar  to  RAI  experts\u2019  practices,  many  of  the  UX  prac-titioners  in  our  interviews  talked  about  their  eforts  to  compile  their  RAI  knowledge  and  experience  into  actionable  RAI  guidelines  and  best  practices.  Some  UX  practitioners  even  integrated  RAI  into  their  standard  practices  and  design  pipeline:  \u201cOne  of  our  team\u2019s  RAI  standard  practices  is  just  how  we  collect  data,  that  we\u2019re  putting  in  a  process  on  collecting  data  fairly  [...]  [F]olks  have  built  out  a  much  more  rigid  pipeline  for  how  we  collect  that  stuf  [...]  it\u2019s  just  part  of  the  standardized  practice  now.\u201d  (U14).  To  further  reinforce  their  RAI  perspective  during  the  design  pro-cess,  UX  practitioners  implemented  responsibility  lifts,  a  series  of  activities  at  the  beginning  of  the  design  process  to  reinforce  RAI  as  a  lens  to  inspect  and  flter  design  ideas.  U4  talked  about  a  design  sprint  on  ideating  about  AI  products  powered  by  LLMs:  \u201c[RAI]  was  called  out  in  the  brief  of  this  sprint  as  a  whole  to  think  about  responsibility  for  any  ideas  that  we  come  up  with.  And  what  are  the  responsibility  implications  of  those?\u201d  Similarly,  U14  who  organized   the  conversational  AI  design  sprint  and  invited  internal  guest  speak-ers  to  talk  about  RAI  at  the  beginning  of  the  sprint,  explained  his  rationale  during  the  interview:  \u201cEven  just  doing  stuf  like  that  [having  an  RAI  lightning  talk  at  the  beginning  of  the  sprint],  [...]  having  a  space  put  in  to  any  workshop  moving  forward,  or  any  team  doing  this  kind  of  work  with  AI  and  everything,  you  make  the  space  for  someone  to  give  a  presentation  like  that,  where  it  can  at  least  put  those  ideas  in  the  designer,  and  the  prototyper,  and  the  engineers\u2019  brains,  so  it  is  at  least  top  of  mind  [...]  you  make  space  for  responsible  design  and  thinking  when  you\u2019re  in  the  thick  of  it.\u201d  4.1.2   Organizational  Challenges  of  Communicating  about  RAI  with  Teams.   As  RAI  is  a  rather  new  and  still-emerging  discipline,  those  with  expertise  are  limited,  and  practitioners  with  demonstrated  expertise  are  often  called  upon  to  help  shepherd  projects  through  the  examination  of  RAI  considerations  and  mitigation  strategies.  Many  RAI  experts  and  UX  practitioners  in  our  study  were  members  of  centralized  teams  and  were  often  positioned  to  apply  RAI  exper-tise  horizontally.  As  such,  both  RAI  experts  and  UX  practitioners  described  \u201cdropping  in\u201d  and  then  out  of  specifc  teams  to  conduct  RAI  work  in  particular  phases.  Due  to  this  form  of  centralized  organizational  structure  (rather  than,  for  instance,  embedded  RAI  experts  and  UX  practitioner  with  permanent  roles  on  a  single  AI  team),  many  RAI  experts  and  UX  practitioners  thus  invested  signifcant  time  in  sensitizing  members  of  the  teams  they  were  \u201cdropping  in[to]\u201d  to  RAI  considerations.  Oftentimes,  RAI  experts  and  UX  practitioners  joined  AI  projects  that  were  still  relatively  early  in  development,  but  which  had  already  begun,  making  it  more  difcult  to  reverse  decisions  that  had  already  been  made  or  shape  the  design  direction  in  fundamental  ways,  particularly  as  they  worked  to  understand  and  navigate  the  power  dynamics  among  the  AI  team  with  whom  they  were  working.  Both  RAI  experts  and  UX  practitioners  talked  about  how  the  norms  and  implicit  values  associated  with  the  larger  organizational  culture  incentivized  \u201cmoving  fast\u201d  and  emphasizing  positive  out-comes  of  AI  systems  [cf.  53,  70,  84].  This  remained  a  challenge  even  for  RAI  experts  who  were  in  formal  positions  as  RAI  reviewers  or  ethics  consultants  that  had  more  power,  sometimes  with  blocking  power,  over  project/product  directions  and  releases.  In  their  inter-views,  many  RAI  experts  said  they  mostly  worked  with  teams  that  voluntarily  reached  out  to  them  for  RAI  consultations  and  reviews.  These  teams  tend  to  be  more  open  to  suggestions  and  critiques  that  RAI  experts  brought  up  during  the  process.  Many  RAI  experts  emphasized  teams\u2019  openness  in  discussing  RAI  and  initiatives  in  making  changes  as  paramount  when  working  with  the  teams  on  RAI  issues:  \u201c[RAI  consultations]  is  very  much  conversation-based.  And  we  steer  the  conversations.  But  the  important  thing  is  that  the  interest  has  to  come  from  [the  product/project  teams].  It  has  to  be  that  they  want  to  change  their  beliefs  and  behaviors.  We  can  try  and  impose  it,  of  course.  It\u2019s  always  better  if  this  kind  of  culture  change  comes  internally  rather  than  externally.\u201d  (R3)  However,  this  was  not  always  the  case  for  UX  practitioners  who  had  less  power  to  sway  the  project  directions  over  potential  RAI  concerns. UX practitioners also worked closely with the teams on the design and development of the products on a day-to-day basis and faced the same time constraints alongside the product teams. In the face of these pressures, UX practitioners described using a combination of communication techniques and product team activities to reconcile these incentives for speed with the introduction of new, more intentional RAI processes that encourage refection on a range of potential outcomes and harms of AI systems. Here, UX practitioners emphasized the importance of communi-cating potential RAI issues strategically and presenting themselves in a supportive role, rather than being seen as a \u201cblocker.\u201d U8 told us, \u201cMaking it as blameless as possible is one of the best things we\u2019ve learned. I\u2019m not trying to tell anyone they\u2019re bad, I\u2019m not trying to freak anyone out. I more want to highlight, here\u2019s something that could go wrong. Here are the ways that people could be afected, here\u2019s the ways the business could be afected. You can choose to act on that or not, but I would strongly advise you to do so.\u201d In the face of organizational pressures to ship products rapidly, UX practitioners took on additional hidden work to sensitize their team and organizational leadership to the potential harms of AI systems and the importance of mitigating those harms prior to deployment\u2014crucial labor that was not always recognized as being core to their work by their organization. When not directly negotiating with team members or organi-zational leadership about RAI concerns, UX practitioners would sometimes create activities or documents meant to enable team members to respond to RAI issues. One strategy included showing potential impacts through user testing that surfaced concerns, or by creating artifacts that illustrate potential harms. For example, U12 had difculty getting their team to respond to potential concerns and ran a team activity to create fake newspaper headlines [cf. 115] to illustrate how things could go wrong: \u201cI had put together a deck of like fake headlines of how this could go wrong. [...] I think at the time I think it had a big efect. People backed of the idea, we didn\u2019t have to go any further with it.\u201d This activity and others like it were part of the additional work that UX practitioners took on to sensitize others in the AI teams they worked with (and the company more broadly) to the range of potential harms from AI systems. 4.2   Responsible  Prototyping:  Ideating  and  Building  with  Machine  Learning  Models  During the interviews, both sets of practitioners described how they understood RAI in terms of anticipating and surfacing harms that AI technology could bring to the end-users, society, and the public. They both brought up similar sets of harms in their interviews that they tried to anticipate and surface, which included, but were not limited to: safety, misinformation, inequity and/or culture and iden-tity erasure, stereotyping, over-reliance on AI, anthropomorphism of AI, toxicity and/or ofensiveness, and privacy. However, RAI experts and UX practitioners approached this operational defnition of RAI diferently based on their respective methods and perspectives. In their interviews, RAI experts told us that given their expertise and the nature of their jobs, they were often invited by the teams to explicitly and intentionally look for RAI issues and concerns, often through question-asking and adversarial testing. As R8 described, \u201cI think the vast majority of  people are not thinking adversarial-y. I\u2019m here to think about the worst of the worst. That\u2019s what I was hired to do.\u201d When asked about how they helped the teams to surface harms, R6 said, \u201cIt\u2019s honestly just asking questions. This job is mostly just knowing what questions to ask. It\u2019s just critical thinking, and it\u2019s helping other teams think critically about their projects. We\u2019re always asking, what is the worst-case scenario?\u201d In contrast, while UX practitioners were not trained or required to surface and anticipate potential AI harms in their day-to-day work, we found that UX practitioners were committed to surface and anticipate harms from their unique human-focused perspective and with their unique skills, methods, and tools during their design and prototyping process. In this section, we describe how UX practitioners anticipated potential harms by envisioning how user interface design decisions could infuence the users\u2019 mental model of AI, and how they at-tempted to surface harms by leveraging both traditional and new approaches to design and prototyping. 4.2.1  Considering the Consequences of Users\u2019 Mental Models of AI.  During the interviews, UX practitioners told us that due to the stochastic nature of ML models they typically work with (in particular, generative language models, which often include some stochasticity to aid in producing variety in the model outputs), they were often concerned about how users would perceive AI applications driven by these models. In part, UX practitioners were concerned that users might overestimate the capabilities of the language model or treat the model as if it were human-like (i.e., anthropomorphizing it [80, 104]). UX practitioners felt responsible for appropriately communicating model capabilities to users, both through how they designed the user studies as well as the design of the application interface. For example, U1 discussed the potential misinterpretations of AI technologies such as LLMs: \u201cI think one of the other things about this technology [LLM] that is like, really a misnomer, is that, a lot of people see it as this, like general-purpose chatbot frst and then, \u2018oh, it can do all this other stuf.\u201d\u2019 To mitigate the potential consequences of people\u2019s mental mod-els of AI technology, UX practitioners leveraged diferent design strategies and techniques. During the interviews, UX practitioners mentioned techniques such as putting constraints on user input and model output, to limit model behavior (e.g., preventing it from generating answers to of-topic user questions) in AI applications; designing LLM-driven AI applications that are not chatbots to ex-pand people\u2019s understanding of LLMs; or changing the appearance of the UI of a chatbot application when they discovered users\u2019 expec-tations of an LLM-based chatbot didn\u2019t match the actual language interaction: \u2018So we had to reconfgure how we show these prototypes visually and also how we communicate about what these prototypes are. And that [higher-fdelity UI] was vital to change up. So the team quickly responded and changed the way that it looked, and they changed it from looking like a product to looking more like, terminal, like a very simple, old-school Lo-Fi terminal chatbot.\u201d (U7). Other UX participants brought up the need to build features and functions to allow users to dig deeper into the AI technology\u2019s ca-pability, to improve their understanding of the model. For example, U14 said \u201cProviding the insight for the user on what features and capabilities are possible helps expand the user\u2019s mental model of how they can interact with the system. That all develops trust [built on] knowing what\u2019s possible and what they can use the system for.\u201d During her interview, U10, a UX researcher, also refected on the UX practitioners\u2019 role and obligation in design to help users with low technology literacy understand the AI system and avoid overes-timating the model\u2019s capability or erroneously anthropomorphizing the LLM: \u201c[W]e might have many users who don\u2019t even know what AI or ML is. And so there were defnitely com-ponents that I think [raised] a very big, open question for me, in interviews where I asked people how they thought it worked, and they\u2019d be like, \u2018There\u2019s a person that\u2019s looking at these, and they get back to you really quickly.\u2019 And so, there\u2019s this question about what obli-gation, if any, do we have to correct that misconception, and what potential downstream harm could having that misconception lead to, and what might onboarding to a system like this look like for people who have lower tech literacy? [...] How do you help people understand that this is an AI system and what that means?\u201d 4.2.2  Examining ML Models through \u201cTest-Driving\u201d.  A large part of UX designers\u2019 job during early-stage AI application design, besides representing the user perspective as part of standard UX practice, was to understand the potential harm and capability of the ML model underlying the AI application. To do this, UX participants described how they supplemented traditional UX design and proto-typing methods with novel emerging methods to surface potential RAI issues that traditional methods may not have been able to uncover. Several participants mentioned using traditional UX design meth-ods such as Wizard-of-Oz studies or toy examples to quickly test out their AI design ideas. However, they also pointed out that these traditional design methods are fawed, as their ability to surface real-world RAI issues is limited. The scope, time frame, and re-searcher supervision constraints of typical user studies don\u2019t allow users to bring in situ, authentic personal data, needs, and use cases to bear on model interaction (U3). In part, the organizational factors that we described in section 4.1.2 impact UX practitioners\u2019 ability to conduct more longitudinal studies of the harms of AI systems in a more ecologically valid context. To work around those con-straints, UX practitioners used toy examples they believed to be representative of usage scenarios. However, as described by U10, the use of toy examples during user studies makes it difcult to surface RAI issues based on real-world usage: \u201c[O]ne of the hardest parts of prototyping a tool like this, is that it\u2019s a highly personal experience when you have a health condition, and your level of anxiety might be incredibly high or your level of investment in fnding relevant information may be way higher than you can get in an actual study [using toy examples]. And so, that\u2019s been a big challenge for us, is reading the tea leaves a little bit...\u201d  to interact with the ML models directly. For example, the Prompt-Maker tool we provided in the interview was already used by several UX participants. During prototyping activities that involved model prompting, UX practitioners performed what many referred to as \u201ctest-driving,\u201d which refers to the activity of continuously probing model outputs using diferent natural language prompts (that serve as input to the model) to understand model capabilities and limita-tions, in order to determine ft between a design idea and model capabilities. UX practitioners considered good prompt design to be an \u201cart form,\u201d in that it was difcult to identify reasons why prompt construction led to certain model outputs, making repeatable best practices and rules hard to distill. U5 recounted: \u201cI think it\u2019s [PromptMaker] a great tool to \u2018scratch pad\u2019 with, to move really quickly to just try something new. But generally, prompting is really weird. It\u2019s a kind of weird art form. Little weird things like just having one space key at the end of your prompt can change the complete output of what comes out. That\u2019s really subtle and hard to have somebody to understand what that means. So it\u2019s a useful tool, I love it, it was the most accessible thing I think that we\u2019ve created so far. But it\u2019s not just self-service yet, or intuitive enough on its own.\u201d This lack of consistency and intuitiveness of model prompting made it challenging to come up with a consistent practice to ad-dress the need to test-drive model interaction during early-stage AI application design. During his interview, U3 also raised the issue that prompting could further introduce bias, requiring intentional eforts to avoid \u201ccodifying our own belief systems. I think what hap-pens is the minute you get a new user that starts to ask questions to your system, then we need diferent sets of belief systems. And so if you\u2019re not thoughtful to [that] fact, your prompt might represent your beliefs in a stronger way...\u201d To mitigate this RAI issue during responsible prototyping, U3 said he liked to crowdsource few-shotexamples, and intentionally eliminated prompts that looked too similar: \u201cBecause what I\u2019m really looking for is, I\u2019m looking for few shots that represent a diversity of types of input that might come in. And sometimes I even model, slightly adversarial examples.\u201d However, most UX practitioners currently lack a standard practice to address this, beyond trying to diversify few-shot examples or deferring to user testing, which we discuss below. 4.3   Responsible  Evaluation  of  AI  Applications:  Involving  Users  to  Assess  Responsible  AI  In our interviews, both sets of practitioners explained how they saw inclusion of a diversity of experiences and perspectives as a core dimension of RAI. Almost all RAI experts in our study pointed out the importance of user research and evaluation in ensuring accountability and responsibility, as well as validating and refecting on the social benefts of the AI products during the early-stage design process. However, several RAI experts called for longer-term engagement with users throughout the AI product lifecycle given that traditional user evaluation and testing might not be sufcient in designing Many of the UX participants in our study were able to get access to emerging UX prototyping tools that allowed UX practitioners RAI. For instance, R5 critiqued the short-term nature of user testing and questioned the assumptions in traditional user testing and evaluation processes: \u201cI think at base level, not making research just be like a short-term, one-of thing, but having it be something that you are doing constantly at all diferent parts of this design process. Making sure that [...] you are not just going in with pre-thought of categories where you are like, \u2018Ok, this is how we defne failure. Has this failure happened? No, it hasn\u2019t. Ok, great.\u2019 but kind of like collectively defning some of those terms with the users. I think just having that process be integrated throughout all of these diferent steps would probably be a little bit more helpful.\u201d Other RAI experts in our study more explicitly called out their aspiration of taking more participatory approaches to involve users as well as external stakeholders in the design and evaluation of RAI. For example, R2 said, \u2018I don\u2019t think responsible AI can be achieved without some form of robust participation in a nutshell. For me, respon-sible AI is the extent to which it can be made participatory. Responsible AI is participatory AI.\u201d Through our interviews with both RAI experts and UX practition-ers, we found that while involving users and taking participatory approaches to evaluate RAI remained largely an aspiration from RAI experts, UX practitioners, with their disciplinary perspective towards involving users and other stakeholders in design and eval-uation, were already carrying out this aspiration through involving prospective users of a future AI application in the design process to surface and assess RAI issues. In this section, we outline UX practitioners\u2019 processes and chal-lenges for involving users in the process of evaluating potential RAI issues: from preparing the model prior to involving users, to coping with unexpected model output during user evaluations. 4.3.1  Preventing Harmful Model Output Prior to User Involvement. Due to the inherent stochasticity of generative language models, UX practitioners frequently witnessed model outputs that they considered to be toxic when testing ideas through direct model interaction. Many UX practitioners, while hoping to get users in-volved as early as possible in RAI research to surface and identify potential harms, were also concerned about exposing users to these outputs. U8 told us that he was primarily \u201cconcerned [with] what can I do before this gets to someone who\u2019s not on the team...\u201d To address the uncertainty associated with potentially harm-ful model outputs, before inviting users in to interact with these models, teams generated a wide range of constraint-based input and output suppression techniques that often relied on classifers and flters. U15 described approaches to handling input and out-put that was \u201cinequitable\u201d in terms of ofending particular groups: \u201cOne [technique] is [to] detect if the user[s] themselves are initiating  something that is inequitable, if the model is giving out something that is inequitable, and [another] thing that we did was reduce the scope of what the model can do in terms of its output. The [other] is reduce the scope of what a person can do in terms of its input into the model.\u201d Sometimes, UX practitioners curated resources used to classify and mitigate toxicity: \u201cWe put in place a bunch of flters to make sure that it\u2019s on topic, that it\u2019s not ofensive, that it doesn\u2019t use a set of words or phrases that we\u2019ve specifcally banned.\u201d (U1) However, it was not always possible to prevent harmful model output entirely, in which case practitioners had to come up with other user study safeguards. U14 described these challenges in the context of their experiences in an AI application design sprint, during which they met milestones related to ideating, designing, prototyping, and conducting user testing with the prototypes all within a matter of days: \u201cI  think  we  had  two  and  a  half  days  until  we  were actually testing in front of [external, prospective] users, so it was so fast and rapid, which is one of the pros about it [...] but if something did go wrong, and if [the model] did come up with bad suggestions, or if we did have an answer that went of the rails, we couldn\u2019t just shut it down ... [all we could do] was just make the screen go blank, and hopefully they didn\u2019t see it in that second.\u201d Although developing guardrails and constraints is a common strategy to mitigate and prevent RAI issues with large-scale models (albeit one that may reproduce existing structural inequities in AI [37, 89]), some practitioners pointed out the importance of explor-ing other approaches, so as to not over-limit the types of inputs and topics that users can discuss. As R1 told us, \u201cI think we need to fgure out how to teach the model in a controlled-generation way, to respond more appropriately so you\u2019re not always taking the sledgehammer and just suppressing results.\u201d However, as R8 told us, UX practitioners and product teams also struggle with developing more fexible mitigation approaches for complex algorithmic assemblages, for which they believed a constraint-based mitigation strategy was the best feasible option for mitigating potentially toxic output: \u201c[for some applications] it\u2019s actually many models at once. [...] For [product] purposes, a blocklist at the code level is really the most feasible thing because [a product application team] can\u2019t go back and retrain the model.\u201d 4.3.2  User Evaluation of the AI Application.  Through our inter-views, we found that UX practitioners ascribed specifc purposes and meaning to user evaluation in order to meet RAI challenges in early-stage AI application design. They saw user testing as an avenue for surfacing and identifying levels of comfort with large-scale model interactions and identifying possible RAI concerns through user evaluation of AI application prototypes. Here, the line between user feedback about a potential AI application, and adversarial testing of the model underlying it, has the potential to blur. As U7 told us, \u201cWe have to change our perspective on how we user test these things [AI prototypes] and think about the greater good because if we just focus on \u2018oh the button needs to change\u2019 and \u2018people didn\u2019t like the font size,\u2019 we\u2019re in trouble.\u201d To surface and mitigate RAI issues during user evaluations, UX practitioners often sought to recruit a broad range of users to im-prove diversity in user testing. This often includes, but is not limited to, recruiting for users across diferent demographics, race, gender, tech literacy, age, etc. However, UX practitioners also needed to balance resource and time constraints while trying to diversify user testing eforts. This often leads to the need to prioritize certain RAI issues for user testing. For example, U15 described how he gave suggestions on prioritizing the RAI issues that the team knew the least about, given limited time and resources: \u201cThere are 21 things that you are doing. My suggestion would be to look at these fve ini-tially and redesign because my understanding of the model is that these fve things have not been tested previously. [...] Let\u2019s focus on things that we know the least about.\u201d Many UX practitioners talked about the importance of prepar-ing users for potentially toxic or otherwise harmful outputs that the AI application might generate during user evaluation sessions. Many also described their anxiety around viewing unpredictable model outputs with the users during these sessions. To mitigate this, UX practitioners discussed the importance of setting expec-tations and communicating with users at the beginning of user testing sessions: \u201cYou are showing them to an end user at the same time you\u2019re seeing them yourself. And so there\u2019s a certain amount of anxiety there. So, in the research protocol, it\u2019s really important to be able to set people\u2019s expectations: \u2018This is early technology, if you see some things that are harmful, I want you to be able to talk to me about it. I\u2019ll have some narrative to help you understand what you\u2019re seeing.\u201d\u2019(U3) UX practitioners also pointed out the difculties users could face in providing honest feedback and surfacing RAI issues that were meaningful to them during user study sessions. User stud-ies, especially those in which the researcher and participant have not built a relationship, have always grappled with the potential for social desirability bias, participant conformity to researchers\u2019 expectations, and preferences to avoid taboo topics or embarrass-ing (or personally invasive) social interactions. These concerns are made more salient when evaluating high-fdelity AI applications, given that large models could generate unpredictable and harm-ful outputs. To help mitigate this challenge, UX practitioners also sought to create a safe and encouraging environment for users to feel comfortable discussing RAI issues. U13 described one strategy they used to accomplish this by matching identity characteristics like practitioners\u2019 race and ethnicity with those of users, to help them feel comfortable talking about potential RAI concerns: \u201cI think one of the things [for] responsible AI consid-eration is when you\u2019re doing user research, does the makeup of your team and the people who are inter-viewing those folks, are users comfortable talking to them about fairness issues? We always try to match the race of the moderator to the race or the ethnicity of the participant for psychological safety. Because a lot of times, users don\u2019t really feel comfortable talking about inequity with a person who may not have experiences with systemic inequity. How are you making sure that  you\u2019re conducting not just research with users responsi-bly and ethically, but also you are pairing them with interviewers that could lead to honest feedback.\u201d Next, we refect on our fndings and the ways in which, taken together, they suggest an evolution of UX praxis. We conclude by refecting on opportunities for HCI research to move our feld closer to designing responsible AI. 5   DISCUSSION  In our fndings, we highlight three types of emerging RAI practices carried out by UX practitioners to meet RAI challenges: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications. Specifcally, we identifed strategies that UX practitioners employed to self-educate and communicate with others about potential RAI issues; challenges during proto-typing, such as communicating model capabilities (and limitations) to users; and current approaches to responsible evaluation of AI applications, including preparing both the models and the users for potentially harmful model outputs. In this section, we frst discuss implications of the hidden work of RAI conducted by UX practitioners, highlighting the labor as-sociated with these practices. We then discuss research opportu-nities and ways to support UX practitioners in their RAI practice, particularly for applying large-scale language models to develop applications. Finally, we identify opportunities to reconfgure the role of the user in designing RAI. 5.1   Supporting  the  Hidden  Work  of  RAI  in  UX  Practice  In light of increasing calls for UX practitioners to be involved in the work of identifying and addressing issues of RAI in technology design [e.g., 13\u201315, 53, 69, 70, 84], we highlight the hidden work that UX practitioners are conducting to meet RAI challenges. We call out this hidden work by situating UX practitioners\u2019 emerging RAI practices in the context of the work conducted by RAI experts in their formal RAI roles. We fnd that UX practitioners and RAI experts share similar conceptualizations and aspirations for the human impacts of AI\u2014they both saw RAI as, in large part, antici-pating, surfacing, and mitigating a variety of potential harms for users and other stakeholders who may be impacted by a given AI system. Through our study, we see RAI experts carrying out the work of developing the RAI agenda, guidelines, toolkits, evaluation, and foundational research at an organizational level throughout the entire AI design and development process, while UX practition-ers operate in specifc application areas, contexts, and domains to actively promote, implement, and adapt RAI to ft their day-to-day work in early-stage AI application design. Drawing upon their unique human-centered values and design techniques, UX practitioners are adapting their UX practices, negoti-ating their respective roles in RAI work, and learning and educating others about human-centered values and RAI concerns. However, carrying out these RAI eforts often requires UX practitioners to devote additional time and eforts in their day-to-day work, or what Star and Strauss [1999] have described as \u201cinvisible work,\u201d or akin to what Strauss [1988] has referred to as \u201carticulation work,\u201d or the work required to make other work happen. We fnd that UX practitioners take on a translation role [cf. 23] to adapt high-level RAI guidelines into specifc practices applica-ble for their teams, given that RAI issues are often domain- and technology-specifc [e.g. 30]. Our fndings thus encourage more HCI research eforts to understand how to design RAI guidelines and frameworks that can be easily tailored to ft into existing UX workfows and methods. A good starting point would be to under-stand how UX practitioners adapt and apply existing RAI guidelines and frameworks to ft their workfows and, accordingly, provide design implications on RAI tools, practices, and frameworks for UX practitioners. Oftentimes, UX practitioners also need to take on additional work to learn more about RAI given that RAI and design ethics are often not included in many HCI and UX curricula [108], as well as educating others on their team about UX methods, values, and impacts of AI systems on people through strategic communica-tion. Our fndings suggest that these emerging roles as learners and educators for the UX aspects of RAI may need to be explicitly articulated as part of UX practice in order to be valued by UX prac-titioners\u2019 organizations [cf. 23]. Organizations could also provide UX practitioners with resources and guidance to help them raise RAI concerns, as prior work has proposed for AI practitioners more broadly [70, 84, 114]. UX-specifc resources might thus be designed to support UX practitioners in taking on more of such an advocate or activist role [cf. 23, 112] in working towards more responsible AI design and development processes. This labor that we outlined above not only requires additional eforts and time from UX practitioners, but also takes the form of emotional (or afective) labor, which Wong [2021] has identifed as being a crucial part of designers\u2019 ethics work, despite not being valued as part of the typical technology design process. The hidden work of RAI conducted by UX practitioners is often not recog-nized or valued by their managers or organizations, and therefore constantly at risk of being deprioritized in the face of competing priorities and limited resources [113], or leading others to view UX practitioners as a \u201cblocker\u201d of the design and development process. In our study, we also fnd that in order to surface potential RAI issues in application design, UX practitioners intentionally and repeatedly generate and witness harmful and toxic outputs through model prompting, as well as prepare participants in their studies to view such toxic content. Much like Gray and Suri [2019] identifed for content moderation, the burden of viewing harmful content often falls to those who are least likely to be protected from or compensated appropriately for handling the toxic externalities of large-scale sociotechnical systems. 5.2   Challenges  and  Opportunities  in  Responsibly  Designing  with  Large  Language  Models  In addition to implications for the work of UX practitioners in RAI more broadly, our fndings suggest specifc implications for the design of new tools and methods for UX work in responsible design and evaluation of AI applications powered by LLMs. Existing ethics-focused design methods [24] or frameworks such as value-sensitive design [42] largely do not focus on AI or language models, while  existing AI ethics toolkits are largely designed to support \u201ctechni-cal\u201d work of RAI, rather than UX practices [114]. Indeed, despite its prevalence in the academic literature, none of our participants mentioned using value-sensitive design as an approach to designing RAI. In our study, we fnd that traditional HCI methods such as Wizard-of-Oz studies are not only insufcient to support UX prac-titioners in their ideation and design processes for AI applications powered by LLMs [38, 118, 119, 122], but they are also insufcient in helping UX practitioners identify and evaluate potential RAI issues through real use cases in early-stage AI application design. Our study sheds light on the opportunities and challenges for RAI from the emerging design and prototyping practice of prompt programming with LLMs. In their interviews, while many UX prac-titioners talked about the many advantages that the prompt pro-gramming tool PromptMaker brought to them (e.g., test-driving model capabilities directly, reduced reliance on other experts like engineers, and data scientists [97, 118]), practitioners also pointed out that it is not yet a mature tool for RAI design. One substantial risk of using prompt programming to uncover RAI issues during the design and prototyping of high-fdelity AI prototypes, as pointed out by the UX practitioners in the interviews, is the danger of embedding their belief systems into users\u2019 experi-ence of the AI system via prompting. Similar concerns were also raised regarding the use of prompting or generating \u201cbehavioral tests\u201d [14, 86] to prompt models to generate harmful outputs, which may be limited by the positionality of the practitioner or researcher creating those tests [14, 123]. While some UX practitioners in our study attempted to mitigate this issue by crowdsourcing prompts [e.g., 75, 77, 78], this mitigation strategy may be hindered by a lack of representative groups of participants as well as a disconnect between the abstract model the tests are created for and the downstream application of that model within a particular sociocultural context and use case. As a result, new tools and methods are needed that can ground the often speculative work of anticipating potential harms of language models in their specifc contexts of use, rather than abstracting that social context away [cf. 117]. Substantial  prior  work  has  identifed  the  potential  harms  of technologies built on LLMs [e.g., 8, 15, 105], while at the same time acknowledging the challenge of evaluating and mitigating such harms [14, 100, 123] and calling for HCI and UX researchers and practitioners to contribute a human-centered perspective to identifying and addressing RAI issues in LLMs [12\u201315, 123]. Our fndings suggest that constraint-based input and output suppression techniques such as blocklists are a commonly used strategy to prevent users from encountering toxic output during user testing, in addition to preventing models from generating such output after deployment as well [37, 89]. As prior work has identifed [37, 89], blocklists are crude instru-ments that may lead to harms of erasure [33, 37] if and when they fail to account for the social context of language use [cf. 11, 14, 123]. However, although practitioners acknowledged in their interviews that such suppression techniques are not ideal, it is often the most feasible option they have, given that product teams often deal with cascading RAI issues from upstream models they may not have access to or control over. As such, more research is needed to under-stand how\u2014in development paradigms where pre-trained models are fne-tuned or used in downstream applications [15, 58]\u2014RAI issues may be propagated from the beginning of the model devel-opment to the design process of AI applications. In addition, more research is needed to understand how constraint-based approaches such as classifers and blocklists are developed and used, and by whom, to interrogate the assumptions underlying their design.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "e5f8aba3-cbaf-433e-ac4a-eb00fe48c2a8",
                    "text": "Recognizing  the  limited  perspectives  of  researchers  and practi-tioners involved in AI design, there are increasing calls for more user-centered or participatory approaches to responsible AI [e.g., 4, 29, 35, 52, 64, 91, 102, 111]. In our study, we found that UX prac-titioners involve members of the public, potentially impacted user groups, and domain experts in the design and evaluation of RAI ap-plications; however, this new practice of involving users and other impacted stakeholders in RAI work poses new challenges and re-search questions for the HCI community, including when and how people should be involved in RAI design and evaluation, as well as how to protect people from any potential harms of participating in those processes. Our fndings suggest opportunities to rethink how UX practi-tioners conceptualize and draw on people\u2019s mental models of AI applications during the responsible design and evaluation of AI systems. For instance, prior work on folk theories of algorithms [e.g., 34, 39, 62, 93] suggests that the accuracy of folk theories of how algorithms work may be less critical for UX practitioners than what those folk theories (or mental models) of algorithms reveal about people\u2019s orientations towards algorithmic systems. In our fndings, UX practitioners\u2019 concerns about people anthropomor-phizing LLMs may suggest opportunities (e.g., more seamful design [21]) to reveal the capabilities and limitations of applications based on LLMs. In addition, our fndings suggest the need to reconsider how UX practitioners confgure the role that users and other poten-tially impacted stakeholders play in identifying and mitigating RAI issues. For instance, we see UX practitioners using user testing sessions to conduct adversarial testing of potential model harms; prior literature has suggested crowdsourcing [77] or using \u201cbias bounties\u201d [49] or \u201ccrowd audits\u201d to identify potentially harmful model outputs [35, 91]. However, these approaches are still nascent, and UX education and praxis has not yet developed robust methods, frameworks, and practices for UX practitioners to either lead such eforts themselves or incorporate their results into UX design and evaluation. Moreover, as we fnd in our work, the nature of identi-fying potential RAI harms may lead to unintended consequences for the participants in such studies who may either have to gener-ate ofensive, toxic output themselves, or be exposed to ofensive  language as a result of prompts that the UX practitioners or other participants create. Future research should thus explore ways to protect participants from these toxic externalities of RAI work. Furthermore, despite calls for broader participation, participa-tory design (PD), or community-based design of AI, the current modes for engaging people in responsible AI evaluation may not deliver on the empowering goals of participatory approaches [29]. For instance, relying on users (and other stakeholders) to identify potential harms may inadvertently relegate them to a more consul-tative or extractive mode of engagement, rather than empowering them to have more generative, creative input into RAI design, as suggested by traditions such as participatory design [e.g., 29, 74], aspirations-based design [65, 101], and community-collaborative design approaches [26]. Moreover, the design paradigm of train-ing large-scale AI models and applying them in downstream AI-powered applications poses serious questions for HCI research about how we might develop modes of participation in RAI de-sign and evaluation that empower participants to have meaningful control over the design of AI applications.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "c006d6c3-c3f2-4c2f-9101-76813b152475",
                    "text": "While our work provides valuable insights into and implications of emerging RAI practices carried out by UX practitioners, our study has limitations. First, all study participants were recruited from one large technology company and their RAI perspectives and prac-tices could be shaped or limited by the organization\u2019s processes and culture around RAI; hence, more research is needed to identify the relevance and applicability of our fndings and implications in other industry contexts\u2014including at smaller technology compa-nies. That almost all participants in our study had prior experience working at diferent technology companies allowed us to draw from their prior work experience during the interviews. Second, our par-ticipants mostly discussed their experiences related to computer vision and language-based ML models, as participants were pri-marily working within these areas. Interviewing participants with expertise in other types of ML models, such as sound-based models, could identify diferent challenges, strategies, or tensions. Third, our study focused on one specifc style of designing and deploying AI systems, i.e., a model-frst trajectory [76], in which ML models were developed before practitioners designed and built AI products around the model. We acknowledge that there are other forms of AI application design and development, such as product-frst, or integrated approaches, and that our fndings might be more or less transferable to these processes. Future studies should replicate our work across organizations of diferent sizes, AI development ap-proaches, and maturity levels [cf. 109], to expand on the practices, challenges, and needs associated with RAI in the UX practices that we identifed through our study.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "4eadd5a2-00b1-4c27-ba29-b15b78e12889",
                    "text": "This paper reports on interviews with ffteen UX practitioners and eight RAI subject matter experts, to understand and situate the emergent RAI practices of UX practitioners in a large technology company. Through the interviews, we identify three emerging RAI practices conducted by UX practitioners in AI application design: building and reinforcing an RAI lens, responsible prototyping, and responsible evaluation of AI applications. We distill challenges and strategies UX practitioners employed to self-educate and commu-nicate RAI issues with the team, to surface and identify potential harms when designing and prototyping with ML models, and to involve users in RAI application design and evaluation processes. Based on our fndings, we discuss and highlight the hidden work of RAI carried out by UX practitioners. We then outline research opportunities and questions for the HCI community, to increase practitioner support for managing RAI challenges, and to move towards best practices for participatory involvement of impacted stakeholders in RAI-related processes.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                },
                {
                    "id": "1872fddf-cfcf-4a53-8bf8-8e9894a3491b",
                    "text": "We thank our study participants. We also thank anonymous review-ers for their valuable feedback on the paper.",
                    "reference": "```\n[1] Qian Yang, Michael Madaio, Sean Kane, Sandeep Kapania, et al. 2023. Designing responsible AI: Adaptations of UX practice to meet responsible AI challenges. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3544548.3581278\n```"
                }
            ]
        },
        {
            "paper_title": "Tools and practices for responsible AI engineering",
            "authors": "R Soklaski, J Goodwin, O Brown, M Yee\u2026",
            "publication_info": "arXiv preprint arXiv \u2026 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2201.05647",
            "chunks": [
                {
                    "id": "523f5518-2853-42e6-85a7-3ddd65f4eb35",
                    "text": "",
                    "reference": "[1] Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
                },
                {
                    "id": "14cb806e-3fd9-4221-bfc8-2ee38b170968",
                    "text": "Responsible Arti\ufb01cial Intelligence (AI) is the practice ofcreating and maintaining AI systems that are not only ac-curate, but also exhibit important qualities such as: robust-ness, safety, security, and privacy; fairness and inclusive-ness; transparency, interpretability, and explainability; ac-countability and governability; and promotion of societaland environmental well-being. There is growing recogni-tion that more attention needs to be given to these criticalprinciples, as evidenced by the publication of more than 400policy documents (Shneiderman 2021), including guidancefrom leading industry and government organizations (Mi-crosoft 2021; Google 2021; IBM 2021; Department of De-fense 2020; High-Level Expert Group on Arti\ufb01cial Intelli-gence 2019). However, doing work in the space of responsi- Figure 1: Framework for Responsible AI Engineering,with new tools introduced in this paper shown in green.hydra-zen simpli\ufb01es the process of making complex soft-ware (SW) applications con\ufb01gurable and reproducible withHydra, thereby increasing the traceability and scalabilityof ML work\ufb02ows. The rAI-toolbox provides \ufb01rst-classsupport for advanced ML techniques (such as robust train-ing) that go beyond the standard train/test/deploy paradigmsupported by popular high-level ML APIs.ble AI puts researchers and engineers at the edge of what iscurrently capable with existing tools.Engineering reliable software systems is already a chal-lenging endeavor, although modern DevOps (and DevSec-Ops) practices and tools help manage risk. Machine learn-ing (ML) applications have the added complexity of beingdata driven, and thus require additional support for carefultracking, management, and monitoring of datasets, experi-ments, and models across the machine learning system life-cycle (MLOps). Additional properties required by respon-sible AI systems, such as robustness and explainability, areactive research areas with a developing ecosystem of tools.In this paper, we present two software libraries thataddress critical needs for responsible AI engineering (seeFigure 1). hydra-zen extends the popular Hydra frame-work (Yadan 2019) to standardize the process of mak-ing complex AI applications con\ufb01gurable and their behav-iors reproducible. It provides specialized tools that dramat-ically simplify the process of adopting the Hydra frame-work and that eliminate major sources of technical debtthat this framework typically creates. The rAI-toolboxis designed to enable methods for evaluating and enhancingthe robustness of AI models in a way that is scalable andthat composes naturally with other popular ML frameworks,such as PyTorch (Paszke et al. 2019).We demonstrate the composability and \ufb02exibility of thetools by showing how two different use cases from adver-sarial robustness and explainable AI can be concisely imple-mented with familiar APIs. We then conclude with a \ufb01nalexample that computes robustness curves for two differentmodels using both hydra-zen and the rAI-toolbox,to show how robustness analyses can be easily scaled. Bothlibraries rely on property-based testing (MacIver 2019) andautomatic test case generation, leveraging the Hypothesislibrary (MacIver, Hat\ufb01eld-Dodds et al. 2019; MacIver andDonaldson 2020), in order to ensure that the tools used forevaluating AI systems are themselves reliable.",
                    "reference": "[1] Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
                },
                {
                    "id": "ea02a3ff-3b69-4e63-8373-8a4281679939",
                    "text": "Conducting experiments and analyses in a reproduciblemanner is a critical facet of responsible AI engineeringpractices. The need to fastidiously document and organizeone\u2019s work in a systematic way is of manifest importanceto any scienti\ufb01c endeavor. That being said, AI systems areoften sprawling software enterprises whose pillars\u2014dataprocessing pipelines, model architectures, and training &testing frameworks\u2014are each complex, hierarchical sys-tems whose states are challenging to record. While somepopular tools, such as Weights & Biases (Biewald 2020),Comet (Comet.ML 2021), and MLFlow (MLFlow 2021),provide support for tracking experiments in the name ofreproducibility, they ultimately provide only \u201cshallow,\u201d in-complete summaries of AI systems. By contrast, the Hy-dra framework (Yadan 2019) has designed a rich, domain-speci\ufb01c language for describing con\ufb01gurations of com-plex software applications; this framework, leveraged viaour user-facing hydra-zen Python library (Soklaski andGoodwin 2021), enables AI systems to be made thoroughlycon\ufb01gurable and reproducible.In general, Hydra and hydra-zen standardize the pro-cess of designing AI systems that are:Con\ufb01gurable: All of one\u2019s Hydra-basedapplication\u2014including deeply nested components\u2014canbe con\ufb01gured from a single interface.aspectsReproducible: Each run of self-documenting; the full con\ufb01guration of the software issaved alongside the results of that run.the application isScalable: Multiple con\ufb01gurations of the application canbe launched\u2014to sweep or search over con\ufb01gurationsubspaces\u2014using a variety of local and distributed job-launcher methods. Listing 1: Using hydra-zen to automatically generatea con\ufb01guration for a popular optimizer from the PyTorchframework. This \u201ccon\ufb01g\u201d can be serialized to a YAML\ufb01le by Hydra in order to record its exact con\ufb01guration forpurposes of reproducibility.At the core of the Hydra framework is a YAML-based,domain-speci\ufb01c language that enables the description andcon\ufb01guration of hierarchical structures in a Python-basedsoftware application. Thus, even nested components of one\u2019sAI system can be con\ufb01gured from a single interface, and acomplete con\ufb01guration of the entire system can be repre-sented via a human-readable YAML \ufb01le. Additionally, onecan identify so-called \u201ccon\ufb01guration groups\u201d in the appli-cation\u2019s structure, so that entire sections of the system\u2014e.g., a particular data pre-processing procedure\u2014can be\u201cswapped\u201d en-masse in an ergonomic way. Reproducibil-ity is a natural consequence of this con\ufb01gurability: eachjob launched by Hydra is documented by\u2014and can be fullyreplicated by\u2014the YAML con\ufb01guration that is automati-cally recorded for that job.hydra-zen provides Hydra users with elegant toolsfor automatically generating and customizing Hydra-compatible con\ufb01gurations. Without these tools, users arefaced with hand-writing and maintaining YAML con\ufb01gu-rations (or their Python-based equivalents) for their entiresoftware application; such a process is manually intensive,error-prone, repetitive, and is a major source of technicaldebt (Soklaski 2021). hydra-zen eliminates these costsby enabling a Python-centric, ergonomic work\ufb02ow for dy-namically populating and automatically validating con\ufb01gu-rations for one\u2019s entire software application. For example,Listing 1 demonstrates the use of hydra-zen\u2019s buildsfunction to dynamically generate a con\ufb01guration for an opti-mizer from the PyTorch library, based on that optimizer\u2019s de-fault values. Thus, hydra-zen makes it tractable for bothlarge-scale AI systems and smaller, rapid-prototype systemsto be designed to be con\ufb01gurable, reproducible, and scalablevia the Hydra framework.hydra-zen\u2019s con\ufb01guration-creation tools also enablea unique, con\ufb01guration-based functionality modi\ufb01cationframework, which can be used to augment the behaviorof the various components of a system without modifyingthe system\u2019s source code. For example, rich runtime type-checks of con\ufb01gured values, and schema-based data vali-dation checks, can be \u201cinjected\u201d into the system\u2019s variousinterfaces\u2014and be subsequently enabled or disabled\u2014bysolely modifying the con\ufb01guration of the system. The abil-ity to effectively patch the behaviors of a system via itscon\ufb01guration is highly valuable from a system-maintenanceperspective; this capability is not native to Hydra, but is a\u201chigher-order\u201d capability that is enabled by the design of thefunctions provided by hydra-zen.Ultimately, Hydra and hydra-zen are uniquely well-suited to enable rigorous reproducibility in AI systems, by-design.",
                    "reference": "[1] Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
                },
                {
                    "id": "9c256b9c-f596-4333-9ac1-9352f177a5c2",
                    "text": "While hydra-zen contributes to responsible AI practicesthrough enabling the ecosystem surrounding AI model de-velopment and deployment to be more transparent, there isalso a need for the AI models themselves to be engineered toobey the set of responsible AI principles. There is a growingeffort to innovate methods for assessing and bolstering therobustness and explainability of AI systems\u2014two key facetsof responsible AI. While a variety of tools exist that im-plement common techniques from the research community,such as Foolbox (Rauber, Brendel, and Bethge 2017), Clev-erHans (Papernot et al. 2018), and IBM\u2019s Adversarial Ro-bustness Toolbox and AI Explainability 360 (Nicolae et al.2018; Arya et al. 2019), these tools have yet to be fully inte-grated into existing MLOps frameworks. Many of the exist-ing tools are designed to provide a large library of techniquesfor evaluating or enhancing robustness in a framework ag-nostic API; this has a handful of limitations: (1) support forrecent techniques is accomplished by relying heavily on re-search implementations, which can result in brittle and spe-cialized code, being domain or even dataset-speci\ufb01c and (2)support for multiple frameworks results in un-scalable codethat is dif\ufb01cult to build from. These observations inspired ahandful of design principles for the rAI-toolbox that weexplain in the following sections.In section 3.1, we identify key low-level problems thatthat often need to be solved for robust AI tasks. Solutionsto these problems form the core of the rAI-toolbox.In section 3.2, we explain the advantages of designing therAI-toolbox around a single deep learning framework.Section 3.3 describes how the rAI-toolbox supportsscalable work\ufb02ows by being compatible with existing toolsthat enable distributed computation. Finally, in section 3.4,we describe how we employ property-based testing (PBT)to ensure that the rAI-toolbox itself is reliable.",
                    "reference": "[1] Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
                },
                {
                    "id": "b6ef239c-ed1b-4d36-bff8-2a3c2af75d13",
                    "text": "Whereas standard model-training frameworks are designedto re\ufb01ne the parameters of the machine learning model (i.e.,architecture and weights), methods for studying the robust-ness (and often other responsible AI properties) of the modelnaturally involve analyses of and optimizations over the data(i.e., inputs to the model and representations extracted by themodel). This optimization over the data space increases thecomplexity of the responsible AI engineering work\ufb02ow overthat of the standard setting. For example, consider the standard optimization objectivefor training a model, f , parameterized by \u03b8, where x and yrepresent the data input and corresponding output sampledfrom a data distribution, D, and L is the loss function to beminimized: E [L(f (x), y)].min (1)Note that here, the data samples are \ufb01xed, and the search isdone over the model\u2019s parameter space. Once the model istrained, its performance is often evaluated using an indepen-dent (but usually \ufb01xed) set of data samples from the samedata distribution.Now, consider the process of solving for a worst-case (i.e.,\u201cadversarial\u201d) perturbation to a data input to fool the modelinto producing an incorrect output, which is common prac-tice when assessing the adversarial robustness of the model(Carlini et al. 2019). The perturbation, \u03b4, is optimized tomaximize loss against the true output, y, subject to a con-straint set, \u2206: max L(f (x + \u03b4), y). (2)Here, the model parameters are held \ufb01xed, and the search isconducted over the data space. The constraint set will varydepending on the goals of the researcher, the data domain,etc., but a common choice for the constraint is the (cid:96) -ball ofradius (cid:15), often with p = 1, 2, or \u221e. Additionally, a plethoraof approaches to solving this objective under different losscon\ufb01gurations have been proposed, with perhaps the mostpopular being iterative projected gradient descent (PGD) onthe negative cross-entropy loss (Madry et al. 2018).To characterize the adversarial robustness of an AI sys-tem, researchers and practitioners alike may be interested insolving Equation 2 under a variety of con\ufb01gurations, whichmay include swapping out constraints, loss functions, andoptimizers. They will need to run this optimization acrossan entire test set, and may also be interested in re-runningthis analysis for multiple ML models to compare their ro-bustness. Clearly, assessing adversarial robustness is a com-plex and computationally-intensive process, requiring toolsfor solving Equation 2 that are just as composable and scal-able as the existing ML tools that solve Equation 1.Moving beyond the af\ufb01ne perturbation model from Equa-tion 2, consider the following broad problems that form thecore of robust AI, where \u03b4 is now used to represent the pa-rameters of a generalized model for applying transforma-tions to data, g (x), such as in (Laidlaw and Feizi 2019):Transforming Data (e.g., augmentations, corruptions):g (x)Optimizing Transformations of Data:max L(f (g (x)), y)Optimizing Transformations of Data Distributions:max E [L(f (g (x)), y)]Optimizing Models on Transformed Data: (3)(4)(5)min E [max L(f (g (x)), y)] (6)Optimizing Models on Transformed Distributions:min max E [L(f (g (x)), y)] (7)Our Responsible AI Toolbox (rAI-toolbox) is de-signed to support all of the \ufb02avors of analysis represented byEquations 3 - 7. The solutions to these sophisticated, data-scrutinizing problems, which often depend on the state of themodel itself, do not naturally \ufb01t into the standard train, test,and deploy stages that are facilitated by popular, high-levelML APIs like PyTorch (Paszke et al. 2019), PyTorch Light-ning (Falcon and Cho 2020), or fastai (Howard et al.2018). As a result, much of the tooling that is currentlyavailable for assessing and enhancing the robustness in AIsystems is research-grade code. Relative to standard APIs,these existing tools lack in composability, scalability, andreliability (i.e., often they are under-tested or have no testswhatsoever). In the remaining three sections, we discuss thedesign principles behind the rAI-toolbox that addressthese challenges, along with illustrative examples.",
                    "reference": "[1] Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
                },
                {
                    "id": "ef6bd9f8-9a6e-4fd3-8c04-990678cdb627",
                    "text": "A key design principle of the rAI-toolbox is that weadhere strictly to the general APIs speci\ufb01ed by the Py-Torch machine learning framework. This enables our li-brary\u2019s functions and boilerplate code to compose elegantlywith the powerful features provided by PyTorch and otherPyTorch-based projects, and it ensures that we expose onlyfamiliar and well-de\ufb01ned interfaces to our users. In contrast,we have found that attempts by robustness libraries to simul-taneously cater to multiple ML frameworks, such as Tensor-Flow (Abadi et al. 2015) and JAX (Bradbury et al. 2018) inaddition to PyTorch, inevitably hinders this essential com-posability.To highlight the utility of this design principle, considerthat the data exploration techniques that are employed inAI robustness analyses (and introduced in Section 3.1) areoften substantially more sophisticated than the gradient-based methods that typically support model training. Forexample, model-dependent line-searches and (cid:96) -norm pro-jected optimizers are essential tools of the trade in thisdomain. Despite this sophistication, the rAI-toolboxdesigns all data exploration work\ufb02ows around the basictorch.optim.Optimizer API. Thus, users can readilyuse PyTorch-based \u201coff-the-shelf\u201d optimizers for robustnessanalysis and enhancement, or they can implement a customoptimizer towards this end.The bene\ufb01ts afforded by this design choice are hardto overstate; we can natively accommodate over data do-mains: line-searches via closures, heterogeneous learning-rate schedulers, higher-order optimization methods, andmore. These capabilities are not available in other AI robust-ness toolkits, which tend to design their APIs in service ofparticular popular research results. Ultimately, we anticipatethat the design of the rAI-toolbox will enable innova-tive engineering solutions for improving and optimizing AIrobustness techniques, as well as the incorporation of thesetechniques at-scale in MLOps pipelines. Listing 2: A standard work\ufb02ow to solve for dataperturbations (i.e., Equation 4) using rAI-toolbox. Notethat it is by design that this work\ufb02ow closely matches astandard model-training loop in PyTorch.Listing 3: Leveraging the solver from Listing 2 to performprojected gradient descent, in search of minimal dataperturbations that \u201cfool\u201d our model. Some representativeresults are depicted in Figure 2.To highlight the utility of the design principles discussedin this section, we provide some simple examples of usingthe rAI-toolbox for two different responsible AI anal-yses that both involve solving Equation 4. We use the Im-ageNet dataset (Russakovsky et al. 2015) and standard androbust models from (Engstrom et al. 2019) for these ex-amples. Listings 3 and 4 both leverage the same data per-turbation solver that is de\ufb01ned in Listing 2, yet they performdistinct tasks. The former generates (cid:96) -constrained adversar-ial perturbations using PGD (depicted in Figure 2), whereasthe latter uses a sparse optimizer to perform concept prob-ing (Roberts and Tsiligkaridis 2021) on a model (depictedin Figure 3). The \ufb02exibility demonstrated here is a testamentto the value of designing the rAI-toolbox around strictadherence to PyTorch\u2019s APIs.Listing 4: Leveraging the solver from Listing 2 to performconcept probing of our model; the model is prompted toprovide an internally generated, visual representation of aconcept. Some representative results are shown in Figure 3.for data-distributed parallelism, model sharding techniques,and more. Additionally, integrating robust AI work\ufb02owswith popular frameworks that support scalability such as Py-Torch Lightning or fastai is as simple as integrating tradi-tional training and testing work\ufb02ows. For example, Listing 5demonstrates the simplicity of building a scalable optimiza-tion loop for generating adversarial perturbations using Py-Torch Lightning\u2019s LightningLite API. Here you willnotice the reuse of code provided in Listings 2 and 3 withthe additional functionality required to setup scalability withLightningLite. Given PyTorch Lightning\u2019s support ofmultiple distributed strategies, e.g. DeepSpeed (Rasley et al.2020), a user developing with rAI-toolbox will have au-tomatic support for multiple distributed strategies.For scaling experimentation work\ufb02ows, such as evaluat-ing the robustness of multiple models trained with differ-ent techniques, it currently is a challenge to launch multi-ple jobs in a way that not only distributes each job to re-duce the computation time, but also ensures each individ-ual experiment is con\ufb01gurable and repeatable. This is wherewe see the power of combining Hydra, hydra-zen, andrAI-toolbox. As mentioned in Section 2, by con\ufb01guringexperiments with hydra-zen, a user can take full advan-tage of Hydra\u2019s ecosystems of plugins for launching mul-tiple jobs by sweeping over \u201cswappable\u201d con\ufb01gurations orranges on parameter values. This type of work\ufb02ow is out-lined in Listing 6 to demonstrate how to launch an experi-ment to compute the performance of a standard and robustmodel against adversarial perturbations of varying strength(in this case, the size of the (cid:96) -ball) using PGD. The outputsof the individual job runs can then be collected and plotted toproduce robustness curves, as shown in Figure 4 for a stan-dard and robust model from (Engstrom et al. 2019) on theCIFAR-10 dataset (Krizhevsky, Hinton et al. 2009). This ex-Figure 2: Some illustrative results showing original (toprow) and adversarially perturbed (bottom row) images andtheir predicted labels from a standard ImageNet model, gen-erated using the rAI-toolbox via Listings 2 and 3.Figure 3: Illustrative results of concept probing for a ro-bust ImageNet model, using rAI-toolbox via Listings 2and 4. Inputs are noisy samples (top) and the optimizationis prompted to render representations of a \u201cBlack Widow\u201d,\u201cJay Bird\u201d, and a \u201cBurrito\u201d (bottom).",
                    "reference": "[1] Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
                },
                {
                    "id": "f93817cd-8fa7-453c-8915-8e555ddda8a9",
                    "text": "Scalability is about performing computationally expensiveoperations in a way that improves productivity and reducescost of training and testing. There are two essential prop-erties of scalability that the toolbox provides support for:frameworks supporting PyTorch distributed API to distributemodels and data, and scaling experimentation work\ufb02ows toexecute jobs that span multiple models, datasets, and param-eters.First, the rAI-toolbox\u2019s tight integration with Py-Torch APIs, such as the torch.optim.Optimizer,provides natural support for frameworks that utilize PyTorchdistributed. This is highly bene\ufb01cial because it helps ad-dress integration challenges of techniques that often havelarge computational costs. Users can take full advantage ofany framework that utilizes the PyTorch distributed interfacescaling rAI-toolboxListing 5: An example ofusing PyTorch Lightning\u2019s LightningLite to distributeoptimization for data perturbations across multiple GPUs.LightningLite handles the distributed computations forthe model and data.ample demonstrates how the combination of hydra-zenand the rAI-toolbox can enable a complex ResponsibleAI activity (e.g., evaluating the robustness of multiple MLmodels) to be conducted at scale and in a traceable manner.",
                    "reference": "[1] Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
                },
                {
                    "id": "0e64f58e-5ff9-46b2-b348-0e7e5b2be626",
                    "text": "A \ufb01nal essential, but oft-overlooked, component to devel-oping analysis tools for AI systems is a comprehensive testsuite, which not only exercises the APIs of one\u2019s tooling butalso checks for the correctness of the software. Given thatthese tools are being used to diagnose the reliability of al-ready dif\ufb01cult-to-interpret AI systems, their trustworthinessis of manifest importance. That being said, there is a nat-ural barrier to testing such software for correctness underdiverse conditions, as there is typically no \u201coracle\u201d againstwhich one can test their implementation; it can appear that,in order to test one\u2019s implementation of a mathematicallysophisticated algorithm, one must obtain an independent im-plementation to compare against. Rather than indulge in thisexercise of tautology, the rAI-toolbox leverages a styleof testing known as property-based testing (MacIver 2019),which empowers us to verify that the critical implementationdetails of our tooling are reliable.A property-based test (PBT) is designed to check thatan expected property of a function holds true under an ex-ceptionally diverse set of inputs to the function. That is,whereas a traditional example-based test (EBT) will checkthat a function produces exactly the expected output whenpassed a concrete input, a PBT makes less precise, but moregeneral, assertions about the function. This is an importantdistinction that enables one to elegantly test complex, scien-ti\ufb01c software. For example, suppose that we have implemented inrAI-toolbox a projection function, \u03a0 : X \u2192 X , thatmaps an input onto a particular sub-domain. An EBT of thisfunction would check that a particular value, x , gets mappedby \u03a0 to a particular value y , i.e., \u03a0(x ) = y . By contrast,a PBT could test that\u2014for any input x \u2208 X \u2014\u03a0 is idempo-tent: \u03a0(x) = \u03a0(\u03a0(x)). Note that the EBT example requiresan oracle (e.g., a human performing a calculation by hand)to provide us with y , whereas we can verify the idempo-tence property of our implementation of \u03a0 in a purely self-consistent way. Thus by identifying and testing a suf\ufb01cientlyrich set of properties of a function, which are often meta-morphic in nature (Segura et al. 2016), we are capable ofverifying the correctness of that function over very diverseuse cases.The quality of one\u2019s PBTs is largely determined by thediversity and volume of inputs that you can use to exercise agiven test. Towards this end, we leverage the test-case gen-eration library Hypothesis (MacIver, Hat\ufb01eld-Dodds et al.2019; MacIver and Donaldson 2020). Hypothesis is a pop-ular Python library that provides powerful test-case genera-tion strategies that are used to \u201cdrive\u201d PBTs. The primitivesthat it offers can be composed to create sophisticated de-scriptions of data, which are then used to adaptively gener-ate diverse test cases in search for falsi\ufb01able assertions in atest. Listing 7 demonstrates a Hypothesis-driven PBT, whichgenerates \ufb01nite-valued 32-bit \ufb02oat arrays of arbitrary shapesto check for idempotence of a particular NumPy (Harriset al. 2020) function. The test-suite for the rAI-toolboxmakes extensive use of Hypothesis; it is an essential aspectcontributing to the reliability of our code. Ultimately, werecommend that other library authors consider adopting Hy-pothesis, and property-based testing in general, to substan-tially improve the quality of software testing methods for AIsystems.",
                    "reference": "[1] Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
                },
                {
                    "id": "287a2e9e-6dd2-43d2-8cfb-ba41fff6d42b",
                    "text": "The availability of powerful, easy-to-use deep learning li-braries greatly accelerated AI research and led to break-throughs in computer vision, natural language, and otherdomains. As the development and deployment of AI-enabled systems becomes more widespread, there is anurgent need for tools that facilitate responsible AI engi-neering practices. In this work, we presented two softwarelibraries\u2014hydra-zen and the rAI-toolbox\u2014that helpresearchers and developers create AI systems that aremore con\ufb01gurable, reproducible, robust, and explainable. Bykeeping the core components in the rAI-toolbox strictlyaligned with standard PyTorch APIs, the toolbox shouldremain interoperable with new developments in the Py-Torch ecosystem (e.g., related to distributed computing), aswell as provide a \ufb02exible foundation for additional researchand development of responsible AI capabilities. Some near-term directions for rAI-toolbox development includeadding domain-agnostic perturbations and support for ad-ditional data augmentation-based training approaches, e.g.,AugMix (Hendrycks et al. 2019), to enable application ofresponsible AI techniques for a broad set of domains and toperform more diverse robustness assessments.Listing 6: Con\ufb01guring and launching an experiment to evaluate performance of a standard and robust model against adversarialperturbations of varying size, \u201cepsilon\u201d. hydra-zen is used to build con\ufb01gurations that will be saved out in YAML format foreach experiment and launch the jobs which sweep over the models and values of (cid:15), while our LightningLite model fromListing 5 is used to distribute each experiment across 2 GPUs using PyTorch\u2019s data distributed parallel.Figure 4: Robustness curves based on Listing 6, showing ad-versarial accuracy on the CIFAR-10 test dataset for a stan-dard and robust model with perturbations of increasing size.",
                    "reference": "[1] Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
                },
                {
                    "id": "eb8929a1-b5e8-4354-b076-f9156d55255c",
                    "text": "DISTRIBUTION STATEMENT A. Approved for public re-lease. Distribution is unlimited. This material is based uponwork supported by the Under Secretary of Defense forResearch and Engineering under Air Force Contract No.FA8702-15-D-0001. Any opinions, \ufb01ndings, conclusions orrecommendations expressed in this material are those of theauthor(s) and do not necessarily re\ufb02ect the views of the Un-der Secretary of Defense for Research and Engineering. \u00a92021 Massachusetts Institute of Technology. Delivered tothe U.S. Government with Unlimited Rights, as de\ufb01ned inDFARS Part 252.227-7013 or 7014 (Feb 2014). Notwith-standing any copyright notice, U.S. Government rights inthis work are de\ufb01ned by DFARS 252.227-7013 or DFARS252.227-7014 as detailed above. Use of this work other thanas speci\ufb01cally authorized by the U.S. Government may vio-late any copyrights that exist in this work.A portion of this research was sponsored by the UnitedStates Air Force Research Laboratory and the United States Listing 7: A property-based test, using the Hypothesis test-case generation library, which validates the idempotentproperty of the NumPy clip function. By default, runningthe test will prompt Hypothesis to adaptively generate 100arrays of various shapes and contents as test cases.Air Force Arti\ufb01cial Intelligence Accelerator and was accom-plished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this docu-ment are those of the authors and should not be interpretedas representing the of\ufb01cial policies, either expressed or im-plied, of the United States Air Force or the U.S. Govern-ment. The U.S. Government is authorized to reproduce anddistribute reprints for Government purposes notwithstandingany copyright notation herein.",
                    "reference": "[1] Ryan Soklaski, Jason Goodwin, Olivia Brown, Maya Yee, Dawei Yang, Victoria Alsina, Jie Xu, and Brent Hecht. 2022. Tools and practices for responsible AI engineering. arXiv:2201.05647. Retrieved from https://arxiv.org/pdf/2201.05647"
                }
            ]
        },
        {
            "paper_title": "Responsible ai pattern catalogue: A collection of best practices for ai governance and engineering",
            "authors": "Q Lu, L Zhu, X Xu, J Whittle, D Zowghi\u2026",
            "publication_info": "ACM Computing \u2026 - dl.acm.org",
            "paper_url": "https://dl.acm.org/doi/pdf/10.1145/3626234",
            "chunks": [
                {
                    "id": "3a45a8dc-e922-46bc-aa23-3d0a50ae548b",
                    "text": "",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "a41eabbb-833c-4b1c-8498-6f024b68640d",
                    "text": "has been transforming our society and listed as the top strategictechnology in many organizations. Although AI has huge potential to solve real-world challenges,there are serious concerns about its ability to behave ethically and make decisions in a responsibleway. Compared to traditional software systems, AI systems involve a higher degree of uncertaintyand more ethical risk due to their dynamic, autonomous, and opaque decision making and histor-ical data-dependent behaviors. Responsible Artificial Intelligence (RAI) refers to the ethical173:2 Q. Lu et al.development of AI systems to benefit the humans, society, and environment. The concept of RAIhas attracted significant attention from governments, organizations, companies, and societies.According to the 2022 Gartner CIO and Technology Executive Survey, 48% of organizationshave already adopted or plan to adopt AI technologies within the next 12 months, whereas 21%of organizations have already deployed or plan to deploy RAI technologies within the next 12months. RAI has been widely considered as one of the greatest scientific challenges of our timeand the key to unlock the market and increase the adoption of AI.To address the RAI challenges, a number of AI ethics principles frameworks have been pub-lished recently [57], which AI systems are supposed to conform to. There has been a consensusmade around the AI ethics principles [37]. A principle-based approach allows technology-neutral,future-proof, and context-specific interpretations and operationalization. However, without fur-ther best practice guidance, practitioners are left with nothing much beyond truisms. For example,it is a very challenging and complex task to operationalize the the human-centered value principleregarding how it can be designed, implemented, and monitored throughout the entire lifecycle ofAI systems. In addition, significant efforts have been put on algorithm-level solutions which mainlyfocus on a subset of mathematics-amenable ethical principles (e.g., privacy and fairness). However,issues (including ethical issues) can occur at any step of the development lifecycle, crosscuttingmany AI, non-AI, and data components of systems beyond AI algorithms and models. To try tofill the principle-algorithmic gap, further guidance such as guidebooks, questions to generatediscussions [66, 67], checklists [44, 62], and documentation templates [1, 4, 52, 55, 94, 127] havestarted to appear. Those efforts tend to be ad-hoc sets of more detailed prompts for practitionersto think about all the issues and come up with their own solutions.In this article, we therefore adopt a pattern-oriented approach and present an RAI PatternCatalogue for operationalizing RAI from a system perspective. In software engineering, a patternis a reusable solution to a problem that occurs commonly within a given context in softwaredevelopment [13]. Rather than staying at the ethical principle level or algorithm level, we focuson patterns that practitioners can utilize in practice to ensure that the developed AI systems areresponsible throughout the entire software development lifecycle. As shown in Figure 1, the RAIRAI Pattern Catalogue 173:3Pattern Catalogue classifies patterns into three groups: (1) governance patterns for establishingmulti-level governance for RAI, (2) process patterns for setting up trustworthy developmentprocesses, and (3) product patterns for building RAI-by-design paradigm into AI systems. Thesepatterns are identified through conducting a systematic Multivocal Literature Review (MLR).The full version of our RAI Pattern Catalogue can be accessed online.The remainder of the article is organized as follows. Section 2 introduces the methodology forbuilding up the pattern catalogue. Section 3 presents the AI system stakeholders and governancepatterns. Section 4 discusses the process patterns for each stage of the development lifecycle. Sec-tion 5 introduces the project patterns. Section 6 discusses the related work. Section 5 concludesthe article.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "6dfa43ba-35c5-4806-b126-eec75fdc54e1",
                    "text": "To build up an RAI Pattern Catalogue, we performed a systematic MLR to collect patterns. Figure 2presents the research design and methodology. The high-level research question that has guidedthis research is this: \u201cWhat RAI solutions can be identified?\u201d The research question focuses onidentifying the reusable patterns for RAI.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "6c9a4ccb-9d4c-4950-95e6-c428e82f76b7",
                    "text": "The benefit of an MLR is to cover both academic literature and grey literature in the study. Greyliterature is written by practitioners (e.g., governments, organizations, companies) and not pub-lished in books or scientific journals/conferences. However, grey literature can provide valuableinsights on the state of practice and may include many industry solutions that are not discussed in173:4 Q. Lu et al.academic papers. Given the nature of patterns, we decided to also review grey literature to under-stand the state of the practice in the field of RAI and collect patterns from industry. In our MLR, weidentify (1) relevant academic peer-reviewed academic literature and (2) relevant grey literaturefor this study.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "9b1d59a0-8357-4137-b4d8-a4c303353dca",
                    "text": "The study has been carried out separately for academic literature and grey literature. We adoptedthe Systematic Literature Review (SLR) guideline in the work of Kitchenham and Charters [60]to review the academic literature and used the guideline of Garousi et al. [39] to perform thegrey literature review. The complete MLR protocol is available as online material. Overall, wefirst tested different search strings in the five well-known search engines and evaluated the totalnumber of studies retrieved, as well as their relevance. The evaluation involved cross checkingthe inclusion of known relevant literature. Once we determined the most effective search string,we proceeded to perform searches on Google Scholar. From the results, we randomly selected20 papers and extracted the relevant answers for each of the research questions.We determined the search strings by deriving relevant keywords from the research question.Before conducting the systematic search, we did a pilot study by experimenting with the searchterms to compare the results. We used \u201cAI,\u201d \u201cResponsible,\u201d and \u201cSolution\u201d as the key terms andincluded synonyms and abbreviations as supplementary terms to increase the search results. Wedesigned the search strings for each primary source to check the title. After completing the firstdraft of search strings, we examined the results of each search string against each database tocheck the effectiveness of the search strings. The finalized search terms are shown in Table 1.We use Australia\u2019s AI ethics principles [27] to identify the supplementary terms for \u201cResponsi-ble\u201d as a close-enough representation of the many similar ones [37, 57] around the world. Theeight AI ethics principles include human, societal, and environmental wellbeing; human-centeredvalues; fairness; privacy protection and security; reliability and safety; transparency and explainabil-ity; contestability; and accountability. We mapped each individual term in the eight principles toits corresponding noun term and adjective forms. Furthermore, to encompass the relevant termsrelated to RAI, we expanded the mapping to include \u201cresponsible\u201d as well as its variations, suchas ethics, ethical, responsibility, trust, trusted, trustworthiness, and trustworthy. By doing so, weensure comprehensive coverage of the terms relevant to RAI. The search strings and the respectivepaper quantities of the initial search for each primary source are listed in our MLR protocol. Weapplied the search string to both scholar search engines for academic literature and Google SearchEngines for grey literature. The scholar search engines includeACM Digital Library, IEEE Xplore,Science Direct, Springer Link, and Google Scholar. The search period is up to July 31, 2022.RAI Pattern Catalogue 173:5We screened the initial results against inclusion and exclusion criteria. The inclusion criteriainclude the following: (1) a paper/article that presents a governance or process or design solutionfor RAI, (2) a paper/article that presents a tool or toolkit for developing RAI systems, and (3) apaper/article that is in the form of a published scientific paper or industry article. The exclusioncriteria are as follows: (1) a paper/article that only discusses high-level principles or frameworks,(2) a paper/article that only focuses on algorithm-level techniques, (3) a paper/article that is notwritten in English, (4) a conference version of a study that has an extended journal version, and(5) Ph.D./Master\u2019s dissertations, tutorials, editorials and books.The snowballing technique has been recommended and used in place of database searches inSLRs. For the academic literature, we identified a set of papers that serve as the starting point (i.e.,seed set) for snowballing. The seed set papers were selected based on the source databases andAustralia\u2019s AI ethics principles to cover various communities. For each of the five source databases,we selected one top-cited paper for RAI in general and each of the eight principles, respectively.For some principles, there is no paper found in one particular database. We only collected thegrey literature from the first 10 Google pages. For the grey literature, snowballing is conductedif related RAI solutions are mentioned on the webpage. We finally identified 205 academic itemsand 69 grey items for the MLR. For the grey literature, we organized the RAI solutions accordingto the corresponding companies. For example, we found 13 RAI tools/solutions on Microsoft\u2019swebsite but only counted Microsoft as one grey item in our data extraction sheet and recorded afew patterns extracted from Microsoft\u2019s tools/solutions.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "71dd49ac-0e5d-4798-baaf-c44e07e953e6",
                    "text": "To realize RAI from a software engineering perspective, we need to make both AI products andtheir development processes trustworthy and responsible. Additionally, compliance with AI stan-dards and laws from a governance perspective is necessary. Thus, we classify the patterns intothree categories: governance, process, and product. Not only should you use product patterns toenforce RAI principles directly in the product and verify/validate the product, but you should alsouse process and governance patterns to complement it further.We extracted data and summarized findings from the selected academic and grey items basedon the pre-defined research question. Based on the answers extracted for the research question,we identified different types of patterns. For example, there are a few papers using federatedlearning to deal with data privacy issues, thus \u201cfederated learner\u201d is identified as a productpattern that can be built into the architecture of AI systems for continuous learning. Some ofthe solutions can be mapped to multiple levels. For example, the software bill of materials can beidentified as an organization-level governance pattern that is interconnected with and supportedby the product pattern \u201cbill of materials registry.\u201d After identifying a pattern, we documented itsdetails on our RAI Pattern Catalogue website according to the traditional pattern structure [74]:context, problem, solution, benefits, drawbacks, related patterns, and known uses. The knownuses were found through data extraction and additional manual search. The pattern users andimpacted stakeholders are identified based on (1) the AI software supply chain and ecosystem, (2)AI standards, and (3) our expertise and knowledge.We also extract some general information, such as author name, organization, publication venue,and publication year. We performed a pilot study on 20 items to test the research question and theway to extract the required data. We stored all the extracted data in a spreadsheet for analysis.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "ed37378b-5c9f-43cf-973f-ba904230882a",
                    "text": "The governance for RAI systems can be defined as the structures and processes that are employedto ensure that the development and use of AI systems meet AI ethics principles. According to the173:6 Q. Lu et al.structure of Shneiderman [104], governance can be built at three levels: industry level, organizationlevel, and team level. As illustrated in Figure 3, we identified the stakeholders for RAI governanceand classified them into three groups:\u2022 Industry-level stakeholders:\u2014 AI technology producers develop AI technologies for others to build on top to produce AIsolutions (e.g., parts of Google, Microsoft, IBM). AI technology producers may embed RAIin their technologies and/or provide additional RAI tools.\u2014 AI technology procurers procure AI technologies to build their in-house AI solutions (e.g.,companies or government agencies buying/using AI platform/tools). AI technology pro-curers may care about RAI issues and embed RAI into their AI technology procurementprocess.\u2014 AI solution producers develop in-house/blended unique solutions on top of technologysolutions and need to make sure the solutions adhere to RAI principles/standards/regu-lations (e.g., parts of MS/Google providing Office/Gmail \u201csolutions\u201d). They may offer thesolutions to AI consumers directly or sell to others. They may use RAI tools (provided byAI technology producers or RAI tool producers) and RAI processes during their solutiondevelopment.\u2014 AI solution procurers procure complete AI solutions (with some further configurationand instantiation) to use internally or offer to external AI consumers (e.g., a governmentagency buying from a complete solution from vendors). They may care about RAI issuesand embed RAI into their AI solution procurement process.\u2014 AI users use an AI solution to make decisions that may impact on a subject (e.g., a loanofficer or a government employee). AI users may exercise additional RAI oversight as thehuman-in-the-loop.\u2014 AI-impacted subjects are impacted by some AI-human dyad decisions (e.g., a loan applicantor a taxpayer). AI impacted subjects may care about RAI issues and contest the decisionon dyad AI grounds.\u2014 AI consumers consume AI solutions (e.g., voice assistants, search engines, recommenderengines) for their personal use (not affecting third parties). AI consumers may care aboutRAI issues and the dyad AI aspects of AI solutions.\u2014 RAI governors are those who set and enable RAI policies and controls within their culture.RAI governors could be functions within an organization in the preceding list or external(regulators, consumer advocacy groups, community).RAI Pattern Catalogue 173:7\u2014 RAI tool producers are technology vendors and dedicated companies offering RAI featuresintegrated into AI platforms or AIOps/MLOps tools.\u2014 RAI tool procurers include any of the preceding stakeholders who may purchase or useRAI tools to improve or check solutions/technology\u2019s RAI aspects.\u2022 Organization-level stakeholders:\u2014 Management teams include individuals at the higher level of an organization who are re-sponsible for establishing RAI governance structure in the organization and achieving RAIat the organization level. The management teams include board members, executives and(middle-level) managers for legal, compliance, privacy, security, risk, and sustainability.\u2014 Employees are individuals who are hired by an organization to perform work for the orga-nization and expected to adhere to RAI principles in their work.\u2022 Team-level stakeholders:\u2014 Development teams include those who are responsible for developing and deploying AIsystems, including product managers, project managers, team leaders, business analysts,architects, UX/UI designers, data scientists, developers, testers, and operators. The devel-opment teams are expected to implement RAI in their development process and embedRAI into the product design of AI systems.As shown in Figure 4, we identify a set of governance patterns and classify them into industry-level governance patterns, organization-level governance patterns, and team-level governance pat-terns based on the governance structure of Shneiderman [104]. The target users of industry-levelgovernance patterns are RAI governors, whereas the impacted stakeholders include AI technol-ogy producers and procurers, AI solution producers and procurers, and RAI tool producers andprocurers. For the organization-level patterns, the target users are the management teams and theimpacted stakeholders are employees, AI users, AI consumers, and AI-impacted subjects. The tar-get users of team-level patterns are the development team, whereas the impacted stakeholders areAI users, AI consumers, and AI-impacted subjects.3.1 Industry-Level Governance Patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "9496dc33-16a3-4cc1-8e02-874b227b27b7",
                    "text": "Laws already apply to AI systems; however, the processes/requirementsto ensure compliance are not always certain, and also some regulations may need to be updated(e.g., administrative law). There is an urgent need for clear guidance to ensure that AI systemsare developed and used responsibly in compliance with existing and upcoming laws (e.g.,173:8 Q. Lu et al.discrimination law). RAI regulations are developed by governments in their jurisdiction to enablethe trustworthy development of AI systems by industry [26, 53, 54, 91, 98, 104, 105]. Organizationswill be required to ensure that they comply with the requirements of the EU AI Act when theapplications fall into the high-risk category. In the United States, the Algorithmic AccountabilityAct of 2022 was introduced in the Senate and House of Representatives, and an AI Bill of Rightsis under development by the White House Office of Science and Technology Policy. The aim ofRAI regulations is to prevent illegal or negligent, malicious use of AI systems. However, thereare many regulations in developments in each jurisdictions, which may cause an interoperabilitychallenge for organizations. In addition, it usually takes a long time to enact AI regulations dueto the lengthy consultation and approval process.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "ff9b9bbb-1037-4289-86d7-e9bcba59a962",
                    "text": "To enable the trial of the innovative AI products in the market, aregulatory sandbox can be designed to allow testing the innovative AI products in the real worldunder relaxed regulatory requirements but with appropriate safeguards in place on a time-limitedand small-scale basis [98]. An AI Regulatory Sandbox is introduced in the EU\u2019s AI Act proposalsubmitted in 2021. The UK Information Commissioner\u2019s Office advised a Regulatory Sandboxfor utilizing personal data. The Australian Government released the Enhanced RegulatorySandbox for innovative financial services. AI products can enter the market under more flexibleregulatory requirements in a faster pace and be tested in the real-world market to ensure that theyare designed ethically. However, it might incur extra cost to apply for a regulatory sandbox. Inaddition, the AI products might not work well with large-scale deployment in different contexts.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "38dc7400-4402-4113-b793-ac8d1ca4eeb0",
                    "text": "AI systems may have various degrees of risk depending on the design andapplication domains. To ensure that AI systems are trustworthy and meet certain minimum stan-dards, building code can be designed to provide mandatory regulatory rules for authority parties(e.g., an independent oversight and advisory committee) to assess the compliance of AI systemsbefore they are allowed to launch [104]. For example, IEEE has released a set of building codesfor developing smart cities, Medical Device Software Security, and Power System SoftwareSecurity. The building code sets out clear compulsory regulatory requirements for developingAI systems. AI systems cannot be sold in the market until an approval is issued by the assessmentauthority.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "f7095dc0-b069-45d4-9446-5fa19c19a9bf",
                    "text": "An AI system may use data or components from multiple jurisdictions thatmay have conflicting regulatory requirements on their usage. To enable interoperability betweenjurisdictions, RAI standards are developed to describe repeatable processes to develop and use AIsystems responsibly that are recognized internationally and can be either mandated by law or bycontract [53, 104, 105]. The ISO/IEC JTC 1/SC42 AI Technical Committee is developing the ISO/IEC42001 IT-AI-Management System Standard, which provides a pathway for the certification ofAI systems and WG3 trustworthiness that covers risk management and bias. IEEE has releasedRAI Pattern Catalogue 173:9the Guide for Architectural Framework and Application of Federated Learning, Standard forTechnical Framework and Requirements of Trusted Execution Environment Based Shared MachineLearning, and IEEE p7000 IEEE Standards for Model Process for Addressing Ethical ConcernsDuring System Design. Those AI standards provide repeatable processes and guidance for theuse and development of AI systems that are recognized internationally.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "d4b58bd0-446f-4763-8d47-5ae72643d8fe",
                    "text": "Organizations can face challenges that can hurt their business ifthey are not aware of their RAI maturity. The RAI maturity model can be used to assess an orga-nization\u2019s RAI capabilities and the degree of readiness to take advantage of AI based on a set ofdimensions [5, 38, 104, 126]. The RAI maturity model can guide organizations on how to increasetheir RAI capabilities. The assessment results depend on the model quality, such as assessmentdimensions and rating methods. There have been a few AI maturity models developed in industry,such as Gartner\u2019s AI Maturity Model, Microsoft\u2019s AI Maturity Model, and IBM\u2019s AI MaturityFramework.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "722f768d-2304-4e3b-9c5d-9e0c36e77c38",
                    "text": "AI is a high-stake technology that requires evidence to prove AIproducts\u2019 compliance with AI standards or regulations to operate in society. RAI certification canbe designed to recognize that an organization or a person has the ability to develop or use an AIsystem in a way that is compliant with standards or regulations [19, 22, 26, 46, 72, 73, 104, 105, 124].The Malta AI-ITA certification is the world\u2019s first AI certification scheme for RAI systems. TheDO-178C Certification has been used to approve commercial software-based aerospace systems.Queen\u2019s University offers an executive education program on Principles of AI Implementation.The evidence of compliance can be provided through RAI certification to improve human trust inAI systems. However, like other types of certificates, RAI certificates may be forged, which makesthe verification of authenticity of certificates challenging. The certification process is usuallycomplex, costly, and time consuming.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "64c2e896-6a38-450d-a503-41fd5c28b673",
                    "text": "Consumers in the market usually do not have professional knowledge aboutAI. To improve public confidence on AI and dispel their ethical concerns, the trust mark, a sealof endorsement, is easy to understand by all consumers and can be used to inform consumersabout the AI system. The trust mark is important for small companies which are often not wellknown in the AI market. However, consumers may not trust that the AI systems with a trust markperform more responsibly than those without one. There have been several trust marks designedfor responsible use of data, such as the Australian Data and Insights Association Trust Mark,the New Zealand Privacy Commissioner\u2019s Privacy Trust Mark, and Singapore\u2019s Data ProtectionTrustmark (DPTM).",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "b5eb55db-c114-4f91-b2b2-4a9b73986216",
                    "text": "Decisions made by AI systems may lead to severe failures due toits autonomous decision-making process. To audit AI systems and investigate failures in a trusted173:10 Q. Lu et al.way, independent oversight can be conducted by independent oversight boards that consist ofexperts who are knowledgeable to perform the review and have no conflict of interest with thereviewed organizations [76, 104, 105, 128]. The U.S. National Transportation Safety Board in-vestigates every civil aviation accident. The U.S. National Artificial Intelligence Advisory Com-mittee advises on AI-related issues. Independent oversight provides a trusted review infras-tructure to gain public confidence. Planning oversight provides early feedback on the new de-velopment proposals. Failures of independent oversight could happen due to lack of sufficientindependence.3.2 Organization-Level Governance Patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "87d0c0d2-8caa-4dd0-a076-1ad500ef1775",
                    "text": "The management teams need to understand the values,cost, and risk for adopting AI in an organization. Commitment needs to be made by the man-agement team to build an RAI culture within an organization [104]. Leadership commitment isachieved by the management team dedicating their time and efforts on establishing ethics princi-ples and governance structure (e.g., appointment of a chief RAI officer, RAI advisory boards) [108],as well as incorporating RAI into an organization\u2019s values, vision, mission [105], board strategyplanning, executives\u2019 performance reviews [98], audit and risk committee\u2019s scope [54], and ESGcommitments. Leadership commitment enables organizational culture on RAI and visible sponsor-ship to build RAI capability. IBM has established an AI ethics board to support a culture of RAIthroughout IBM. Axon has assembled an independent AI ethics board to provide guidance on AIsystem development. Schneider Electric has appointed its first chief AI officer to advance its AIstrategy.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "37f358b5-db66-4af3-92be-d189a20c0069",
                    "text": "Organizations need to build capability incorporating multiple areas ofexpertise to address RAI issues. An AI ethics committee is an AI governance body that is estab-lished to develop standard processes for decision making, as well as to approve and monitor AIprojects [19, 73]. Adobe has created an AI ethics committee that includes experts from differentbackgrounds. Sony has established an AI ethics committee to ensure the ethically developmentof AI systems. The ethics committee provides feedback and guidance to the project team after re-viewing the proposal. However, the committee might not have the expertise to review a particularcase, which might cause bias issues.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "baed0a44-2f29-4256-818c-e8d8c45f1147",
                    "text": "AI may make wrong decisions or behave inappropriately (e.g.,impact human lives or buy the wrong product). To guide AI-related activities in an organization, acode of ethics is a set of rules that employees should uphold when developing an AI system [48, 53,73, 104]. AAAI has issued the Code of Professional Ethics and Conduct for all members. Boschsets out a code of ethics to establish guidelines for the development of AI. BMW has releaseda code of ethics for AI. A code of ethics provides employees with the same concrete rules onRAI Pattern Catalogue 173:11developing AI systems, but it relies on individuals to do the right thing with limited monitoringand enforcement.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "2fa97228-2eea-44ba-85db-5540cd197e04",
                    "text": "Although there are increasing concerns on AI ethics, RAI reg-ulation is still at a very early stage. To assess the ethical risks associated with AI systems, anorganization needs to extend the existing IT risk framework or design a new one to cover AIethics [4, 14, 25, 31, 54, 64, 73, 94, 104]. The ISO/IEC JTC 1/SC 42 committee is developing ISO/IEC23894 on Artificial Intelligence and Risk Management. NIST released the initial draft of the AIRisk Management Framework that provides a standard process for managing risks of AI systems.The Canadian government has released the Algorithmic Impact Assessment tool to identify therisks associated with automated decision-making systems. The Australian NSW government ismandating all of its agencies that are developing AI systems to go through the NSW AI AssuranceFramework. Singapore launched the AI Verify Toolkit to test RAI. UK ICO released the AI andData Protection Risk Toolkit, which is built up on their guidance for organizations using AI sys-tems. Although ethical risk assessment has the potential to prevent the majority of incidents andincrease awareness of RAI, it is often a one-off type of risk assessment with subjective judgmenton measurement [95].",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "9f9d4867-990c-4d34-81a7-8a5cfb7b887f",
                    "text": "Standardized reporting is essential to address the opaque blackbox issue of AI systems. Organizations should set up standardized processes and templates for in-forming the development process and product design of AI systems to different stakeholders (e.g.,AI governors, users, consumers) [100]. RAI regulations may request such obligations to ensure thetransparency and explainability of AI systems. The Cyberspace Administration of China publishedtransparent disclosure requirements for online service providers. The service providers arerequested to file with the regulators (i.e., AI governors) for impact assessment when realizing newservices. In addition, the online services must inform users when AI is being used to recommendcontent to them and explain the purposes and design of recommended systems. In the EU\u2019s AIAct, the incidents of AI systems are required to be reported and disclosed by AI system providers(i.e., AI technology or solution producers). Helsinki and Amsterdam released AI registersdescribing where and how the two cities are using AI, how AI is built, which data and algorithmsare used, how the applications impact the citizens\u2019 daily lives, and the development team\u2019s contactinformation.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "923c3cab-5469-4dd1-bba5-b61a242cc695",
                    "text": "It is necessary that organizations have an appropriateapproach to enable accountability throughout the entire lifecycle of AI systems. Role-level ac-countability can be established through formal contracts to define the boundary of responsibilityand identify who should be held accountable when an AI system misbehaves [128]. For example,Australia\u2019s National Data Commissioner created a data sharing agreement template for using173:12 Q. Lu et al.Australian Government data. Developers primarily focus on the technique aspects of AI systemsand may not be familiar with the ethical principles. Role-level accountability contracts make thedevelopers keep ethics in mind at every step, but they may create stress for employees at all levelswithin an organization.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "8cd4dc4b-26ac-4af9-9eef-101bdafff663",
                    "text": "From a software supply chain angle, the development of AIsystems involves a complex and dynamic software supply chain. Many organizations procure AItechnologies/solutions to build their AI systems. The AI systems are often assembled by using com-mercial or open source AI and/or non-AI components from third parties. Despite cost efficiency,the underlying security and integrity issues of the third-party components have attracted signifi-cant attention. According to Sonatype\u2019s report, 2021 State of the Software Supply Chain, softwaresupply chain attacks increased 650% in 2021, whereas it was 430% in 2020. The RAI software bill ofmaterials keeps a list of components used to create an AI software product, which can be used byAI solution procurers and consumers to check the supply chain details of each component of in-terest and make buying decisions [12]. The supply chain details should at least include componentname, version, supplier, dependency relationship, author of software bill of materials data, andtimestamp [86]. This provides traceability and transparency about components and allows AI so-lution procurers and consumers to easily check component information (e.g., supply chain detailsand context information) and track ethical issues. The RAI software bill of materials enables fastervulnerability identification but may need to be updated frequently since AI systems may evolveover time. Dependency-Track is widely used by practitioners to track components\u2019 supply chaininformation and identify known vulnerabilities. Software Package Data Exchange (SPDX) andCycloneDX are two standards for exchanging software bill of material information for securityanalysis.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "077ca649-78b0-4fd2-be23-17f81bd5d3bf",
                    "text": "It is urgent that the employees of organizations begin to think throughthe potential implications of AI on their work and make ethical choices during the developmentand use of AI systems. Ethics training provides employees with knowledge on how to deal withethical issues during development [6, 31, 53, 59, 97, 98, 104, 105, 128]. MIT offers a 3-day course,\u201cEthics of AI: Safeguarding Humanity,\u201d introducing the ethics of AI development and deployment.The University of Technology Sydney (UTS) designed a short course, \u201cEthical AI: from Principlesto Practice,\u201d for business executives. The University of Helsinki created a free online course, \u201cTheEthics of AI,\u201d for anyone interested in AI ethics. Halmstad University provides a short course oncritical design and practical ethics for AI. Ethics training can improve organizational awarenesson RAI and sharpen the employees\u2019 RAI skills. However, RAI covers a broad range of knowledgeand skills, and ethics training may only offer a subset of knowledge and skills within limited time.3.3 Team-Level Governance Patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "ffb09ba6-3146-4c58-bb09-b948179789cc",
                    "text": "Agile development has been increasingly adopted by organiza-tions to incrementally and iteratively develop software systems, including AI systems. However,RAI Pattern Catalogue 173:13the existing agile development methods mainly focus on business value and largely neglect the AIethics principles. To address ethical issues in the AI system development process, agile methodsneed to be extended and customized to allow consideration of ethics principles. Extension pointscould be artifacts, roles, ceremonies, practices, and culture [51]. Microsoft\u2019s Azure DevOps allowsthe customization of inherited processes. Atola Technology provides a customized agile method-ology that contains different development practices. Apptio Targetprocess is a web-based visualtool for managing projects with flexibility at various levels.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "e8bb0cf3-f544-41e7-8e32-a8803215dba7",
                    "text": "AI system development involves the de-velopment of both AI and non-AI components with rapid iterations. This requires more frequentintegration of AI and non-AI components. Compared with non-AI components, the development ofAI components that support the AI model pipeline is more experimental with still limited method-ological support and mostly done by data scientists and data engineers who are not familiar withsoftware engineering. To bridge the methodological gap between AI and non-AI development, boththe AI team and the non-AI team need to be clear about what exactly is being delivered by a projectand share the same sprints and use a common co-versioning registry to track the progress [71].The close coupling of AI and non-AI development results in improved trust within the projectteam and better communication on both system-level and model-level ethical requirements. Thechallenge for the tight coupling might be that the non-AI component development is applicationcentric, whereas the AI component development is mostly data centric. There have been a fewattempts in industry on continuously integrating AI components/models into the software, suchas Microsoft Team Data Science Process, Amazon SageMaker Pipelines, and Azure Pipelines.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "e4144644-7e08-4c71-9a3b-343c82188333",
                    "text": "AI pipelines are built by humans and thus may imply bias (e.g., racismand sexism) and produce discriminating results. In addition, the code of AI systems is written bydevelopers who are primarily focused on technical aspects. Building a diverse project team caneffectively eliminate bias and improve diversity and inclusion in AI systems [31, 72, 131]. The di-versity can be across gender, race, age, sexual orientation, expertise, and so on. A diverse team candrive creative thinking for greater innovation, but communication could become challenging dueto different background and preference. Google published the 2022 Diversity Annual Report,which introduces the actions to build an inclusive workplace. Microsoft aims to integrate diversityand inclusion principles into their organization. Meta has been working on creating diverse andinclusive work communities.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "c3feba98-fa1a-44fe-ac59-893970ddc243",
                    "text": "Stakeholders may have various ethical concerns about the de-velopment and use of AI systems. Keeping stakeholder engagement throughout the AI project isessential to building AI systems responsibly. Stakeholder engagement allows AI systems to betterreflect their stakeholders\u2019 needs and expectations [17, 104, 129, 131]. There are various mannersto engage stakeholders, such as interviews, online and offline meetings, project planning/review,173:14 Q. Lu et al.participatory design workshops, and crowd sourcing. Stakeholders may help the project team iden-tify potential ethical risks before they become threats, but there maybe conflicting opinions fromdifferent stakeholders. The Association for Project Management published 10 stakeholder engage-ment principles. The Australian Public Service Commission released stakeholder engagementguidelines. Deloitte published a report on stakeholder engagement.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "d3cacb37-c47f-4852-9fa2-29584337fba6",
                    "text": "Developers primarily focus on the code andoften neglect updating the documentation during rapid iterations. The project teams need to createand continuously update documentations for the key artifacts of AI systems that may lead to ethicalissues, such as data and models. Continuous documentation using templates helps track the evolu-tion of artifacts and clarify the context in which AI systems are trustworthy [1, 4, 52, 55, 94, 127].Google\u2019s Model Cards enables transparent model reporting on model provenance and ethicalevaluation [81, 119]. Microsoft\u2019s datasheets for datasets tool allows every dataset to be accompa-nied with a datasheet document [41]. IBM\u2019s AI service FactSheets maintains AI services\u2019 perfor-mance, safety, security, and provenance information [9]. Meta\u2019s method cards provide prescriptivemodel specification templates that provide guidance on how to mitigate potential issues [1, 2].",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "3068c09c-6b58-4628-aea6-bc454d5283b6",
                    "text": "Ethical defects in AI systems are often detected throughextensive simulation and testing in the later stages of development. However, this may lead tosignificant delays to timelines and additional development cost. Failure Mode and Effects Anal-ysis (FMEA) is a bottom-up risk assessment method that can be used to identify ethical risks andcalculate their priorities at the beginning of the development process [30]. FMEA was originallyproposed in U.S. Armed Forces Military Procedures document MIL-P-1629 in 1949. Ford MotorCompany first introduced FMEA to the automotive industry in the mid-1970s. FMEA has beenextended and adopted by Toyota\u2019s Design Review Based on Failure Modes (DRBFM) for assess-ing potential risk and reliability for automotive and non-automotive applications. FMEA replies onexperts to apply their professional knowledge and experience to the ethical risk assessment pro-cess. In addition, FMEA is better suited for bottom-up analysis and not able to detect system-levelcomplex ethical failures.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "e332e147-64ba-44b3-80b4-f4a1e9491f13",
                    "text": "Undesired system behaviors or decisions could lead to serious con-sequences and even cause loss of human lives. Fault Tree Analysis (FTA) [30] can be used todescribe how system-level ethical failures are led by small ethical failure events through an analyt-ical graph (i.e., fault tree). The development team can easily capture how ethical failures propagatein the AI system. FTA can be done during the design or operation stage to anticipate the potentialethical risks and to recommend mitigation actions. FTA was first introduced by Bell Laboratoriesin 1962 to assess the safety of a missile launch control system. Boeing started using FTA to de-RAI Pattern Catalogue 173:15sign civil aircrafts in 1966. FTA was included in the U.S. Army Materiel Command\u2019s EngineeringDesign Handbook on Design for Reliability. FTA assists in analyzing the ethical issues related toAI system artifacts and prioritizes the issues to address that contribute to an ethical risk. However,it is complex to use for large system analysis, which may involve many ethical events and gates.In addition, time can hardly be captured in FTA.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "b8bc16f1-1a7c-49e0-a62d-891be71209cd",
                    "text": "The potential users of AI systems need methodsfor assessing an AI system\u2019s ethical properties and comparing the system to other systems.A verifiable claim platform can be built to support developers in making claims on ethicalproperties [40] and conducting the verification [124]. Such platform must consider the disparityof the stakeholder\u2019s views. For example, developers might focus on reliability, whereas usersmight be interested in fairness. A verifiable claim is a statement about an AI system or an artifact(e.g., model or dataset) that is substantiated by a verification mechanism. The platform itselfprovides management capabilities such as claim creation and verification, access control, anddispute management. The W3C Verifiable Claims Working Group aims to make expressing and ex-changing claims. The Open Web Application Security Project has published a Verifiable Claimsdocumentation. The Ethereum Verifiable Claims is a method for off-chain variable claims.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "b9c7e3fb-4be7-4a1c-ba6f-c957577ce5c0",
                    "text": "In this section, we discuss the process patterns that can be incorporated into RAI system develop-ment processes. The process patterns are reusable methods and best practices that can be used bythe development team during the development process. Figure 5 illustrates the process patternscollected for each stage of the development process.4.1 Requirement Engineering",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "6b97c1d0-f68b-4ff1-abb7-a0452ffd0b81",
                    "text": "AI has a huge potential to provide effective solutions to tacklecritical problems. However, it does not necessarily add value to every software system. Before173:16 Q. Lu et al.starting to build a software system with AI, the development team first needs to identify the rightproblem to solve and the corresponding user needs. Once the problem is found and the environ-ment where the system will be situated fully explored, the development team needs to analyzewhether the system and the users benefit from AI of if they are potentially degraded by AI [89].It is essential to make sure that AI adds value to the design. Oftentimes, a heuristic-based designmay be easier and cheaper to develop and may work better than an AI-based design in terms ofpredictability and transparency. AI suitability assessment can help the development team under-stand whether AI can add unique value to the design but may incur additional cost and requireextra resources.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "097c09d3-e301-4728-ba34-63123024c914",
                    "text": "The development of AI systems needs to adhere to AIethics principles which are generally abstract and domain agnostic. Ethical requirements needto be derived from the AI ethics principles to fit into a specific domain and system con-text [16, 49, 93, 118, 128]. Every ethical requirement specified in a requirements specification docu-ment should be put into a verifiable form (i.e., with acceptance criteria). This means that a personor machine can later check that the AI system meets the ethical requirements that are derivedfrom AI ethics principles and grounded in users\u2019 needs. Vague or unverifiable statements shouldbe avoided [110]. If there is no way to determine whether the AI system meets a particular ethi-cal requirement, then this ethical requirement should be revised or removed. Ethical risk can bereduced via considering ethical requirements from the beginning of the development process andexplicitly verifying ethical requirements. Some ethical principles/requirements may not be easilyquantitatively validated [128], such as human-centered values. There may be tradeoffs betweensome ethical principles or requirements. The current practice to deal with the tradeoffs is usuallythe developers following one principle while overwriting the others rather than building balancedtradeoffs through patterns.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "eb92841d-2b8b-42a6-b205-8d0a5dc54939",
                    "text": "The quality of an AI model is largelydependent on the quality of the data used to train or evaluate. The lifecycle of data consists of sev-eral phases, including data collection, cleaning, preparation, validation, analysis, and termination.Unfortunately, the scope of data requirements [104, 118] often focuses on the data analysis phaseand largely neglects the other key phases in the data lifecycle. This may lead to downstream ethicalconcerns such as AI model reliability, accountability, and fairness. AI systems can hardly be trustedwhen the data lifecycle is poorly managed. Data requirements need to be listed explicitly and spec-ified throughout the data lifecycle (i.e., collection, cleaning, preparation, validation, analysis, andtermination), taking into account ethical principles and involved stakeholders (i.e., data providers,data engineers, data scientists, data consumers, data auditors). Data requirements can be managedthrough data requirements specification. The specification could include detailed requirements foreach phase in the data lifecycle, such as data collection requirements including data sources andcollection methods. Google has created a template for dataset requirements specification [52].",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "091dc25d-fd25-4835-a84a-40d812d7f162",
                    "text": "Requirements elicitation methods are needed to collect detailed ethi-cal requirements from stakeholders to capture AI ethics principles. In agile processes, ethical userstories [43, 93] can help the development team elicit ethical requirements for AI systems and im-plement AI ethics principles from the early stage of development. Ethical user stories are createdto serve as items of the product backlog that is to be worked on by the development team in itera-tions (i.e., sprints). Card-based toolkits can be used to list questions related to AI ethics principles.The answers to those questions are integrated into ethical user stories to be included in sprintbacklogs. The development team or users can write ethical user stories on cards or notes usinga pre-defined template and assign them to different sprints based on priority. Ethical user storiesRAI Pattern Catalogue 173:17make ethical requirements traceable both backward and forward, but they are difficult to scale forlarger projects. The Guide for Artificial Intelligence Ethical Requirements Elicitation consists of25 cards which are used by the development team to answer questions related to ethical principles.The answers are used to create ethical requirements in the form of ethical user stories which areincluded in sprint backlogs. ECCOLA [43] consists of 21 cards which are divided into eight themesand with questions to be answered by the development team.4.2 Design",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "8258f95a-f724-4d2d-9b3b-53d8fb68080f",
                    "text": "Compared with traditional software, the architecture of AIsystems is more complex due to different levels of integration. On the one hand, AI models aredeveloped by data scientists/engineers via an AI model pipeline. The AI model pipeline usuallyis composed of a sequence of automatic steps including data collection, data cleaning, featureengineering, model training, and model evaluation. These steps can be viewed as software compo-nents for producing AI models from a software architecture perspective. On the other hand, theproduced AI models cannot work alone and need to be integrated into software systems that are tobe deployed in the real world [82]. The decisions made by the AI model need to be executed as ac-tions via other software components. The architecture of an AI ecosystem consists of three layers:AI software supply chain, AI system, and operation infrastructure. The focus of the AI softwaresupply chain layer is about developing and managing AI and non-AI components [69], includingAI model pipeline components, deployment components, co-versioning components, provenancetracking components, and credential management components, among others. The AI system layercomprises AI components that embed AI models and non-AI components that use the outputsof AI components for overall system functionalities [65]. The operation infrastructure layer ismainly about monitoring and feedback components. Multi-level co-architecting is required to en-sure the seamless integration of different components, including co-architecting AI componentsand non-AI components and co-architecting of different AI model pipeline components. Multi-level co-architecting allows both system- and model-level requirements to be considered in designdecision making.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "60e0ab6b-bdc0-47e2-b214-ca467323b70d",
                    "text": "AI ethics principles, including the human-centered values principles,are too high level for developers who often lack the technical means to assure human values andethics. Envisioning cards [17, 115] are designed to help the development team operationalize hu-man values during design processes of AI systems. The design of envisioning cards is based onfour envisioning criteria, including stakeholder, time, value, and pervasiveness. The stakeholdercriterion helps the development team takes into account the effects of an AI system on both directstakeholders and indirect stakeholders. The time criterion emphasizes the long-term implicationof AI systems on human, society, and environments. The value criterion guides the developmentteam to consider the impact of AI systems on human values. The pervasiveness criterion discussesthe challenges encountered if an AI system is widely adopted in terms of geography, culture, de-mographics, and so on. The adoption of envisioning cards comes at a relatively low cost, in termsof both money and time. However, envisioning cards are hard to scale when the number of partic-ipants is large or the AI systems are complex.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "48a00251-f7b5-4e78-b085-3e28994e07f2",
                    "text": "To reduce ethical risks, AI ethics principles need to be ad-hered to during the design process. Design modeling methods can be extended and used to supportthe modeling of AI components and the ethical aspects, including using UML to describe the ar-chitecture of AI systems and represent their ethical aspects [114], designing formal models taking173:18 Q. Lu et al.into account human values [36], using ontologies to model the AI system artifacts for account-ability [10, 85], establishing RAI knowledge bases for making design decisions considering ethicalconcerns [101], and using logic programming to implement ethical principles [8]. UML is an op-tion to describe the AI systems and represent their ethical aspects [114]. The UML extension couldbe a declarative graphic notation for AI system architecture. Additional stereotypes/metamodelelements can be added for RAI-by-design reference architecture (e.g., to describe AI pipeline com-ponents). Use case diagrams can help define the stakeholders and explain the functions they use,which are valuable for achieving accountability. State diagrams are useful to analyze the systemstates and identify the states that may cause ethical failures. Design patterns like the AI modeswitcher can take effect to change the state of an AI system to a more human-controlled state.Sequence diagrams describe the human-AI interactions to ensure all the required explanations areprovided. Using design modeling methods is helpful to capture and analyze ethical principles indesign. One disadvantage when using modeling languages is the time to create and manage themodels. In addition, the modeling languages do not scale up for large and complex systems.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "948ecda6-8718-4237-a9a5-56ed9e344fd3",
                    "text": "To avoid ethical disasters and gain public trust, it is nec-essary to model the real-world situations of AI systems without ethical risk. System-level simula-tion (e.g., [29, 30, 96, 106]) is a cost-effective way to imitate real-world situations and assess thebehaviors of AI systems before deploying AI systems in the real world. A simulation model needsto be built to mimic the possible behaviors and decisions of the AI system and assess the ethicalimpacts. The assessment results can be sent to the development team or potential users before theAI systems are deployed in the real world. System-level simulation can predict potential ethicalrisks and avoid serious ethical disasters before deploying AI systems in the real world. However,the simulation model cannot represent all the behaviors and ethical impacts of AI systems in thereal world. The accuracy of assessment results is limited by the quality of the simulation model.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "4b3c65ac-8893-4d37-834a-6526cfa97ea2",
                    "text": "End users oftendo not understand how decisions are made by AI systems and are not aware of the capabilitiesor limitations of the AI systems. The missing explainability may lead to a lack of trust and hasbeen identified as one of the most urgent challenges of RAI to be addressed. Explainable Ar-tificial Intelligence (XAI) can be viewed as a human-AI interaction problem and achieved byeffective human-centered interface design. Checklists (e.g., a question bank) are often used to helpdesign the explainable user interfaces [62, 66, 67] and understand the user needs, choices of XAItechniques, and XAI design factors [67]. For example, the checklist questions could consider thefollowing aspects [66] for different stakeholders: input, output, how, performance (can be extendedto ethical performance), why and why not, what if, and so on. The design of conversational inter-faces can be experimented via a Wizard of Oz study [56] in which users interact with a system thatthey believe to be autonomous but is actually being operated by a hidden human, called the Wizard.The conversation data is collected and analyzed to understand requirements for a self-explanatoryconversational interface. There can be several ways to increase human trust in AI systems througha human-centered user interface, including anthropomorphism [72] and proactive informing (e.g.,capability/limitation of AI systems, ethics credentials, explanations of decisions/behaviors, poten-tial outcomes, data use information).4.3 Implementation",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "428dd700-1f74-4f3e-8b0f-de3b940ec2bb",
                    "text": "APIs allow developers to solve problems more efficiently andcan effectively reduce the development cost and time of AI systems. However, there may beethical quality issues with APIs (e.g., data privacy breaches or fairness issues). Ethical compliancechecking for APIs is needed to detect if any ethics violation exists [50]. A knowledge-drivenRAI Pattern Catalogue 173:19approach can be adopted to detect ethics issues through ethical knowledge graphs. Ethicalknowledge graphs make meaningful entities and concepts, and their relationships in developmentof AI systems. With the ethical knowledge graph, the rich semantic relationships between entitiesare explicit and traceable across heterogeneous high-level documents and various AI systemsartifacts. Ethical knowledge graphs can be built based on the ethical principles and guidelines(e.g., a privacy knowledge graph based on GDPR [35, 90]) and technical documents (e.g., APIdocumentation) to support the ethical compliance checking for APIs.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "31f38420-7c5b-4384-a08c-ae87d07567e1",
                    "text": "Some AI systems may provide high-risk capabilities, which canbe used or modified to implement harmful tasks. To avoid harmful dual uses in AI systems [80],developers should carefully design how their AI systems can be directly used and indirectly used(i.e., potential ways their systems can be adapted). Developers must restrict the way AI systemsare used and preventing the users from getting around of restrictions by unauthorized reverse en-gineering or modification to the system design. Rather than fully opening the access to AI systemsby allowing AI systems to run locally, developers could provide AI services on the cloud and con-trol the interactions with the AI services via APIs [103]. For example, OpenAI\u2019s language modelGPT-3 can be only integrated with AI systems through an API by approved users. Google VisionAI limits its facial recognition feature to a few celebrities through API.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "aa42c86c-e9d9-46ee-a9a1-23574dcc6289",
                    "text": "Building AI systems from scratch can be very complexand time consuming. Very big companies usually have massive AI investments and large volumesof data to compete in the market, whereas smaller companies may only have a couple of datascientists and can hardly keep up with larger companies. To speed up the development and re-duce cost, it is highly desirable and valuable to reuse the AI artifacts (i.e., AI components and/orAI pipeline artifacts) across different applications. However, there might be ethical quality issueswith the reused AI artifacts, which requires further assurance mechanisms. Ethical constructionwith reuse means to develop RAI systems with the use of existing AI artifacts that are compliantwith AI ethics principles, such as from an organizational repository or an open source platform. Amarketplace can be built up to trade the reusable AI artifacts, including component code, models,and datasets. Blockchain can be adopted to design an immutable and transparent marketplace en-abling auction-based trading for AI artifacts and material assets (e.g., cloud resources) [107]. Ethicscredentials might be required to be attached to the traded AI artifacts. In addition, tooling supportmight be needed, such as model migration tool pytorch2keras, and glue code for compatibility.Low/no code tools can also help to achieve ethical construction with reuse.4.4 Testing",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "86d4a11d-c62a-4ea0-bba9-1ea994e5cd97",
                    "text": "Since the AI ethical principles are very high level, they needto be captured through ethical requirements, which can be viewed as the agreed commitments bythe development team and customers. Ethical acceptance testing (e.g., bias testing) is designed todetect the ethics-related design flaws and verify the ethical requirements (e.g., whether the datapipeline has appropriate privacy control, fairness testing for training/validation data) [3, 20, 123].In an agile process, the ethical requirements can be framed as ethical user stories and associatedwith ethical acceptance tests. The ethical acceptance tests are a contract between the customer and173:20 Q. Lu et al.development team. The behavior of the AI system should be quantified by the acceptance tests, andthe acceptance criteria for each of the ethical principles should be defined in a testable way. Thehistory of ethical acceptance testing should be recorded and tracked, such as how and by whomthe ethical issues were fixed. A testing leader may be appointed to lead the ethical acceptancetesting for each ethics principle. For example, when bias is detected at runtime, the monitoringreports are returned to the bias testing leader [28, 104]. Ethical acceptance tests capture the ethicalrequirements and measure how well the AI system meets ethical requirements, but they may needto be amended frequently as ethical requirements change.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "eab042bb-acb8-4861-a488-857b5de03586",
                    "text": "The ethical quality assurance for AI systems is heavilydependent on ethical acceptance testing, which is aimed at detecting and solving ethical issues inthe AI system. A collection of test cases with expected results should be generated [83] and main-tained to detect possible ethical failures in a variety of extreme situations [42]. However, theremight be ethical issues within the test cases. For example, the test data may introduce fairness orprivacy issues [77]. Preparing quality test cases is an integral part of ethical acceptance testing. Atest case usually is composed of the ID, description, preconditions, test steps, test data, expected re-sults, actual results, status, creator name, creation date, executor name, and execution date. All thetest cases for verification and validation should pass the ethics assessment. This includes ethicalrisk assessment for test steps and test data. The creation and execution information are essentialto track the accountability of ethical issues with test cases. Ethical assessment for test cases im-proves the ethical quality of the development process of AI systems, but new test cases need to becontinually added and assessed when a new ethical requirement is added or the operation contextchanges.4.5 Operation",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "5baa60d7-8d64-4cc8-8626-1ac1d9316991",
                    "text": "AI systems may frequently evolve due to their data de-pendency. When ethical performance degradation occurs over time, AI models need to be retrainedwith new data or features and reintegrated into AI components. The non-AI component may alsoneed to be upgraded to meet new requirements or changing context. New versions of AI systemsneed to be frequently and continuously deployed into production environments. However, AI sys-tems involve a higher degree of uncertainty and risks associated with the autonomy of the AIsystems. Thus, there is a strong desire for various deployment strategies to support continuous de-ployment [75, 128]. There are various deployment strategies for AI systems. Phased deploymentmeans deploying AI systems for a subset group of users initially to reduce ethical risk [47]. Thenew version of AI systems rolls out incrementally and serves alongside the old version. Phaseddeployment can also be about automating decisions in phases to better supervise and control au-tomation. This usually depends on the stakes of the situations and the level of confidence that usersmay have with automatic decisions made by AI systems. Further, A/B testing deployment [58] is acommon deployment strategy undertaken in industry, where different versions of the AI model aredeployed to production. The models are compared and selected based on their ethical performance.In addition, the existing reliability practices, like redundancy, are also applicable to AI componentsin an AI system. Multiple AI models work independently to improve the ethical performance ofthe AI components. Applying various deployment strategies helps to reduce the ethical risk. Userscan be quickly redirected to the older version or the other version of AI systems/models. However,it is complex and expensive to adopt different deployment strategies during operations.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "f9cf890a-887b-4244-8e31-054dd3818f53",
                    "text": "The current risk-based ap-proach to ethical principles is often a done-once-and-forget type of algorithm-level risk assess-ment [45, 73, 99, 104, 130] and mitigation for a subset of ethical principles (e.g., privacy orRAI Pattern Catalogue 173:21fairness ) at a particular development step (e.g., Canada\u2019s Algorithmic Impact Assessment Tool ),which is not sufficient for the highly uncertain and continual learning AI systems. In addition, thecontext of AI systems varies with the application domains, organizations, culture, and regions. Itis essential to perform continuous risk assessment and mitigation of RAI systems [25, 111]. Theethical risk assessment framework can be built with guided extension points for different contexts(e.g., culture context). The risk mitigation can be designed from three aspects: reducing frequencyoccurrence, consequence size, and consequence response. Extensible, adaptive, and dynamic riskassessment can effectively ensure that an AI system adheres to AI ethics principles throughoutthe whole lifecycle, but it might be hard to measure some of the ethical principles, such as human-centered values.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "4906d7d5-05e8-4a15-acb4-ba0541c8ff2f",
                    "text": "AI systems involve two levels of relationships and depen-dencies across various AI artifacts, including the supply chain level and the system level. At thesystem level, there are multiple versions of AI components and non-AI components. At the supplychain level, there are different versions of data, model, code, and configuration, which are usedto produce different versions of AI components [65]. At the system level, the AI components thatembed AI models are integrated into AI systems and interact with non-AI components. However,the retraining of AI models introduces new versions of data, code, and configuration parameters.If federated learning is adopted, for each round of training, a global model is ensembled based onlocal models sent from participating clients [70]. It is important to capture all of these dependen-cies during the development process. Multi-level co-versioning provides end-to-end traceabilityand accountability throughout the whole lifecycle of AI systems, but the collection and documen-tation of co-versioning information incur additional development cost. There have been manyversion control tools in industry focusing on supply chain level co-versioning, such as MLflowModel Registry on Databricks and the Amazon provenance tool, and Data Version Control(DVC).",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "d230484e-58e7-4ae0-828f-ee3519e40b78",
                    "text": "This section provides system-level guidance on how to design the architecture of RAI systems. Wepresent a collection of product patterns (i.e., design patterns) for building RAI-by-design into AIsystems (Figure 6). Broadly, an AI system is composed of three layers: (1) the supply chain layer thatgenerates the software components which compose the AI system, (2) the system layer which is thedeployed AI system, and (3) the operation infrastructure layer that provides auxiliary functions tothe AI system. Figure 7 presents the identified products patterns for each of the three layers. Thoseproduct patterns can be embedded into the AI ecosystems as product features. Figure 7 illustratesa state diagram of a provisioned AI system and highlights the patterns associating with relevantstates or transitions, which show when the product patterns could take effect.5.1 Supply Chain Patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "0569fe7d-ddfa-4bb7-b963-32a352424ddf",
                    "text": "The bill of materials registry [12, 88] can be designed to keepa formal machine-readable record of the supply chain details of the components used in building173:22 Q. Lu et al.an AI system, including component name, version, supplier, dependency relationship, author, andtimestamp. In addition to supply chain details of the components, context documents (like modelcards [81] for reporting AI models, and datasheets for the datasets [41] used to train AI models)can be integrated to the bill of materials registry.The main purpose of the bill of materials registry is to provide traceability and transparency intothe components within AI systems so that ethical issues can be tracked and addressed [11]. Someplatforms manage the bill of materials registry, such as OpenBOM, Codenotary, and SnorkelRAI Pattern Catalogue 173:23Flow. An immutable data infrastructure can store the bill of materials to enable integrity. Forexample, the manufacturers of autonomous vehicles could maintain a material registry contracton blockchain to track their components\u2019 supply chain information (e.g., the version and supplierof the third-party navigation component). Stakeholders can access the supply chain details of eachcomponent of interest in AI systems via the bill of materials registry. As AI systems evolve overtime, the bill of materials may need to be updated frequently. The cost of managing the bill ofmaterials of all the components depends on the complexity of the AI system.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "912c160f-97e2-4587-93cc-3055bf7f947b",
                    "text": "Verifiable ethical credentials can be used as evidence of eth-ical compliance for AI systems, components, models, developers, operators, users, organizations,and development processes [21, 72, 92]. Verifiable credentials are data that could be cryptograph-ically verified and be presented with strong proofs [23]. Publicly accessible data infrastructureneeds to be built to support the generation and verification of the ethical credentials on a neutralplatform. Before using AI systems, users may verify the systems\u2019 ethical credential to check if thesystems are compliant with AI ethics principles or regulations [21]. However, the users may berequired to provide the ethical credentials to use and operate the AI systems (e.g., to ensure theflight safety of drones). The verifiable ethical credential helps increase user trust toward an AIsystem through conferring the trust that the user has with the authority that issues the creden-tial to AI systems, organizations that develop AI systems, and operators who operate AI systems.Such transitive trust relationship is critical to the efficient functioning of the AI system. With anethical credential, an AI system could provide proof of compliance as an incentive for the users touse the AI system, thus increasing AI adoption. An ethical credential may be forged, which makesthe verification of authenticity of the ethical credentials become challenging. Blockchain could beadopted to build the credential infrastructure to ensure data integrity. For example, SecureKeyis a blockchain-based infrastructure for ID management with support of a verifiable credential.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "4ab36746-70ed-43b4-bb6e-7ed88f6daecb",
                    "text": "Compared with traditional software, AI systems involve differentlevels of dependencies and may evolve more frequently due to their data-dependent behaviors.From the viewpoint of the AI system, it is important to know the version of the AI componentintegrated into the system. From the viewpoint of the AI component, it is important to know whatdatasets and parameters were used to train the AI model and what data was used to evaluate the AImodel. Co-versioning of the components or AI artifacts of AI systems provides end-to-end prove-nance guarantees across the entire lifecycle of AI systems. The co-versioning registry can trackthe co-evolution of components or AI artifacts [65, 70]. There are different levels of co-versioning:co-versioning of AI components and non-AI components, co-versioning of the artifacts withinthe AI components (i.e., co-versioning of data, model, code, and configurations, and co-versioningof local models and global models in federated learning). Co-versioning enables effective mainte-nance and evolution of AI components because the deployed model or code can be traced to theexact set of artifacts, parameters, and metadata that were used to develop the model and code. TheMLflow Model Registry is a model repository and set of APIs that enable management of the fulllifecycle of MLflow Models, including model lineage and versioning.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "95c49d4d-4e36-4f45-b3b7-6f9f50491106",
                    "text": "Despite the widely deployed mobile or IoT devices generating mas-sive amounts of data, lack of training data is still a challenge for AI systems given the increasingconcern in data privacy. The federated learner trains an AI model across multiple edge devices or173:24 Q. Lu et al.servers with local data samples. The federated learner [15, 18, 68\u201370, 112, 113, 117] preserves thedata privacy by training models locally on the client devices and formulating a global model on acentral server based on the local model updates (e.g., train the visual perception model locally ineach vehicle). Decentralized learning is a variant of federated learning, which could use blockchainto remove the single point of failure and coordinate the learning process in a fully decentralizedway [120]. TensorFlow Federated is an open source framework for machine learning on decen-tralized data sources. FATE is an open source project that supports the federated AI ecosystem.5.2 System Patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "e672013e-fba5-43aa-8037-d35178e8a618",
                    "text": "When to use AI at decision-making points can be a major architecturaldesign decision when designing an AI system. Adding an AI mode switcher to the AI systemoffers users efficient invocation and dismissal mechanisms for activating and deactivating the AIcomponent whenever needed, thus defer the architectural decision to the execution time that isdecided by the end user or the operator of the AI system. The AI mode switcher is like a kill switchof an AI system that could immediately shut down the AI component and thus stop its negativeeffects [78, 89, 116] (e.g., turning off the automated driving system and disconnecting it from theinternet). The decisions made by the AI component can be executed automatically or reviewed bya human expert before being executed in critical situations. The human expert serves to approveor override the decisions (e.g., skipping the path generated by the navigation system). Humanintervention can also happen after acting on the AI decision through the fallback mechanism thatreverses the system back to the state before executing the AI decision. A built-in guard can be usedto ensure that the AI component is only activated within the pre-defined conditions (e.g., domainof use, boundaries of competence). The end users or the operators can ask questions or reportcomplaints/failures/near misses through a recourse channel after observing a bad decision fromthe AI component. Tesla Autopilot has multiple driver assistance features that can be enabled ordisabled during the driving. Users maintain control of the vehicles and can override the operationsby these features at runtime. The Baidu autonomous mini-bus Robobus requires a staff in theseat to supervise the self-driving operations, and the bus can be switched to manual driving modeby braking.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "73bac2be-60d9-48cc-8996-28109a4078e4",
                    "text": "In the reliability community of software systems, traditionalarchitecture-based software reliability is based on a software component. The existing reliabilitypractices, like redundancy, are also applicable to AI components in an AI system. In addition, a rea-sonable combination of multiple AI models that are normally work independently could improveperformance (e.g., accuracy) of the AI component. The multi-model decision maker employs differ-ent models to perform the same task or enable a single decision (e.g., deploying different algorithmsfor visual perception). It improves the reliability by deploying different models under different con-texts (e.g., different geo-location regions) and enabling fault tolerance by cross validating ethicalrequirements for a single decision [24, 84]. Different consensus protocols could be defined to makethe final decision\u2014for example, taking the majority decision. Another strategy is to only acceptthe same results from the employed models. In addition, the end user or the operator could step into review the output from the multiple models and make a final decision based on human exper-tise. Scikit-learn is a Python package that supports using multiple learning algorithms to obtainRAI Pattern Catalogue 173:25better performance through ensemble learning. The AWS Fraud Detection Using Machine Learn-ing solution trains an unsupervised anomaly detection model in addition to a supervised model, toaugment the prediction results. IBM Watson Natural Language Understanding uses an ensemblelearning framework to include predictions from multiple emotion detection models.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "b4245a88-1926-46b9-8943-78d268b1eac8",
                    "text": "-version programming is a software design pattern toensure fault tolerance of software [61]. Similarly, deploying multiple redundant and identical AIcomponents (e.g., two brake control components) can be a solution to tolerate the individual AIcomponent with high uncertainty that may make unethical decisions or the individual adversaryhardware component that produces malicious data or behaves unethically [84]. A cross checkcan be conducted for the outputs provided by multiple components of a single type. The resultsare accepted only as there is a consensus among the redundant components. The results thatare not accepted automatically according to a consensus protocol can be further reviewed by theend user or the operator of the AI system. Waymo contains multiple redundant componentsat various levels, including redundant braking, steering, and inertial measurement systems forvehicle positioning.5.3 Operation Infrastructure Patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "f627ca39-34fd-43bc-bfff-eb057ebe6d8a",
                    "text": "AI components of an AI system often require continuallearning based on new data collected during operation of the AI system. The continuousethical validator deployed in an AI system continuously monitors and validates the outcomesof AI components (e.g., the path recommended by the navigation system) against the ethicalrequirements [58, 111]. The outcomes of AI systems are about whether the AI system providesthe intended benefits and behaves appropriately given the situation. The time and frequencyof validation can be configured. Version-based feedback and rebuild alert are sent when thepre-defined conditions regarding the ethical requirement are met. AWS SageMaker ModelMonitor continuously monitors the bias drift of the AI models in production. Qualdo isan AI monitoring solution that monitors data quality and model drift. Azure Machine Learn-ing uses Azure Monitor to create monitoring data. Azure Monitor is a full-stack monitoringservice.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "129e7e84-78ca-40f0-80a0-d6ce9ad9e531",
                    "text": "Given that AI systems are of high stake, it is risky to run the entiresystem in the same execution environment. Ethical sandbox can be applied to isolate an AIcomponent from other AI components and non-AI components by running the AI componentseparately in a safe environment [63] (e.g., sandboxing the unverified visual perception compo-nent). Thus, the AI component could execute without affecting other components and the outputof the AI system. The ethical sandbox is an emulated environment with no access to the rest ofthe AI system. An emulation environment duplicates all the hardware and software functionalityof an AI system. Thus, developers could run an AI component safely to determine how it worksand whether it is responsible before widely deploying the AI component. Maximal tolerableprobability of violating the ethical requirements should be defined as the ethical margin for thesandbox. A watchdog can be used to limit the execution time of the AI component to reduce theethical risk (e.g., only activating the visual perception component for 5 minutes on the bridges173:26 Q. Lu et al.built especially for autonomous vehicles). Fastcase AI Sandbox provides a secure platform forusers to upload dataset and do data analysis in a safe environment. AI Sandbox provides an AIexecution and RESTful interface that could be used by modern programming languages.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "7e735444-d220-4fd1-bb62-f038dd6f4e70",
                    "text": "The ecosystem of AI systems involves broad ethical knowledge,such as AI ethics principles, regulations, and guidelines. Such ethical knowledge is scattered andis usually implicit or abstract to end users or even developers and data scientists who primarilyare without a legal background and focus more on the technical aspects of AI systems. An ethicalknowledge base, such as a knowledge graph, makes meaningful entities and concepts, and theirrelationships in design, implementation, deployment, and operation of AI systems [32, 85, 101].With the ethical knowledge base, the rich semantic relationships between entities are explicit andtraceable across heterogeneous high-level documents on one hand and different artifacts across theAI system lifecycle on the other hand. Thus, ethical requirements of the AI system can be system-atically accessed and analyzed using the ethical knowledge base. Awesome AI Guidelines aimsto provide a mapping between ecosystem of guidelines, principles, codes of ethics, standards, andregulation around AI. The RAI community portal is provided by AI Global, which is an evolvingrepository of reports, standards, models, government policies, datasets, and open source softwareto inform and support RAI development. Responsible AI Knowledge-base is a knowledge baseof different areas using and developing AI in a responsible way.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "99ea7dbe-53fd-4ded-ba5e-58a865bc2f97",
                    "text": "Simulation is designed to imitate a real-world situation. Before run-ning an AI system in the real world, it is important to perform system-level simulation throughan ethical digital twin running on a simulation infrastructure to understand the behaviors of theAI system and assess ethical risks in a cost-effective way. Digital twin [102] was introduced byNASA as a digital representation of a real system used in lab-testing activities. The digital twinof an AI system could be used to represent the behaviors of the AI system and forecast changeimpacts. The ethical digital twin can also be used during operation of the AI system to assess thesystem\u2019s runtime behaviors and decisions based on the simulation model using the real-time data.The assessment results can be sent back to alert the system or user before the unethical behavioror decision takes effect [29]. Vehicle manufacturers can use the ethical digital twin to explore thelimits of autonomous vehicles based on the collected real-time data, such as NVIDIA DRIVE Simand rfPro.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "814f657f-ffdf-4ec8-8867-319531afab23",
                    "text": "Incentive mechanisms are effective treatments in motivating AIsystems and encouraging the stakeholders involved in the AI system ecosystem to execute tasksin a responsible manner. An incentive registry records the rewards that correspond to the AIsystem\u2019s ethical behavior and outcome of decisions [121, 125] (e.g., rewards for path planningwithout ethical risks). There are various ways to formulate the incentive mechanism, such asusing reinforcement learning or building the incentive mechanism on a publicly accessible datainfrastructure like blockchain [125]. Traditional incentive mechanisms for human participantsinclude reputation based and payment based. However, it is challenging to formulate the formof rewards in the context of RAI, as the ethical impact of AI systems\u2019 decisions and behaviorsRAI Pattern Catalogue 173:27might hardly be measured for some of the ethical principles (e.g., human values). Furthermore,the incentive mechanism needs to be agreed on by all stakeholders, who may have different viewson the ethical impact. In addition, there may be tradeoffs between different principles, whichmakes the design harder. The Open Science Rewards and Incentives Registry incentivizes thedevelopment of an academic career structure that fosters outputs, practices, and behaviors tomaximize contributions to a shared research knowledge system. FLoBC is a tool for federatedlearning over blockchain that utilizes a reward/punishment policy to incentivize legitimatetraining, and to punish and hinder malicious trainers.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "3b8f9c6d-c853-45bb-a200-8ff62eb2ce1b",
                    "text": "The black box was introduced initially for aircraft several decades agofor recording critical flight data. The intention of adding a black box to aircrafts is to collect ev-idence of the actions of system and the surrounding context information for analysis after nearmisses and failures. The near misses and failures are specific to the use cases. Although the pri-mary usage of a black box is accident investigation, black boxes are useful for other purposes. Datacollection and the analysis could support improvement of the system. The purpose of embeddingan ethical black box in an AI system is to investigate why and how an AI system caused an acci-dent or a near miss. The ethical black box continuously records sensor data, internal status data,decisions, behaviors (both system and operator), and effects [33, 34, 122]. For example, an ethicalblack box could be built into an automated driving system to record the behaviors of the systemand driver and their effects [34]. All of these data need to be kept as evidence with the timestampand location data. Designing the ethical black box is challenging, as the ethical metrics need to beidentified for data collection. In addition, design decisions need to be made on what data shouldbe recorded and where the data should be stored (e.g., using a blockchain-based immutable log ora cloud-based data storage). RoBoTIPS aims to develop an ethical black box for social robots, toenable the explainability of their behavior.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "82fa214e-3574-475f-ad45-561e98ce728d",
                    "text": "When an accident happens, there might be more than one AI systemor multiple AI components within one AI system involved (e.g., multiple autonomous vehicles inan accident). The data collected from each involved AI system/component might conflict with eachother since the individual AI system/component may have their own perception. The global-viewauditor is a component that collects information from multiple AI components/AI systems andprocesses the information to identify discrepancies among the information collected [79]. Basedon the result, the global-view auditor may alert the AI system/component to a wrong perception,thus avoiding negative impacts or identifing liability when negative events occur. This pattern canbe also used to improve the decision making of an AI system by taking the knowledge from othersystems. For example, an autonomous vehicle may increase its visibility using the perceptions ofothers to make better decisions at runtime. The global-view auditor enables accountability thatcovers different perceptions of AI components/systems that are involved and redresses the con-flicting information collected from multiple AI components/systems.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "82e3709b-6809-400d-bb98-1cb4a91a5214",
                    "text": "The challenge of RAI has attracted significant attention in both industry and academia. Toachieve RAI, there have been nearly 100 high-level AI ethics principles and guidelines issued bygovernments, organizations, and companies [57]. Some degree of consensus around AI ethicsprinciples has been achieved [37]. A principle-based approach allows technology-independent173:28 Q. Lu et al.operationalization of RAI. However, these principles are very abstract and high level forstakeholders of AI systems to use in practice.Significant efforts have been put on algorithm-level solutions which mainly focus on a subsetof principles. Figure 8 lists the top five major industry players on RAI according to their numberof RAI tools based on the results of our MLR study. Most of these tools focus on privacy, security,reliability, safety, fairness, and explainability from an AI model perspective. More work is neededon transparency, accountability, contestability, and human-centered values, and human, societal,and environmental wellbeing, particularly from a system perspective.RAI Pattern Catalogue 173:29Overall, AI ethics principles need to be operationalized in the form of concrete patterns andbest practices that are usable by AI developers and other stakeholders to build up RAI systems.Some ad-hoc sets of guidebooks, question banks, checklists, and templates have started to appear.Microsoft\u2019s Human AI Interaction (HAX) Toolkit provides a set of HAX guidelines and patterns.However, those guidelines and patterns only focus on interaction design and do not provideany guidance on development and governance. Google\u2019s PAIR (People + AI Research) guidebooksummarizes 23 design patterns which mainly address some of the AI ethics principles for AImodels, including explainability, privacy, and reliability. Process and governance guidelines arenot discussed in Google\u2019s PAIR. Although OECD provides a framework of tools for trustworthyAI [87], the framework largely contains categorized but disjointed software tools, lacking process-related linkages. Thus, a systematic and operationalized guidance for AI system stakeholders isrequired throughout the entire lifecycle of AI systems.There have been a few survey papers on operationalizing RAI [7, 99, 109]. However, the findingsand insights in these papers are still around principles and do not provide concrete and actionableguidelines for stakeholders to use in practice. Our previous roadmap paper [71] discusses the cur-rent state and identifies the critical research challenges in the area of software engineering forRAI based on an initial SLR. This pattern catalogue paper is built on top of the published roadmapand provides a comprehensive list of concrete patterns from multi-level governance patterns toprocess and product patterns based on the results of an MLR. In the RAI Pattern Catalogue, wepresent structured knowledge about the patterns, including context, problem, solution, benefits,drawbacks, and known uses. The full version of the RAI Pattern Catalogue is available online.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "4dd77ac6-ad64-4c3c-982d-6a91fcab4c3b",
                    "text": "First, \u201cresponsible artificial intelligence\u201d is loosely defined with many terms thatrefer to emerging technologies in this area. There is a set of terms currently being used in thecommunity to mean largely the same thing: RAI, AI ethics, ethical AI, trustworthy AI, and trust inAI. This issue has been addressed by including search terms that are being used interchangeablyin the search string to ensure that all relevant works were covered. Another issue is that manysolutions were initially designed only for addressing one of the AI ethics principles but could beidentified as a pattern and extended to implement RAI. To mitigate this threat, we included all AIethics principles in the search string as supplementary terms.Internal Validity. To mitigate the threat of not finding all relevant studies, we performed a rigor-ous search using defined keywords and executed snowballing that allows us to recover the missingstudies from the literature. To address the bias, one researcher performed the screening of titles,abstracts, and full texts. The other researcher evaluated a random sample of the selected studiesafter screening to check the consistency of their inclusion/exclusion decisions.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                },
                {
                    "id": "f5274f2c-f6d0-469f-95ad-68eebf59af89",
                    "text": "To operationalize RAI, this article adopts a pattern-oriented approach and presented a comprehen-sive RAI Pattern Catalogue that AI system stakeholders can utilize to ensure that the developed AIsystems are trustworthy throughout the entire governance and engineering lifecycle, from multi-level governance patterns to concrete process and product patterns. These patterns offer a system-atic and actionable system-level guidance with consequence analysis and well-known uses for AIsystem stakeholders to reference during the governance and development processes. We are cur-rently building a Question Bank and a software tool for AI risk assessment, which will use the RAI173:30 Q. Lu et al.Pattern Catalogue as one of the knowledge sources to recommend mitigation strategies. We alsoplan to validate the utility, usability, and effectiveness of the pattern catalogue and the supportingtools in industrial projects.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiaoxi Xu, Jon Whittle, Didar Zowghi, Jia Zhang, and Laura Rutgersson. 2023. Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. ACM Computing Surveys. https://doi.org/10.1145/3626234"
                }
            ]
        },
        {
            "paper_title": "Responsible-AI-by-design: A pattern collection for designing responsible AI systems",
            "authors": "Q Lu, L Zhu, X Xu, J Whittle",
            "publication_info": "IEEE Software - ieeexplore.ieee.org",
            "paper_url": "https://arxiv.org/pdf/2203.00905",
            "chunks": [
                {
                    "id": "9455bd99-6327-4b9f-bdec-6de671089bad",
                    "text": "Although AI has signi\ufb01cant potential to transform society, there are serious concerns about its abilityto behave and make decisions responsibly. Many ethical regulations, principles, and guidelines forresponsible AI have been issued recently. However, these principles are high-level and dif\ufb01cult toput into practice. In the meantime much effort has been put into responsible AI from the algorithmperspective, but they are limited to a small subset of ethical principles amenable to mathematicalanalysis. Responsible AI issues go beyond data and algorithms and are often at the system-levelcrosscutting many system components and the entire software engineering lifecycle. Based on theresult of a systematic literature review, this paper identi\ufb01es one missing element as the system-levelguidance \u2014 how to design the architecture of responsible AI systems. We present a summaryof design patterns that can be embedded into the AI systems as product features to contribute toresponsible-AI-by-design.Key words: Responsible AI, ethical AI, trustworthy AI, AI engineering, software architecture, MLOps, AIOps",
                    "reference": "[1] Qian Yang, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. Retrieved from https://arxiv.org/pdf/2203.00905."
                },
                {
                    "id": "a1a235bc-e41d-4ad1-be00-18ef3e805463",
                    "text": "Although AI has signi\ufb01cant potential and capacity to stimulate economic growth and improve productivity across agrowing range of domains, there are serious concerns about the AI systems\u2019 ability to behave and make decisions in aresponsible manner. According Gartner\u2019s recent report, 21% of organizations have already deployed or plan to deployresponsible AI technologies within the next 12 months .Many ethical principles, and guidelines have been recently issued by governments, research institutions, and compa-nies [1]. However, these principles are high-level and can hardly be used in practice by developers. Responsible AIresearch has been focusing on algorithm solutions limited to a subset of issues such as fairness[2]. Ethical issues canenter at any point of the software engineering lifecycle and are often at the system-level crosscutting many componentsof AI systems. To try to \ufb01ll the principle-algorithmic gap, some development guidelines have started to appear. However,those efforts tend to be high-level development process checklists and ad-hoc sets lacking of state-related linkages for\ufb01nal products [3].Therefore, in this paper, rather than staying at the ethical principle-level or AI algorithm-level, we take a pattern-orientedapproach and focuses on the system-level design patterns to build responsible-AI-by-design into \ufb01nal AI products. Thedesign patterns are collected based on the results of a systematic literature review (SLR) and can be embedded intothe design of AI systems as product features to contribute to responsible-AI-by-design. We identify the lifecycle of aprovisioned AI system in which the states or state transitions are associated with design patterns to show when the designpatterns can take effects. The lifecycle along with the design pattern annotations provide an responsible-AI-focusedview of system interactions and a guide to effect use of design patterns to implement responsible AI from a systemA - S 21, 2022perspective. To the best of our knowledge, this is the \ufb01rst study that provides a concrete and actionable system-leveldesign guidance for architects and developers to reference.Figure 1: Methodology.",
                    "reference": "[1] Qian Yang, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. Retrieved from https://arxiv.org/pdf/2203.00905."
                },
                {
                    "id": "f0bf0c54-2129-4d72-8703-0b2edf08d4f5",
                    "text": "To operationalize responsible AI, we performed an SLR to identify design patterns that architects and developers canuse during the development process.Fig. 1 illustrates the methodology. The research question is: \u201dWhat solutions forresponsible AI can be identi\ufb01ed?\u201d The research question focuses on identifying the reusable patterns for responsible AI.We used \u201dAI\u201d, \u201dResponsible\u201d, \u201dSolution\u201d as the key terms and included synonyms and abbreviations as supplementaryterms to increase the search results. The main data sources are ACM Digital Library, IEEE Xplore, Science Direct,Springer Link, and Google Scholar. The study only includes the papers that present concrete design or process solutionsfor responsible AI, and excludes the papers that only discuss high-level frameworks. The complete SLR protocol isavailable as online material . We use the ethical principles listed in Harvard University\u2019s mapping study [4]: Privacy,Accountability (professional responsibility is merged into accountability due to the overlapping de\ufb01nitions), Safety &Security, Transparency & Explainability, Fairness and Non-discrimination, Human Control of Technology, Promotionof Human Values.",
                    "reference": "[1] Qian Yang, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. Retrieved from https://arxiv.org/pdf/2203.00905."
                },
                {
                    "id": "bd6d572f-957d-45ee-9c92-629644b5e6aa",
                    "text": "Fig. 2 illustrates the lifecycle of a provisioned AI system using a state diagram and highlights the patterns associatingwith relevant states or transitions, which show when the design patterns could take effect. We have limited the scope tothe design patterns that can be embedded into the AI systems and provisioned supply chain tool-chain as \ufb01nal productfeatures. The best practices of the development process including some patterns related to of\ufb02ine model training is outof the scope of this paper. Before an AI system is provisioned, the supply chain information can be accessed throughbill of materials. The users can be required to provide the veri\ufb01able ethical credentials to show their capability tooperate the systems, while the users can examine the system\u2019s veri\ufb01able ethical credentials for ethical compliancechecking. Once the AI system starts serving, it is important to perform system-level simulation through an ethicaldigital twin. ethical sandbox can be used to physically separate AI components from non-AI components. When an AIsystem is requested to execute a task, decision-making is often needed before executing the task. AI component can beactivated or deactivated through AI model switcher to automatically make the decision or involve human experts toreview the suggestion. Multi-model decision maker can use different models to make a single decision and cross-checkthe results. Similarly, homogeneous redundancy can be applied to the system design to enable fault-tolerance. Both the2 A - S 21, 2022Figure 2: Lifecycle of a provisioned AI system.behaviors and decision-making outcomes of the AI system are monitored and validated through continuous ethicalvalidator. Incentives for the ethical behaviors can be maintained by an incentive registry. If the system is failed to meetthe requirements (including ethical requirements) or a near-miss is detected, the system need to be updated. Federatedlearner retrains the model locally at each client to protect data privacy. Co-versioning registry can be used to track theco-evolution of AI system components or assets. An ethical knowledge base can be built to make the ethical knowledgesystematically accessed and used when developing or updating the AI system. The AI system needs to be auditedregularly or when major-failures/near-misses occur. An ethical black box can be designed to record the critical data thatcan be kept as evidence. A global-view auditor can be built on top to provide global-view accountability when multiplesystems are involved in an accident. The stakeholders can determine to abandon the AI system if it no longer ful\ufb01ls therequirements.",
                    "reference": "[1] Qian Yang, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. Retrieved from https://arxiv.org/pdf/2203.00905."
                },
                {
                    "id": "736c41f4-da0b-47e4-a362-681238d9d0eb",
                    "text": "To operationalize responsible AI, Fig. 3 lists a collection of patterns for responsible-AI-by-design. The full version ofdesign patterns is available online .\u2022 Bill of materials: AI product vendors often create AI systems by assembling commercial or open source AIand/or non-AI components from third parties. The AI users often have ethical concerns about the procuredAI systems/components. Before an AI system is provisioned, the supply chain information can be accessedthrough bill of materials , which keeps a formal machine-readable record of the supply chain details ofthe components used in building an AI system, such as component name, version, supplier, dependencyrelationship, author and timestamp. The purpose of bill of material is to provide traceability and transparencyinto the components that make up AI systems so that ethical issues can be tracked and addressed. There havebeen many tools to generate SBOM for practitioners, such as Dependency-Track . To ensure traceability andintegrity, immutable data infrastructure [5] is needed to store the data of bill of materials. For example, themanufacturers of autonomous vehicles can maintain a material registry contract on blockchain to track theircomponents\u2019 supply chain information, e.g., the version and supplier of the third-party AI-based navigationcomponent. 3 A - S 21, 2022Figure 3: Operationalized design patterns for responsible AI systems.4 A - S 21, 2022\u2022 Veri\ufb01able ethical credential: Veri\ufb01able ethical credentials are cryptographically veri\ufb01able data that can beused as strong proof of ethical compliance for AI systems, components, artifacts, and stakeholders (such asdevelopers and users). Before using the provisioned AI systems, users verify the systems\u2019 ethical credential tocheck if the systems are compliant with AI ethics principles or regulations [6]. On the other hand, the usersis often required to provide the ethical credentials to use and operate the AI systems. Publicly accessibledata infrastructure needs to be built to support the generation and veri\ufb01cation for ethical credentials . Forexample, before driving an vehicle, the driver is requested to scan her/his ethical credential to show she/he hasthe capability to drive safely, while verifying the ethical credential of the vehicle\u2019s automated driving systemshown on the center console.\u2022 Ethical digital twin: Before running the provisioned AI system in a production environment, it is critical toconduct system-level simulation through an ethical digital twin running on a simulation platform to monitorthe behaviors of AI systems and predict potential ethical risks. Ethical digital twin can also be designed as acomponent at the operation infrastructure level to examine the AI systems\u2019 runtime behaviors and decisionsbased on the abstract simulation model using the real-world data. The risk assessment results can be used bythe system or users to take further actions to mitigate the potential ethical risk. For example, the manufacturersof autonomous vehicles can use the ethical digital twin to explore the limits of autonomous vehicles based onthe collected run-time data, such as NVIDIA DRIVE Sim and xFpro .\u2022 Ethical sandbox: It is risky to execute the whole system including AI components and non-AI componentsin the same environment. When an AI system is being served, ethical sandbox can be used to physicallyseparate AI components from non-AI components by running the AI component in a self-contained emulationexecution environment [7], e.g. sandboxing the unveri\ufb01ed visual perception component. The AI componentsplaced in the ethical sandbox has no access to the rest of the AI system. All the hardware and softwarefunctionality of the AI component are duplicated in the ethical sandbox. Thus, the AI component can runsafely under supervision before being deployed at scale. For example, Fastcase AI Sandbox provides asecure AI execution platform for analysing data safely in a secure environment. Maximal tolerable probabilityshould be set as an ethical margin for the sandbox against the ethical requirements. A watch dog can be addedto restrict the execution time of the AI component to avoid the potential ethical risk, e.g., only executing thevisual perception component for 10 minutes on the roads designed especially for autonomous vehicles.\u2022 AI mode switcher: When to activate AI is a major architectural design decision when designing a softwaresystem. When an AI system is making a decision, AI mode switcher enables ef\ufb01cient invocation and dismissalmechanisms for activating or stopping the AI component when needed. Kill switch is a special type ofinvocation mechanism which immediately turns off the AI component and terminates its negative effects, e.g.switching off the autopilot functionality and its internet connection. The AI component can make decisionsautomatically or provide suggestions to human experts in high risk situations. The decisions can be approvedor overridden by human expert (e.g. skipping the path suggested by the navigation system). If the systemstate after acting an AI decision is not expected by human experts, fallback can be triggered to reverse thesystem back to the previous state. A built-in guard ensures that the AI component is only being used under theprede\ufb01ned risk categories.\u2022 Multi-model decision-maker: The reliability of traditional software is dependent on the design of softwarecomponents. One of the reliability practices in the reliability community is redundancy, which can be appliedto AI components. When decisions are being made by an AI system, multi-model decision-maker can rundifferent models to make a single decision [8], e.g., using different algorithms for visual perception. Reliabilitycan be improved by using different models under different context (e.g., different user groups or regions). Inaddition, fault tolerance can be enabled by cross-checking the results given by multiple models (e.g., onlyaccepts the same results from the deployed models). IBM Watson Natural Language Understanding makepredictions using an ensemble learning framework that includes multiple emotion detection models .\u2022 Homogeneous redundancy: Ethical failures in AI systems can cause serious damage to the humans orenvironment. N-version programming is a design pattern for dealing with reliability issues of traditionalsoftware. This concept can be adapted and applied to AI system design. Homogeneous redundancy (e.g.,two brake control components) can be applied to tolerate the highly uncertain AI system components thatcan make unethical decisions or the adversary hardware components that produce malicious data or behave5 A - S 21, 2022unethically [9]. When an AI system is executing a task, a cross-check can be performed for the outputs givenby multiple redundant components of a single type.\u2022 Incentive registry: Incentives are effective in motivating AI systems to execute tasks in a responsible manner.When executing a task, an incentive registry records the rewards that are given for the behavior and decisionsand behaviors of AI systems [10], e.g., rewards for the recommended path without safety risk. There aredifferent ways to enforce the incentive mechanism, e.g., designing the incentive mechanism on blockchainbased data infrastructure that is publicly accessible, using reinforcement learning. However, it is challengingto design the mechanisms in the responsible AI context since it dif\ufb01cult to measure the ethical impact ofdecisions and behaviors of AI systems on some ethical principles (such as human values). Besides, consensusneeds to be reached on the incentive mechanism by all the stakeholders. Additionally, in some cases, ethicalprinciples are con\ufb02icting with each other, making the design of incentive mechanism harder. FLoBC is atool that uses blockchain to incentivize training contribution for federated learning.\u2022 Continuous ethical validator: AI systems often need to conduct continual learning when data drift orunethical behavior is detected in production. When an AI system executes tasks, continuous ethical validatormonitors and validates the outcomes of AI systems (e.g., the path suggested by the navigation system) againstthe ethical requirements. The outcomes of AI systems are the consequences of decisions and behaviors of thesystems, i.e., whether the AI system behaves ethically or provides the promised bene\ufb01ts in a given situation.The time and frequency of validation can be prede\ufb01ned within the continuous validator. Version-based feedbackand rebuild alert can be sent when the ethical requirements are met or breached. Incentive registry can be usedto reward or punish the ethical/unethical behavior or decisions of AI systems.\u2022 Ethical knowledge base: AI systems involve broad ethical knowledge, including AI ethics principles, regu-lations, unethical use cases, etc. Unfortunately, such ethical knowledge is scattered in different documents(e.g., AI incidents) and is usually implicit or even unknown to developers who primarily focus on the technicalaspects of AI systems and do not have ethics background. This results in negligence or ad-hoc use of relevantethical knowledge in AI system development. Ethical knowledge base is built upon a knowledge graph tomake meaningful entities, concepts and their rich semantic relationships are explicit and traceable acrossheterogeneous documents so that the ethical knowledge can be systematically accessed, analysed, used whendeveloping or updating AI systems [11]. For example, an ethical knowledge base can be used to supportcontinuous ethical risk assessment. Ethical knowledge base can be built based on the AI ethics principles,frameworks, and actual AI use cases discussed in the existing papers.\u2022 Co-versioning registry: AI systems involve different levels of dependencies and need frequent evolutionwhen data drift or unethical behavior occurs. Co-versioning of the components of AI systems or AI assetsgenerated in AI pipelines provides provenance guarantees across the entire lifecycle of AI systems. There havebeen many version control tools for managing the co-versioning of data and models, such as DVC . Whenupdating an AI system, co-versioning registry can track the co-evolution of components or AI assets. Thereare different levels of co-versioning: co-versioning of AI components and non-AI components, co-versioningof the assets within the AI components (i.e., co-versioning of data, model, code, con\ufb01gurations). A publiclyaccessible data infrastructure can be used to maintain the co-versioning registry to provide a trustworthytrace for dependencies. For example, a co-versioning registry contract can be built on blockchain to managedifferent versions of visual perception models and the corresponding training datasets.\u2022 Federated learner: Despite the widely deployed mobile or IoT devices generating massive amounts of data,data hungriness is still a challenge given the increasing concern in data privacy. When learning or updating AImodels, federated learner preserves the data privacy by performing the model training locally on the clientdevices and formulating a global model on a central server based on the local model updates , e.g., train thevisual perception model locally in each vehicle. Decentralized learning is an alternative to federated learning,which uses blockchain to remove the single point of failure and coordinate the learning process in a fullydecentralized way. In the event of negative outcomes, the responsible humans can be traced and identi\ufb01ed byan ethical black box for accountability.\u2022 Ethical black box: Black box was introduced initially for aircraft several decades ago for recording critical\ufb02ight data. The purpose of embedding an ethical black box in an AI system is to audit an AI system andinvestigate why and how the system caused an accident or a near miss. The ethical black box continuouslyrecords sensor data, internal status data, decisions, behaviors (both system and operator) and effects [12]. Forexample, an ethical black box could be built into the automated driving system to record the behaviors of the6 A - S 21, 2022system and driver and their effects. Design decisions need to be made on what data should be recorded andwhere the data should be stored (e.g. using a blockchain-based immutable log or a cloud-based data storage).\u2022 Global-view auditor: There can be more than one AI systems involved in an ethical incident (e.g. multipleautonomous vehicles in a car accident). During auditing, it is often challenging to identify the liability as thedata collected from each of the involved systems can be con\ufb02icting to each other. Global-view auditor canenable accountability by analysing the data discrepancies between the involved AI systems and identifyingthe liability for the ethical incident. This pattern can be also applied to improve the reliability an AI systemby taking the data from other systems. For example, an autonomous vehicle increases its visibility using theperception data collected from the other vehicles. [13]. All the historical data of AI systems can be recordedby an immutable log for third-party auditing.",
                    "reference": "[1] Qian Yang, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. Retrieved from https://arxiv.org/pdf/2203.00905."
                },
                {
                    "id": "26d9359a-dae2-422e-ac17-51d4a94ac861",
                    "text": "To operationalize responsible AI, we take a pattern-oriented approach and collect a set of product design patternsthat can be embedded into an AI system as product features to enable responsible-AI-by-design. The patterns areassociated to the states or state transitions of a provisioned AI system, serving as an effective guidance for architectsand developers to design a responsible AI system. We are currently building up a responsible AI pattern cataloguethat includes multi-level governance patterns, trustworthy process patterns (i.e., best practices and techniques), andresponsible-AI-by-design product patterns.",
                    "reference": "[1] Qian Yang, Liming Zhu, Xiwei Xu, and Jon Whittle. 2023. Responsible-AI-by-design: A pattern collection for designing responsible AI systems. IEEE Software. Retrieved from https://arxiv.org/pdf/2203.00905."
                }
            ]
        },
        {
            "paper_title": "Towards a roadmap on software engineering for responsible AI",
            "authors": "Q Lu, L Zhu, X Xu, J Whittle, Z Xing",
            "publication_info": "\u2026 1st International Conference on AI \u2026 - dl.acm.org",
            "paper_url": "https://arxiv.org/pdf/2203.08594",
            "chunks": [
                {
                    "id": "61cf2331-ee29-48cc-a41e-bcac8f1137ac",
                    "text": "Although AI is transforming the world, there are serious concerns about its ability to behave andmake decisions responsibly. Many ethical regulations, principles, and frameworks for responsibleAI have been issued recently. However, they are high level and dif\ufb01cult to put into practice. On theother hand, most AI researchers focus on algorithmic solutions, while the responsible AI challengesactually crosscut the entire engineering lifecycle and components of AI systems. To close the gapin operationalizing responsible AI, this paper aims to develop a roadmap on software engineeringfor responsible AI. The roadmap focuses on (i) establishing multi-level governance for responsibleAI systems, (ii) setting up the development processes incorporating process-oriented practices forresponsible AI systems, and (iii) building responsible-AI-by-design into AI systems through system-level architectural style, patterns and techniques.Key words: AI, machine learning, responsible AI, ethics, software engineering, software architecture, MLOps, DevOps,requirement engineering",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "6183aaf1-0abb-4064-94ac-bb9304fe6bd8",
                    "text": "Arti\ufb01cial intelligence (AI) is considered one of the major driving forces to transform society and industry and has beensuccessfully adopted in data-rich domains. Although AI is solving real-world challenges and improving our quality oflife, there are serious concerns about its ability to behave and make decisions in a responsible manner. Responsible AIhas become one of the greatest scienti\ufb01c challenges of our time. Both legal and ethical aspects need to be considered toachieve responsible AI. Since the law establishes the minimum standards of behaviour while ethics sets the maximumstandards, in this paper, we use the terms responsible AI, ethical AI and ethics to cover the broader set of requirements.A large number of ethical principle frameworks have been recently issued by governments, research institutions, andenterprises [1], which responsible AI technologies and systems are supposed to adhere to. A degree of consensusaround the principles has been achieved [2]. However, these principles are high-level and do not provide operationalizedguidance and software engineering methods on how to develop responsible AI systems. This leaves unansweredquestions such as how can these principles be designed for, implemented and tracked in developing and operating anAI system. On the other hand, signi\ufb01cant research has gone into ethical algorithms where the formulation of someethical principles is amenable to mathematical de\ufb01nitions, analysis and theoretical guarantees. These algorithm-levelmechanisms mainly focus on a small subset of ethical principles (such as privacy [3] and fairness [4]) relying ontheoretical heuristics. There is a lack of linkage to the software development processes, especially requirementsengineering, system design methods, or operations.Therefore, this paper presents a research roadmap on software engineering for operationalizing responsible AI. Ratherthan staying at the ethical principle level or going straight down to the AI algorithm level, this paper focuses on thesoftware engineering approach to operationalizing responsible AI. We perform a systematic literature review (SLR) onsoftware engineering for responsible AI to summarize the current state and identify the critical research challenges. Asshown in Fig. 1, the proposed roadmap focuses on (i) establishing multi-level governance for responsible AI systems,A - M 17, 2022Figure 1: Overview of the roadmap.(ii) setting up the development processes incorporating process-oriented best practices for responsible AI systems,and (iii) building responsible-AI-by-design into the AI systems through system-level architectural style, patterns andtechniques.The remainder of the paper is organized as follows. Sec. 2 discusses the methodology. The rest of the paper is dividedinto three parts: governance perspective (Sec. 3), process perspective (Sec. 4), and system perspective (Sec. 5). Foreach perspective, we present the current state and the challenges being faced by the community.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "75ab2d8a-e45b-4510-81c5-abfdecb5ddfb",
                    "text": "To develop a roadmap, we performed an SLR following the guideline in [5]. Fig. 2 illustrates the methodology. Thetwo research questions are de\ufb01ned for the SLR: 1) What responsible AI principles are addressed by the study; 2) Whatsolutions for responsible AI can be identi\ufb01ed. The data sources include ACM Digital Library, IEEE Xplore, ScienceDirect, Springer Link, and Google Scholar. The study only includes papers presenting concrete solutions for responsibleAI and exclude papers discussing high-level frameworks. A set of 159 primary studies was identi\ufb01ed. The completeSLR protocol is available as online material . We use the ethical principles listed in Harvard University\u2019s mappingstudy [2]. Fig. 3 lists an adapted summary of the principles (responsibility is merged into accountability given theoverlapping de\ufb01nitions). Figure 2: Methodology.2 A - M 17, 2022",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "4cdbf981-5181-4ee5-bd45-609b316370fe",
                    "text": "The governance for responsible AI systems can be de\ufb01ned as the structures and processes that are designed toensure the development and use of AI systems are compliant to ethical regulations and responsibilities. As shown inFig. 4, the governance can be built into three levels based on Shneiderman\u2019s governance structure [6]: industry-level,organization-level, and team-level.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "044060ff-1842-46a9-b2fc-f1a905bdeaf4",
                    "text": "The industry-level governance requires the governments and AI industry to act collectively via regulation, policy,standards to make the AI systems acceptable by society. One form of regulation is analogous to building codes to whichdevelopers can adhere when developing AI systems [7]. Incentives/penalties may be applied for ethical/unethicalsoftware development [6]. Given the laws and public policies usually take a long time to enact, governments can consideradopting agile regulatory sandbox on a time-limited basis for the emerging AI technology. For example, the Singaporegovernment and EU have applied a regulatory sandbox to allow autonomous vehicles to be legally on the roads withoutchanging the national laws [8]. As there may be various sector/domain-speci\ufb01c risk concerns (e.g. military or health),there is a need to extend and adapt generic regulations to sector/domain-speci\ufb01c regulations [9]. Professional andnon-governmental organizations and research institutions have been making signi\ufb01cant efforts on developing standards,guidelines, and open-source tools and platforms[6]. Regulators could incentivize organizations for responsibleAI innovation. AI project funding bodies could require applicants to include responsible AI statements in theirfunding applications and implement ethical checklist driven monitoring and management of grant funding.Independent oversight is essential to the accountability of AI systems. External audits could be conducted byindependent third parties (such as AI Safety Commission) during the development or post-hoc. The inspection of the AIsystems\u2019 behaviours and decision-making is required either by reviewing interpretable AI models or having access tothe artifacts of AI systems, such as datasets, source code, and documents. For example, Z-inspection process [10] is ageneric inspection processes to assess the trustworthiness of AI systems. When an accident happens, causal analysiscan be performed using the why-because method for accident investigation. Insurance companies can play a role of aguarantor for responsible AI and compensate for the failures of AI systems [6].AI capability maturity model is being introduced to examine organizations\u2019 AI capability [11, 12]. Like theconventional software engineering capability maturity model, the AI capability maturity model has different levelsof maturity based on development processes and desired responsible AI metrics. The results assessed by the AIcapability maturity model could be used for ethical certi\ufb01cation [13] to certify an organization\u2019s ability to achieveethical principles. In addition to organization-level assessment and certi\ufb01cation, both the AI capability maturity modeland ethical certi\ufb01cation could be extended to cover a broader view to provide veri\ufb01able evidence for improving humantrust in AI systems, e.g., assessing and certifying for organizations, development processes, developers, operators, AIsystems, components, models.Figure 3: An adapted summary of AI Ethics Principles [2].3 A - M 17, 2022Figure 4: Multi-level governance for responsible AI.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "5c7d8de4-e8d5-4817-9074-c659489cf5c5",
                    "text": "Achieving responsible AI in an organization requires the establishment of AI governance that includes setting upfounding principles, an ethics committee, a governance structure, governance metrics, external committees,organization-wide training, and promotion of diversity and dissent [14]. The ethics committee should oversee theoverall AI-driven decision-making processes (not only the AI systems). Code of conduct (such as speci\ufb01c code ofethics) should be developed for the employees (e.g. developers or operators) to follow [6].Leadership commitment is essential to organization-level governance. Responsible AI statements should be de-scribed explicitly in an organization\u2019s values, vision, and mission [6]. The establishment of responsible AI governancecould be incorporated into CEO\u2019s contracts and performance reviews [8]. The management can enforce the organi-zational culture on responsible AI and use ceremonies to celebrate responsible AI successes (e.g. certi\ufb01cate granted).AI transformation workshops can be organized to assess the impact, e.g. using human-centered AI canvas .Internal audit review is required to cover the complete lifecycle of AI systems and include continuous monitoringinstruments (e.g. checklist for retrospective meetings and data/code reviews). Extensive reporting of failures andnear misses should be produced with why-because analysis. Ethical risk assessment checklists [15] need to beco-designed with practitioners [16] (e.g. for ethics committees, team, or prospective purchasers) for each of theethical principles (e.g. fairness risk identi\ufb01cation questionnaire [17], reproducibility checklists, fairness checklists [15]),taking into account the application category and automation level for risk and timeliness.Role-level accountability [18] is established in the organization through formal contractual mechanisms (e.g., legalagreements between system users, data contributors, and the project team) to hold each other accountable [19]. Diversetypes of ethical quality constraints are enforced as a contract, such as service contract, model contract, and data contract.The provenance of data, model, and code allows examining role-level accountability.AI ethics training programs should be introduced within the organizations, including technical and non-technicalethics and human rights training for different roles (such as the management, developers, data scientists, op-erators) and organizational awareness. The content of an AI ethics training course could include governance forresponsible AI, ethical operations of AI systems, trustworthy development processes, and responsible-AI-by-design with case studies (e.g. using the design of an ethical/unethical agent [20], autonomous vehicles [21]).4 A - M 17, 2022",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "dca081c0-68b2-4f1b-90ce-8059e804954c",
                    "text": "Team-level governance is mainly for managing the AI projects and overall development processes. The developmentprocesses should be conformance to standard processes. Agile development processes can be adapted and cus-tomised by incorporating responsible AI principles [22]. The AI and non-AI development sprints and standupsneed to be closely coupled [23]. One effective way to ensure fairness, human-centred values, and HSE wellbeingin AI systems is to make the development teams diverse (e.g., gender, age, race, culture) and engage stakehold-ers throughout the lifecycle of AI systems. Culture needs to be explicitly considered in the design when there isculture-sensitive data or context involved. For example, culture could determine whether it is ethical or unethical for aconversational AI system to tell a lie to humans (e.g. when negotiating a price) [24]. For indigenous projects, indigenouspeople need to be involved in the development process to help incorporate culture concerns into development anddecision-making (e.g., following CARE ). Ethicists can be included into the development team to promote a moreethical development of AI systems [25]. Con\ufb02ict/trade-off resolution process is needed to achieve consensus during thedevelopment.The day-to-day work\ufb02ow of software engineers (e.g. training and deployment pipeline [23]) is expected to be uni\ufb01edand automated by using the standardized tools (e.g., Jupiter notebooks, python, Github) [26] to improve productivityand integrate AI ethics tools. Model-based process assessment checks the process compliance by linking work\ufb02owmodels with assessment criteria [27]. Customized visual IDE tools may help developers with varying levels ofexperience [23]. The project team can consider publishing the \ufb01ndings through scienti\ufb01c publication following thescienti\ufb01c norms of research integrity and knowledge sharing to provide public transparency [19]. Open process acrossthe development lifecycle (e.g. full access to artifacts from stakeholders or authorized third-parties) and Artifactspeci\ufb01c lifecycle management process are helpful to improve transparency and accountability [28]. Decentralizedcomputational infrastructure is built for veri\ufb01able ethical credentials which can be used as proofs of responsibleAI compliance [29]. Developers are suggested to prepare standardized documentation (e.g. ISO AI standards ). Thereare various types of documentation for managing the development of responsible AI systems, such as model cards,data statements, datasheets for datasets, AI service factsheets [15], requirements speci\ufb01cation, design document,implementation diary, testing report, maintenance plan [28].",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "697d4f53-5da1-44b4-b59d-d63d243154ba",
                    "text": "Despite nearly one hundred frameworks for ethical principlesbeing issued [1], these principles are too high-level and hard to operationalize. A systematic and holistic governanceframework connected with software development lifecycles is highly desirable to translate ethical principles intopractices throughout the entire lifecycle of AI systems. The governance structure and processes need to be designedand organized into multiple levels, including industry-level (which can be further divided into international-level andcountry-level), organization-level, and team-level. So the governments, organizations and development teams canfollow the framework to governance AI systems.Governance mechanisms linking with stakeholders and lifecycle processes. The governance mechanisms need tobe designed in a way that links with stakeholders and process stages. This makes the practitioners easy to adopt thegovernance mechanisms in practice. Also, to identify and track accountability, we need to understand the stakeholdersof AI systems and their roles in the governance throughout the entire lifecycle of AI systems.The communication between the SE people and the rest (i.e. people who are not SE-people). Responsible AIchallenges are broader than software engineering and need to be addressed by a multidisciplinary team with expertise insoftware engineering, machine learning, social science, human-machine interaction, and user experience. However, asthe \ufb01nal deliverable to the society is a responsible AI system, we believe the software engineering people are the keyforce to drive the research and collaboration.Education for students. As there is a serious concern about the social impact of AI systems, education on responsibleAI is urgently required for students across all across all levels (such as K-12 and university education).5 A - M 17, 2022Figure 5: Development process practices.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "77e9d204-6ef5-4c9f-b1ec-30c4bbf0fdcb",
                    "text": "Fig. 5 summarises the methods and best practices that can be incorporated into development processes, so the developerscould consider to apply them during the development.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "4c13f6e2-354f-4b7c-9ffc-2c284f22ae70",
                    "text": "Responsible AI requirements are either omitted or mostly stated as high-level project objectives in practice. The existingmethodologiesshould be extended and adopted for AI systems to ensure that the requirements captured are as accurateand complete as possible while recognizing the special characteristics of AI systems such as autonomy, continuouslearning and the value-alignment problem.Requirements Elicitation: Elicitation is the process of collecting requirements from stakeholders and othersources, including goals, domain knowledge, business rules, operational environment, organizational environment [30].Ethical principles are an essential source for identifying ethical requirements types and relevant and inclusive stakehold-ers. Some of the important stakeholders for collecting ethical requirements include not only business owners, systemusers, ethical/legal experts, regulators but also data providers,impacted data subjects, operators, advocacy groups andeven concerned public. Culture safety is a critical ethical requirement for AI systems that involve culture sensitivedata. There are a variety of requirements elicitation techniques that can be adapted to gather ethical requirements(e.g. training/validating data fairness requirements and secondary data usage requirements), such as interviews [31],scenarios, requirement workshops, interactive demos/prototypes [31], and user stories. Ethical user stories (e.g.utilizing ECCOLA cards [32]) is an effective way to transform the abstract ethical requirements into tangible outcomes.Requirements Analysis: One of the most important activities in requirement analysis is requirements classi\ufb01ca-tion [30] and resolution of requirements con\ufb02icts. We group the ethical principles (in Fig. 3) into two requirementscategories based on their nature and characteristics. The \ufb01rst group includes privacy, safety & security, fairness, andhuman values, which are similar to software qualities [33] and can be considered as non-functional requirements.Some principles, such as safety & security, are the quality attributes well studied in the dependability community [34]and can be identi\ufb01ed and considered early in the development lifecycle. Recurring requirement problems and associatedreusable design fragments, like patterns/tactics, could be applied to meet those quality attributes [35]. Privacy is not astandard software quality attribute [33], but has been treated as an increasingly important non-functional requirementto realize regulatory requirements, like GDPR . Recurring requirements and reusable patterns/practices have beensummarized in for privacy [36]. Fairness is another requirement that the developers should collect and design forfrom early development life cycles. Technical mechanisms/practices to remove bias at different stages of the pipelinehave been designed [4]. There has been recently emerging research on casting human values into requirements in6 A - M 17, 2022software engineering and their operationalization [37, 38], including the extension of value-based design methods [39],extension of human factor research on productivity and usability into human values consideration. But these efforts arestill limited to a small subset of human values [40]. The reason to group these principles is that they can be handledand validated using how non-functional quality attributes are handled in software system design. Some principlescan be quantitatively validated, whereas others can be qualitatively handled by the widely used design patterns andprocess-oriented practices.The second group includes transparency & explainability, human control of technology, and accountability, which aremeta-level governance issues and can be classi\ufb01ed as functional requirements for improving users\u2019 con\ufb01dence inthe AI system. Transparency & explainability can be ful\ufb01lled by designing functions for receiving an explanationof a prediction or decision and having access to the artifacts of AI systems. Human control of technology requires afunction that allows the users to challenge the output or use of the AI system. To meet accountability, the AI systemshould include a function for tracing and identifying those who are responsible for the various phases of the AI systemlifecycle and the outcomes of the system.Requirements Speci\ufb01cation: AI systems are complex software systems that involve hardware components. Thus, bothsoftware requirements and system requirements (e.g., requiring certi\ufb01ed hardware components) are needed. It\u2019s worthnoting that AI systems try to solve the problems autonomously with a level of independence and agency and cannot befully speci\ufb01ed. It is important to judiciously make ethical requirements quanti\ufb01able or measurable, and avoid vagueand unveri\ufb01able requirements [30]. Scope of responsibility need to be clearly de\ufb01ned in the requirements [41]. Bothteam diversity and the choice of ethical requirements veri\ufb01cation/validation techniques can be speci\ufb01ed as processrequirements. Data requirements [42] need to be listed explicitly and speci\ufb01ed throughout the data lifecycle (i.e.,collection, management, survey, consumption, termination) taking into account all the involved roles (i.e., data provider,manager, analyser, consumer) and ethical concerns, e.g. training/validation data fairness requirements and secondarydata usage requirements. Functional requirements should be stated with performance measures [42] and theirexplanations in the context of the domain. Speci\ufb01c examples of the desired outcomes can be listed for the giveninputs [6]. Negative requirements with misuse, abuse and confuse cases can be described in the speci\ufb01cation [43].Requirements Veri\ufb01cation and Validation: The ethical requirements should be speci\ufb01ed traceable both backwardto the stakeholders and forward into the design modules, code pieces, and test cases. Requirements validation becomesan activity that needs to be performed continuously during operation, which includes both monitoring and analysis ofproduction data from the ethical perspective [42]. Awareness of potential mismatches between training data andreal-world data is necessary to prevent the trained model from being unsuitable for its intended purpose. Model updateor recalibration on new data is important for the trustworthiness of AI systems. The conditions of retraining need to beconsidered during the speci\ufb01cation phase, such as time, frequency, data characteristics, user feedback. The intendedoperating environment should be understood as complete as possible. For example, the knowledge about the inputdata fed into the decision-making component of the AI system and the data sources used to obtain the input datashould also be learnt [44].",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "fc0ae547-60d3-4b98-b444-64a411c028a2",
                    "text": "Signi\ufb01cant efforts have been put on ethical algorithms where ethical properties have been enabled via mathematicalde\ufb01nitions, analysis and theoretical guarantees, such as fairness [4] and privacy [3]. However, these algorithm focusedmechanisms have limited theoretical heuristic and are con\ufb01ned to a small subset of ethical principles that can be easilytranslated to quanti\ufb01able ethical properties. Most of the time, these ethical-aware algorithms are too complicated toexplain to less numeracy-equipped stakeholders [45] and hardly connected to the development processes.There are different ways in the design process to reduce the ethical risk by managing: 1) Frequency of occurrence: Thefrequency of automatic decision-making by AI systems can be reduced. Instead, AI systems can make suggestions tohuman and ask for human\u2019s approval on the \ufb01nal decision; 2) Consequence size: The consequence size can be managedthrough deployment strategies, e.g. only deploying the new model version to a group of users; 3) Consequenceresponse: This can be done through worst case analysis (e.g., FMEA), the resilient design to recover the state, orpunishing through the incentive mechanism, or overriding the recommended decisions, or undoing the actions.There has been emerging research on the design process that considers ethical principles. Value sensitive design[39] isa design approach that incorporates human value into the whole design process. To drive the value sensitive design forresponsible AI systems, participatory co-design workshops can be organized for developers and stakeholders [46]using different types of toolkits and methods, such as card-based toolkits [46, 47, 48], ethical matrix [49], ethicalhackathon [50], user journeys, sketches, low-\ufb01delity paper prototypes, high-\ufb01delity clickable wireframes [51],role play scenarios, table-top poll, low-\ufb01delity storyboards [52], value sensitive algorithm design [53].7 A - M 17, 2022There are a variety of card-based toolkits that can be used in the design workshops. Envisioning cards are one of themost adopted card-based toolkits in practice, which help designers envision of the long term effect their systems willhave on stakeholders. [46, 47]. In addition to envisioning cards, there are other card-based toolkits that can be usedin the value sensitive design workshops[46, 48], including model cards describing models with the inherent designtrade-offs, data cards analyzing possible data sources, people/persona cards discussing potential stakeholders andtheir values and interests in the system, criteria/checklist cards considering different social and technical criteria,ethics cards re\ufb02ecting on ethical implications of implementing an AI system, situation cards describing problematicsituations, inspiration cards supporting participants to generate innovative and design concepts with new technologies.Some card-based toolkits are designed speci\ufb01cally for a type of applications, such as Tiles for designing IoT applicationsand PlutoAR for designing augmented reality (AR) applications [46].Modelling has been adapted to re\ufb02ect ethical concerns and relevant design decisions, including using SysML torepresent the architecture and describe the ethical aspects of AI systems [54], designing formal models incorporatinghuman values [55], using ontologies to represent the artifacts of AI systems to make them accountable [56], buildingup ethical knowledge bases recommending design paths considering ethical concerns [57], using inductive logicprogramming to codifying ethical principles [58].Before deploying AI systems in real-world, it is important to perform system-level simulation to understand thebehavior of AI systems and evaluate ethical quality attributes in a cost-effective way. ethical scenario simulation [59]is an effective simulation-driven design method. A digital twin simulates what is happening to an AI system running inthe real world, which can help \ufb01nd unethical issues at run-time and improve the design.Trust factors should be analyzed and considered during the design, such as capability of systems, availability ofinterface, personality of agent (e.g., voice embodiment, visual virtual \ufb01gure or physical representation) [60]. The fourmajor factors for trustworthiness-by-design include data, algorithm, architecture, software [61].Many efforts for explainable AI (XAI) user interface design have been spent on checklists [62, 63, 64]. The question-driven checklists are an effective way to understand the user needs, choices of XAI techniques, and XAI designfactors [63]. The main factors for the XAI design include information included, information delivery, and interactionincluded [64]. The questions can be classi\ufb01ed into the following groups: input, output, performance (can be extendedto ethical performance), how, why, why not, what if, how to be that, how to still be this [62]. The conversationalinterface design can be experimented via a wizard of oz in which users interact with a system that the users believe tobe autonomous but is actually being operated by an unseen human [65]. There are ways to increase the level of humantrust in AI systems through XAI user interface: i) integrating human realism (i.e. anthropomorphism), includingappearance and other behavioural characteristics, into the interface design [66]; ii) proactively informing users andthe public data use information; iii) providing measurable bene\ufb01ts to users; iv) informing users capabilities, scope ofuse, and limitations of AI systems; v) informing users credentials of AI systems and operators; vi) explanations ofdata, algorithm, models, system decisions and behaviours; vii) considering explanation audiences\u2019 explainability needsacross the AI project lifecycle.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "0818551b-34ee-4ba4-897b-902f3a50440e",
                    "text": "are collections of implementation rules from the ethical perspective, includingcommunication methods (i.e., documentation), programming languages, coding, interface, and tooling [30]. Developersneed to follow the communication standards to continuously maintain high-quality, up-to-date code documentationthat covers both AI components and non-AI component. The author of each line and who made changes need tobe recorded and maintained in a repository to enable accountability and traceability. Code review can be conductedfollowing the prede\ufb01ned ethical standards. Ethical principles and metrics need to be de\ufb01ned and added into thedevelopments tools to automate ethical quality checks. Initial attempts have been made on responsible AI toolingsupport, mainly on algorithm-level rather than system-level, such as Google and Microsoft . In particular, there aremany industry fairness toolkits, such as IBM\u2019s AI Fairness 360, Google\u2019s Fairness Indicators, Microsoft\u2019s Fairlearn,and UChicago\u2019s Aequitas.There may be ethical quality issues with APIs (e.g., data privacy breaches or fairness issues). Thus, ethical compliancechecking for APIs is needed to detect if any ethics violation exists. Ethical knowledge graphs can be built based onthe ethical principles and guidelines (e.g. privacy knowledge graph based on GDPR [67]) to automatically examinewhether APIs are compliant with regulations for AI ethics. Call graph might also be needed for code analysis as theremight be interactions between different APIs. 8 A - M 17, 2022Construction with reuse means to develop responsible AI systems with the use of existing AI assets, e.g. from anorganizational repository or an open-source platform. A marketplace can be built up to trade the reusable AI assets,including component code, models, and datasets. Blockchain can be adopted to design an immutable and transparentmarketplace enabling the auction-based trading for AI assets and material assets (e.g., cloud resources) [68]. Toensure the ethical quality, credentials can be bound with the AI assets or developers, which can also be supported byblockchain platforms. If different frameworks are used in model migration, guidelines and tooling support (such aspytorch2keras ) are needed to automatically migrate a model from one framework to another. Glue code may beused to integrate AI components with non-AI components to eliminate incompatibility since there are various inputsand outputs for the AI component .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "8f094c13-16ea-4323-9b19-1461ea878004",
                    "text": "Veri\ufb01cation and validation are used together for checking whether an AI system meets the requirements described in thespeci\ufb01cation and ful\ufb01lls its intended purpose in a responsible way. Ethical acceptance testing (e.g. Bias testing) canbe designed for detecting the ethics-related design \ufb02aws [69] and verifying the ethical requirements (e.g. whether thedata pipeline has appropriate privacy control, fairness testing for training/validation data). The behaviour of the AIsystem should be quanti\ufb01ed by the acceptance testing and the acceptance criteria for each of the ethical principlesshould be de\ufb01ned in a testable way. A testing leader may be appointed to lead the testing for each ethics principle.For example, When bias detected at runtime, the monitoring reports are returned to the bias testing leader [6]. Anethical scoring system using a set of actionable tests can be used to measure how ready for production a given AIsystem is from the ethics perspective. When certifying ethical aspects of an AI system/component, benchmark testingmay be needed. Formal veri\ufb01cation can be used to prove that a system matches its ethical requirements through acomprehensive mathematical analysis [70, 71].A collection of test cases with expected results should be maintained to detect possible ethical failures in a wide varietyof extreme situations. New test cases need to be added when there is a new requirement added or the operation contextchanges [6]. All the test cases for veri\ufb01cation and validation should pass the ethics assessment. The history of testingshould be recorded and tracked, such as how and by whom the ethical issues were \ufb01xed.The traditional testing techniques can be adapted for testing AI systems. Unit testing can be performed for both AIcomponents and non-AI components according to the speci\ufb01cation (including model-level speci\ufb01cation). Interactionsbetween AI system components, particularly in between AI components (i.e. for AI pipeline) and between AI com-ponents and non-AI components, need to be veri\ufb01ed through incremental integration testing along the developmentprocess [43]. Infrastructure testing is also part of the integration testing. Sanity testing is performed after thesoftware build to ensure that the code changes introduced are working aligned with ethical principles. Usabilitytesting [6] measures stakeholder performance and satisfaction and is essential to ensure the systems is easy to use anddoes what users and indirect stakeholders expect in terms of responsible AI. Data tests need to be performed to checkwhether the serving data is the data we expect in the operating environment, e.g., check the inputs for each variablematch what the model expects. Skew tests checks how representative the training data is of the serving data, e.g.,through the percentage of missing data in the serving data compared to the training data. AI4SE4AI can be applied toautomate the testing through AI testing and/or cognitive testing [43].Both failure mode and effects analysis (FMEA) and fault tree analysis (FTA) are tools to understand ethical failuresand risk of AI systems [43]. Assurance cases [72] (e.g. safety case [73]) provide evidence and arguments that supportclaims about ethical properties or behaviors of AI systems. There are four levels of evaluation assurance [70], includingbasic disclosure, tested claims, active testing, and formal veri\ufb01cation.Scoring tools allow the team to measure the level of trustworthiness [74] or trust (e.g., trustworthiness/trust score) byassessing the outcomes of AI systems based on the contextual data or capturing users\u2019 experiences with AI features.Arti\ufb01cial intelligent quotient [75] tracks a conversational AI system\u2019s level of competence and capabilities over time(e.g. knowledge, language, reasoning, creative and critical thinking, working memory). The agents\u2019 humanness factors(e.g. speaking and listening) can help improve the trust in conversational AI systems [76].",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "f8baad11-b737-48e9-8c2e-284e7e2570a8",
                    "text": "Given the continual learning of AI systems based on new data and the higher degree of uncertainty and risks associatedwith the autonomy of the AI systems, there is a strong desire for deployment strategies and continuous validation of9 A - M 17, 2022responsible AI requirements. The deployment strategies include phased deployment (i.e., deploying AI systems for asubset group of users initially to reduce ethical risk), homogeneous redundancy, etc.The existing work on the continuous monitoring and validation mainly focuses on the AI system outputs (e.g.,performance metrics - accuracy, precision, and recall ) rather than the outcomes (i.e. whether the AI system behavesand make decisions responsibly) [77]. With different context data (e.g., users, traf\ufb01c, weather) in operation, AI systemsare continuously evolving to address ethical risks. The current practice for ethical risk assessment is often one-offtype of risk assessment (e.g. Five Safes Framework ). Ethical risk assessment is expected to be dynamic, adaptive,and extensible for different context e.g., culture. . Version-based feedback (e.g., ethical violation) should be reportedto the development team and other stakeholders continuously. Incentives can be applied to encourage the ethicaloutcomes of AI systems in terms of both decisions and behaviors. The time and frequency of validation and conditionsof necessary retraining should be prede\ufb01ned [42].An AI system usually involves co-evolution of data, model, code, and con\ufb01gurations. Data/model/code/con\ufb01gurationco-versioning traces exactly what datasets and con\ufb01guration parameters were used to train and evaluate the model.Co-evolution may also happen to AI components and non-AI components, thus co-versioning is required to facilitatethe maintenance and communication between the AI component development team and non-AI component team.Bill of materials (BOM) [78, 79] enables the transparency and accountability by keeping a formal record of the supplychain details of the components used in building an AI system, such as component name, version, supplier, dependencyrelationship, author and timestamp. Ethical audit trail records every step of AI systems from the ethics perspective.Developers need to consider trade-offs between accountability and performance when making design decisions on whatdate is placed on blockchain/cloud. Accountability knowledge graphs support capturing accountability informationthroughout the lifecycle of AI systems and auditing them programmatically. To build such knowledge graphs, ontologycan be used to describe and model accountability information [56, 80].",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "138d8c63-c9b1-4a87-b18a-881fe714e619",
                    "text": "There is a methodological gap between the AI model development pipeline that produces the AI model and softwaredevelopment process that develops the non-AI components and AI components embedding AI model [81]. Given the AImodel pipeline is more experimental with still limited methodological support, integrating the AI model pipeline intothe agile software development process needs better understanding of artifacts, activities and roles involved. Althoughinitial tooling attempts have been made on the integration (such as Microsoft Team Data Science Process and IBMWatson Studio ), the integrated AI system development process still need to be standardized and supported by softwaretools taking into account responsible AI principles.Capturing ethical principles by requirement engineering. Compared with traditional software and general responsi-ble software, AI systems also need to consider requirements about models, training data, system autonomy oversightand may emphasize certain ethical requirements more due to AI-based autonomous and potentially opaque behaviourand decision making. In particular, some of the ethical principles are hard to de\ufb01ne and quantify. To make the ethicalprinciples be captured by requirements engineering for AI systems, the community should put further efforts onrequirements engineering methods and provide a concrete guidance on requirement engineering for responsible AIsystems.Designing AI systems for improving both trustworthiness and trust. Trustworthiness is the ability of an AI systemto meet ethical principles, while trust is users\u2019 subjective estimates of the trustworthiness[18]. Trust in AI systemsinvolves trust duality that includes trust in providers and trust in technologies that can be further classi\ufb01ed into trust inan AI technology and a base technology (e.g. vehicle) [82]. Both trust in a provider and trust in a technology needto be considered in parallel. A user\u2019s trust in an AI system may have a signi\ufb01cant gap compared to the AI system\u2019sinherent trustworthiness, i.e., a user underestimates or overestimates a system\u2019s trustworthiness and has inadequate orexcessive trust into the system. More efforts need to be made on the design to improve both trustworthiness and trust inAI systems. Process and product (i.e. system perspective) mechanisms can be leveraged to achieve trustworthiness fordifferent ethical principles, whereas process and product evidence need to be offered for different types of trusters in aproper communication way to drive trust. These will help close the gap between their subjective estimation and thesystem\u2019s more objective trustworthiness. Instead of focusing on veri\ufb01able product trustworthiness via mathematicalalgorithm-level guarantees, researchers need to systematically explore a broader variety of mechanisms in system-levelproduct design and development processes to improve both trustworthiness and trust.10 A - M 17, 2022Figure 6: Architectural patterns for responsible-AI-by-design.Continuously monitoring and validating the outcomes of AI systems against responsible AI requirements. Thereare two forms of AI system development de\ufb01ned in [83]: requirement-driven development and outcome-drivendevelopment. The latter is about the real-world operation and outcome of AI systems. Seamless integration ofrequirement-driven development and outcome-driven development requires understanding the unique characteristics ofAI systems. The development of AI systems is a continual and iteration process. Validation of outcomes (i.e. whetherthe system provides the intended bene\ufb01ts and behave appropriately given the situation) is required for AI systems ratherthan just outputs (e.g. precision, accuracy and recall). Also, since all principles need to be instantiated, it is necessary tomake risk assessment and management extensible , adaptive, and dynamic for different context, with guided extensionpoints. For example, some principle might be automatically instantiable for different culture context and extendedfollowing guided extension points. Further work is needed on MLOps for continuously monitoring and validating theoutcomes of AI systems against the responsible AI requirements.5 System Perspective5.1 Current State",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "38d4b29b-f566-4bb8-b006-8df0026861e7",
                    "text": "Responsible-AI-by-design architectural style can be de\ufb01ned by a set of design principles and patterns. Some designprinciples include co-architecting of AI components and non-AI components, minimum complexity, design withreuse. The architecture of AI systems includes AI components that produce and embed the AI models and non-AIcomponents that uses the outputs of AI components for overall system functionalities. As the AI components are ofteniteratively experimented by data scientists/engineers who are not familiar with software engineering, co-architecting ofthe AI components and the non-AI components can ensure the seamless integration of the two types of componentsand consideration of both system-level and model-level (ethical) requirements when making design decisions. Scenario-based evaluation methods can be adapted to evaluate the architecture of AI systems. Developers should follow theprinciple of minimum complexity that includes both the design of AI systems (e.g. software/system architecture,whether adopting AI or not, selection of AI techniques) and the future operating environment [44]. One principleis design with reuse. AI system components, particularly the AI components for producing AI models, should be11 A - M 17, 2022evaluated against responsible AI requirements and reused as much as possible to improve productivity [44]. Ethicalcerti\ufb01cates may be needed for reusing the AI components developed by the third party.Fig. 6 lists a set of architectural patterns for responsible-AI-by-design, which could be embedded as the product featuresof AI products. Adopting AI or not can be considered as a major architectural design decision when designing asoftware system. For example, AI mode switcher offers users ef\ufb01cient invocation and dismissal mechanisms foractivating or deactivating the AI component when needed [84, 85, 86, 87]. Kill switch is a special type of invocationmechanism which immediately shuts down the AI component and its negative effects. The decisions made by the AIcomponent can be executed automatically or reviewed by a human expert in critical situations. The human expertserves to approve or override the decisions. Human intervention can also happen after acting the AI decision throughthe fallback mechanism that reverses the system back to the state before executing the AI decision. A built-in guardensures that the AI component is only activated within the prede\ufb01ned conditions (such as domain of use, boundaries ofcompetence). Users can ask questions or report complaints/failures/near misses through a recourse channel.Ethical sandbox can be applied to run the AI component separately (e.g. from untrusted third parties) [88, 89].Maximal tolerable probability of violating the responsible AI requirements should be de\ufb01ned as ethical margin for thesandbox [90]. A watch dog can be used to limit the execution time of the AI component to reduce the ethical risk [91].Multi-model predictor employs two or more models to perform the same task [92, 93, 94]. This pattern can improvethe reliability by deploying different models under different context (e.g., different regions) and enabling fault-toleranceby cross-validating ethical requirements for a single decision (e.g., only accepts the same results from the employedmodels) [95, 96]. Federated learner preserves the data privacy by training models locally on the client devices andformulating a global model on a central server based on the local model updates [97, 98, 99]. Decentralized learningis an alternative to federated learning, which uses blockchain to remove the single point of failure and coordinate thelearning process in a fully decentralized way.In the event of negative outcomes, the responsible humans can be identi\ufb01ed by an ethical black box for accountabil-ity [100]. The ethical black box continuously records sensor data, internal status data, decisions, behaviors (both systemand operator) and effects [101]. All of these data need to be kept as evidence with the timestamp and location datausing an immutable log (e.g. using blockchain)[102]. Cross-system auditor provides global-view accountability by\ufb01nding discrepancies among the data collected from a set of AI systems and identifying liability when negative eventsoccur [103, 104]. Co-versioning registry can be applied to ensure accountability in two cases: 1) co-versioning ofAI components and non-AI components; 2) co-versioning the artifacts within the AI components, i.e., data, model,code, and con\ufb01guration [105, 106, 107]. Continuous validator supports continuous ethical risk assessment by mon-itoring and validating the prede\ufb01ned ethical metrics [108, 109, 110]. The time and frequency of validation shouldbe con\ufb01gured within the continuous validator. Version-based feedback and rebuild alert should be sent when theprede\ufb01ned conditions are met. Incentive mechanisms can be designed to reward/punish the ethical/unethical behavioror decisions of AI systems.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "a29bfef1-7199-4326-aa67-0467e4bfcf64",
                    "text": "There are many system-level techniques, which could be embedded as components:\u2022 Fairness: demographic parity, data augmentation, weighted data sampling, re-sampling, re-weighting, swappinglabels, removing dependencies, equalized odds checking;\u2022 Privacy: data sanitizing, federated learning, decentralized learning, differential privacy, secure multipartycomputation, homographic learning, fog computing;\u2022 Safety and security: sandboxing, trusted execution environments; safety margin; data representativenesschecking, approximate computing;\u2022 Explainability: global explanations, local explanations, feature-based explanations (contrastive, counterfactual,rule-based explanations), exemplar-based explanations, post hoc explanations for black box models, prospectiveexplanations, surrogate models (LIME, SHAP, PyExplainer [111]), what-if, con\ufb01dence scores.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "387bbf9f-1247-4e1c-be21-c9752f38c199",
                    "text": "An AI system consists of AI components and non-AIcomponents that are interconnected and work together to achieve the system\u2019s objective [112]. An AI model needs tobe trained and integrated into the inference component of the system to perform the required functions. Both the AIpipeline components and the inference component can be considered as AI components. Combining AI and non-AIcomponents may create new emergent behavior and dynamics. Therefore, ethics need to be considered at system-level,including AI components, non-AI components and their connections. For example, the new data gathered by the data12 A - M 17, 2022collector need to be fed into the model trainer, whereas the effect of actions decided by the AI model may changethe behavior of non-AI components and could be collected through the feedback component built into the AI system.One way to achieve responsible-AI-by-design is to de\ufb01ne an architectural style through a set of architectural designprinciples taking into account ethical principles. Additional principles need to be explored in addition to the initialattempt.Dealing with ethical requirement trade-offs using design patterns. There are trade-offs between functional require-ments and ethical principles or in between some of the ethics principles. Privacy and utility are often con\ufb02icting. Forexample, to ful\ufb01ll the privacy requirements, the datasets can be de-identi\ufb01ed and aggregated so that individuals cannotbe uniquely identi\ufb01ed [113], which may lead to worse distributional properties and affect the reliability. Requirementinconsistency also happens to accountability and privacy. For example, when particular activities are not compliant tosome regulatory standards, we need to \ufb01nd out where this happened and who to blame. In this case, there might bean issue about data privacy. The current practice dealing with con\ufb02icting ethics principles is usually the developersfollowing one principle while overriding the other rather than building balanced trade-offs with stakeholders making theultimate value and risk call. Patterns can be used to deal with the system-level trade-offs among con\ufb02icting responsibleAI principles and other requirements in an inclusive manner. A pattern catalogue is needed to provide concrete guidanceon how to design responsible AI systems.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                },
                {
                    "id": "5e583581-58a8-43e2-9f33-dd94475714f0",
                    "text": "To operationalize responsible AI, we present a research roadmap on software engineering for responsible AI. The \ufb01ndingscould be organised as operationalized guidelines for the stakeholders of AI systems (e.g. regulators, management,developers). Some \ufb01ndings could be framed as tools (e.g. ethical risk assessment) or embedded as product features ofAI systems to reduce the ethical risk and unlock the market where there is currently little trust (e.g. ethical black box).Although the major industry solutions (such as model cards) have been identi\ufb01ed in our study through Google scholarand the snowballing process, we plan to do an industry landscape that will cover the complete state of the practice.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing, and others. 2022. Towards a roadmap on software engineering for responsible AI. In 1st International Conference on AI. Retrieved from https://arxiv.org/pdf/2203.08594."
                }
            ]
        },
        {
            "paper_title": "Behavioral use licensing for responsible ai",
            "authors": "D Contractor, D McDuff, JK Haines, J Lee\u2026",
            "publication_info": "Proceedings of the \u2026 - dl.acm.org",
            "paper_url": "https://dl.acm.org/doi/pdf/10.1145/3531146.3533143",
            "chunks": [
                {
                    "id": "578525e5-3f9d-4211-9370-0d50604372ce",
                    "text": "With the growing reliance on artificial intelligence (AI) for manydifferent applications, the sharing of code, data, and models is im-portant to ensure the replicability and democratization of scientificknowledge. Many high-profile academic publishing venues expectcode and models to be submitted and released with papers. Further-more, developers often want to release these assets to encouragedevelopment of technology that leverages their frameworks andservices. A number of organizations have expressed concerns aboutthe inappropriate or irresponsible use of AI and have proposedethical guidelines around the application of such systems. Whilesuch guidelines can help set norms and shape policy, they are noteasily enforceable. In this paper, we advocate the use of licensingto enable legally enforceable behavioral use conditions on softwareand code and provide several case studies that demonstrate thefeasibility of behavioral use licensing. We envision how licensingmay be implemented in accordance with existing responsible AIguidelines.CCS CONCEPTS\u2022 Social and professional topics \u2192 Licensing.KEYWORDSAI licensing, ethical guidelines and principles, enforceable mecha-nisms",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "2c11467a-dacd-4002-b59d-389835cd684b",
                    "text": "Academic communities rely on the sharing of ideas, materials, anddata. \u201cOpenness\u201d of this kind is vital because it allows methodsto be verified, saves time and resources, and allows researchers tomore effectively build upon ideas that have come before. It lowersthe bar to entry for academics and reduces friction in translatingresearch to applications.Accordingly, openness around code and other intellectual prop-erty has become one of the core values of computer science. Highprofile computer science conferences, such as NeurIPS and CVPR,expect code to be submitted and released with published submis-sions [46] and the policy of some journals is that \u201cauthors are re-quired to make materials, data, code, and associated protocols promptlyavailable to readers without undue qualifications\u201d [6]. Within thehuman-computer interaction (HCI) research community specifi-cally, there has been discussion of how to support open researchpractices for HCI [68, 70] and how HCI can help other fields practiceopen computing research [26].Advances in computing have introduced new challenges in main-taining openness. A new generation of end-to-end neural approachesto AI (e.g., convolutional neural networks, recurrent neural net-works, generative adversarial networks, transformers) is leadingto systems becoming increasingly generalized, rather than limitedto feature sets tailored for specific domains. While AI systems arenot limited to neural models, the need for control over use hasbeen accelerated due to advances in model performance. For ex-ample, they can generate text [17], images [37], videos [21, 47],and audio [69] that in some conditions are indistinguishable fromthose created by a human. Recent progress in generative models,such as those based on Generative Adversarial Networks (GANs)for images [32], and Transformer Architectures [64] for text, haveto led to fears of these models being exploited for malicious usevia Deep Fakes [47], or the creation of machine-generated fakenews at scale [73]. In addition, the recent progress made in thedevelopment of neural architectures has resulted in models thatcan learn rich representations of data. They can thus be applied toa broad range of discriminative and generative tasks. For example,the same (or similar) model architectures trained on separate imagedatasets can be used to achieve good performance on biometricidentification (e.g., face recognition), detection of cancerous regionsin a mammogram [57], or for classifying buildings and people froman unmanned vehicle [27].The context in which a model is applied can be far removed fromthat which the developers had intended, a major point of concernfrom the perspective of human-centered machine learning [31].However, there are few mechanisms that allow someone to sharetheir work broadly while also restricting use in applications thatmay be of concern, such as large-scale surveillance or the creationof \u201cfake\u201d media. In some cases, the developers or technology cre-ators may legitimately want to control the use of their work due toconcerns arising out of the data that it was trained on, the technol-ogy\u2019s underlying assumptions about deploy-time characteristics, orthe lack of sufficient adversarial testing and testing for bias. Thisis especially true of AI models that are difficult or expensive torecreate. For example, given that models such as GPT-3 [17] report-edly cost over $10 million (U.S.) to train, very few organizationsare positioned to train (and potentially, need to retrain) a model ofsimilar size.The computing community has begun to respond to the conflictbetween powerful new technologies and the value of openness. Anumber of organizations have expressed concerns about incorrector irresponsible use of AI and have proposed AI ethical guidelines(see an example from Microsoft [7]) and Responsible AI initiatives[53]. While such guidelines are useful and help shape policy, theyare not directly enforceable, leading to fears of \u201cethics washing\u201d[16]. Governments have also taken note of the risks associated withcertain types of AI applications and have passed legislation such asdata protection laws in the European Union [1] and San Francisco\u2019sAcquisition of Surveillance Technology Ordinance, banning theuse of facial recognition technology [22]. While these are legallyenforceable, government action requires prolonged deliberationby policymakers who often lack direct experience in computerscience, let alone machine learning or artificial intelligence. Becausedevelopment of regulations in the technology space tends to lagbehind the development of new technologies, it is important forthe computing research community to explore avenues for changethat complement slower, top-down regulation.In this paper we advocate for the use of licensing as a mecha-nism for enabling legally enforceable responsible use. Licenses canhelp democratize the definition of responsible use by enabling AIdevelopers and other stakeholders in AI systems (e.g., people whocreate data) to incorporate permissive- and restrictive-use clauses,based on their view of how their system(s) should or should notbe re-purposed. We further demonstrate how existing AI ethicalguidelines may be operationalized through licensing. We argue thatlicenses could serve as a useful strategy for creating, expanding,and enforcing responsible behavioral norms as a complement togovernmental legislation, and further serve to demonstrate that the computing community is itself committed to the responsibleuse of the technologies it creates. This approach is highly \u201chuman-centered\u201d [40], as it focuses on the interests of both developers andthe general public.Our paper makes the following contributions:\u2022 We discuss licensing as a mechanism for supporting theresponsible use of AI, using behavioral use clauses.\u2022 We present exemplar clauses that buttress concepts com-monly found in widely-adopted AI ethical guidelines.\u2022 We demonstrate how information from AI FactSheets [11]can be incorporated into a license for an AI system.Together, these contributions advance a new approach to thornyresponsible AI challenges.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "b55f15e4-1f4b-4fee-ae0b-20b3dc884920",
                    "text": "Here, we present a brief overview of the different approaches com-monly used to promote the responsible use of AI.In reviewing this work, we emphasize the unique challengesAI systems create. Compared to traditional software systems, AIsystems do not offer the same degree of control and run-time guar-antees of system behaviour. While traditional software systemsare also deployed in social systems that are highly complex andunpredictable, they can be systematically debugged, thoroughlytested, and versioned. In contrast, the output characteristics of AIsystems are dependent on the nature of the dataset(s) used to trainthem, as well as the design and training procedure of the model.This makes it hard to be completely certain about how an AI modelis likely to respond to known unknowns and unknown unknowns.While there is ongoing work that attempts to solve these problemsfor specific classes of models (e.g., [55] and [58]), there remainchallenges, and there are no generalizable solutions.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "1803e87f-9d24-4db2-bec7-e51fd947acf7",
                    "text": "To help build more robust AI systems, researchers have built severaltoolkits to study and address bias in datasets [14, 62, 63]. In addition,behavioral testing frameworks [56] have been developed, includingthose that adversarially attack machine learning models to assesstheir sensitivity to input features [48], alongside various othertools that help visualize the processing of inputs in an AI system.Technology companies and policy institutions have begun torelease principles and guidelines for the use of AI. According to a re-cent survey report [36] that studied 84 AI Responsible AI and ethicalguidelines, 73-86% of the guidelines surveyed included principles ofTransparency, Trust and Accountability and Justice & Fairness. Prin-ciples of \u2018Transparency\u2019 and \u2018Trust\u2019 in guidelines include aspectsrelated to the acquisition, use, and transformation of data [13, 43],the model design [5] and its end-use application [25]. Furthermore,some guidelines suggest that AI systems should be \u2018Accountableand Fair\u2019 where they suggest that decisions of an AI system shouldbe interpretable and explainable to a trained human worker [4]. Ad-ditionally, some guidelines also recommend that AI systems shouldexplicitly identify themselves as such [4] and that decisions takenby an AI system, or with the assistance of an AI system, shouldhave a human being accountable [2].How does one ensure that such principles are followed? Today,enforcement is based on self-regulation by technology developersto voluntarily comply with such guidelines. In other words, there isno external enforcement. This has led to fears that, in the absenceof regulatory mechanisms that enforce the spirit of such guidelines,the reliance on self-regulation is resulting in shallow appearancesof ethical behavior (colloquially referred to as \u201cethics washing\u201d)[16].",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "f03ed40b-819e-4485-bcfa-02296769dcc6",
                    "text": "To understand the roles licenses might play in governing the use ofAI, it is important to consider the varying degree to which modelscan provide interpretability regarding their decisions, and the rolethat interpretability and explainability research initiatives haveplayed so far in the pursuit of \u2018responsible\u2019 AI.Depending on the underlying architecture used by machine learn-ing models, the interpretability of downstream systems will vary.For instance, models based on Decision Trees allow users to studythe decision made by each intermediate node in the tree; this canhelp determine the reasoning employed. Other models, such as thosebased on Lasso Linear Regression, can provide some explainabilityespecially when used with easy to interpret features. Such modelshave been used in disease risk prediction and studies of clinical fac-tors. [18, 61]. Bayesian formulations are another class of machinelearning models that can provide explanations using the conditionalprobabilities and dependencies between sets of variables [38]. Forexample, such models are used to help indicate the likelihood of apatient having a particular disease given their medical symptomsand health parameters [10].In order to provide a degree of explainability in deep learningbased AI systems, methods such as LIME [54] offer locally inter-pretable, model-agnostic explanations by learning a feature basedlinear regression model trained to mimic (predict) the outputs ofthe deep learning model. The models are trained by generatingsamples after perturbing the instance under observation and thecorresponding outputs. Other methods include SHAP [42] whichalso builds locally-explainable models, and uses techniques fromcooperative game theory to model how different features contributetowards the final output. Instead of building simpler ML methodsto model the output of black-box neural networks, developers havedone this by enabling visualizations of model components - such asheatmaps of words [39] or images [71]. Probing (studying the effectof small changes in the input) [49] as well as, using counterfactuals[19], are other ways in which the neural network characteristicsare studied.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "967268c8-808a-47eb-92ba-9d0d3274f662",
                    "text": "Better documentation for AI systems is another tool researchersinvestigated for advancing responsible AI. Providing details suchas the data used for the development and testing of AI systems, thenature of testing and error characteristics, and risks of bias couldhelp end-users determine the applicability or \u201cfit\u201d of a model for their needs. AI Factsheets [11] have introduced questionnaires seek-ing details about fairness and robustness checks and performancebenchmarks, and they require developers to list intended uses inorder to populate a summarized template sheet. Similar suggestionsare included in AI Model cards [45], which advocate for quantita-tive evaluation to be broken down by individual, demographic, andother domain-relevant conditions.The datasets used for training and testing AI systems are keyaspects to consider during development. The Datasheets [29] forDatasets proposal introduces a questionnaire that captures thedetails about the purpose and nature of a system\u2019s data choices,including details around labeling, sources of noise, source of thedataset, who the creators are, and whether there might be relation-ships between data samples (e.g., social connections).",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "ee91ce0d-da41-4421-b4ba-24af88c24ae5",
                    "text": "The growing capability of AI systems, as well as the potential fortheir misuse, have made creators of AI systems increasingly con-cerned about sharing their work. OpenAI, for example, only selec-tively disclosed elements of GPT-2 when it released GPT-2 [52]:\u201cDue to concerns about large language models being used to generatedeceptive, biased, or abusive language at scale, we are only releasinga much smaller version of GPT-2 along with sampling code. We arenot releasing the dataset, training code, or GPT-2 model weights.\u201dOpenAI was similarly circumspect in releasing GPT-3 by provid-ing researchers with access to GPT-3 only through its own cloud-hosted API. Other AI technology providers such as IBM, Amazonand Microsoft recently announced [44] they will no longer offergeneral-purpose facial recognition technology to law enforcementagencies.Not all AI technology providers want to \u201chold back\u201d their tech-nology from potential users, and not all AI technology providersare positioned to release only bits and pieces. For those technologyproviders who wish to release their work, and yet simultaneouslyimpose behavioral constraints on future users, licensing appears toprovide a pathway that is less extreme than limited-releases andprice-based mechanisms that create a barrier to entry [3].In the next section we provide an introduction to licensing as alegal framework. We then describe exemplar clauses based on someethical AI principles and information shared in AI Factsheets [11].These proof-of-concept exemplars help to demonstrate how the useof behavioural-use clauses could offer an enforceable mechanismfor promoting the responsible use of AI, grounded in the existinglegal framework for protecting intellectual property (\u201cIP\u201d).",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "35a8428d-f73b-4510-ba9b-733774c2dbec",
                    "text": "Let us continue by establishing the basis for the use of licensing asa mechanism for controlling end-user behavior. We describe thestructure of a license and the key elements for making it effective.Governments have created intellectual property rights in order togive authors and inventors control over the use and distributionof their ideas and inventions. Intellectual property (IP) rights thusact as an incentive for creators to engage in creative activities(with a potential cost of limiting the diffusion of ideas). Examplesof such intellectual property rights include patents, copyrights,and trademarks. By offering new licensing opportunities to AIdevelopers and stakeholders (e.g. data creators), these groups cantake advantage of existing IP frameworks.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "c73fdd77-8ce9-49f6-897f-cf335a67294f",
                    "text": "In general, a license is a contractual agreement that grants permis-sion by a qualified authority, such as the owner of IP rights, to alicensee. Licenses may have territorial limits, temporal limits, orother restrictions. These permissions are, of course, limited by thelicensor\u2019s power to confer such permissions. When IP owners li-cense the use of their IP to others, they forfeit their right to excludethose licensees from using the IP. IP owners benefit from licensingbecause they maintain ownership over their IP while profiting fromthe licensee\u2019s use of their IP, either in the form of royalties or inthe co-development of new IP.IP licenses are formed contractually in the form of license agree-ments. The terms of a license are typically dictated by those whoown or control the IP. \u201cIP\u201d can include many different intangiblerights \u2013 e.g. patents (inventions), copyright (works of authorshipincluding technical manuals, software, specifications, formulae,schematics, and documentation), know-how (e.g. expertise, skilledcraftsmanship, training capability), trade secrets (a protected for-mula or method, undisclosed customer or technical information,algorithms, etc.), trademarks (logos, distinctive names for productsand services), industrial designs (the unique way a product lookssuch as a computer\u2019s molding), and semiconductor mask works(the physical design of semiconductor circuits).Licenses may be for certain IP rights only (e.g. a license to prac-tice an identified patent or to copy and distribute a certain work).Licenses may be for all the IP rights that are necessary to sell,market, and/or use a specific type of technology (e.g. a license todevelop a new widget that is created from a patented process andhas a distinctive design).Thus, an IP owner could permit users to create derivative worksbut only with attribution to the original author (e.g., Creative Com-mons License). Alternatively, an IP owner could require users torefrain from using the licensed technology in a particular geographyor in a particular type of application.In this paper, we seek to encourage entities and individuals whocreate AI tools and applications, to leverage the existing IP licenseapproach to restrict the downstream use of their tools and applica-tions (i.e., their \u201cIP\u201d). Specifically, IP licensors should allow othersto use their IP only if such licensees agree to use the IP in waysthat are appropriate for the IP being licensed. While contractualarrangements are not the only means to encourage appropriatebehaviour, it is a mechanism that exists today, is malleable to differ-ent circumstances and technologies, and acts as a strong signalingmechanism that the IP owner takes their ethical responsibilitiesseriously. Put another way, licenses are one readily available toolthat developers and technology creators can use today to take astep towards responsible AI.While there are benefits of licensing regimes, one must also bemindful of the fact that violations of the licensing terms are en-forced by the IP owner (or its designee) via legal action in the formof litigation. At least two causes of action could be brought in thiscontext \u2013 breach of contract and infringement of IP rights. Thus, those who opt for a licensing regime to enforce responsible behaviorin the AI space must be prepared to enforce such terms using avail-able dispute resolution mechanisms. In the US, those mechanismsare primarily the judicial system and private arbitration.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "b71d91b8-e7a4-4885-9577-cb140f6a50e6",
                    "text": "The basic structure of a license requires, at minimum, the following:\u2022 Licensor: A person, business or organization with exclusivelegal rights over the assets,\u2022 Licensee: A person, business or organization that has beengranted legal permission by the licensor,\u2022 Identification of the subject Intellectual Property, and\u2022 Restrictions: geographic, temporal, behavioral, ability to sub-license, royalty terms, etc.Multiple examples of behavioral restrictions exist in commonsoftware licenses, such as limiting the licensee\u2019s access for \u2018inter-nal use\u2019 only; prohibiting modification of the software code; andrestricting use of the software to specific countries. These licenserestrictions can be tailored for each type of technology being in-troduced. For example, in the novel field of \u201crobot law\u201d, at leastone set of \u201cethical license terms\u201d has been proposed with restric-tions requiring default privacy settings and a kill switch [24]. Suchbehavioral restrictions generally originate with the licensor, andmay or may not be subject to negotiation between the licensor andlicensee.An IP owner who wishes to act as an IP licensor should be cog-nizant of the following three questions in monitoring compliancewith the license terms: (1) How can I detect license term violations?(2) What are the potential consequences of such violations? (3)What are the available enforcement mechanisms for the licenseterms?",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "2e1c36ff-352c-4246-bccd-14bb3e4db035",
                    "text": "How does an IP licensor detect when alicensee has exceeded the permissible scope of the license? That is,will it be obvious to the IP licensor if or when a licensee of softwarethat is licensed for \u201cinternal use only\u201d is shared by the licenseewith third parties? Certain types of IP are more readily policedthan others. For example, it would arguably be much more difficultfor a licensor to detect third parties\u2019 unlicensed use of spelling-autocorrection software as opposed to third parties\u2019 unlicenseduse of an online photo. Depending on the difficulty of detectingviolations of IP license terms, the licensor may wish to incorporateother clauses in the contract to ensure compliance, such as annualaudits or periodic reports of IP usage. Standardization of licensecomponents reduces the burden on developers when it comes toreleasing assets. A good example of this is the Creative CommonsLicense which provides a set of options for the licensor to choosefrom about how the assets can be used by the licensee.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "10a75901-3163-4056-95d6-f4d95980717f",
                    "text": "The potential consequences ofbreaching the license terms vary widely. In some instances, thebreach \u2013 especially if it was made inadvertently \u2013 may be correctedby a simple notice from the licensor that the licensee has breachedthe contract. Other breaches can be catastrophic, such as exposinglicensor\u2019s source code, which can dramatically limit the ability fora licensor to control the use of that code. A licensor should thinkthrough the possible ranges of consequences for different types ofbreaches, and be prepared to identify ways for each type of breachto be cured. This further determines how other clauses in the licenseagreement may be drafted.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "326c2a3d-2adf-4d36-a2e8-31210661a1cd",
                    "text": "The available enforcement mechanisms caninfluence the licensees\u2019 willingness to stay strictly within the licenseterms. Some IP licensors have technical mechanisms to enforce theirlicense terms. In the event a licensee breaches the license terms,the licensor can cut off access to its IP. This is feasible, for example,where the IP licensor owns a digital platform (such as a MobileApp Store) or provides software as a service via the cloud. OtherIP licensors, who lack such technical mechanisms of enforcement,must rely on the threat of legal action to effect compliance withthe license terms. In the United States and in other countries withrobust judicial systems, an IP licensor can rely on its lawyers andthe court system to enforce the rules dictated by its license terms.We note that apart from enabling enforcement, license restric-tions may also serve as a deterrent; corporations and organizations,including those tasked with the deployment of high risk AI systems,have stringent legal reviews of tools and software for the licenseterms associated with them. This includes open source softwarewhere some flavours of licenses are viewed more favourably thanothers with regard to commercialization. Thus, any restrictions onuse included in licenses and contracts are likely to be self-enforcedto avoid the risk of expensive litigation.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "00244690-c5ca-49c0-9f7e-fc29e3e55b38",
                    "text": "The advent of open source software in the 1990s popularized thenotion that software should be developed collaboratively and freelyshared within the developer community. The Open Source Defini-tion [23] presents a mature open-source philosophy and defines theterms of use, modification, and redistribution of open-source soft-ware. These terms include the following: (1) free redistribution ofsource code; (2) modifications are permitted and must be distributedunder the same terms as the original license; (3) no discriminationagainst any person or groups of persons; (4) no discriminationagainst fields of endeavor; and (5) technology neutrality. Today,most if not all participants in the software development communityare familiar with open source ideas, under which software licensesgrant rights to other users which would otherwise be reservedto the writer of such software under copyright laws. One of thepopular types of restrictions is the \u201cDo No Evil\" restriction, popu-larized by JSON (JavaScript Object Notation), a format for storingand transporting data between servers and web pages: \u201cPermissionis hereby granted, free of charge, to any person obtaining a copy ofthis software and associated documentation files (the \u201cSoftware\"), todeal in the Software without restriction, including without limitationthe rights to use, copy, modify, merge, publish, distribute, sublicense,and/or sell copies of the Software, and to permit persons to whom theSoftware is furnished to do so, subject to the following conditions: Theabove copyright notice and this permission notice shall be includedin all copies or substantial portions of the Software. The Softwareshall be used for Good, not Evil.\u201dThe last sentence is understood to violate the open source def-inition, given that it restricts usage. Furthermore, most users arelikely able to convince themselves that they are not engaged in evilactivities, and that they are thus compliant even with the \u201cDo No Evil\" terms of a JSON license. This example highlights challengesin creating enforceable licenses.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "85123397-2f34-412c-80c0-59e417172bc2",
                    "text": "In contrast to the the highly developed community norms aroundopen source, discussions of AI negative impacts lack standards.While academics have pointed out the need for new frameworksin this growing space (see, e.g., the case for \u201cdata justice\u201d [60]), thecurrent vacuum has yet to be filled with government regulation.To work towards licensing as a topic of norm-building, we presenttwo case-studies where we adapt some recommendations from AIethical guidelines as well as AI FactSheets [11], as license clauses.The goal here is to illustrate the types of clauses that could existand not necessarily argue for or against these specific clauses. Inother words, a key contribution of these case studies is to makethe point that licenses for responsible AI are highly plausible andachievable in the near-term.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "4c2b84e6-4855-4cd0-9411-f1e60dc50d09",
                    "text": "As previously stated, a number of companies and countries havereleased ethical guidelines for the use of AI, with many overlappingconcepts. How do companies, researchers, and developers of AIsystems \u2013 all in the position of a technology licensor \u2013 implementthese guidelines? In this section, we list a few elements from theMontreal Declaration on Responsible AI [4] and show how eachguideline listed below can be operationalized in license clauses.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "02742bd8-567b-4d2f-904d-3651fec3bef6",
                    "text": "\u201cArtificial Intelligencesystems (AIS) must not be developed to spread untrustworthy in-formation, lies, or propaganda, and should be designed with a viewto containing their dissemination\u201d (Principle 2, Statement 5).Example Clause: \u201cLicensee will not use the Licensed Technology toenable the distribution of untrustworthy information, lies or propa-ganda, and if Licensee discovers that such distribution is unintention-ally occurring, Licensee will put in place countermeasures, includinghuman agents, to prevent or limit such distribution.\u201dImitation of Human Characteristics. \u201cThe development of4.1.2AIS must avoid creating dependencies through attention-capturingtechniques or the imitation of human characteristics (appearance,voice, etc.) in ways that could cause confusion between AIS andhumans\u201d (Principle 2, Statement 6).Example Clause: \u201cThe licensee will not use the Licensed Technologyin a manner that would imitate human characteristics and cause thirdparty confusion between AIS and humans.\u201d",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "c40643aa-380a-4db1-b023-f856cbf7ccb3",
                    "text": "\u201cThe integrity ofone\u2019s personal identity must be guaranteed. AIS must not be usedto imitate or alter a person\u2019s appearance, voice, or other individualcharacteristics in order to damage one\u2019s reputation or manipulateother people.\u201d (Principle 3, Statement 8)Example Clause: \u201c The licensee will not utilize the Licensed Tech-nology in applications that imitate or alter a person\u2019s likeness, voice,or other identifiable characteristics in order to damage his/her reputa-tion.\u201d",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "b9a0fd58-a3f0-4758-bdfe-48eaf4d958be",
                    "text": "\u201cAny person using a service should know if adecision concerning them or affecting them was made by an AIS.\u201d(Principle 5, Statement 8)Example Clause: \u201cThe licensee will disclose to an end-user if andwhen use of the Licensed Technology affected a decision such end-user,including by stating whether the Licensed Technology itself is an AI.\u2019As can be seen, in the examples above, statements in AI ethicalprinciples or guidelines can be transformed into license clausesand the use of such clauses can help enforce the principles laid outin such guidelines. How such license terms are phrased can varywidely, based upon the goals of the licensor, its industry, the typeof technology being licensed, the potential for misuse or harmfulapplications, and the ease of enforceability. We include an exemplarlicense based on some of these usage restrictions in Section 7.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "a494118e-3af0-4021-9135-761b3beafb8b",
                    "text": "AI FactSheets [11] are a form of Supplier\u2019s Declaration of Confor-mity (SDoC) that can be released by AI developers to describe thecharacteristics of the associated product or service. FactSheets mayinclude sections that discuss the robustness of models to adversar-ial attacks, bias due to datasets used, Optimal and Poor operatingconditions, training and test data use for evaluation, its domainof application etc. We consider the five examples available on theIBM AI FactSheets webpage to demonstrate how clauses may becreated. The models are: (i) Audio Classifier, (ii) Object Detector,(iii) Image Caption Generator, (iv) Text Sentiment Classifier, and (v)Weather Forecaster.To present exemplar clauses derived using AI FactSheets weuse one or more of the following fields: (i) Intended Domain (ii)Performance Metrics (iii) Bias (iv) Robustness (v) Domain-Shift (vi)Optimal Conditions (vii) Poor Conditions",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "3774f081-d6ae-46e5-b6fc-8794423ec5f0",
                    "text": "The Audio Classifier classifies an audiostream into day-to-day sounds such as \u201coutdoors\u201d, \u201cspeech\u201d, \u201cmusic\u201detc. It has been trained using the AudioSet Dataset [30] and usesa multi-level attention based classifier [72]. The AudioSet datasetused to train the model contains a vast majority of audio files fromYouTube that contain speech and music. The factsheet reports thata study of different voice types and music genres has not beenperformed. The factsheet further reports that the classifier onlyperforms well when the audio file contains only one or two distinctclasses and the file does not have a high degree of noise.Given the information in the factsheet and the limitations of themodel, one could specify a clause that requires adhering to optimaloperating conditions, as well as disallowing the use of the classifierfor legal and security applications, due to the sensitivity to noiseand the risk of bias in voice-types.Example Clause: \u201cLicensee will not utilize the Licensed Technologyin applications that involve the identification or classification of hu-man beings for legal or security purposes if the intent of such use isto impact such human beings\u2019 livelihoods or personal liberty.\u201d",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "ebe11350-2898-409b-8b5e-0d632eeee9d9",
                    "text": "The Object Detector recognizes 80 differ-ent classes of objects from the COCO dataset [41] using the SSD MobileNet v1 Model [34]. The factsheet reports that the trainedmodel has been studied for gender bias but has not been evaluatedfor other forms of bias. The model is sensitive to the image patternsseen during training and also exhibits performance degradationwhen image transformations add noise. Further, the model may notdetect objects or may classify them if that object-type was not seenduring training (as opposed to identifying an object with genericclass-label \u2018thing\u2019). Finally, poor resolution and lighting also resultsin low performance.Given the information provided in the factsheet, a license for aservice using this model could restrict its use in autonomous agentsdue to the nature of errors and sensitivity to lighting, and also pro-hibit use on security camera footage (which often has poor qualityand could also contain low-light footage). In addition, a clause couldalso be included that introduces certain compliance requirements- such as requiring derivative applications to prominently displaythe limited set of object classes a system may be used for.Example Clause: \u201cLicensee will not use the Licensed Technologyin the context of law enforcement, security, or surveillance. In theevent that Licensee sublicenses any of the Licensed Technology inthe permitted uses contained in the original license, Licensee willprominently display in its license terms all of the limitation of itssystem, including those contained in this license.\u201dImage Caption Generator. This model generates a caption4.2.3using a limited vocabulary to describe the contents of an image. Ithas been trained using the COCO dataset and uses the Inceptionv3 model [59]. As in the previous model, this model is known tohave exhibited gender biases due to the dataset. The model does notperform well on low-light footage and always makes a predictioneven if the object was not something seen in training data.Thus, given the information in this FactSheet, a clause could pro-hibit use in assistive technologies for the visually-impaired whichcould incorrectly describe a scene in a way that could endangerlives.Example Clause: \u201cLicensee will not use the Licensed Technology inorder to perform a visual identification function for humans that arevisually impaired.\u201d",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "d0c9c241-1779-4b1c-a868-132e9de70a04",
                    "text": "The Text Sentiment Classifier isbased on the BERT pre-trained model and it has been fine-tunedon a corpus of annotated sentences from Wikipedia. It classifies asentence based on its sentiment polarity (positive/negative). TheFactsheet indicates that the model may express racial and genderbias. This model works well when sentences are well structured likeformal Wikipedia articles and express sentiment unambiguously.The model does not work well on sentences that includes elementslike sarcasm, passive-aggressive statements, multiple polarities, orsocially or racially offensive language.Given the operational characteristics of the model, clauses that re-strict its use in automated content moderation would be applicable.It may also make sense to restrict the use of models for applicationsthat interact with text data that are quite different from Wikipediaarticles (e.g. conversational applications, social media).Example Clause: \u201cLicensee will not use the Licensed Technology inconnection with the identification of gender or race. Further, Licenseeagrees to supplement the use of Licensed Technology with humanintermediaries in contexts involving informal speech (such as sarcasmor idioms) or unfiltered social media (including but not limited toTwitter).\u201d",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "598feb98-b678-4b00-8974-efa68b2ab881",
                    "text": "The Weather Forecaster model usesweather-related time series data to predict temperature, visibility,dew point, humidity, wind speed, etc. The models have been trainedusing data from the northeastern US. The factsheet reports that themodel has very poor prediction performance on non-northeasternUS weather data.Given the limitations of this model, clauses could be defined toprevent use on data from other regions especially in applicationsthat could put lives at risk, e.g. alerts for air/sea navigation or stormalerts for evacuation.Example Clause: \u201cLicensee will not use the Licensed Technology toprovide weather prediction in any geographic location outside of thenortheastern sector of the United States.\u201dAs can be seen, information described in AI FactSheets can behelpful in defining system-specific license clauses. Recently therehave also been calls for AI researchers to consider the negativeimpacts of their work [33]. Some AI conferences such as NeurIPSnow require authors to consider the potential broader impacts ofresearch projects. We note that, similar to AI Factsheets and EthicalPrinciples, such impact statements could also be used to informrestrictions or appropriate-use clauses in licenses.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "0530ebf3-e5d9-4b61-b04a-374a5212400c",
                    "text": "The performance of AI systems is dependent on the nature of train-ing dataset(s) and the accessibility of such datasets (i.e., \u201cOpenness\").In our discussion so far, we have considered the primary IP being li-censed to be AI models. However, datasets are routinely licensed asstand-alone IP . Many widely-used datasets contain errors, reflectand reinforce problematic societal biases, and spread misinforma-tion [50] \u2013 any of which can create opportunities for harm.Due to the growing potential for irresponsible use of datasetsby AI systems, there is an accelerating effort within academic com-munities to develop mechanisms that restrict how datasets arecollected, used, and shared. One example mechanism is a Data Li-cense Agreement (\u201cDLA\u201d), which is a specific type of license thatdefines the arrangement of data exchange between a licensor and alicensee. In a typical scenario, the licensor will seek to limit use ofa dataset by the licensee by including provisions in the DLA thatidentify: 1) a specifically tailored definition of the dataset beinglicensed; 2) specific persons permitted to use the licensed dataset(i.e., the licensee); and 3) specific purpose(s) for which the licenseddataset may be used. For example, the Montreal Data License Gen-erator [15] uses a questionnaire to generate intellectual property(IP) licensing terms that can be attached to datasets to govern itsdistribution. The framework provided by the Montreal Data Licenseauthors provides a well developed starting point for attempts to use data licensing to drive responsible AI efforts. As another example,datasets hosted by Stanford\u2019s Center for Artificial Intelligence inMedicine and Imaging requires end users to agree to a \u2018ResearchUse Agreement\u2019 (e.g. [35]).Similar to other IP agreements, DLAs could be extended withresponsible use clauses, or behavioral use restrictions. Doing socould further minimize the potential for irresponsible or harmfuluse, while simultaneously maintaining the ability to derive valuefrom the dataset. The introduction of responsible use clauses orbehavioral use restrictions into DLAs may also encourage compli-ance with various state and federal regulations (e.g., data privacyregulations) governing the collection, use, and disclosure of certaindata, such as personal information. Implementation of DLAs is alsoin line with efforts to support data justice, i.e. efforts to achieve\u201cfairness in the way people are made visible, represented and treatedas a result of their production of digital data\u201d [60]. Introducing re-sponsible use clauses helps to ensure that the downstream uses ofdatasets reflect the values of those who produce data.There are several challenges that will need to be addressed tosupport early adoption of data license practices. First, some of themost important datasets are already created under various platform-dependent legal contexts. For instance, Wikipedia, which is widelyused in AI research [17, 66], is licensed as CC-BY-SA. This meansthat Wikipedia contributors are not able to decide upon a newlicense to control how the data they produce is used; the choicehas already been made by the Wikipedia community. Similarly, thedigital records produced when people use popular technologieslike search engines and social media would be difficult to license.Currently, many of the people who help produce such data (e.g. byusing a search engine) may not even realize they are generatingvaluable data, and companies would have no incentive to offer thepublic the ability to license such data.Another challenge is the possibility that data licenses will conflictwith licenses chosen by AI developers. That is, a \u201cdownstream\u201d AImodel may have a more permissive license than an upstream dataset.Negotiating these kinds of conflicts will be an important challengefor proving the viability of the data license concept.An ideal solution to license conflicts would be for AI developersand stakeholders to support democratic governance approachesto deciding on licenses and clauses. For instance, a system thatis highly dependent on data from a particular community (e.g.Wikipedia, GitHub, Mechanical Turk workers) could give inter-ested data contributors the ability to vote on various aspects of thebehavioral use license or otherwise participate in a deliberation pro-cess. This approach would be highly in line with calls to empowerpeople who do \u201cdata labor\u201d to generate valuable data [12, 66, 67].",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "c0f98912-7774-4d28-aa7d-0569bb0e806f",
                    "text": "We propose licensing as one additional tool in the growing toolkitof sociotechnical mechanisms available for actualizing responsi-ble AI goals. Licensing has attractive properties, such as providingresearchers and developers with more control over how their arti-facts are used while potentially maintaining many of the desirableproperties of openness. However, there are several challenges to,and limitations of, licenses as an approach that must be addressedbefore licensing can be a maximally effective.Adoption. Perhaps the most significant challenge facing licensingas a tool for responsible AI is adoption. There are many differenttypes of licenses that exist and norms around how they are used. Forinstance, in the case of open sourced systems, studies have shownthat the adoption of more permissive licenses leads to an increase incommunity contributions and might even result in abandonment ofcompeting projects that use more restrictive licenses [51]. A surveyon the use of open source licenses by Vendome et al. found thatdevelopers currently have strong intrinsic beliefs that affects theirchoice of licenses. Developers also face difficulties in understandinglicensing terms as well as in dealing with incompatible licenses[65].The value of behavioral use clauses must therefore be communi-cated to those releasing software. The clauses must also not conflictwith the other value people see in licenses (e.g., permitting or re-stricting redistribution of code, etc.). To help address some of thesechallenges, it may be helpful to consider the development of li-censes that correspond to existing ethical guidelines. Creating arepository of such licenses could help potential licensors selectexisting pre-defined licenses that align with the principles to whichthey want to adhere. Further, the creation of such community assetscould help with adoption by easing the burden of drafting legallysound license clauses.In addition to the creation of such repositories, licenses couldalso be made modular using license generators wherein AI devel-opers could select clauses (or ethical principles) as well as otherlicense elements, such as the terms of commercial/non-commercialdistribution, description of penalties or conditions of violation etc,that they would like to apply with the release of their AI systems.License generators would also help the groups who develop AI sys-tems to navigate challenging consensus-finding discussions aboutwhich specific uses should be restricted. Furthermore, license gen-erators could help the community coalesce around relatively simplelicenses, to mitigate potential effects where complicated licensesare difficult to enforce. As noted above, an ideal system would incor-porate the interests and perspectives of all stakeholders, includingpeople who contribute data.Incompatibility with Open Source. Certain open source licensessuch as GPL V3 require that freedoms received much be passed onwhen software or code is modified and/or re-licensed. This impliesthat any additional restrictions on use or distribution that were notoriginally present in the GPL license cannot be added. Licenses withresponsible-use clauses or usage restrictions can therefore not beapplied to any software or code that was originally distributed withlicenses such as GPL V3 making them incompatible. On the otherhand, other open-source licenses such as Apache 2.0 are more liberaland permit re-licensing of derivative works under new clauses. IPlawyers in software companies routinely scrutinize licenses whenpermitting use of software or source code for development (open-source or otherwise), and licenses with usage restrictions wouldnow need to be subject to the same legal scrutiny.Enforcement. Licenses are only useful if they are enforced. Asdiscussed earlier, licenses include consequences of breaching theterms of contract. Similar to ethics washing via ethical guidelinesand principles [16], if licenses are not enforced they run the risk of accentuating that problem. In some instances terms may bedifficult to enforce \u2013 for instance if a large corporation violatesthe responsible use clauses of an AI system shared publicly by anindependent developer (with limited resources) using a GitHubRepository. Furthermore, more complicated licenses will be moredifficult to enforce. We note that these problems are not uniqueto our approach and apply to licensing in general including open-source licenses.For AI systems offered as cloud based APIs (or similar) termina-tion of service could serve as an easier mechanism of enforcementas opposed to litigation which can be time consuming and expen-sive. This means an API-based approach could provide an appealingapproach to enforcement.To support AI developers with more limited means, researchersmight try to bootstrap collaborative communities that publiclylist violations along with details such as the date, the nature ofviolation, and response received. Beyond \u2018naming\u2019 and \u2018shaming\u2019which may have limited efficacy, such communities could engagethe services of pro-bono legal initiatives to help enforce terms ofbreaches collectively (if one entity/organization/individual has beennamed in multiple violations or if one clause of a license instancehas been violated by multiple parties), based on an assessment ofthe merits of the case, the severity of impact of violation etc.Scaffolding support for AI developers will be critical, becauseenforcement will inevitably create new responsibilities and laborfor developers, many of whom are volunteers and likely to havelimited resources for enforcement. If leaders of the responsible AIinitiatives can lower this burden, a licensing approach is likely tobe more effective.Customization. There are many possible behavioral use limita-tions people may desire and therefore heavily customized licensesmay seem attractive. However, this could lead to a complex land-scape of different behavioral licenses that might be confusing tolicensees and require extensive conversations with legal profession-als. Apart from creating clause/license repositories, as suggestedpreviously, it may be useful to consider the development of com-munity standards that define and standardize license elements tobe used in AI license. Drawing parallels from the Open Sourcecommunity - while there are hundreds of licenses hosted on theSoftware Package Data Exchange webpage [8], there are a few opensource licenses that have managed to become very popular. Wehope that similar community initiatives along with the developmentof AI licensing standards could help address some of the challengesdiscussed in this section.Reinforcing Existing Power Structures. Many legal frameworksreinforce or support existing power structures. Do license agree-ments provide a way for people to say \u201cno\u201d? We would argue thatlicenses in the form we propose do not provide a direct mechanismfor people to say \u201cno\u201d, while still receiving the benefits that thecode or data being distributed may offer. Therefore, people thatdecline to agree to these terms may be disadvantaged as a result.We recognize that this inability to say \u201cno\u201d can lead to people beingdeprived of their will [9] and that the politics of refusal [20, 28]are very important to consider. Licenses do not directly addressthese issues, and may be viewed as entrenching them. However,we would also argue that behavioral use licenses for code and datado acknowledge that these assets are in need of interpretation (theimportance of which is highlighted in [20]) and that restrictingcertain behaviors can be viewed as a way of helping make thatinterpretation more transparent and concrete.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "7040064f-bf07-4d9e-be41-9f8c741e0223",
                    "text": "Distribution of code, data, models, and access to APIs leaves re-searchers and developers in a conundrum. On one hand, this dis-tribution fosters democratization, verification, reproduction, andadvancement of scientific knowledge. On the other hand, these as-sets could be used for purposes that the developers never intended;some of these might be harmful or irresponsible. These issues arecompounded in light of large-scale pre-trained models (such asGPT-3) that can be used for many downstream applications butrequire vast amounts of training data and computational resources.Sharing access is the only way that people would be able to buildon such a system in practice. Given that there is a well-establishedframework for IP licensing that exists today, we believe that licens-ing could be a powerful tool for developers in preventing negativeimpacts of technologies immediately. The example clauses we haveidentified apply to a range of different types of AI or machine learn-ing systems. We believe that normalizing the use of behavioralrestrictions via licenses will encourage responsible use of powerfulAI tools and systems by downstream users, while recognizing thatthere are limitations in enforcing such terms.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "e62942b4-7372-40e3-ba67-bef4d65151d3",
                    "text": "This section presents a license that has been adapted from theApache 2.0 license to include usage restrictions. The \u201cDefinitions\u201dstandardize language throughout the license, while the clauses in\u201cRestrictions\u201d include some examples of restrictions from our casestudy in Section 4.1. As this example shows, licensing frameworksare fairly flexible and they can support the inclusion of customuse-case and software-specific clauses with ease. The section onrestrictions is prefaced with text which ensures that downstreamapplications as well as other modification/use/redistribution con-tinues to include the restrictions. This approach is similar to theone taken by open source licenses such as GPL V2 to ensureredistribution/re-use remains open-source. \u201cTerminations\u201d showrequirements to be fulfilled in the event of a breach of the licenseterms. The remaining sections includes text that is typically presentin software licenses to minimize risk arising from legal liabilities.Exemplar license text below:",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "59c4f5cd-7333-4064-a02b-6af35f7f3c71",
                    "text": "",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "38614a3c-76c7-4909-bcc6-1b67bff40028",
                    "text": "",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "43571f14-defb-4220-9f37-1f3c799e9e6c",
                    "text": "",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "2ffbd329-749d-4e28-a3f0-74abd961d7a4",
                    "text": "",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "8c951c9a-c21d-492b-ae51-bc998075e5b7",
                    "text": "",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "b8a44218-e97f-4e70-ab2a-909d449bbd4a",
                    "text": "",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                },
                {
                    "id": "e6c0ff9b-e688-47c9-9d0b-a5cc5c1bae7e",
                    "text": "ACKNOWLEDGMENTSWe would like to thank Stephen Ibaraki for his continuous sup-port during the course of this work. We would also like to thankFrancesca Rossi, Michael Hind, Sriram Raghavan, and Jim Spohrerfor their helpful feedback and suggestions.Funding/Support: The authors declare no additional sources offunding.",
                    "reference": "[1] Daniel McDuff, Jaron Lee, and Jeannette K. Haines. 2022. Behavioral use licensing for responsible AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). ACM, New York, NY, USA, Article 35, 1-10. https://doi.org/10.1145/3531146.3533143"
                }
            ]
        },
        {
            "paper_title": "Software engineering for responsible AI: An empirical study and operationalised patterns",
            "authors": "Q Lu, L Zhu, X Xu, J Whittle, D Douglas\u2026",
            "publication_info": "Proceedings of the 44th \u2026 - dl.acm.org",
            "paper_url": "https://arxiv.org/pdf/2111.09478",
            "chunks": [
                {
                    "id": "bf878221-890d-4f48-a1bc-574fc8b94c07",
                    "text": "Although arti\ufb01cial intelligence (AI) is solving real-world challenges and transforming industries,there are serious concerns about its ability to behave and make decisions in a responsible way. ManyAI ethics principles and guidelines for responsible AI have been recently issued by governments,organisations, and enterprises. However, these AI ethics principles and guidelines are typicallyhigh-level and do not provide concrete guidance on how to design and develop responsible AIsystems. To address this shortcoming, we \ufb01rst present an empirical study where we interviewed 21scientists and engineers to understand the practitioners\u2019 perceptions on AI ethics principles and theirimplementation. We then propose a template that enables AI ethics principles to be operationalised inthe form of concrete patterns and suggest a list of patterns using the newly created template. Thesepatterns provide concrete, operationalised guidance that facilitate the development of responsible AIsystems.arti\ufb01cial intelligence, AI, machine learning, responsible AI, ethics, software engineering, software architecture, DevOps",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "fe91350b-d48b-4e2e-9938-fe4be3d6b47d",
                    "text": "Arti\ufb01cial intelligence (AI) continues demonstrating its positive impact on society and successful adoptions in data richdomains. The global AI market was valued at approx. USD 62 billion in 2020 and is expected to grow with an annualgrowth rate of 40% from 2021 to 2028 [58]. Although AI is solving real-world challenges and transforming industries,there are serious concerns about its ability to behave and make decisions in a responsible way.To achieve responsible AI, both ethical and legal aspects may need to be considered. As law is usually considered to setthe minimum standards of behaviour while ethics establishes the maximum standards, throughout this paper we usethe terms responsible AI, ethical AI and ethics to cover the broader set of requirements. Trustworthy AI refers to AIsystems that embody the responsible AI principles and requirements [53]. Many AI ethics principles and guidelinesfor responsible AI have been recently issued by governments, organisations, and enterprises [26, 35]. However, theseprinciples are typically high-level and do not provide concrete guidance on how to develop responsible AI systems. Toaddress this, we pose the research questions listed below.RQ1: What are the current states and potential challenges developers are facing in dealing with responsible AIissues during the development of AI systems? We perform an empirical study where we interviewed 21 AI scientistsand engineers with various backgrounds and expertise. We asked participants what ethical issues they have consideredin their AI projects and how the ethical issue were addressed or they envisioned can be addressed. Based on theinterview results, we reveal several major \ufb01ndings: (1) The current approach is often a done-once-and-forget type ofrisk assessment at a particular development step, which is not suf\ufb01cient for the highly uncertain and continual learningAI systems; (2) Responsible AI requirements are either omitted or mostly stated as high-level objectives, and notspeci\ufb01ed explicitly in veri\ufb01able way as system outputs or outcomes; (3) Although responsible AI requirements have thecharacteristics of cross-cutting quality and non-functional requirements amenable to architecture and design analysis,system-level architecture and design are under-explored; (4) There is a strong desire for continuous monitoring andA - N 19, 2021Figure 1: Overview of the employed methodology.validation of AI systems post deployment for responsible AI requirements, where current MLOps/AIOps practicesprovide limited guidance.RQ2: How can AI ethics principles be operationalised into concrete practice that AI developers can use through-out the lifecycle of AI systems? We design a pattern template that enables AI ethics principles to be operationalised inthe form of concrete patterns. We then suggest a list of process and design patterns using the newly created templatethroughout the lifecycle of an AI system based on the results of the interviews, literature review, as well as existingsoftware development and design practices.The major contributions of our study are as follows:\u2022 To the best of our knowledge, this is the \ufb01rst in-depth study that explores practitioners\u2019 perceptions on AI ethicsprinciples and their implementation.\u2022 We identify the AI system development and operation (AIOps/MLOps) process that integrates with the AI modeldevelopment process that includes data collection, feature engineering, model training, evaluation and updates.\u2022 We propose a template to de\ufb01ne patterns for operationalising responsible AI and summarise a list of ethical assurancepatterns using the newly designed template throughout the lifecycle of an AI system. The patterns provide a concrete,operationalised guidance that can be easily applied and extended by AI developers to develop responsible AI systems.We continue the paper as follows. Section 2 overviews the methodology. Section 3 identi\ufb01es the development process.Section 4 discusses the \ufb01ndings. Section 5 suggests a list of patterns. Section 6 discusses the threats to validity. Section 7covers related work. Concluding remarks are given in Section 8.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "c8172ad5-9d46-41b0-88d3-582271434169",
                    "text": "An overview of the methodology is given in Fig. 1. The major \ufb01ndings were extracted through interviews, while the AIsystem development process and ethical assurance patterns were identi\ufb01ed based on the interview results, literaturereview on software engineering for AI and machine learning (SE4AI/SE4ML), and existing software engineering (SE)practices.The interviewees were from a research institute and sought via \u201ccall for participation\u201d emails as well as via follow-uprecommendations given by the interviewees, until a saturation of perspectives were reached. 21 interviews wereconducted from February to April 2021. The interviewees are from various backgrounds, with a large variation inthe interviewees\u2019 degree of experience and responsibility. 10 interviewees worked primarily in computer science,6 interviewees worked in the health & biosecurity area, and 5 interviewees worked in the land & water area. The jobpositions of the interviewees included: postgraduate student (1), research scientist (1), senior research scientist (4),principal research scientist (2), principal research engineer (1), team leader (8), group leader (4). The gender split wasapproximately 24% females and 76% males.The interviews were conducted by three project team members with various research backgrounds (machine learning,software engineering, ethics in AI, respectively), in a face-to-face setting and/or via video teleconferencing. Prior to2 A - N 19, 2021Table 1: Incidence of themes related to AI ethics principles.PrinciplePrivacy Protection & SecurityReliability & SafetyTransparency & ExplainabilityAccountabilityContestabilityFairnessHuman-Centred ValuesHuman, Societal and Environmental (HSE) Wellbeing Incidence17 / 21 (81%)19 / 21 (90%)18 / 21 (86%)13 / 21 (62%)8 / 21 (38%)10 / 21 (47%)3 / 21 (14%)11 / 21 (52%)each interview, each interviewee was given a summary of Australia\u2019s AI ethics principles [20] (as shown in Fig. 2),to ensure all interviewees are aware of the principles. The interviews ranged from approximately 22 to 59 minutesin length, with a median length of approximately 37 minutes. We followed the methodology employed in [3] to stopinterviews when saturation of \ufb01ndings was reached.The transcripts were analysed using theoretical thematic analysis [16]. This analysis used a theoretical approach tocoding the interview data by using the eight AI ethics principles as themes. Concepts identi\ufb01ed in discussions ofspeci\ufb01c principles were recorded as sub-themes related to that principle. We summarised the \ufb01ndings based on theinterview analysis data. Table 1 shows the incidence of themes related to AI ethics principles across the interviews. Thetop three principles covered in the interviews are Reliability & Safety, Transparency & Explainability, and PrivacyProtection & Security. Principles which were covered in roughly half the interviews are Accountability, HSE Wellbeing.The Human-Centred Values principle was covered the least in the interviews.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "29089bc7-5f4c-4540-adf5-1d9d973f6dea",
                    "text": "Fig. 3 illustrates an overview of AI development process. The process starts with requirement analysis. In this phase,we need to identify the requirements and constraints placed by stakeholders. In recent years, responsible software,responsible technology and human values in software has become an important \ufb01eld of study [72]. Responsible/ethicalAI (system) is a sub-\ufb01eld within the responsible technology (software) \ufb01eld. However, compared with traditionalsoftware, AI systems also need to consider requirements about models, training data, system autonomy oversight andmay emphasise certain ethical requirements more due to AI-based autonomous and potentially opaque behaviour anddecision making.Figure 2: An adapted summary of 8 voluntary high-level ethics principles for AI, as promulgated by the AustralianGovernment [20]. 3 A - N 19, 2021Figure 3: AI system development process.Once the requirements are identi\ufb01ed, the process is divided into two sub-process for non-AI part and AI part, respectively.The non-AI part sub-process includes design, implementation, and testing of non-AI components. The AI part sub-process is the AI development process for model production, which covers data engineering, feature engineering,model training, model evaluation and updates. The converged phase for non-AI part and AI part is the deploymentand operation of the AI system. Some key differences in the deployment and operation of AI systems are often thecontinual learning of AI components based on new data, the higher degree of uncertainty and risks associated withthe autonomy of the AI component, and validation of outcomes (i.e. did the system provide the intended bene\ufb01ts andbehave appropriately given the situation?) rather than just outputs (e.g. precision, accuracy and recall) [10].",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "8bd5e7ec-35ab-4b17-95d1-074ba0418f79",
                    "text": "In this section, we report our \ufb01ndings for each of the categories that were identi\ufb01ed using open card sorting on interviewcontents. For each category, we select the most meaningful comments and highlight our observations.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "e8e7f960-5734-4386-a1ca-2caaa8b8b015",
                    "text": "Understanding and managing risk is particularly important for AI systems as they may behighly uncertain and may involve continual learning. We found some ethical risk assessment frameworks were usedin practice. One interviewee stated \u201cThere was a privacy impact assessment. We went through a lengthy process tounderstand the privacy concerns and build in provisions to enable privacy controls and people to highlight things thatthey didn\u2019t want to be visible\u201d (P10). However, such kind of approach is a done-once-and-forget type of risk assessmentand not suf\ufb01cient for AI systems that are highly uncertain and continually learn. Furthermore, various practitionersapproach risk differently. One interviewee suggested fail-safe by design should be considered and noted that \u201cthere\u2019sonly so much you can think ahead about what those failure modes might be\u201d (P16). One interviewee argued \u201cOnce Iknow that it works most of the time I don\u2019t need explainability, I don\u2019t need transparency. It\u2019s just temporary to establishthe risk pro\ufb01le\u201d (P11).Finding 1: The current practice is a done-once-and-forget type of risk assessment at a particular development step,which is not suf\ufb01cient for the highly uncertain and continual learning AI systems.Trust vs. trustworthiness. Trustworthiness is the ability of an AI system to meet AI ethics principles, while trustis users\u2019 subjective estimates of the trustworthiness of the AI system [77]. Even for a highly trustworthy AI system,gaining the trust from humans is another challenge that must be addressed carefully for the AI system to be widelyaccepted. This is because a user\u2019s subjective estimates of the AI system\u2019s trustworthiness may have a signi\ufb01cant gapcompared to the AI system\u2019s inherent trustworthiness. It can also be the other way around when a user overestimates asystem\u2019s trustworthiness and put excessive trust into it. We found many interviewees have recognised the importanceof human trust in AI. One interviewee stated: \u201cA lot of the work that we do trust comes as an important factor here,4 A - N 19, 2021that a user or somebody who takes that information, wants to be able to trust it\u201d (P9). One of the obstacles for thedevelopment of AI systems is gaining and maintaining the trust from the data providers. One interviewee noted \u201cyoubuild the trust with the data providers, so more people can give you data and increase your data representability\u201d (P2).One interviewee pointed out evidence need to be offered to drive trust: \u201cBecause you justi\ufb01ably want to trust thatsystem and not only ask people do you trust it? I mean they need some evidence. You can build this into your system tosome degree. So that\u2019s very important\u201d (P12).Finding 2: The inherent trustworthiness of an AI system for various ethical principles and the perceived trust of thesystem are often mixed in practice. Even for a highly trustworthy AI system, gaining the trust from humans is achallenge that must be addressed carefully for the AI system to be widely accepted. Process and product mechanismscan be leveraged to achieve trustworthiness for various ethical principles, whereas process and product evidence needto be offered to drive trust.Ethical credentials. AI industry requires responsible AI components and products at each step of the value chain. AIsystem vendors often supply products by assembling commercial or open-source AI and/or non-AI components. Someinterviewees agreed credential schemes can enable responsible AI by attaching ethical credentials to AI components andproducts. One interviewee commented \u201cGetting those certi\ufb01cates, it always helps. As long as there is standardisationaround it.\u201d (P13). There have been certi\ufb01cates for the underlying hardware of AI systems. One interviewee pointed out\u201cA lot of hardware is actually certi\ufb01ed. I mean in (...) full size aviation. you have at least a certi\ufb01cation. So when youbuy something you get some sort of guarantees\u201d (P12).Finding 3: Human trust in AI can be improved by attaching ethical credentials to AI components/products since thevendors often supply products by assembling commercial or open-source AI or non-AI components.Requirement-driven development vs. outcome-driven development. We observed there are two forms of develop-ment mentioned by the interviewees: requirement-driven development and outcome-driven development [15]. Amongthe ethical requirements/principles, privacy and security is one of the most discussed requirements. One intervieweenoted privacy requirements: \u201cTo protect those privacy and de-identi\ufb01cation requirements, you\u2019ll be aggregating so thatpeople can\u2019t be uniquely identi\ufb01ed\u201d (P1). In relation to outcome-driven development, one interviewee emphasised thedevelopment is a continual process: \u201cThis is a continual and iteration process, human need to continually to evaluatethe performance, identify the gap and provide insight into what\u2019s missing. Then go back to connect data and re\ufb01ne themodel\u201d (P2).Finding 4: Developing AI systems requires seamless integration of requirement-driven development and outcome-driven development.End-to-end system-level development tools. An AI system consists of AI components and non-AI components thatare interconnected and work together to achieve the system\u2019s objective. An AI model needs to be integrated with thesystem to perform the required functions. Combining AI and non-AI components may create new emergent behaviorand dynamics. Therefore, ethics need to be considered at system-level, including AI components, non-AI componentsand their connections. For example, the effect of actions decided by the AI model could be collected through thefeedback component built into the AI system. Although most of the interviewees are research scientists/engineers whomainly worked on research projects and focused on model development, some of them did recognise the signi\ufb01cance ofsystem-level thinking in AI projects. One interviewee commented \u201cWell, it\u2019s just that the design ways in which that AIwas designed and deployed as an end-to-end solution, it wasn\u2019t that AI sat in the middle, right? It actually had to sitwithin the system\u201d (P14). We also found that the management of AI ethics principles was heavily relied on manualpractice. One interviewee pointed out \u201cWe had to go through a lot of data and make sure that there was not a singleframe with a person in it\u201d (P13). This accidental collection of sensitive data issue could be addressed automaticallyusing AI enabled human detection tools.Finding 5: An AI model needs to be integrated with the system to perform the required functions. Combining AI andnon-AI components create new emergent behaviour and dynamics, which require system-level ethical consideration.Implementation of AI ethics principles still heavily relied on manual operations. There is lack of end-to-enddevelopment tools to support continuous assurance of AI ethics.5 A - N 19, 2021",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "56b4335e-6e85-4023-89c8-93875d0dfbee",
                    "text": "We found some ethics principles, such as human, societal, environmental well-being, weresometimes omitted and stated only as a project objective rather than veri\ufb01able requirements or outcomes. Oneinterviewee stated \u201cPeople are presented with a clear project objective upfront, and the project leader might framethe project with we\u2019re working on improving [a grass specie] yield forecasting using machine learning. You do feelgood about working on projects that provide environmental bene\ufb01t\u201d (P9). Ethical AI requirements need to be analysed,veri\ufb01ed and validated by a wide range of experts beyond the software developers (such as hardware engineers, cultureexpert, end users).Finding 6: Responsible AI requirements are either omitted or mostly stated as high-level objectives, and not speci\ufb01edexplicitly in a veri\ufb01able way as expected system outputs (to be veri\ufb01ed/validated) and outcomes (e.g. bene\ufb01ts).Requirements engineering methods need to be extended with ethical aspects for AI systems.Scope of responsibility. Based on our interview results, we found that there were various perceptions on the meaningof responsible AI. One interviewee raised a question of meaning of responsibility in the context of autonomous dronesystems \u201cThe question is what happens if [the] remote pilot is really there, \ufb02icks the switch [to disable the system] andthe system doesn\u2019t react? The remote pilot is not always in full control of [the drone] because of technical reasons[such as a failed radio link]\u201d (P12). The various meanings and interpretations of the word \u201cresponsible\u201d have alreadyreceived considerable attention. Tigard et al. [69] introduce three varieties of responsibility, including the normativeinterpretation (i.e. behaving in positive, desirable and socially acceptable ways), the possessive interpretation (i.e.having a duty and obligation) and descriptive interpretation (i.e. worthy of a response - answerable). Lima et al. [38]summarise eight meanings of responsibility. We observe interviewees touched on all the three varieties of Tigard\u2019smeanings [69] and considered all of them as important. Furthermore, timeliness needs to be considered for responsibility.One interviewee commented \u201cwhether the stuff works in 10 years, it\u2019s not under our control (...) and we shouldn\u2019treally care about it\u201d (P11).Finding 7: The various meanings and interpretations of the word \u201cresponsible\u201d have already received considerableattention. There are three varieties of responsibility including the normal interpretation, the possessive interpretation,and descriptive interpretation.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "f64305a7-6d18-482f-aba8-023aa656cd4b",
                    "text": "AI is an uncertain and complex technology which is often hard to explain thus making detailed riskassessment dif\ufb01cult. One interviewee commented \u201cWhen do you have a complete assessment really? Especially withsystems that change over time and based on sensory input. [...] It\u2019s very dif\ufb01cult\u201d (P12). Adopting AI or not can beconsidered as a major architectural design decision when designing a software system. An architect can also designan AI component that can be switched off during run-time or changed from decision mode to suggestion mode. Itis necessary to let humans make judgement throughout the lifecycle of an AI system, e.g. whether to adopt AI indesign or whether to accept the recommendations made by the AI systems. One interviewee explained overriding therecommended decisions with an clinical system example: \u201cthere was actually a de\ufb01ned process where if a patient wasnot \ufb02agged as being high risk, [...] clinicians were still allowed to include the patient into the next step clinical review\u201d(P18).Finding 8: AI is an uncertain and complex technology which is often hard to explain, thus making detailed riskassessment dif\ufb01cult. Adopting AI can be considered as a major architectural design decision when designing a softwaresystem. Furthermore, an AI component can be designed to be \ufb02exibly switched off at run-time or changed fromdecision mode to suggestion mode.Trade-offs between ethical principles in design. Several interviewees pointed out there are trade-offs between ethicsprinciples (e.g. privacy vs. reliability/accountability, fairness vs. reliability). One interviewee commented \u201cIf you\u2019ve gotother ways of protecting privacy that don\u2019t involve aggregating, then you can be actually getting better distributionalproperties\u201d(P01). However, there was not much discussion about the methods on how to deal with the trade-offs.The reliability of AI depends on the quantity and quality of the training data. One interview noted that \u201cif you\u2019retraining a model without a lot of data, you can actually get some really weird results\u201d (P9). Obtaining a suf\ufb01cientnumber of samples can be challenging, as obtaining one sample can be high in terms of both \ufb01nancial/time cost and6 A - N 19, 2021privacy issues in domains such as genomics (P3). Federated learning was mentioned to deal with privacy and securityethical concerns in addition to the data hungriness issues \u201cdifferent research institutions from around the world cancollaborate, because they don\u2019t have to give up their data. They don\u2019t have to share their data\u201d (P3). There was adesire to use such architecture styles and speci\ufb01c patterns to handle some ethical AI requirements.Finding 9: There are trade-offs between some AI ethics principles. The current practice to deal with the trade-offs isusually the developers following one principle while overwriting the other rather than building balanced trade-offswith stakeholders making the ultimate value and risk call.Design process for ethics. We found the reuse of models and other AI pipeline components is desired since trainingmodels and building various components in the model development pipeline is time-consuming and costly. There wasextension to the reuse of the overall architecture and design of the AI system due to its dependency on the costly andcomplex pipeline. Similar issues were reported in literature regarding architecture degradation and accumulating ofhigh technical debt over time [63]. However, the ethical AI consequence of the reuse was not well understood. Oneinterviewee highlighted \u201cWhat we have gone beyond the project we hope to achieve is we\u2019ll have the whole pipeline inplace. Once we have different data from a different environment that\u2019s not associated to that particular company thatthey labelled and they recorded. We already have something in place that we can train with different data. As long asit\u2019s not the same data - it\u2019s a different type of environment - that\u2019s \ufb01ne\u201d (P13). It would be helpful to develop modellinglanguages and representation tools to capture various ethical concerns and represent the AI components to stakeholdersto improve explainability. The representation is not only about model structure, maybe depending on who developerswork with, show various views and ethical concerns of AI components.Finding 10: Although responsible AI requirements have the characteristics of cross-cutting quality and non-functionalrequirements amenable to architecture/design analysis and reusable patterns, they were under explored in the projects.Design for explainability and interpretability. Explainability and interpretability are two emerging quality attributesfor AI systems. We found some interviewees have considered explainability and interpretability in practice and adoptedhuman-centred approaches taking into account users\u2019 background, culture, and preferences to improve human trust.Explainability de\ufb01nes being able to come up with features in an interpretable domain that contribute to some explanationabout how an outcome is achieved. The recommendations made by the AI systems are often not that useful to assistusers to make decisions, unless the system shows the indicators and factors for why that prediction was given. Oneinterviewee noted that \u201cthere have been instances where we\u2019ve chosen an explainable model which has slightly loweredperformance to a non-explainable model which has higher performance but would be harder to convey the reasoningbehind the prediction\u201d (P18). Interpretability is the ability of an AI system to provide a understandable descriptionof a stimulus (e.g., model output) in stakeholders\u2019 familiar terms. One interviewee stated \u201cI\u2019m really experimentingnow with how we actually show the data so that it can be interpreted by people? So we\u2019re playing around with datavisualisation tools now to say how do we bring that data to bear and going out there and saying does this make sense toyou? We designed all these reports which just show the data in different ways and part of that was - do you like the waythis is going or is there things you\u2019d like to see?\u201d (P14).Most of the actions for explainability that were discussed by the interviewees were around the interface design of AIsystems. One interviewee commented \u201cThat interface was really responsible behind - nobody seems to ask about,what\u2019s the predictive performance of the algorithm [in the initial stakeholder meeting]? It\u2019s around, can I look at yourinterface and look at - see a couple of patient risk pro\ufb01les and then understand that.\u201d (P18).It is necessary to calibrate trust over time to match AI systems\u2019 trustworthiness. One interviewee stated \u201cThere isno need to explain anything if you know the risk and and if you have a long enough time to look over it. So thisexplainability thing, it\u2019s just a temporary requirement until the risk is known\u201d (P14).Finding 11: Human-centred approaches have been adopted for explainability and interpretability taking into accountusers\u2019 background and preferences to improve human trust in AI.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "8ed1027c-9417-4ffd-8719-0c02b11d4a1a",
                    "text": "There is a strong desire for continuously monitoring and validating AI systemspost deployment for ethical requirements. One interviewee commented: \u201cIt\u2019s up to us to come with technology that7 A - N 19, 2021makes it acceptable for them to implement measurements in that respect and being able to prove compliance or evensignal a trend like you\u2019re compliant now, but because we can see that your [values] are slowly going up and that\u2019s yourthreshold, so you\u2019re approaching it\u201d (P7). Awareness of potential mismatches between training data and real-worlddata is necessary to prevent the trained model from being unsuitable for its intended purpose (P4). Model update orrecalibration on new data were seen as important for the reliability of AI systems. The models may need to be retrainedor recalibrated to properly take advantage of user feedback, newer and/or more comprehensive data which was notconsidered during the initial deployment. One interviewee noted \u201cIf you build a model on 10 year old data, thenyou\u2019re not representing the current state of risks for certain disease. As a minimum, [recalibration] on new data wouldprobably be more meaningful\u201d (P18). In addition to reliability, continuous validation and improvement of other ethicsprinciples may occur at run-time. System-level updates is necessary to address unethical issues.Finding 12: There is a strong desire for continuously monitoring and validating AI systems post deployment forresponsible AI requirements but current MLOps practices provide limited guidance.Traceability of artifacts. One approach often identi\ufb01ed by the interviewees are related to traceability, provenanceand reproducibility, which are useful to building trust in AI systems. It is necessary to track the use of an AI systemand model provenance to improve explainability and accountability.One interviewee mentioned \u201cThings that I wason that had very \u2013 very strict rules about the provenance. So basically, every piece of code and every output had togo somewhere and have metadata tagged with it, so that if anyone wanted to audit what we did they could\u201d (P4).It is well perceived that version control and immutable log are important for model provenance. One intervieweementioned \u201cWhen the system gets complex, you have to keep more evidence along the way. Version control, and theimmutable log. You don\u2019t want people to tamper this since after things went wrong\u201d (P2). This improves both thetrust and trustworthiness of AI systems. We found most of the interviewees used Git repository management softwaretools (e.g. GitHub or Bitbucket) for code version code. \u201cAny software we are developing is in Bitbucket, internalcon\ufb01guration management system\u201d (P17). However, an AI system usually involve co-evolution of data, model, code,and con\ufb01gurations. Thus, data/model/code/con\ufb01guration co-versioning with model dependency speci\ufb01cation is neededto ensure data provenance and traceability. If AI models are based on domain knowledge models, the underlying domainknowledge models need to be co-versioned with the AI models. There has been a lack of tools to use these traceabilityand provenance data to help with ethical AI concerns.Finding 13: An AI system usually involve co-evolution of data, model, code, and con\ufb01gurations. Data / model / code /con\ufb01guration co-versioning with model dependency speci\ufb01cation is needed to ensure data provenance and traceability.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "a738c75c-12ab-4e27-9a46-c841f8c5cb1c",
                    "text": "To operationalise responsible AI, as shown in Fig. 4, we de\ufb01ne a pattern template which provides an integrated viewof the the following aspects: categorisation, scope, alignment with AI ethics principles, mechanisms, applicability,consequences, and know uses. In Fig. 5-6 we summarise a list of operationalised responsible AI assurance patternsusing the newly de\ufb01ned template based on the interview results, literature review, and existing software developmentpractices.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "3e042aa6-e2dd-4bf8-adba-f993f7fbfbb5",
                    "text": "In our study, the interviewees were selected via \u201ccall for participation\u201d emails and recommendations within oneorganisation. Although selection bias is always a concern when the interviewees are not randomly sampled, theprocedure partially alleviates the threat since the interviewers have no contact with interviewees before the interviews.Furthermore, given that our interviews include practitioners with various backgrounds, roles, and genders, the threat haslimited in\ufb02uence.We stopped our interviews when we achieved a saturation of \ufb01ndings after interviewing 21 persons. To avoid the riskof missing information and interviewer subjectivity, each interview included three interviewers with various researchbackgrounds. The three interviewers worked together to ask questions and take notes during interviews. This canaid in reducing the likelihood of subjective bias on whether the saturation of \ufb01ndings has been achieved, as well asmaximising the capture of as much relevant data as possible.8 A - N 19, 2021Figure 4: Template of patterns for responsible AI.The operationalised patterns we recommended may not cover all the existing solutions for some of the developmentstages and AI ethics principles, e.g. testing, technologies for reliability, as they have been well studied in signi\ufb01cantliterature. The emphasise of our work is mainly on the stages and ethics principles that are still under explored and hardto be operationalised, e.g. requirement engineering, architecture design, and DevOps.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "4c19cf49-e947-4602-9b74-1e1021e689fe",
                    "text": "This study is conducted within one organisation, which may introduce a threat to external validity. While we recognisethat having more organisations would be desirable, we believe our study is generalisable to most AI system developmentteams. All the interviewees are from a national science agency with teams working on multiple areas serving variouscustomers, and having various products/projects and cultures. We acknowledge that the opinions provided by ourinterviewees may not be representative of the whole community. To reduce this threat, we ensured that our intervieweeshold various roles and have various levels of expertise. We believe that their opinions and comments uncovered variousinsights into the challenges developers are facing in dealing with responsible AI issues during development.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "49a7c494-13a7-493e-89cd-ff9346635732",
                    "text": "The challenge of responsible AI has gathered considerable attention [76]. Nearly 100 high-level principles andguidelines for AI ethics have been issued by governments, organisations, and companies [35]. A degree of consensusaround high-level principles has been achieved [26]. Certain AI ethics principles, such as privacy & security, reliability& safety, and fairness, can be considered as software quality attributes. Security, reliability and safety are well-studiedin the dependability research community [5] and can be speci\ufb01ed as non-functional requirements to be considered in the9 A - N 19, 2021Figure 5: Operationalised patterns for responsible AI \u2014 part 1.10 A - N 19, 2021Figure 6: Operationalised patterns for responsible AI \u2014 part 2.11 A - N 19, 2021development. There are reusable design methods (e.g., patterns) that could be applied to address these principles [11].Although privacy is not a standard software quality attribute [34], it has been increasingly taken into consideration as animportant requirement of a software system in the design to conform with regulation, e.g., General Data ProtectionRegulation (GDPR) [33]. Patterns have been summarised to address privacy concerns and realise privacy-by-design [57].Fairness is a quality attribute that AI developers should consider throughout the AI system lifecycle. Many methods andtools have been introduced into AI pipeline to achieve fairness more at model-level rather than system-level [43], suchas IBM\u2019s AI Fairness 360 [31], Microsoft Fairlearn [14], and Linkedin Fairness Toolkit [70].Human, societal and environmental wellbeing, as well as human centered values can be treated as functional re-quirements. However, there is lack of work on the operationalisation of these two principles. For example, for thehuman-centered values principle, which values are considered and how can these values be designed for, implementedand tracked in an AI system. Risk mitigation mechanisms and the existing approaches on operationalising humanvalue in software can be applied to achieve these two principles in AI systems [47, 72]. Transparency & explainability,contestability, and accountability can be viewed as meta-level governance-related meta-level functional requirements.New design and process patterns are needed to ful\ufb01l these principles, particularly from a governance perspective.Overall, AI ethics principles need to be operationalised in the form of concrete practices that are usable by AI developerswhen developing AI systems. Although OECD provides a tool framework for trustworthy AI [53], the frameworklargely contains categorised but disjointed software tools and guidelines, lacking process-related linkages and the trustside in addition to trustworthiness. Thus, an operationalised guidance for developers is required throughout the entirelifecycle of AI systems.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                },
                {
                    "id": "63bf3733-0c3b-472f-914c-776d846a635d",
                    "text": "AI ethics principles are typically high-level and do not provide concrete guidance to developers on how to develop AIsystems responsibly. In this study, we \ufb01rst perform an empirical study to understand the practitioners\u2019 perceptions onAI ethics principles and their implementation. We then suggest a list of patterns to provide a concrete, operationalisedguidance that are usable by AI developers to develop responsible AI systems.",
                    "reference": "[1] Qian Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Damith C. Ranasinghe, and David Douglas. 2022. Software engineering for responsible AI: An empirical study and operationalised patterns. Proceedings of the 44th International Conference on Software Engineering (ICSE 2022). https://arxiv.org/pdf/2111.09478"
                }
            ]
        },
        {
            "paper_title": "Data cards: Purposeful and transparent dataset documentation for responsible ai",
            "authors": "M Pushkarna, A Zaldivar, O Kjartansson",
            "publication_info": "Proceedings of the 2022 ACM \u2026 - dl.acm.org",
            "paper_url": "https://dl.acm.org/doi/pdf/10.1145/3531146.3533231",
            "chunks": [
                {
                    "id": "c1f2b0a2-7891-4969-9610-6cc421ef03fa",
                    "text": "As research and industry moves towards large-scale models capable of numerous downstream tasks, the complexity of understanding multi-modal datasets that give nuance to models rapidly increases. A clear and thorough understanding of a dataset\u2019s origins, devel-opment, intent, ethical considerations and evolution becomes a necessary step for the responsible and informed deployment of models, especially those in people-facing contexts and high-risk domains. However, the burden of this understanding often falls on the intelligibility, conciseness, and comprehensiveness of the doc-umentation. It requires consistency and comparability across the documentation of all datasets involved, and as such documentation must be treated as a user-centric product in and of itself. In this paper, we propose Data Cards for fostering transparent, purposeful and human-centered documentation of datasets within the prac-tical contexts of industry and research. Data Cards are structured summaries of essential facts about various aspects of ML datasets needed by stakeholders across a dataset\u2019s lifecycle for responsible AI development. These summaries provide explanations of pro-cesses and rationales that shape the data and consequently the models\u2014such as upstream sources, data collection and annotation methods; training and evaluation methods, intended use; or deci-sions affecting model performance. We also present frameworks that ground Data Cards in real-world utility and human-centricity. Using two case studies, we report on desirable characteristics that support adoption across domains, organizational structures, and au-dience groups. Finally, we present lessons learned from deploying over 20 Data Cards.CCS CONCEPTS\u2022 Social and professional topics \u2192 User characteristics; \u2022General and reference \u2192 Evaluation; \u2022 Software and its en-gineering \u2192 Software creation and management; \u2022 Human-centered computing;KEYWORDSdata cards, dataset documentation, transparency, responsible AI,datasheets, model cards",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "06e25bd9-b2c5-4dd4-95dc-46a3e6c78193",
                    "text": "The challenge of transparency in machine learning (ML) models anddatasets continues to receive increasing attention from academiaand industry [1, 2]. Often, the goal has been to attain greater vis-ibility into ML models and datasets by exposing source code [4],contribution trails [8], introducing ML-drive data analysis meth-ods [19], and introducing diverse oversight [18]. Transparency andexplainability of model outcomes through the lens of datasets hasbecome a huge concern in regulation from government bodies inter-nationally. However, attempts to introduce standardized, practicaland sustainable mechanisms for transparency that create value atscale meet limited success in research and production contexts. Thisreflects real world constraints of the diversity of goals, workflows,and backgrounds of individual stakeholders participating in the lifecycles of datasets and artificial intelligence (AI) systems [11, 13, 14].As a step towards creating value that connects dataset success toresearch and production experiences, we propose a new frameworkfor transparent and purposeful documentation of datasets, calledData Cards [26]. A Data Card contains a structured collection ofsummaries gathered over the life cycle of a dataset about observable(e.g., dataset attributes) and unobservable (e.g., intended use cases)aspects needed for decisions in organizational and practice-orientedcontexts. Beyond metadata, Data Cards include explanations, ratio-nales, and instructions pertaining to the provenance, representa-tion, usage, and fairness-informed evaluations of datasets for MLmodels.Data Cards emphasize information and context that shape thedata, but cannot be inferred from the dataset directly. These aredesigned as boundary objects [28] that should be easily availablein accessible formats at important steps of a user journey for adiverse set of readers. Data Cards encourage informed decisionmaking about data usage when building and evaluating ML modelsfor products, policy and research. Data Cards complement otherlonger-form and domain-specific documentation frameworks forethical reporting (See Appendix A) , such as Model Cards [23], DataStatements [9], Datasheets for Datasets [15], and [6] FactSheets.Data Cards are accompanied by frameworks to adapt them to avariety of datasets and organizational contexts. These frameworksare pivotal to establishing common ground across stakeholders andenable diverse input into decisions. Our case studies demonstratethat creators of Data Cards were able to discover surprising futureopportunities to improve their dataset design decisions, such asconsidering reasons for a high percentage of unknown values andthe need to create a shared understanding of lexicons used in datasetlabeling during problem formulation.In summary, our contributions are four-fold:Our collective efforts suggest that in addition to comprehensivetransparency artifacts , the creation of structured frameworks arenot only beneficial in adding nuance to the dataset documentationprocess itself, but also transformational in introducing human-centric and responsible practices when using datasets in ML appli-cations.",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "c8e45bf2-be32-46bd-bdf2-fe578d5015d0",
                    "text": "Over the course of 24 months, multiple efforts were employedto design Data Cards and its supporting frameworks, borrowingfrom methods in human-centered design, participatory design, andhuman-computer interaction. We worked with dataset and MLteams in a large technology company to iteratively create DataCards, refining our design decisions to respond to challenges inproduction contexts. In parallel, we ran studies and workshops toidentify opportunities and challenges in the implementation of DataCards. In this section, we detail the various efforts and describetheir impact on the development of Data Cards.Specifically, we worked with 12 teams in a large technology com-pany to create 22 Data Cards that describe image, language, tabular,video, audio, and relational datasets in production settings. Teamsranged in size from four to over 20 members, and were comprisedof some combination of research software engineers, research sci-entists, data analysts and data program managers. This allowedus to observe each teams\u2019 documentation workflows, collabora-tive information gathering practices, information requests fromdownstream stakeholders, review and assessment practices. Ourco-creative approach in conjunction with feedback received acrossother studies yielded continuous improvements in the usability andutility of each new Data Card created.As we worked with ML dataset and model owners to produceprototypical transparency artifacts, drafts were evaluated in anexternal focus group with nine participants. These participants rep-resented non-expert, technical use cases from User Experience (UX)and Human-Computer Interaction (HCI) research, Policy, Prod-uct Design & Development, Academia, and Law. Participants wereasked to complete a paper-based questionnaire to reflect on their ideals of transparency, used as a basis for broader discussions on transparency. Participants were then provided with printed drafts which they annotated with their feedback. This allowed us to cap-ture specific feedback and establish relationships across themes and topics in the artifacts. We concluded with a discussion reflecting on their use of transparency artifacts and an offline survey to capture their overall expectations. Through this focus group, we were able to arrive at a working definition and values of transparency relevant to domains within AI product life cycles. We further synthesized feedback on the transparency artifacts into an initial set of recom-mendations to combat common reader-side challenges, which were then offered as guidance to teams creating Data Cards.Based on our experience in co-creating Data Cards with teams, we were able to consolidate recurring and overlapping questions into a canonical template that documents 31 different aspects of data sets. Questions that are were modality-specific were consolidated into appendable blocks, but largely left out of the canonical tem-plate. A follow-up internal MaxDiff survey (n=191) was conducted to  understand  the  information  needs  in  dataset  documentation within our company. Through this survey, we learned the relative importance of the 31 aspects documented in a Data Card, how these vary by dataset modality and job function, and further incorporated insights into our design of Data Cards. We observed the need for a generative framework that Data Card creators could use to add or tailor question to new datasets without compromising the readabil-ity, navigability, comparability and transparency intrinsic to the Data Card.Our internal study recruited 30 experts spanning sixteen teams within our company. Participants represented stakeholders who (a) create datasets designed for ML use cases and (b) use or review datasets for applied and foundational model development. Over the course of three days, this group engaged in various participatory activities to articulate use cases for transparency artifacts, infor-mation requirements, and strategies for evaluation of transparency artifacts. Participants were then invited to actively contribute to fu-ture discussions of Data Cards and their development as it related to the participant\u2019s specific data domains. We found that despite their deep expertise and experience, participants were unable to provide examples of exemplary documentation, but were quick to furnish \"excellent\" examples of poor documentation. This pointed us to the need for a set of dimensions that can be used to assess transparency and documentation without conflating documentation with the dataset.Further,  we  developed  a  structured  participatory  workshop-based approach to engage cross-functional stakeholders when cre-ating transparent metadata schema for dataset documentation [25]. This methodology was open-sourced and tested in the data domains of human computation, geo-spatial ML, multi-modal data opera-tions, healthcare data, community-engaged research, and large-scale multitask language models. Common to all workshops, we found that participating teams often started with an intuition about the benefits of transparency in dataset documentation. We found that teams needed to necessarily align on a shared definition of transparency, audience, and the audience\u2019s requirements as pre-requisites define the content, infrastructure, and processes to scale Data Card creation. We observed organization-specific factors that can impact long-term sustainability of scaling Data Cards, suchas knowledge asymmetries between stakeholders, organizationalprocesses that incentivize the creation and maintenance of docu-mentation, infrastructure compatibility and readiness, and com-munication culture across and within stakeholder groups. While adetailed discussion of our participatory methodology to developingtransparency metadata schemas and survey is beyond the scopeof this paper, we introduce relevant critical frameworks from ourmethodology.",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "83e968ac-19ec-4a64-a2f5-c8b1da895abc",
                    "text": "Despite the diverse backgrounds of participants across studies, the shared dominant perception was that transparency artifacts were ironically opaque. The opacity in documentation, quite simply, in-creases when language used is technical, dense, and presumptive of a reader\u2019s background, making it difficult for non-technical stake-holders to interpret. This, in turn, leads to sub-optimal decision making, and propagates asymmetries in power structures and my-opic AI data practices. Further, focus group and workshop partici-pants described transparency as \"subjective\", \"audience-specific\" and \"contextual\". To that end, we frame our definition of transparency as \u201ca clear, easily understandable, and plain language explanation of what something is, what it does and why it does that\u201d, to emphasize the domain-agnostic and inclusive prerogative of transparency ar-tifacts. In Table 1, we present eight characteristics of transparency that are vital for a robust discussion of the benefits, values, ethics, and limitations of AI datasets. Data Cards aim to provide a single scalable,  artifact  that  allows  non-traditional  stakeholders  across product, policy, and research to understand aspects about datasets and how they are used to make informed decisions. We found that stakeholders review role-related topics in Data Cards with ampli-fied  scrutiny,  and  follow-up  questions  progressively  increase  in specificity, which suggests that transparency is attained when we establish a shared and socratic understanding of datasets based on the ability to ask and answer questions over time.",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "1948b85d-c907-4be8-8bc0-0f5242ea804b",
                    "text": "At first, our audience for Data Cards was fairly broad, comprising a mix of experts and non-experts. Frameworks proposed by Suresh, et al [29] have distinguished higher-level domain goals and objectives from  lower-level  interpretability  tasks,  but  are  limited  by  their epistemological framing and vast scope. We created a broad yet decomposable typology describing three stakeholders groups in a dataset\u2019s life cycle, allowing us to consider how cross-functional stakeholders engage in decision-making on the basis of a single transparency artifact.In  our  typology,  Producers  are  upstream  creators  of  dataset and documentation, responsible for dataset collection, ownership, launch and maintenance. We observed that producers often sub-scribe to a single, informal notion of \u201cusers\u201d of Data Cards\u2014loosely characterized by high data domain expertise, familiarity with simi-lar datasets, and deep technical knowledge. However, in practice, we find that only a few readers or Agents actually meet all these requirements.Agents are stakeholders who read transparency reports, and possess the agency to use or determine how themselves or others might use the described datasets or AI systems. After testing pro-totypes and proof of concepts with different audience groups, itbecame clear that agents with operational and reviewer needs weredistinct categories. Reviewers include stakeholders who may neverdirectly use the dataset, but will engage with the Data Card (for e.g.reviewers or non-technical subject matter experts). Agents may ormay not possess the technical expertise to navigate informationpresented in typical dataset documentation, but often have accessto expertise as required.Additionally, agents are distinct from Users, who are individualsand representatives who interact with products that rely on modelstrained on dataset. Users may consent to providing their data as apart of the product experience, and require a significantly differentset of explanations and controls grounded within product experi-ences. We therefore suggest the use of Data Card target agents withaccess to technical expertise, and encourage the use of alternativetransparency artifacts for users that are designed exclusively forthat purpose.We further dis-aggregate these high-level groups to generateawareness and emphasize the unique decisions that each sub-groupmust make (Fig[3]). However, these groupings exist on a continuumand stakeholders may fall into more than one group concurrently,depending on their context. We used this typology to unearth as-sumptions that are often made about the rich intersectional at-tributes of individual stakeholders, such as expertise (e.g. novice orexpert), data fluency (e.g. none to high), job roles (e.g. Data Scientist,Policy Maker), function performed vis-\u00e0-vis the data (Data Contrib-utor, Rater), and goals or tasks (Publishing a dataset, Comparingdatasets) when conceptualizing Data Cards. Usability studies acrossthese groups revealed guidelines for the successful and appropriateadoption of Data Cards in practice and at scale. These are distilledinto the following objectives for Data Cards:Table 1: Characteristics of transparency surface through participatory sessions",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "05a2fa59-4ff5-4929-afbb-e9e657bf7760",
                    "text": "",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "a33a0a6f-e316-4f17-aa01-6712e28dd30d",
                    "text": "",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "7d6047b6-1dfc-4752-8176-09bc338f2647",
                    "text": "Figure 2: A Data Card Template Section: This section is titled \"Dataset Overview\", and contains two rows. The first row has threeblocks, whereas the second row spans the entire width of the section. Blocks contain (A) A Title, (B) A prompting question,and (C) an answer input space populated with predetermined choices or suggested answer structures.",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "4086f16a-5b8c-4a50-a900-c44c69b789c1",
                    "text": "Table 2: Content themes in the Data Card template. Our content schema extends the constitution of traditional dataset doc-umentation to include explanations, rationales, and instructions pertaining to 31 themes. We anticipate that not all themeswill be uniformly relevant to all datasets or equally applicable to features within a single dataset.",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "6189a849-2c4a-47d4-a0de-9ed0104fcf6d",
                    "text": "Table 3: The OFTEn framework",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "487eea45-3bfe-48eb-9aa7-62557c8e1d68",
                    "text": "",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "781f2d29-c677-4ae4-9bb5-0486ff604ce8",
                    "text": "5 DISCUSSION",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                },
                {
                    "id": "0959b2a2-83c3-44d0-8275-6c6f2bf89ec8",
                    "text": "ACKNOWLEDGMENTS",
                    "reference": "[1] Mehtab Singh Pushkarna, Alejandro Zaldivar, and Ondrej Kjartansson. 2022. Data cards: Purposeful and transparent dataset documentation for responsible AI. In Proceedings of the ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3531146.3533231"
                }
            ]
        },
        {
            "paper_title": "AI and ethics\u2014Operationalizing responsible AI",
            "authors": "L Zhu, X Xu, Q Lu, G Governatori, J Whittle",
            "publication_info": "\u2026 AI: Productivity, well-being \u2026 - Springer",
            "paper_url": "https://arxiv.org/pdf/2105.08867",
            "chunks": [
                {
                    "id": "76666830-6090-4803-a776-555bb9bf0493",
                    "text": "In the last few years, AI continues demonstrating its positive impact onsociety while sometimes with ethically questionable consequences. Building andmaintaining public trust in AI has been identi\ufb01ed as the key to successful and sus-tainable innovation. This chapter discusses the challenges related to operationalisingethical AI principles and presents an integrated view that covers high-level ethicalAI principles, general notion of trust/trustworthiness and product/process support inthe context of responsible AI, which helps improve both trust and trustworthinessof AI for a wider set of stakeholders.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "05b24c4d-d09a-4659-acd8-f96697a728fa",
                    "text": "When it comes to AI and Ethics/Law , there are two interrelated aspects of thetopic. One is on how to design, develop, and validate AI technologies and systemsresponsibly (i.e., Responsible AI) so that we can adequately assure ethical and legalconcerns, especially pertaining to human values. The other is the use of AI itself asa means to achieve the Responsible AI ends. In this chapter, we focus on the formerissue.In the last few years, AI continues demonstrating its positive impact on societywhile sometimes with ethically questionable consequences. Not doing AI respon-sibly is starting to have devastating effect on humanity, not only on data protec-tion, privacy and bias but also on labour rights and climate justice [8]. Building andmaintaining public trust in AI has been identi\ufb01ed as the key to successful and sus-tainable innovation [6]. Thus, the issue of ethical AI or responsible AI has gatheredhigh-level attention. Nearly one hundred principles and guidelines for ethical AIhave been issued by private companies, research institutions, and public organisa-tions [25, 10] and some consensus around high-level principles has emerged [15].On the other hand, principles and guidelines are far from ensuring the trustwor-thiness of AI systems [31]. Complicating the issue further, humans and societiesperceive trust in AI in intricate ways, which does not necessarily closely match thetrustworthiness of a particular AI system [17, 28, 30].The remainder of the paper is organized as follows: Section 2 discusses the chal-lenges of existing works on ethical AI. The framework with an integrated view ofthree aspects of ethical AI is discussed in Section 3. Section 4 shares our experienceand observations in a crop yield prediction project. Section 5 talks about the high-level ethical principles and their operationalization. Finally, section 6 concludes thechapter.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "1a1b39cc-f3f4-4fd6-bc5e-b34c57c06080",
                    "text": "Signi\ufb01cant research has gone into addressing ethical AI challenges. In this section,we discuss the existing works, which fall into three large categories:1. High-level ethical principle frameworks. A large number of high-level eth-ical principle frameworks [25] (e.g., Australian AI Ethics Principles ). Theyidentify the important ethical and legal principles responsible AI technologiesand systems are supposed to adhere to. Some effort, such as [34], further di-vides these high-level principles into guidelines at the team, organisational andindustry level. These high-level principles are hard to operationalise for manyreasons [31] we will discuss later.2. Ethical algorithms. Signi\ufb01cant research has gone into ethical algorithms wherethe formulation of some ethical/legal properties is amenable to mathematicalde\ufb01nitions, analysis and theoretical guarantees. These include properties suchas privacy [24] and fairness [29] or for speci\ufb01c types of AI systems [12]. Thiscovers mechanisms that deal with pre-processing of data (to remove bias or in-dividualistic characteristics), the learning process itself (to take into considera-tion of ethical constraints), learned models (to be further compliant with ethicalconstraints) and predictive results (to correct for residual bias or revealing in-dividualistic information). However, these mechanisms are algorithm focusedwith limited theoretical heuristic, con\ufb01ned to a small number of quanti\ufb01cation-amenable properties, and a small subset of ethical principles and human val-ues [33]. Most of the time, these ethical-aware algorithms are too complicatedto explain to less numeracy-equipped stakeholders and not connected to thebroader decision making process [6]. They are also not linked to the softwaredevelopment processes, especially system design methods, requirements engi-neering or user-centred design (UCD) processes.3. Human values in software engineering and their operationalisation. Re-cently, there has been emerging research in human values in software engineer-ing and their operationalisation [20, 21], including:a. Extension of value-based design methods (e.g., value-sensitive design -VSD) [19]b. Extension of human factor research on productivity and usability into hu-man values consideration but still limited to a small subset of human values[33]c. Software engineering methods for embedding human values and ethicalconsideration throughout the software development life cycle (SDLC) [20,36, 14]d. Architecture and design patterns that can improve (qualitatively or quanti-tatively) [7] or assure (with strong mathematical guarantees), \u201cby-design\u201d,certain ethical or human-value related quality attributes such as privacy andnon-male\ufb01cence (e.g. security, safety and integrity) [38, 27]",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "2779175d-a792-4edc-9e80-9e661d45273e",
                    "text": "We identify three issues in current research work regarding operationalising ethicalprinciples to achieve the ultimate trust from stakeholders.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "4ff4deae-7ebd-40fc-8552-72262cad43de",
                    "text": "The inherent and technical \u201ctrustworthiness\u201d of an AI system can be directly re-\ufb02ected in technologies/products via code, algorithms, data or system design or in-directly re\ufb02ected via the software development processes). On the other hand, trustis a stakeholder\u2019s (i.e., truster\u2019s) subjective estimation of the trustworthiness of theAI system. This subjective estimation is based on a truster\u2019s expected and preferredfuture behaviour of the AI system. Mixing the two in terms of identifying assur-ance mechanisms and presenting trustworthy evidence can overlook the additionaland special mechanisms required to gain trust (different from the ones for gainingtrustworthiness).A highly technically trustworthy system may not be trusted by trusters for onereason or another, rationally or irrationally. This is because a truster\u2019s subjectiveestimation of the system\u2019s trustworthiness and expectations may have a signi\ufb01cantgap compared to the system\u2019s inherent trustworthiness. It can also be the other wayaround when a truster overestimates a system\u2019s trustworthiness and puts undue trustinto it.The reasons for this gap may be related to several issues:\u2022 a truster\u2019s numeracy (impacting the understanding of different types of trust-worthiness evidence);\u2022 a truster\u2019s prior beliefs and experiences;\u2022 a truster\u2019s preferences and expectations on acceptable behaviours, types of evi-dence and explanation[30];\u2022 a system\u2019s observable behaviours to a truster.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "3809c93f-ac1c-43f4-aeaf-16329f9a7538",
                    "text": "There are many reasons why we still lack systematic methods to operationalise thehigh-level ethical principles. Some are due to the AI \ufb01eld\u2019s relatively short history(e.g., compared with the medical \ufb01eld) thus lacking professional norms, legal andprofessional accountability mechanisms, clear common aims, \ufb01duciary duties andimportantly proven methods to translate principles into practices [31]. Others aredue to the lack of consideration of a wider set of human values [32] such as politicalself-determination and data agency beyond technical dependencies [22].We believe another important factor is due to the relatively narrow attempt to op-erationalise human values and ethical principles into veri\ufb01able \u201cproduct\u201d trustwor-thiness (via mathematical guarantees) without systematically exploring a wider va-riety of mechanisms in development processes to improve both trustworthiness andtrust. Looking at process mechanisms can include highly tailored evidence gather-ing and communication mechanisms for different types of trusters. These will helpclose the gap between their subjective estimation and the system\u2019s more objectiveinherent trustworthiness.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "8f1ec3eb-cdfc-4d27-845d-fabdc03c4477",
                    "text": "Finally, many of the works do not actively consider the unique characteristics of AIduring operationalisation. Referring to one [11] of the many de\ufb01nitions of AI, AIis a collection of interrelated technologies used to solve problems autonomously,and perform tasks to achieve de\ufb01ned objectives, in some cases without explicitguidance from a human being. AI has its own agency [10] re\ufb02ected in its auton-omy (i.e., acting independently), adaptability (i.e., learning in order to react \ufb02exiblyto unforeseen changes in the environment) and interactivity (i.e., perceiving andinteracting with other agencies, human or arti\ufb01cial). So by AI\u2019s de\ufb01nition and itsinherent autonomy-related characteristics, it would be impossible (not simply hard)to accurately and completely specify all the goals, undesirable side-effects and con-straints (including ethical ones) at its \ufb01nest level of details. This is known as thevalue alignment problem: given an optimisation algorithm, how to make sure theoptimisation of its objective function results in outcomes that we actually want, inall respects? As one saying goes, \u201cIt never does just what I want, but only what I tellit.\u201d This inherent under-speci\ufb01cation issue is both a boon and a bane of AI. Thus itis important not just to use guarantee mechanisms but to introduce a range of prod-uct and process-related risk mitigation mechanisms. This will include things likecontinuous validation and monitoring of systems [35] after deployment, broadeningspeci\ufb01cations and real-world validation [9].",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "a3f16f46-08a4-4c7a-807d-bec5e45102fa",
                    "text": "Although previous work has produced high-level ethical AI principles, general no-tions of trust vs trustworthiness and product vs process support, they have not beenintegrated into the context of responsible AI. The contribution of this book chapteris the integrated view of the three aspects and how they help improve both trust andtrustworthiness of AI for a wider set of stakeholders. This integrated view includesthree components:\u2022 the difference between trust and trustworthiness in the context of ethical AIprinciples;\u2022 how different product and process mechanisms can achieve trustworthiness fordifferent ethical principles;\u2022 how different product and process evidence can be presented to different typesof trusters to improve the accuracy of their subjective estimation so they matchthe inherent trustworthiness of the systems.Our work also takes into consideration of the autonomy characteristics of AI andits inherent under-speci\ufb01cation challenges. Figure 1 gives a graphical representationof our framework.For conceptualising the relationship between trust and trustworthiness, we usethe de\ufb01nitions and concepts from Bauer\u2019s work [4]:Trust P is truster A \u2019s subjective estimate of the probability P that B willdisplay A \u2019 preferred behavior X i.e., of B \u2019s trustworthiness.In our work, B represents an AI system. The behaviours X displayed by Bcan include functional behaviours, behaviours that handle ethical constraints (e.g.privacy, security, reliability, safety and other human values and wellbeing) andmeta-level behaviours (e.g.transparency, explainability [18] and accountability). Atruster\u2019s subjective estimate at t is about the AI system\u2019s future behaviour at t .There are some arguments around the concept of trust being binary (rather thanprobabilistic) in reality as you either trust something or not. When you eventuallydecide to accept a system to be trusted, you then accept the associated harm if thetrusted party fails. We believe this binary notion is consistent with the probabilis-tic notion if you introduce a thresh-hold along the probability spectrum to discerntrusted or not trusted.We use Australia\u2019s ethical AI principles [11] and their de\ufb01nitions as a close-enough representation of the many similar ones [25, 15] around the world. Aus-tralia\u2019s ethical AI principles contain eight key principles.\u2022 P1: Human, social and environmental wellbeing: Throughout their lifecycle, AIsystems should bene\ufb01t individuals, society and the environment.\u2022 P2: Human-centred values: Throughout their lifecycle, AI systems should re-spect human rights, diversity, and the autonomy of individuals.\u2022 P3: Fairness: Throughout their lifecycle, AI systems should be inclusive andaccessible, and should not involve or result in unfair discrimination against in-dividuals, communities or groups.\u2022 P4: Privacy protection and security: Throughout their lifecycle, AI systemsshould respect and uphold privacy rights and data protection, and ensure thesecurity of data.\u2022 P5: Reliability and safety: Throughout their lifecycle, AI systems should reli-ably operate in accordance with their intended purpose.\u2022 P6: Transparency and explainability: There should be transparency and respon-sible disclosure to ensure people know when they are being signi\ufb01cantly im-pacted by an AI system, and can \ufb01nd out when an AI system is engaging withthem.\u2022 P7: Contestability: When an AI system signi\ufb01cantly impacts a person, commu-nity, group or environment, there should be a timely process to allow people tochallenge the use or output of the AI system.\u2022 P8: Accountability: Those responsible for the different phases of the AI sys-tem lifecycle should be identi\ufb01able and accountable for the outcomes of the AIsystems, and human oversight of AI systems should be enabled.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "d9c35506-deba-43e0-a388-98b4ab38cfb5",
                    "text": "We group the eight principles in two categories based on their nature and charac-teristics. The \ufb01rst group includes the \ufb01rst \ufb01ve principles (P1\u2013P5), which are humanvalues and ethical constraints similar to the non-functional software qualities [23] tobe considered. The second group includes the last three principles (P6, P7 and P8),which are meta-level governance issues.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "cc2cad4e-a220-4e90-aaee-acd55813c4e1",
                    "text": "These principles sometimes can be framed as functional requirements of a softwaresystem. If that is the case, in the context of software engineering, the methodology ofrequirement engineering could be adopted to ensure that the requirements capturedare as accurate and complete as possible. It\u2019s worth noting again that AI systemscan not be fully speci\ufb01ed and will try to solve the problems autonomously witha level of independence and agency. On the other hand, there will be con\ufb02ictingrequirements whereby tradeoff decisions need to be made.Some principles, such as security, reliability and safety, are the non-functionalproperties well studied in the dependability research community [2]. These princi-ples can be captured as non-functional requirements and considered from the earlystage of system design. There are technical mechanisms or reusable design frag-ments, like patterns and tactics, that could be applied to ful\ufb01l the quality require-ments [3]. Privacy is not a standard software quality [23], but has been treated asan increasingly important property of a software system to realise regulatory re-quirements, like GDPR (General Data Protection Regulation) and Australia Pri-vacy Act, into technical artifacts. Reusable practices and patterns have been sum-marised in both industry and academia for privacy [7]. Fairness is a concern that themachine learning developers should consider from the early stage of the data pro-cessing pipeline. Similar to before, collections of best practice and mechanisms toremove bias at different stages of the pipeline have been compiled [29]. The reasonto group these principles is that they can be handled and validated using a similar ap-proach: the methodology of how non-functional properties are handled in softwaresystem design. Some principles can be validated in a quanti\ufb01able way, like reliabil-ity. Others could be validated against process-oriented best practices, methods andwidely used patterns.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "828882e9-852b-4657-82b9-7b20a820f9d0",
                    "text": "The principles within this group are largely governance issues and designed to im-prove truster\u2019s con\ufb01dence in the AI system. They can be seen as clear requirementsfor certain functionality provided by the software system or entities providing thesystem. For example, contestability requires a system or entity function that allowsthe trusters to challenge the output or use of the AI system. Transparency and ex-plainability are similar in that the users can have access to the system, data, algo-rithms to understand it, including receiving an explanation of a decision or predic-tion.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "8b0307dd-fc28-4197-b6eb-cedeb3e65b27",
                    "text": "Each principle can have different implications in terms of trust vs. trustworthiness.Essentially, the difference is between what an AI system can objectively perform(trustworthiness) and what a truster/stakeholder \u201cprefers/wants\u201d (trust expectation)and their subjective estimation of the behaviour of the AI system. And it has beenobserved that human may have very different expectations of AI or human eventheir trustworthiness are similar [17]. We list these differences in Table 1.As we can see, trust is about subjective estimation and perception and often notlimited to the AI system\u2019s trustworthiness properties (whether via software artifactsor development processes). As identi\ufb01ed in [28], multiple factors contributes to peo-ple\u2019s trust in an AI system, including factors not related to a speci\ufb01c AI system suchas current society safeguards such as regulations, overall AI uncertainty, job impactand familiarity of AI. Some factors may be related to the organisations that build theAI systems, use the AI system or evaluate the AI system. The speci\ufb01c characteristicof the AI system only plays a minor role in trust.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "3af37012-8198-4ac2-9bcd-8344a8f3e3e6",
                    "text": "We continue using the same grouping as the last section to analyse the eight princi-ples to demonstrate different product and development process (including the peo-ple aspect) mechanisms to improve trustworthiness. This differentiation teases out abroader set of considerations to improve trustworthiness, which subsequently helpsunderstand what are the different ways in which these mechanisms could be com-municated to different stakeholders/trusters to improve trust.For human, social and environment wellbeing and human-centred values, wealso consider organisational culture and SDLC methods including roles and agilepractices. For quality attributes related ethical principles, we apply architecture anddesign processes, patterns and tactics and refer to a range of data, model [9] andalgorithmic considerations. [26]. We consider design process (\u201cin design\u201d), designartifacts (\u201cby-design\u201d) and designers (\u201cfor designers\u201d). [10] For governance issues,we build upon the principles in [10, 22, 5, 21].We introduce some considerations and examples across the Product and Pro-cess/People dimension in Table 2. Many of the mechanisms presented in the tablerely on industry-wide and society-wide work, in particular:\u2022 Multi-stakeholder sector-speci\ufb01c [6] and domain-speci\ufb01c guidelines (such asbanned practices in digital platforms [13], regulating software-based medicaldevices [1], technical solutions and empirical knowledge bases [31]).\u2022 AI and data ethics board or other governance mechanisms at the organisationlevel [5] to oversee the overall AI-driven decision-making processes (not justalgorithms and products).\u2022 Regulator levers that incentivise organisations and create a level playing \ufb01eldfor ethical innovation [6] including validation and certi\ufb01cation agencies.\u2022 Technical and non-technical ethics and human rights training for different rolesand organisational awareness.\u2022 Incentives for employees to play a role in identifying AI ethical risks [5].\u2022 Ongoing monitoring and engagement with stakeholders [35, 5].Due to the under-speci\ufb01cation and value alignment problem of AI, none of theassurance mechanisms can guarantee a desirable outcome alone. Each helps reducethe risk and the ongoing monitoring and engagement post-deployment plays a criti-cal role in identifying and mitigating undesirable side effects early.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "8b58a5cd-98e2-46a4-831d-025c32a941d0",
                    "text": "It is important that we present trustworthiness evidence (e.g. product artifacts assur-ances and process/people assurances) to different types of trusters to help with theirsubjective estimation so the estimation matches the inherent trustworthiness of theAI systems. Here we have to take into considerations of two different aspects.Truster Preference: Based on expectation and past experiences, different types oftrusters, such as AI algorithm developers, AI system developers, professional usersof AI systems, affected subjects (whereby AI systems have impacts on them), reg-ulators and certi\ufb01ers, or the general public may have different preferences of an AIsystem\u2019s behaviour.Truster-Speci\ufb01c Veri\ufb01able Evidence: Different types of trusters may expect dif-ferent types of evidence for assuring the trustworthiness of an AI system. They mayhave different abilities to understand and assess the evidence and expect differenttypes of explanations [30]. For example, an AI expert can assess the algorithmswhile a general public would have no use of the algorithms. A system developermay understand an AI algorithm and the data associated but have limited ability toevaluate the algorithm and bias in data.When presenting evidence, such as assurance mechanisms in the last section, thefollowing factors should be taken into consideration:\u2022 Consider a stakeholder\u2019s technical abilities to understand the evidence.\u2022 Consider a stakeholder\u2019s resources and time to assess the evidence.\u2022 Allow both stakeholders and 3rd parties who represent the stakeholders to ex-amine the evidence.\u2022 Allow broader ethical and legal \u201dstanding\u201d so the evidence can be produced andexamined at an individual decision, group and society level by a wide range ofstakeholders or their delegates.\u2022 Present both process and product assurance mechanisms to the right stakehold-ers in the right ways.\u2022 Focus more on process mechanisms when ethical principles can not be easilyde\ufb01ned and quanti\ufb01ed.\u2022 Focus more on the product mechanisms when ethical principles can be de\ufb01nedand quanti\ufb01ed and technically assured.\u2022 Improve overall awareness/familiarity of AI, and understanding of broader AIissues such as current society safeguards, AI\u2019s overall uncertainty and job im-pact.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "71dfa112-606f-4eda-b7d7-9a084d53a972",
                    "text": "In this section, we share our experience and observations in a crop yield predictionproject. In the project, we applied a data-driven approach using machine learningalgorithms to develop an improved version of a crop yield prediction model. Theprevious model was built using a domain model-driven approach and integratedinto a commercialised software product. The project combines commercial satellitedata and data collected from different farms, largely owned by individual farm-ers. The resulting machine learning model predicts future yields of farms in boththe farm data collection region but also in new regions signi\ufb01cantly different fromthe original regions. There are three types of stakeholders in the project: the tech-nical team (a research organisation playing the role of both AI system developerand operation-time learning coordinator), data contributor (i.e., participating farm-ers who provided the data), and model user (i.e., farmers who use the models foryield prediction). We used a federated machine learning approach to deal with thedata ownership problems and the non-IID (independent and identically distributed)data distribution problem. The learning coordinator (i.e., the technical team) designsand operates the model training process on multiple, distributed data contributors.The model was \ufb01rst trained locally on a local server and then sent to a central serverfor aggregating and improving a global model. The model user performs inferenceusing the aggregated global model. Trustworthiness and trust issues manifested dif-ferently for different stakeholders. We applied our approach retrospectively to theproject and made the following observations in term of trust and trustworthiness.\u2022 Human, social and environmental wellbeing\u2013 Positive wellbeing requirements were considered for the project itself butnot for wider issues outside the project.\u2013 Potential misuse of data collected was considered but not for the predictivemodels.\u2013 Both data contributors and model users have high-level trust in the technicalteam as an entity but have very little trust in any third parties who may alsouse the data (even for claimed wellbeing improvement reasons).\u2013 Wellbeing requirements were speci\ufb01ed as high-level goals, not quanti\ufb01ablemetrics.\u2022 Human-centred values\u2013 Human factors on usability and accessibility were considered, consultedand designed in the original prediction app.\u2013 No additional human-centred values were captured apart from the explicitones below.\u2022 Fairness\u2013 Fairness issues were considered at the training data, learning algorithm andmodel level but not explicitly explained to data contributors and modelusers.\u2013 A range of product assurance mechanisms was used, such as:\u00b7 Counterfactual analysis to discover potential hidden or proxy variablesthat lead to different yields.\u00b7 Sensitive-variable-aware data pre-processing including random splitcrop separation (wheat vs. barley), location separation (at paddock andregion level). Each attribute may have associations, thus becoming aproxy variable regarding sensitive and protected attributes.\u2013 Two team members built fairness-aware models in parallel as a processassurance.\u2013 Used domain attributes that were understandable to farmers (data contribu-tors and model users) in both training and explanation so farmers can havea higher level trust in the model regarding fairness across groups (not justaccuracy).\u2022 Privacy and Security\u2013 The personal privacy of individual farmers and privacy/con\ufb01dential info offarms were both identi\ufb01ed as key requirements.\u2013 Potential misuses and harms of privacy info leakage were identi\ufb01ed.\u2013 Both data contributors and model users have high-level trust in the technicalteam as an entity.\u2013 Federated learning, including strong privacy guarantees, was used as anarchitectural pattern ful\ufb01lling data privacy requirements.\u2013 The exchanged model updates was encrypted to improve model security.A model registry was established locally to maintain the mappings of en-crypted models to the decrypted models.\u2013 The concept of federated learning, privacy guarantee and security mech-anisms were dif\ufb01cult to explain to farmers, but the high-level trust in theproject team coupled with the notion that their data never left the localserver and strong encryption was used gained adequate trust.\u2022 Transparency and Explainability\u2013 Transparency and explainability requirements were identi\ufb01ed in the project.\u2013 Although the farmers would not be able to evaluate data, code and modelsdirectly, the project team nevertheless made all artifacts open to stakehold-ers (under privacy and con\ufb01dentiality constraints).\u2013 Code, model and data provenance were captured in Git and Bitbucket.\u2013 On the data contributor side, a data contributor registry was built to storeand manage the information of the participating farmers and their paddocksthat joined the learning process. Further, a model versioning registry wasimplemented to keep track of all local model versions of each data contrib-utor and the corresponding global model. Both of the design mechanismshelped increase the trust of both model user and learning coordinator on theparticipating data contributors.\u2013 On the learning coordinator side, a decentralised aggregator was designedto replace the central server of the learning coordinator, which could be apossible single point of failure. This improved the trust of farmers (i.e., bothdata contributors and model users) on the learning coordinator.\u2022 Contestability\u2013 Data contributors can always withdraw from the projects with their datadeleted. However, the model trained by their data will not be immediatelyupdated to remove the data.\u2013 Model users always have the right not to use the prediction or allow theprediction to be used by others.\u2022 Accountability\u2013 The accountability was largely governed by the legal agreement betweendata contributors, model users and the project team.\u2013 No role-level accountability was established, but the provenance of data,model and code allowed accountability to be examined.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "481c4af7-1312-4e3b-9651-a2950dbe2db2",
                    "text": "Mappings of different ethical principles and guidelines have been recently studied in[25, 15]. Although a global consensus emerges on the core principles (e.g. privacy,transparency, fairness), there have been debates on the classi\ufb01cation and de\ufb01nitionof AI ethical principles. As we are using Australia\u2019s AI ethical principles, we discussour interpretation and observations of these principles.\u2022 Autonomy. Autonomy is not discussed explicitly in the framework. Instead, abroader principle about human-centred values is given, which covers humanrights, diversity, and the autonomy of individuals. Autonomy refers to the free-dom of AI system users to a range of activities, including self-sovereignty/determination,the establishment of relationships with others, selection of a preferred plat-form/technology, experimentation, surveillance, and manipulation [25]. Thismight con\ufb02ict with AI systems\u2019 inherent autonomy which refers to indepen-dent actions without a human being. Further, users (i.e., data owners) expect toexert full control over their data and activities without having to rely on others.To ensure autonomy, a distributed ledger technology like blockchain can playa vital role in the design and implementation of a decentralised AI platform, inwhich secure and self-determined interactions between stakeholders are enabledwithout a central coordinator.\u2022 Explainability. Explainability is listed together with transparency in Australia\u2019sAI ethical principles. Transparency refers to disclosure of all related AI artifacts,such as source code, data, algorithms, models, and documentation. Althoughtransparency enables explainability, explainability is more than helping stake-holders understand how an AI system works through responsible disclosure.Stakeholders expect to understand the reasons for AI system behaviour and in-sights about the causes of decisions. However, it is challenging to present theexplanations (e.g., representation of data in a network - roles of layers/neurons)to gain the trust of stakeholders [16, 30] .\u2022 Accountability. Accountability principles are covered by most of the ethical AIprinciples and guidelines, including Australia\u2019s AI ethical principles. Ethical AIis often present together with responsible AI. However, the difference betweenresponsibility and accountability is rarely discussed. Responsibility refers to theduty to complete a task throughout the lifecycle of an AI system. For example,a developer may be responsible for implementing an algorithm in an AI project.Accountability is the duty to be accountable for a task after it is completed,which happens after a situation occurs. For example, an AI startup company\u2019sCEO may be accountable for the inaccurate or biased decisions made by theirAI system product and has to take the role to explain how the decision is made.Role-level accountable entities and humans should be clearly identi\ufb01ed in AIprojects. Product features and management processes that ensure provenanceand traceability of AI artifacts and decisions can increase accountability of AIsystems.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "63c2372d-3623-4645-93b9-3bbd015c083c",
                    "text": "AI ethics can be operationalised in a variety of different ways. There are more ap-proaches and mechanisms beyond high-level principles and low-level algorithms.\u2022 Requirements. In addition to conventional functional requirements and non-functional requirements, ethical principles can be de\ufb01ned as a subset of require-ments of AI systems. As discussed in Section 3.1, the eight principles can beclassi\ufb01ed into three groups: 1) P1 and P2 principles as functional requirements,2) P3, P4 and P5 as non-functional requirements (software qualities), 3) P6,P7 and P8 as meta-level governance-related functional requirements to improvetruster\u2019s con\ufb01dence. However, it is hard to justify whether the P1 and P2 (i.e.,human, social and environmental well being and human-centred values) are ful-\ufb01lled adequately. Risk mitigation mechanism might be needed to deal with AIautonomy and ensure the ful\ufb01lment of P1 and P2.\u2022 Design. Design patterns/mechanisms can be proposed to address the ethicalprinciples. Existing principles intend to protect users and the external world ofAI system users. In distributed learning systems, trust issues also exist in be-tween participating nodes. For example, in federated learning systems, learningcoordinator might become a single point of failure. Thus, all the stakeholdersshould be carefully considered in design patterns/mechanisms.\u2022 Operations. Continuous validation and monitoring mechanisms are needed tocheck the ethical principles continuously. Engaged stakeholders need to iden-tify the threshold for the signi\ufb01cant impact which triggers the validation mech-anisms.\u2022 Governance.\u2013 Ethical maturity certi\ufb01cation. An AI ethics maturity certi\ufb01cation scheme/system could be developed to assess an organisation\u2019s ethical maturity ofAI project management. Based on the review of AI projects, different lev-els of ethical maturity certi\ufb01cates could be issued to an organisation. Thelevel/type of certi\ufb01cate could be upgraded later.\u2013 Ethics review. Internal and external reviews on ethical impact can be con-ducted to address ethical principles. Representatives of stakeholders are ex-pected to join the reviews.\u2013 Project team. When the development team is set up, team members\u2019 diver-sity (e.g., background, cultures, and disciplines) should be considered.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                },
                {
                    "id": "0bb6803d-5b0e-41bf-95be-0dbba3877999",
                    "text": "In this chapter, we explored the interaction of three issues related to humanity andAI: hard-to-operationalise ethical AI principles, general notion of trust vs trust-worthiness and product vs process support for trust/trustworthiness. We providedan integrated view of them in the context of AI ethical principles and responsibleAI. It points to additional mechanisms especially process mechanisms and trust-enhancing mechanisms for different stakeholders. By using the example of cropyield prediction involving different types of data and stakeholders, we elicited themissing elements in operationalising ethical AI principles and potential solutions.We envision some future directions in ethical AI such as quantifying trust and itslink with trustworthiness and novel process mechanisms for improving trust andtrustworthiness.",
                    "reference": "[1] Xiuwen Zhu, Qinghua Xu, Guoxiang Lu, Guido Governatori, Jon Whittle. 2022. AI and ethics\u2014Operationalizing responsible AI. AI: Productivity, well-being. arXiv:2105.08867. Retrieved from https://arxiv.org/pdf/2105.08867"
                }
            ]
        },
        {
            "paper_title": "Towards responsible AI for financial transactions",
            "authors": "C Maree, JE Modal, CW Omlin",
            "publication_info": "2020 IEEE symposium series \u2026 - ieeexplore.ieee.org",
            "paper_url": "https://arxiv.org/pdf/2206.02419",
            "chunks": [
                {
                    "id": "1dbe732d-5602-45f0-8cb9-8086c889c321",
                    "text": "",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "ee25435b-ee83-4c1c-8fa1-5d2716b572ef",
                    "text": "",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "5ac67d4e-8044-42a3-a7f4-9f02388b939c",
                    "text": "",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "7bf2fd09-1769-4c36-88b7-407b4ebe90ee",
                    "text": "",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "37942855-d621-4f19-bee7-ed2dd37c91e0",
                    "text": "",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "7b69069d-fc71-4594-8524-96d7482a5192",
                    "text": "The features in the dataset include categorical, numerical and text attributes. The target model is a series of two opaque models:  a  word2vec  encoder  followed  by  a  deep  neural network  (DNN).  In  the  first  model,  the  transaction  text  is encoded into a vector representation \ud835\udc4b = {\ud835\udc4b }, \u205f\ud835\udc56 \u2208 {1, \u2026 , \ud835\udc5b} \u2282 \ud835\udc41  (1) where \ud835\udc41 is the number of features in the feature space, \ud835\udc5b is the dimensionality of the vector representation of the text, i.e.  the  product  of  the  size  of  embedding  vector \ud835\udc58,  and  the number of  words in  the text \ud835\udc59,  i.e. \ud835\udc5b = \ud835\udc58 \u00d7 \ud835\udc59.  This vector  is concatenated with one-hot encodings of the transaction code \ud835\udc4b  and  day  of  week \ud835\udc4b ,  normalized  transaction  amount \ud835\udc4b  and  customer  age \ud835\udc4b  as  well  as  binary  series  representing whether  the  transaction  amount  is  negative  or  positive (payment  vs  deposit) \ud835\udc4b  and  whether  the  amount  includes cents \ud835\udc4b .  This  concatenated  dataset \ud835\udc4b,  which  is  sent  into  a DNN is formally represented by: \ud835\udc4b = {\ud835\udc4b , \ud835\udc4b , \ud835\udc4b , \ud835\udc4b , \ud835\udc4b , \ud835\udc4b , \ud835\udc4b }  (2) The model is a classification net, producing a probability distribution \ud835\udc4c \u2208 \ud835\udc4c, \u205f\ud835\udc56 \u2208 {1, \u2026 , \ud835\udc5a} where \ud835\udc5a is  the  number  of output classes.  The training set was labelled using a mixed technique of defined  rules  and  manual  labelling.  The  rules  did  not accurately  classify  all  transaction;  misclassified  transactions had to be hand labelled.",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "25f99b22-a13d-43ac-adf2-27ce5c0b7a54",
                    "text": "Shapley additive explanations (SHAP) [10] is based on the collaborative  game  theory  method,  Shapley  values  [14].  It clarifies the prediction of an instance \ud835\udc65 \u2208 \ud835\udc4b, where \ud835\udc4b is the set of all instances, by computing the contribution of each input feature  \ud835\udc65 \u2208 \ud835\udc65, \u2004\ud835\udc56 \u2208 {1, \u2026 , \ud835\udc41}  where  \ud835\udc41  is  the  number  of features in the dataset. SHAP values assign weights to each feature cluster, where a feature cluster can be either a single feature, e.g. in numeric data, or a group of features, e.g. several words in a sentence. SHAP uses these weights in an additive linear model to explain the overall contribution of all features, thus  elegantly  blending  elements  from  Shapely  values  [14], LIME [15] and others. In [10], the authors define a given explanation model \ud835\udc54 as \ud835\udc54(\ud835\udc67 ) = \ud835\udf19 + \u2211 \ud835\udf19 \ud835\udc67 (3) where \ud835\udc67 \ud835\udf16 {0,1}  is the feature space vector indicating the presence of each feature, \ud835\udc41 is the size of the feature space and SHAP values \ud835\udf19  \ud835\udf16 \u211d is the individual feature contribution for a  feature \ud835\udc57.  The  feature  space  refers  to  a  simplified  feature space  that  maps  to  the  original  feature  space  through  a mapping  function  \ud835\udc67 =   \u210e (\ud835\udc67 ) .  The  individual  feature contributions  \ud835\udf19  \ud835\udf16 \u211d  are  estimated  using  the  collaborative game theory approach Shapley [14]. Shapley explores a game where the prediction of a model \ud835\udc53(\ud835\udc65)  is  seen  as  the  result,  or  payout,  of  the  game.  The individual  features  \ud835\udc65 \u2208 \ud835\udc65  are  the  players.  The  goal  is  to determine the contribution that each player has to the payout. Shapley determines how to fairly distribute the payout among the  players  through  comparison  of  the  model  outputs  for different  coalitions  of  feature  values.  Feature  coalitions  are made by randomly sampling values from the feature space, i.e. a coalition is a fictitious instance \ud835\udc65 \u2209 \ud835\udc4b, where feature values \u2208 \ud835\udc65 are  drawn  randomly  from  the  feature of  the  instance \ud835\udc65space. The Shapley value of a feature is defined as the average change  in  the  prediction  \u0394\ud835\udc53\u0302(\ud835\udc65) = \ud835\udc53\u0302(\ud835\udc65\u2032) \u2212 \ud835\udc53\u0302(\ud835\udc65\u2032\u2032)  that  a \u2208 \ud835\udc65 joins coalition \ud835\udc65  receives  when  a  new  feature  value \ud835\udc65the coalition.",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "1b9f7334-0e4d-4f53-8ae4-7bca995704aa",
                    "text": "Text  is  typically  clustered  using  a  spatial  clustering algorithm,  such  as  the  density  based  spatial  clustering algorithm  with  noise  (DBSCAN)  [16],  [17].  It  starts  with  a random  instance  and  identifies  all  its  nearest  neighbors. Proximity  to  other  instances  is  determined  through  a  given distance measure, e.g. Euclidean, Hamming, Cosine, etc. If a point has a minimum of \ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc43\ud835\udc61\ud835\udc60 neighbors within a distance of \ud835\udf16,  then  a  new  cluster  is  defined.  The  algorithm  will  also identify outliers that do not fall in any cluster as noise. When text is represented as word vectors, through e.g. a word2vec  encoder,  the  similarity  between  two  sentences corresponds  to  the  distance  between  the  vectors.  This  is generally  quantified  as  the  cosine  of  the  angle  between  the vectors [18] i.e. the cosine similarity. Given two sentences, the cosine similarity is defined as: \ud835\udc60\ud835\udc56\ud835\udc5a (\ud835\udc61 , \ud835\udc61 ) = \ud835\udc61   \u22c5 \ud835\udc61|\ud835\udc61 | \u00d7 |\ud835\udc61 | (4) Where \ud835\udc61 ,\u2006\ud835\udc61 \u2208 \ud835\udc47, are \ud835\udc5b-dimensional vectors in the term set \ud835\udc47 = {\ud835\udc61 , \u2026 , \ud835\udc61 }  and  \ud835\udc60\ud835\udc56\ud835\udc5a \u2208 [0,1] .  When  two  terms  are identical,  the  cosine  similarity  is  1  i.e.  \ud835\udc60\ud835\udc56\ud835\udc5a (\ud835\udc61 , \ud835\udc61 ) =1,  \u2200 \ud835\udc61 = \ud835\udc61 . DBSCAN can therefore be used with cosine similarity as a clustering method for texts.  V.  E M  A.  Data Throughout  this  study,  we  used  an  initial  dataset  of roughly 10 million financial transactions. These transactions were labelled using the target model and resampled without replacement to provide a more uniform representation of the labelled  classes.  The  final  dataset, \ud835\udc4b ,  had  a  cardinality  of roughly 5 million transactions, i.e. |\ud835\udc4b| \u2245 5 000 000. B.  Explanation by Decision Trees Global  surrogate  modelling  is  a  well-documented approach to model explainability [4]. In this study, we trained both  a  single  decision  tree  and  a  random  forest  as  global surrogates to explain the model. We used a random sample of 10% of the total dataset (about half a million transactions) for training,  \ud835\udc4b \u2282 \ud835\udc4b  \u2227   |\ud835\udc4b | = 0.1 \u00d7 |\ud835\udc4b| ,  while  testing was done on a randomly sampled set of 100 000 transactions, \ud835\udc4b \u2282 \ud835\udc4b  \u2227   \ud835\udc4b \u2209 \ud835\udc4b   \u2227   |\ud835\udc4b | = 100 000.  We  used  these  train  and  test  sets  to  fit  a  decision  tree classifier  and  a  random  forest  classifier  with  50  individual trees.  The  performance  and  human  understandability  of  the tree  and  forest  were  used  as  a  baseline  to  compare  with  a hybrid clustering / decision tree approach discussed below. C.  Feature Importance through SHAP Analysis We estimate the feature importance using SHAP [10]. The feature  importance, \ud835\udf19  was  estimated  for  each  input  feature \ud835\udc65 \u205f\ud835\udc56 \u2208 {1, \u2026 , \ud835\udc41}  where  \ud835\udc41  is  the  total  number  of  encoded features.  Note  that  due  to  encoding, \ud835\udc41 > 7 where  7  is  the number of original features. Equations  (1)  and  (2)  illustrate  how  the  features  are prepared, with equation (1) referring to the word vectors for the transaction text. SHAP values provide an estimate of the ; however, this is importance of individual features, \ud835\udf19 \u2192 \ud835\udc4bnot useful when the feature of interest is a superfeature: \ud835\udc4b =}, \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc5b}.  In  order  to  derive  the  importance  of  the {\ud835\udc4bsuperfeature  \ud835\udc4b ,  we  aggregate  the  SHAP  values  through addition [10]:   \ud835\udf19 = \u2211 \ud835\udf19 (5) D.  Explanation through Clustering and Decision Trees Having identified the most important feature, we clustered the data according to this feature. In Section VI, we show that the most important feature in classification is the transaction text \ud835\udc4b ; we therefore used the DBSCAN algorithm with cosine similarity  as  the  distance  measure.  We  trained  a  set  of \ud835\udc5a superclusters, \ud835\udc50 \u2208 \ud835\udc36, \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc5a},  where \ud835\udc5a is  the  number of classes in the output \ud835\udc66 \u2208 \ud835\udc4c, \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc5a}.  From  these  superclusters,  we  considered  the  individual words from the texts contained in each cluster. We created a list of keywords \ud835\udc58 \u2208 \ud835\udc3e for each supercluster \ud835\udc56 by extracting unique words from each cluster. Stop words such as place and street names were removed from the keyword lists. Formally, \ud835\udc58 \u2208 \ud835\udc3e\u205f \u2227 \u205f \ud835\udc58 \u2229 \ud835\udc58 = \u2205 \ud835\udc56, \ud835\udc57 \u2208 {1, \u2026 , \ud835\udc5a} \u2227  \ud835\udc56 \u2260 \ud835\udc57  (6)  The keywords  were used as rules that associate a  given transaction  text  with  a  given  supercluster.  For  any  given transaction text \ud835\udc61, each word in the text \ud835\udc64 \u2208 \ud835\udc61 was given the opportunity  to  vote  for  a  supercluster \ud835\udc50;  we  considered  the keyword list for each supercluster; if a word \ud835\udc64 appears in the keywords list \ud835\udc58 , the word voted for supercluster \ud835\udc50 . The votes for  all  words  were  tallied  and  the  supercluster  was  selected through  majority  vote.  If  no  supercluster  was  found,  i.e.  no words  appear  in  the  keyword  list,  a  default  supercluster representing the class \u201cother\u201d was selected. We used shallow decision trees to filter out those instances that did not belong to the homogeneous majority. This is similar to the approach in [12] and [13]; we intended to simplify the final explanation while simultaneously attaining improved accuracy compared to a single large classifier.  E.  Model Robustness against Evasion Attacks In order to test the robustness of the model, we subjected the  model  to  a  targeted  evasion  attack,  leveraging  the the  model.  A  successful newfound  knowledge  about analysis; we used DBSCAN with the distance parameter \ud835\udf16 =0.07. The intent was to train tight clusters. The result was a set  of  clusters  with  high  homogeneity  (95%)  and  a  low percentage of noise (2%), with a total of 12 734 clusters. We then grouped the clusters using the labelled training data into \ud835\udc5a superclusters. Fig. 2 shows a 2-dimensional representation of the supercluster for transactions relating to alcohol. Finally, we fit a small, interpretable decision tree to each supercluster  with  less  than  100%  homogeneity;  the  shallow decision tree provides the final separation and explanation. An example tree is shown in Fig. 3, for cluster number 10 relating to expenditure on kindergartens: adversarial attack therefore suggests not only a vulnerability in the model, but also a working knowledge of the model by the attacker.  The  adversarial  examples  were  generated  by  slightly perturbing  existing  instances,  along  the  feature  of  highest importance,  i.e.  where  the  impact  would  be  greatest.  The perturbations  therefore  targeted the  transaction  text, \ud835\udc4b . The perturbed  set  of  adversarial  examples \ud835\udc4b \u2208 \ud835\udc4b is  therefore defined by:  \ud835\udc65 = {\ud835\udc65 \u2032, \ud835\udc65 , \ud835\udc65 , \ud835\udc65 , \ud835\udc65 , \ud835\udc65 , \ud835\udc65 } \ud835\udc65 \u2208 \ud835\udc4b\u205f \u2227 \u205f \ud835\udc65 \u2208 \ud835\udc4b   (7) Words from the texts were selected by matching the words with the keyword dictionary, \ud835\udc3e. If a word appeared in one of the  keyword  lists \ud835\udc58 \u2208 \ud835\udc3e,  then  that  word  was  replaced  by  a word  from  another  list  \ud835\udc58 \u2208 \ud835\udc3e ,  where  \ud835\udc56 \u2260 \ud835\udc57\u205f \u2227 \u205f\ud835\udc56, \ud835\udc57 \u2208{1, \u2026 , \ud835\udc5a}. VI.  R  The labelled set of transactions was divided into training (80%),  validation  (10%)  and  test  (10%)  sets.  The  trained model  achieved  a  mean  accuracy  of  98.2%,  with  a  95% confidence interval of 0.04% in 20 experiments. As a baseline to an explanation, we trained a decision tree and a random forest as surrogate models on data labelled by the DNN. The decision tree achieved an accuracy of 95.9%, while  the  random  forest  (with  50  estimators)  achieved  an accuracy  of  96.7%.  Both  the  single  decision  tree  and  the random  forest  had  in  excess  of  50  000  nodes.  Even  though decision  trees  inherently  explained  the  rules  they  have derived,  they  clearly  do  not  provide  interpretability  in  this instance.  A.  Feature Importance and Model Explanation The results from the SHAP feature importance evaluation are  clear  evidence  of  the  model\u2019s  bias  towards  the  text features.  As  seen  in  Fig.  1,  the  transaction  text  is  largely responsible  for  the  predictions.  This  is  consistent  with  the importance of transaction text for the partial labelling of the original dataset.  Knowing  that  the  text  is  the  most  important  feature  for model  classification  is  not  an  adequate  explanation  of  the functioning of the model. To determine how the model uses the  text,  we  used  its  vector  representation  in  a  clustering  In  Fig.  3,  the  tree  distinguishes  between  transactions relating  to  kindergarten  and  those  relating  to  property management.  The  transaction  code  \u201c014\u201d  is  the  most important feature in this classification, while the amount and day  of  week  also  play  roles.  In  Fig.  4,  we  plot  the  feature importance for the decision tree shown in Fig. 3.  Fig. 4 shows that the transaction code is the most important feature. This correlates well to our previous estimated of the overall and average feature importance in the original model (Fig.  1).  The  SHAP  feature  importance  coincides  with  the feature importance observed in Fig. 4.  Finally,  we  briefly  investigated  the  robustness  of  the model  by  subjecting  it  to  an  evasion  attack.  The  large influence observed for text perturbations correlates well with our SHAP analysis which suggests a large model dependence on the text. We find that the model is vulnerable to changes in the transaction text. However, since vendors seldomly change their formulas for generating transaction texts and companies seldomly change their names, the text is mostly an immutable property  of  the  transactions.    This  vulnerability  is  therefore deemed  low  risk  for  such  transactions.  In  the  case  of  bank transfer  transactions  where  customers  may  enter  free  text, there could be risk of masking fraudulent or money laundering transactions. If the classifier was ever to be used to detect such transactions this would be a point to address. A  We  would  like  to  thank  the  SpareBank  1  Alliance  for useful  discussions  and  SpareBank  1  SR-Bank  for  providing anonymized transaction data. R  The  additive  SHAP  values  allowed  us  to  identify  the transaction  text  as  the  dominant  feature  for  transaction classification.  Among  the  remaining  features,  the  shallow decision trees identified the transaction code as the feature that filters  the word2vec text embedding. In the large decision tree, the words from the transaction text were scattered throughout the nodes. It remains to be seen whether this is a general property. transaction  from  heterogeneous  clusters  of We  evaluated  the  fidelity  of  the  explanations  by comparing their prediction with those of the transaction model [19].  The  explanation  model  typically  made  the  same prediction  as  the  transaction  model  for  98%  of  the  labelled data.  To evaluate the transaction model\u2019s robustness to changes in the transaction text, we scored a perturbed dataset and found that  the  model  prediction  typically  changed  for  80%  of transactions with a single word replaced. We then repeated the experiment with a new set of perturbed transactions, where we replaced more than one word; this typically resulted in 90% of the transactions being classified differently.  VII. C D F W  In  this  paper,  we  introduced  a  transaction  classification model which is the basis for future value adding products for banking customers, with the end goal of developing a digital financial  advisor.  It  is  thus  imperative  that  the  transaction classifier  be  implemented  in  accordance  with  two  of  the principles of responsible AI: explainability and robustness. We found that decision trees and random forests derived from the transaction model may offer explainability, but their complexity (> 50 000 nodes) limits their interpretability.  We  mitigated  the  complexity  of  the  feature  space  by identifying the transaction text as  salient. The text  was then used to cluster the dataset, before fitting a small tree to each cluster  where  necessary.  These  decision  trees  offered improved interpretability as they were smaller and easier for a human to understand.",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "cf33d6c6-7662-4b27-8afb-490bf3da8bdd",
                    "text": "",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "b5faf3b7-f6d6-4f8e-b649-b7860d228589",
                    "text": "Global  surrogate  modelling  is  a  well-documented approach to model explainability [4]. In this study, we trained both  a  single  decision  tree  and  a  random  forest  as  global surrogates to explain the model. We used a random sample of 10% of the total dataset (about half a million transactions) for training,  \ud835\udc4b \u2282 \ud835\udc4b  \u2227   |\ud835\udc4b | = 0.1 \u00d7 |\ud835\udc4b| ,  while  testing was done on a randomly sampled set of 100 000 transactions, \ud835\udc4b \u2282 \ud835\udc4b  \u2227   \ud835\udc4b \u2209 \ud835\udc4b   \u2227   |\ud835\udc4b | = 100 000.  We  used  these  train  and  test  sets  to  fit  a  decision  tree classifier  and  a  random  forest  classifier  with  50  individual trees.  The  performance  and  human  understandability  of  the tree  and  forest  were  used  as  a  baseline  to  compare  with  a hybrid clustering / decision tree approach discussed below.",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "39e9e6b1-e1d6-4167-b131-1495d1007a2a",
                    "text": "We estimate the feature importance using SHAP [10]. The feature  importance, \ud835\udf19  was  estimated  for  each  input  feature \ud835\udc65 \u205f\ud835\udc56 \u2208 {1, \u2026 , \ud835\udc41}  where  \ud835\udc41  is  the  total  number  of  encoded features.  Note  that  due  to  encoding, \ud835\udc41 > 7 where  7  is  the number of original features. Equations  (1)  and  (2)  illustrate  how  the  features  are prepared, with equation (1) referring to the word vectors for the transaction text. SHAP values provide an estimate of the ; however, this is importance of individual features, \ud835\udf19 \u2192 \ud835\udc4bnot useful when the feature of interest is a superfeature: \ud835\udc4b =}, \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc5b}.  In  order  to  derive  the  importance  of  the {\ud835\udc4bsuperfeature  \ud835\udc4b ,  we  aggregate  the  SHAP  values  through addition [10]:   \ud835\udf19 = \u2211 \ud835\udf19 (5)",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "d0c55039-83ff-4cdb-b81d-4112ce70fd6d",
                    "text": "Having identified the most important feature, we clustered the data according to this feature. In Section VI, we show that the most important feature in classification is the transaction text \ud835\udc4b ; we therefore used the DBSCAN algorithm with cosine similarity  as  the  distance  measure.  We  trained  a  set  of \ud835\udc5a superclusters, \ud835\udc50 \u2208 \ud835\udc36, \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc5a},  where \ud835\udc5a is  the  number of classes in the output \ud835\udc66 \u2208 \ud835\udc4c, \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc5a}.  From  these  superclusters,  we  considered  the  individual words from the texts contained in each cluster. We created a list of keywords \ud835\udc58 \u2208 \ud835\udc3e for each supercluster \ud835\udc56 by extracting unique words from each cluster. Stop words such as place and street names were removed from the keyword lists. Formally, \ud835\udc58 \u2208 \ud835\udc3e\u205f \u2227 \u205f \ud835\udc58 \u2229 \ud835\udc58 = \u2205 \ud835\udc56, \ud835\udc57 \u2208 {1, \u2026 , \ud835\udc5a} \u2227  \ud835\udc56 \u2260 \ud835\udc57  (6)  The keywords  were used as rules that associate a  given transaction  text  with  a  given  supercluster.  For  any  given transaction text \ud835\udc61, each word in the text \ud835\udc64 \u2208 \ud835\udc61 was given the opportunity  to  vote  for  a  supercluster \ud835\udc50;  we  considered  the keyword list for each supercluster; if a word \ud835\udc64 appears in the keywords list \ud835\udc58 , the word voted for supercluster \ud835\udc50 . The votes for  all  words  were  tallied  and  the  supercluster  was  selected through  majority  vote.  If  no  supercluster  was  found,  i.e.  no words  appear  in  the  keyword  list,  a  default  supercluster representing the class \u201cother\u201d was selected. We used shallow decision trees to filter out those instances that did not belong to the homogeneous majority. This is similar to the approach in [12] and [13]; we intended to simplify the final explanation while simultaneously attaining improved accuracy compared to a single large classifier.",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "cfdcb41c-869a-4137-bd79-3d184b8ea0a0",
                    "text": "In order to test the robustness of the model, we subjected the  model  to  a  targeted  evasion  attack,  leveraging  the the  model.  A  successful newfound  knowledge  about analysis; we used DBSCAN with the distance parameter \ud835\udf16 =0.07. The intent was to train tight clusters. The result was a set  of  clusters  with  high  homogeneity  (95%)  and  a  low percentage of noise (2%), with a total of 12 734 clusters. We then grouped the clusters using the labelled training data into \ud835\udc5a superclusters. Fig. 2 shows a 2-dimensional representation of the supercluster for transactions relating to alcohol. Finally, we fit a small, interpretable decision tree to each supercluster  with  less  than  100%  homogeneity;  the  shallow decision tree provides the final separation and explanation. An example tree is shown in Fig. 3, for cluster number 10 relating to expenditure on kindergartens: adversarial attack therefore suggests not only a vulnerability in the model, but also a working knowledge of the model by the attacker.  The  adversarial  examples  were  generated  by  slightly perturbing  existing  instances,  along  the  feature  of  highest importance,  i.e.  where  the  impact  would  be  greatest.  The perturbations  therefore  targeted the  transaction  text, \ud835\udc4b . The perturbed  set  of  adversarial  examples \ud835\udc4b \u2208 \ud835\udc4b is  therefore defined by:  \ud835\udc65 = {\ud835\udc65 \u2032, \ud835\udc65 , \ud835\udc65 , \ud835\udc65 , \ud835\udc65 , \ud835\udc65 , \ud835\udc65 } \ud835\udc65 \u2208 \ud835\udc4b\u205f \u2227 \u205f \ud835\udc65 \u2208 \ud835\udc4b   (7) Words from the texts were selected by matching the words with the keyword dictionary, \ud835\udc3e. If a word appeared in one of the  keyword  lists \ud835\udc58 \u2208 \ud835\udc3e,  then  that  word  was  replaced  by  a word  from  another  list  \ud835\udc58 \u2208 \ud835\udc3e ,  where  \ud835\udc56 \u2260 \ud835\udc57\u205f \u2227 \u205f\ud835\udc56, \ud835\udc57 \u2208{1, \u2026 , \ud835\udc5a}. VI.  R  The labelled set of transactions was divided into training (80%),  validation  (10%)  and  test  (10%)  sets.  The  trained model  achieved  a  mean  accuracy  of  98.2%,  with  a  95% confidence interval of 0.04% in 20 experiments. As a baseline to an explanation, we trained a decision tree and a random forest as surrogate models on data labelled by the DNN. The decision tree achieved an accuracy of 95.9%, while  the  random  forest  (with  50  estimators)  achieved  an accuracy  of  96.7%.  Both  the  single  decision  tree  and  the random  forest  had  in  excess  of  50  000  nodes.  Even  though decision  trees  inherently  explained  the  rules  they  have derived,  they  clearly  do  not  provide  interpretability  in  this instance.  A.  Feature Importance and Model Explanation The results from the SHAP feature importance evaluation are  clear  evidence  of  the  model\u2019s  bias  towards  the  text features.  As  seen  in  Fig.  1,  the  transaction  text  is  largely responsible  for  the  predictions.  This  is  consistent  with  the importance of transaction text for the partial labelling of the original dataset.  Knowing  that  the  text  is  the  most  important  feature  for model  classification  is  not  an  adequate  explanation  of  the functioning of the model. To determine how the model uses the  text,  we  used  its  vector  representation  in  a  clustering  In  Fig.  3,  the  tree  distinguishes  between  transactions relating  to  kindergarten  and  those  relating  to  property management.  The  transaction  code  \u201c014\u201d  is  the  most important feature in this classification, while the amount and day  of  week  also  play  roles.  In  Fig.  4,  we  plot  the  feature importance for the decision tree shown in Fig. 3.  Fig. 4 shows that the transaction code is the most important feature. This correlates well to our previous estimated of the overall and average feature importance in the original model (Fig.  1).  The  SHAP  feature  importance  coincides  with  the feature importance observed in Fig. 4.  Finally,  we  briefly  investigated  the  robustness  of  the model  by  subjecting  it  to  an  evasion  attack.  The  large influence observed for text perturbations correlates well with our SHAP analysis which suggests a large model dependence on the text. We find that the model is vulnerable to changes in the transaction text. However, since vendors seldomly change their formulas for generating transaction texts and companies seldomly change their names, the text is mostly an immutable property  of  the  transactions.    This  vulnerability  is  therefore deemed  low  risk  for  such  transactions.  In  the  case  of  bank transfer  transactions  where  customers  may  enter  free  text, there could be risk of masking fraudulent or money laundering transactions. If the classifier was ever to be used to detect such transactions this would be a point to address. A  We  would  like  to  thank  the  SpareBank  1  Alliance  for useful  discussions  and  SpareBank  1  SR-Bank  for  providing anonymized transaction data. R  The  additive  SHAP  values  allowed  us  to  identify  the transaction  text  as  the  dominant  feature  for  transaction classification.  Among  the  remaining  features,  the  shallow decision trees identified the transaction code as the feature that filters  the word2vec text embedding. In the large decision tree, the words from the transaction text were scattered throughout the nodes. It remains to be seen whether this is a general property. transaction  from  heterogeneous  clusters  of We  evaluated  the  fidelity  of  the  explanations  by comparing their prediction with those of the transaction model [19].  The  explanation  model  typically  made  the  same prediction  as  the  transaction  model  for  98%  of  the  labelled data.  To evaluate the transaction model\u2019s robustness to changes in the transaction text, we scored a perturbed dataset and found that  the  model  prediction  typically  changed  for  80%  of transactions with a single word replaced. We then repeated the experiment with a new set of perturbed transactions, where we replaced more than one word; this typically resulted in 90% of the transactions being classified differently.  VII. C D F W  In  this  paper,  we  introduced  a  transaction  classification model which is the basis for future value adding products for banking customers, with the end goal of developing a digital financial  advisor.  It  is  thus  imperative  that  the  transaction classifier  be  implemented  in  accordance  with  two  of  the principles of responsible AI: explainability and robustness. We found that decision trees and random forests derived from the transaction model may offer explainability, but their complexity (> 50 000 nodes) limits their interpretability.  We  mitigated  the  complexity  of  the  feature  space  by identifying the transaction text as  salient. The text  was then used to cluster the dataset, before fitting a small tree to each cluster  where  necessary.  These  decision  trees  offered improved interpretability as they were smaller and easier for a human to understand.",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "967d4c47-93fc-4c41-b556-d79944c8c0ec",
                    "text": "",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                },
                {
                    "id": "cf374058-c4c0-4b4a-9c0b-1e0c54eaa3ed",
                    "text": "The results from the SHAP feature importance evaluation are  clear  evidence  of  the  model\u2019s  bias  towards  the  text features.  As  seen  in  Fig.  1,  the  transaction  text  is  largely responsible  for  the  predictions.  This  is  consistent  with  the importance of transaction text for the partial labelling of the original dataset.  Knowing  that  the  text  is  the  most  important  feature  for model  classification  is  not  an  adequate  explanation  of  the functioning of the model. To determine how the model uses the  text,  we  used  its  vector  representation  in  a  clustering  In  Fig.  3,  the  tree  distinguishes  between  transactions relating  to  kindergarten  and  those  relating  to  property management.  The  transaction  code  \u201c014\u201d  is  the  most important feature in this classification, while the amount and day  of  week  also  play  roles.  In  Fig.  4,  we  plot  the  feature importance for the decision tree shown in Fig. 3.  Fig. 4 shows that the transaction code is the most important feature. This correlates well to our previous estimated of the overall and average feature importance in the original model (Fig.  1).  The  SHAP  feature  importance  coincides  with  the feature importance observed in Fig. 4.  Finally,  we  briefly  investigated  the  robustness  of  the model  by  subjecting  it  to  an  evasion  attack.  The  large influence observed for text perturbations correlates well with our SHAP analysis which suggests a large model dependence on the text. We find that the model is vulnerable to changes in the transaction text. However, since vendors seldomly change their formulas for generating transaction texts and companies seldomly change their names, the text is mostly an immutable property  of  the  transactions.    This  vulnerability  is  therefore deemed  low  risk  for  such  transactions.  In  the  case  of  bank transfer  transactions  where  customers  may  enter  free  text, there could be risk of masking fraudulent or money laundering transactions. If the classifier was ever to be used to detect such transactions this would be a point to address. A  We  would  like  to  thank  the  SpareBank  1  Alliance  for useful  discussions  and  SpareBank  1  SR-Bank  for  providing anonymized transaction data. R  The  additive  SHAP  values  allowed  us  to  identify  the transaction  text  as  the  dominant  feature  for  transaction classification.  Among  the  remaining  features,  the  shallow decision trees identified the transaction code as the feature that filters  the word2vec text embedding. In the large decision tree, the words from the transaction text were scattered throughout the nodes. It remains to be seen whether this is a general property. transaction  from  heterogeneous  clusters  of We  evaluated  the  fidelity  of  the  explanations  by comparing their prediction with those of the transaction model [19].  The  explanation  model  typically  made  the  same prediction  as  the  transaction  model  for  98%  of  the  labelled data.  To evaluate the transaction model\u2019s robustness to changes in the transaction text, we scored a perturbed dataset and found that  the  model  prediction  typically  changed  for  80%  of transactions with a single word replaced. We then repeated the experiment with a new set of perturbed transactions, where we replaced more than one word; this typically resulted in 90% of the transactions being classified differently.  VII. C D F W  In  this  paper,  we  introduced  a  transaction  classification model which is the basis for future value adding products for banking customers, with the end goal of developing a digital financial  advisor.  It  is  thus  imperative  that  the  transaction classifier  be  implemented  in  accordance  with  two  of  the principles of responsible AI: explainability and robustness. We found that decision trees and random forests derived from the transaction model may offer explainability, but their complexity (> 50 000 nodes) limits their interpretability.  We  mitigated  the  complexity  of  the  feature  space  by identifying the transaction text as  salient. The text  was then used to cluster the dataset, before fitting a small tree to each cluster  where  necessary.  These  decision  trees  offered improved interpretability as they were smaller and easier for a human to understand.",
                    "reference": "[1] Craige Maree, Jethro E. Modal, and Christian W. Omlin. 2020. Towards responsible AI for financial transactions. In 2020 IEEE Symposium Series. IEEE, 2020. Retrieved from https://arxiv.org/pdf/2206.02419"
                }
            ]
        },
        {
            "paper_title": "A human rights-based approach to responsible AI",
            "authors": "V Prabhakaran, M Mitchell, T Gebru\u2026",
            "publication_info": "arXiv preprint arXiv \u2026 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2210.02667",
            "chunks": [
                {
                    "id": "910ea3ee-78f2-42bd-8f39-a170ad457236",
                    "text": "",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "d2eda089-7f45-4a2b-b7c0-e03bceeef03d",
                    "text": "Fairness, accountability, transparency and ethics (FATE) research in AI contends with questions around how AI modelsmight be problematically biased, unfair, or unethical, and how to make them \u201cfairer\u201d and more \u201cethical\u201d. However, theresearch community working in this area greatly lacks in terms of geo-cultural diversity [27], resulting in the researchbeing primarily framed in the Western context, by researchers mostly situated in Western institutions/organizations,to mitigate social injustices prevalent in the West, using data from the West, and implicitly imparting Western valuesystems [66]. On the other hand, these research insights are meant to intervene on platforms that are globally present,serving a global population from diverse societies, cultures and values, with their own forms of injustices.A core concern in this arrangement is that of value imposition, where local values, i.e., values that are local tothe regions where the interventions are built, implicitly shape and inform global systems without any or much roomfor discussion or contestation from those a\ufb00ected by those interventions. More speci\ufb01cally, interventions designed toaddress FATE failures necessarily impart a normative value system, but the values that guide the proposed solutionsare rarely recognized as sites of contestation. This is problematic because while there may be ethical principles for MLthat garner a degree of consensus across di\ufb00erent value systems, in a pluralistic world this consensus is not somethingthat should be assumed. Instead, we need to be explicit about the values that underpin the quest for ethical and justAI, and to cultivate an active debate about those values, critically examining and evaluating claims about them [28].Another shortcoming of not being explicit about what normative value systems shape the interventions is thevagueness it entails, making it harder to arrive at a common vocabulary and shared understanding between computerscientists and civil society. Such a shared understanding is crucial to bridge the gap between research and practice,especially in a way that e\ufb00ectively supports the priorities of the latter constituency. This is especially important giventhe need for critical examinations that require deeper understanding of the societal contexts in which interventions are2 Vinodkumar Prabhakaran, Margaret Mitchell, Timnit Gebru, and Iason Gabrielenvisioned [7, 9, 36, 66] and the need for participatory methods to incorporate marginalized stakeholder perspectives[22, 44, 51] when shaping these interventions.In this paper, we argue that the doctrine of human rights can serve as a starting point to help address some ofthese gaps. The role of human rights as a legal framework, through which AI-related harms may be identi\ufb01ed, hasbeen explicitly evoked by (or implicitly shaped) various AI accountability initiatives within the industry as well asgovernmental and civil society bodies (e.g., [16, 48, 52, 61]). However, we take a broader view of human rights andargue that, in addition to this legal role, they may play three further functions. First, human rights, understood as a setof moral claims, have a measure of intercultural and cross-cultural validity, which means that they can support valuealignment for AI systems across a range of di\ufb00erent national and social contexts. Second, they can help illuminatethe concurrent responsibilities of various actors, given that they apply to states, organizations and individuals. Third,they provide a shared vocabulary and framework that technologists and practitioners may productively invoke toaddress the claims and concerns of the global civil society and the people impacted by these technologies. To clarifythis distinction, we start by distinguishing three di\ufb00erent aspects of human rights \u2014 (1) as a set of moral claims, (2) asa legal regime and set of instruments, and (3) as a cultural practice and global social movement. We then delve deeperinto three speci\ufb01c rights enshrined in the Universal Human Rights Declaration (UDHR), in order to illustrate howhuman rights might shape responsible AI development and deployments. While we acknowledge that a human rightsperspective is not a panacea for addressing all issues in AI use, and that the provenance of UDHR in particular hasbeen contested, we argue that a cross-cultural set of human rights, exempli\ufb01ed here by UDHR, can be a groundingframework for value alignment in AI.",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "771421b7-4879-49e5-93ea-5bb847918904",
                    "text": "From a philosophical perspective, human rights are the fundamental rights every human being holds simply by virtueof their birth, regardless of their age, ethnic origin, location, language, religion, nationality, ethnicity, or any otherstatus. The doctrine of human rights\u2014which maps out their content, meaning, and consequences\u2014has a number ofdi\ufb00erent parts.First and foremost, human rights can be understood as a set of moral claims anchored in the notion that human lifehas value and that there are aspects of personhood that must be protected. These claims can be grounded in a numberof di\ufb00erent ways but they tend to draw support from recognition that there are a set of core human interests thatare widely shared, from common appreciation of the need to respect human autonomy and freedom, and from sharedrecognition of the dignity of human life [6, 24, 26, 34, 73]. In each case, there is a consideration that is strong enoughto create a duty on other actors to only treat people in certain ways. In the words of Henry Shue, those who possesshuman rights can make justi\ufb01ed demands that the actual enjoyment of a good be socially guaranteed against standardthreats to those things [73]. Taken together, the idea of human rights holds that we owe it to each other to build aworld in which the ability to enjoy certain goods, such as physical security, health and education, are widely enjoyedby all.Second, human rights are part of a legal regime and set of instruments that aim (in part) to make these values a reality[12]. While the idea of universal rights has a long intellectual lineage, the modern conception of human rights emergedlargely after the Second World War in response to the atrocities that had been committed and the moral trauma ofA Human Rights-Based Approach to Responsible AI 3the Holocaust [18]. This experience helped seed the desire to forge international agreement around the protectionstates owed to their citizens (and more widely), culminating in the adoption of the Universal Declaration of HumanRights (UDHR) by the United Nations General Assembly in 1948. The UDHR consists of 30 di\ufb00erent articles a\ufb03rmingan individual\u2019s rights, such as the right to life, liberty and security, right to privacy, right to be free of discrimination,and right to freedom of expression \u2014 rights that are critically relevant to building AI-based interventions responsibly.Drafted by a committee that included representatives from China, Chile, the Soviet Union, Lebanon, and India (inaddition to the Western powers at the time), the UDHR was subsequently augmented by the International Covenanton Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR)in 1966. These agreements incorporated insights from a still larger group of countries after the widespread success ofdecolonization movements. They now form part of customary international law and have been adopted by a numberof national and transnational legal bodies such as the International Criminal Court (ICC).Third, human rights are part of a cultural practice and global social movement that focuses on advocacy, empower-ment and critique of existing institutions. From the very beginning, civil society has played a vital role in developinghuman rights doctrine, embedding norms, and monitoring outcomes [45]. As human rights advocates have long un-derstood, it is not enough for rights to exist only in moral or juridical form: people also need to be informed abouttheir rights and exercise them, if they are to have a signi\ufb01cant bearing upon outcomes. Human rights advocacy hashad a number of important successes, including in the \ufb01eld of women\u2019s rights, indigenous rights, and disability rights,helping to shape global norms around how people may and may not be treated [64]. Civil society organizations havealso explicitly and successfully used the human rights mechanisms to contest government policies (e.g., by makingsubmissions to the UN\u2019s Universal Periodic Review [31]).E\ufb00orts are now underway to incorporate human rights into the design of AI systems, and to identify novel ways inwhich AI can support human rights practices. In applied contexts, there have been a number of attempts to integrateAI with human rights monitoring. These include e\ufb00orts to improve human rights reporting in con\ufb02ict zones usingautomated analysis of satellite imagery [47]. Additionally, in the context of discussions about fairness and AI ethics,human rights frameworks have been proposed as a tool or mechanism to promote greater accountability to those inneed [16, 48, 52, 61]. In particular human rights advocates have criticised the tendency, sometimes present in FATEdiscourse, to imply that the relevant ethical norms for AI technology need to be discovered for the very \ufb01rst time \u2014or to focus on sophisticated statistical analysis of algorithms without paying due attention to the way in which theirdeployment may result in actual societal harms [70]. Against these viewpoints, human rights doctrine contains a setof well-established principles that are robustly centered upon human vulnerabilities and human needs. Finally, therehas been notable engagement with speci\ufb01c challenges to human rights posed by new technologies such as fake newsmoderation [49] and predictive policing [50]. For example, Data & Society organized a multidisciplinary workshopin April 2018 exploring how the human rights framework can e\ufb00ectively inform, shape, and govern AI research, de-velopment, and deployment. More recently, [17] organized tutorials at the FAT* 2020 conference on this topic. Wedraw inspiration from these groundbreaking e\ufb00orts, and argue that more substantial engagement from the machinelearning research community with the human rights paradigm can address three major challenges that the \ufb01eld of MLfaces: alignment, the allocation of responsibilities, and participation.4 Vinodkumar Prabhakaran, Margaret Mitchell, Timnit Gebru, and Iason Gabriel",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "eaf96b21-f2fd-4104-8861-088933c70e7c",
                    "text": "Any e\ufb00ort to try and create ethical AI involves imparting a set of human values on the behavior of the AI-based systemsunder consideration, many of which are deployed across the world. While there has been plenty of research withinAI sub-communities such as AI safety [2], machine learning [35], and natural language processing [15, 60], on thechallenges involved in encoding human values into AI systems, less attention has been paid to the normative questionof which set of values/principles should be encoded. Sometimes this question is overlooked altogether; in other casessystem designers proceed on the basis of a preferred principle or theory. Both approaches are problematic given thewide variation in the moral beliefs people actually hold. Indeed, since most FATE research in AI is situated in theWest, it does not meaningfully engage with the conditions, values, and histories of non-West contexts [41, 57, 66]. AsBirhane and Cummins note in [8], \u201cit is possible that what is considered ethical currently and within certain domainsfor certain societies will not be received similarly at a di\ufb00erent time, in another domain, or for a di\ufb00erent society.\u201d Thus,the Western values implicitly encoded into AI systems may be at odds with other value systems, creating the risk ofproblematic value imposition when these technologies are deployed globally [28].In order to address this challenge, Gabriel [28] argues that we need to identify fair processes for selecting values toencode in AI systems. In this context, one promising approach focuses on the possibility of identifying an \u201coverlappingconsensus\u201d between the di\ufb00erent moral belief systems around the world. To make progress in this direction we need toask: are there any ideals that command widespread, or even global, assent? Although no candidate doctrines is withoutlimitations, with respect to cross-cultural validity, the doctrine of universal human rights is particularly promising [23,p. 150]. Indeed, while the modern notion of human rights has the speci\ufb01c historical lineage that we have outlined, astrong case can be made for the notion that these rights are no longer time-bound or culturally parochial. To startwith, there is evidence that these beliefs \ufb01nd a degree of cross-cultural support in African, Islamic, Western, andConfucian traditions of thought [19, p335-343]. Moreover, human rights have been adopted and actively claimed bypeople around the world across a wide range of contexts ranging from indigenous rights movements [56] to the ArabSpring [38]. Hence, we believe human rights can serve as a legitimate goal for building value-aligned AI systems, arequirement that entails both that they need to respect human rights directly and also that they are deployed in waysthat strengthen human rights practice.Yet, the proposed focus on human rights faces four objections. First, it might be thought to lead to certain knowledgegaps: can a human rights framework adequately factor in collective goods, for example, or concerns about distributivejustice? This criticism has often been made by Marxist scholars and contemporary critics of human rights doctrinesuch as Samuel Moyn [54]. In certain respects, the concerns they raise are well-founded: we agree that there may wellbe goods and considerations that are not adequately expressed in the language of rights [63]. Yet, our claim is not thathuman rights capture the full space of AI ethics but only that they capture a set of particularly important claims \u2014a \u2018morality of the depths\u2019 [73] \u2014 for which there is widespread support. Understood in this way, respect for humanrights would be necessary but not su\ufb03cient to ensure the ethical design and deployment of AI systems.Second, scholars have challenged the purported \u201cuniversalilty\u201d of human rights by mobilising arguments that drawupon cultural relativism [80]. Proponents of this viewpoint note that moral norms vary signi\ufb01cantly across cultures,and that what is \u201cregarded as a human rights violation in one society may properly be considered lawful in another,and Western ideas of human rights should not be imposed upon Third World societies\u201d [76]. However, as Donnellynotes, a more nuanced analysis of this matter resists the reduction of human rights doctrine to this single axis, andA Human Rights-Based Approach to Responsible AI 5asks instead \u201chow human rights are (and are not) universal and how they are (and are not) relative\u201d [24]. In this context,he notes that UDHR has a very strong claim to what he terms \u201crelative universality\u201d, insofar as it represents a minimalresponse to basic cross-cultural human values and the threats to human dignity posed by modern institutions. At thesame time, from a practical standpoint, the ways in which human rights interface with more speci\ufb01c cultural values isan important aspect to consider in the doctrine\u2019s integration into AI ethics [82]. For instance, human rights doctrinemay serve as an useful starting point that provides a baseline set of values that enjoy cross-cultural recognition, thatcan then be built on for speci\ufb01c contexts, especially when rights of speci\ufb01c communities are in question.Third, despite what has been said, we might worry that human rights are still geographically parochial, or worse, aform of neo-imperialism [81]. In this context, it is important to recognize the historicity of human rights, the evolvingcharacter of human rights discourse, and the political undercurrent that in\ufb02uence how these norms are operationalizedin practice. There is certainly evidence that human rights claims have been deployed in a strategic or politicized wayby Western states or NGOs at certain times, and that they still operate under the burden of this legacy today [37].However, this does not detract from the reality of their widespread support, or from the fact that they have been usedequally in an opposing manner \u2014 to oppose authoritarian regimes, and in anti-colonial movements to resist externalintervention [13, 40]. Moreover, Kathryn Sikkink notes that \u201cvoices and actors from the Global South were deeplyinvolved in demanding the international protection of human rights and in building the institutions that started tomake enforcement of these rights possible\u201d [74, p. 25]. As a result of this process of contestation, something of valuehas emerged: modern human rights doctrine is often invoked by people to resist interference with their political rightsor ability to govern themselves.Finally, it might be thought that a human rights framework is too anthropocentric, emphasising the value of humanlife while neglecting non-human life and other bearers of value. However, as we noted before, insisting on the impor-tance of human rights is not the same as asserting that only human rights matter: we agree other things matter aswell. Moreover, we suggest that a renewed focus on human rights \u2014 and on the ways in which people can be seriouslyharmed \u2014 is particularly important for AI research at the present moment given the tendency to approach ethicalchallenges as technical problems [10]. Afterall, a great deal has been written about the formal properties of modelsand the biases they embody, however, the need to better understand the harms these models cause is critical [1, 4].By way of illustration, Blodgett et al. conducted a survey of 146 research papers analyzing bias in natural languageprocessing models and found only limited engagement with the question of why bias is harmful, in what ways, and towhom [11]. As a consequence, it is hard to be con\ufb01dent that proposed mitigations will have a positive impact on thosewho encounter these harms. By way of contrast, a human rights based approach to fairness research can help forgea stronger connection between models, the socio-technical systems they operate in, and salient harms. It supports areorientation away from formal principles and towards human welfare and a person\u2019s capacity to \ufb02ourish.",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "05fc62e3-fbdd-4387-bb8a-8f292089fa34",
                    "text": "The moral responsibilities entailed by human rights treaties apply to a wide range of actors and environments. Statesare often the primary duty-bearers, with respect for human rights being a key element of regime legitimacy [62]. Indi-viduals also have duties not to violate human rights and there are a number of national and supranational mechanisms,such as the ICC and the European Court of Human Rights, to address this. Increasingly, human rights are also under-stood to apply to companies or organizations and to those employed by them \u2014 including for people working in thetechnology sector. These frameworks can therefore help us to understand who has a moral responsibility to do what,serving as a well-spring for responsible innovation in AI.6 Vinodkumar Prabhakaran, Margaret Mitchell, Timnit Gebru, and Iason GabrielTo begin with, human rights have rami\ufb01cations for the way in which scienti\ufb01c and commercial research is conducted,especially when research concerns human subjects. This connection between bioethics and human rights was explicitlyrecognized in the aftermath of the Holocaust [3], leading to the development of safeguards for participants such asthose described in the 1964 Helsinki Declaration and the 1979 the Belmont Report. More recently, the the UniversalDeclaration on Bioethics and Human Rights (UDBHR), adopted by UNESCO\u2019s General Conference in 2005, furthera\ufb03rmed the importance of human dignity, human rights and fundamental freedoms when it comes to research. Givenrecent controversy surrounding numerous publications in the \ufb01eld of ML and existing data collection practices, theseprinciples, and the institutional review protocols they necessitate, serve as a valuable precedent for the broader MLresearch community [78].The focus on human rights can also help us understand the responsibilities technology companies owe within thewider ecosystem of duty bearers. The 2011 United Nations Guiding Principles on Business and Human Rights (UNGP)outline the speci\ufb01c responsibility of businesses to respect human rights, including by identifying, preventing, andmitigating salient human rights risks [61]. As the UNGPs make clear, when it comes to human rights, it is important tomove beyond good intentions: those developing new technologies need to make an informed e\ufb00ort to understand theimplications that a technology will have for rights holders \u2014 and to put in place measures that ensure that the rightsare upheld through processes of evaluation, review and assessment.",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "01d1acf3-a179-4ca8-b36e-4407922f2069",
                    "text": "A major challenge for the AI ethics community centers upon the inclusion of relevant voices when addressing thedesign and governance of AI systems, and the cultivation of an e\ufb00ective lingua franca \u2014 or participatory processes\u2014 that make it easier for needs of historically marginalized communities to be articulated and for their claims to bee\ufb00ectively met. Against this backdrop, a key cluster of human rights centers upon and recognize the value of wideparticipation. For example, Articles 20 and 21 of the UDHR guarantee \u201cfreedom of peaceful assembly and association\u201dand \u201cfreedom to participate in political processes\u201d respectively. Moreover, the idea that people are \u201crights-holders\u201dserves as a basis for engagement and source of authority that is quite di\ufb00erent from engagement as \u201cconsumers\u201d,\u201ccitizen[s]\u201d, \u201cstakeholders\u201d or \u201ca\ufb00ected parties\u201d. Taken together, these two interrelated elements of participation andempowerment explain why approaches towards responsible AI, pioneered by civil society organizations, have tendedto invoke human rights more often than fairness research situated within the machine learning community.For instance, the Mapping Consensus in Ethical and Rights-based Approaches to Principles for AI report publishedby the Berkman Klein Center for Internet & Society [25] found that among thirty-six di\ufb00erent sets of AI principlespublished by private and public agencies, human rights are a major focus \u2014 with civil society and trans-nationalgovernmental agencies relying most heavily on this framework. Documents drafted by trans-national governmentalagencies such as AI for Europe by the European Commission and Ethics Guidelines for Trustworthy AI by the EuropeanHigh Level Expert Group on AI also foreground human rights. And out of the \ufb01ve civil society-drafted AI principlesdocuments, three of them \u2014 Toronto Declaration by Amnesty International & Access Now, Universal Guidelines forAI by The Public Voice Coalition, and Human Rights in the Age of AI by Access Now, \u2014 explicitly adopt a humanA Human Rights-Based Approach to Responsible AI 7rights framework, while a fourth one, Top 10 Principles for Ethical AI by UNI Global Union, includes discussion ofhuman rights risks.By way of contrast, research done within the ML fairness, accountability, transparency and ethics research com-munity rarely invokes a rights-based framework. Drawing upon a survey of 138 papers/abstracts published in theFAT* conference in 2019 and 2020, we found only one research paper [43] and 2 tutorial abstracts [17, 75] that engagewith the human rights scholarship. More precisely [43] proposes an impact assessment methodology that they situatewithin the human rights assessment literature, [17] conducted a translation tutorial between human rights scholarshipand FATE research and [75] conducted a hands-on tutorial where they used a human rights framing to test academicconcepts and their formulation in policy initiatives around algorithmic accountability and explainability.In addition to the substantive value of the human rights framework, the salience that human rights have both forpeople a\ufb00ected by new technologies and in policy circles provides additional reason to close this signi\ufb01cant gap in theAI FATE literature. For AI researchers to build e\ufb00ective partnerships with civil society, bridging work needs to be done.In certain domains bridging work is already underway, for instance between the community of NLP researchers work-ing in the space of detecting online abuse and the RightsCon community [59]. Yet, failure to engage with human rightsscholarship more widely, risks leading to a situation in which FATE researchers end up \u2018speaking a di\ufb00erent language\u2019from those their products a\ufb00ect, making it harder to conduct participatory research with civil society organizationsand foregoing a major opportunity to strengthen the practice of AI ethics more widely.",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "0d6a3f74-3e96-4c95-914b-8593077cc8ba",
                    "text": "The potential impact of AI on human rights is wide-ranging, something that is recognized by recent commentary onthe human right to science by the UN Economic and Social Council which notes \u2018applications of arti\ufb01cial intelligencein industry or services can lead to enormous gains in productivity and e\ufb03ciency\u2019 while also expressing concern thatalgorithms could be incorporated into weapon systems or used to reinforce discrimination. Rather than attempt tocatalogue the full range of impacts AI might have, we focus in this section on three human rights in particular, to showwhat a human rights framework may add to the responsible AI discussion. For each right, we walk through speci\ufb01cexamples of how the values at play may in\ufb02uence decisions in an algorithmic context.",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "4e07fdc4-5e4f-43eb-878f-a2ec45cd515d",
                    "text": "The right to be free from discrimination is a negative right not to be harmed in certain ways, and it is heavily impactedby prevalent forms of algorithmic bias. This right is enshrined in various universal and regional legal instruments ofhuman rights, and forms one of the core rights in the UDHR. In particular, article 2 of the UDHR states: \u201ceveryone isentitled to all the rights and freedoms set forth in this Declaration without distinction of any kind, such as race, colour,sex, language, religion, political or other opinion, national or social origin, property, birth or other status\u201d, essentiallyextending all the rights enshrined in the declaration to all humans without discrimination.The right to be free from discrimination is also often the right that is most directly relevant to a majority of work inthe FATE community. Fairness research has identi\ufb01ed numerous types of biases in various algorithmic systems [5, 11].An algorithmic system that treats individuals di\ufb00erently based on an attribute such as race or gender, with negative8 Vinodkumar Prabhakaran, Margaret Mitchell, Timnit Gebru, and Iason Gabrielconsequences for group members and without due cause, is in and of itself a violation of the right to be free of dis-crimination. However, certain instances of discrimination that result in withholding other rights have a compoundinge\ufb00ect. For instance, an algorithmic content moderation system that disproportionately censors individuals speaking acertain dialect [67] not only risks interfering with their right to freedom of expression, but also potentially impedestheir right to be free from discrimination in exercising that right.Since early research into algorithmic fairness dealt with the applications of AI in US law enforcement such as predic-tive policing and recidivism prediction, and regulations around anti-discrimination in housing, loans, and educationin the US, the FATE community\u2019s inquiries into this space draws largely on US legal frameworks such as the CivilRights Acts and Fair Housing Act, as well as on US legal concepts of discrimination [33]. One side e\ufb00ect of this isthat the scope of this conversation has also been largely limited to discrimination in the US context, compared to themore global human rights framework. For instance, while the right to be free of discrimination based on religion orlanguage has equal standing within the global human rights framework, these axes of discrimination are rarely dealtwith within the FATE research community, compared to discrimination based on race or gender, which are prominentconcerns in the US American public discourse. This gap, in terms of understanding the full range of characteristics,that may serve as axes of unjust discrimination, can be addressed, in part, through re\ufb02ection on the more expansivecategorisation invoked by the UDHR.Furthermore, indexing FATE research on the legal framework of a particular country carries with it additional risk,when viewed form a global standpoint. For example, a single country\u2019s legal frameworks may not give all groupsadequate protection against discrimination, an issue that looms particularly large in the context of unjust laws ornational practices. To guard against these pitfalls, a more universal set of principles such as the universal humanrights doctrine, may serve as a useful reference point, and as means of checking national laws this kind of gaps ordenial of equal rights to citizens.",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "e48510ae-1449-4466-b467-367afdfb8f74",
                    "text": "The human right to health enshrines \u201cthe right of everyone to the enjoyment of the highest attainable standard ofphysical and mental health\u201d (ICESAR, 1966, Art. 12.1). It works primarily as a positive right that creates a duty forstates to lower infant mortality, promote child development, provide medical services to their populations, and sharemedical knowledge, among other things. Given the growth of ML-enabled services and diagnostic tools within thehealthcare sector [20, 83], AI research has the potential to intersect with this right in important ways.The right to health grounds an entitlement to access health care on terms that are free from discrimination, withparticular attention being paid to protected groups such as women and those with physical disabilities. Given evidencethat racial bias a\ufb00ects algorithms deployed in a healthcare context [55], measures to mitigate the harms that biasescan create prior to an algorithm\u2019s deployment can be understood as a human rights obligation. Additionally, the dutiesthat correspond to the right to health make reference to that \u201chighest attainable standard\u201d of care, a speci\ufb01cation thatacknowledges that the content of the right will vary dynamically according to time and place. For example, govern-ments have an obligation to give their citizens the highest attainable standard of health care. But if the country is verypoor, then the required standard of treatment might still be quite low. Contrastingly, if AI services can be used to bringdown the cost of healthcare, or the lack of access to it, then what is \u2019attainable\u2019 for the country may begin to rise,and the right to health may become a right to access and bene\ufb01t from AI services. This invites us to think about theA Human Rights-Based Approach to Responsible AI 9relationship between healthcare and AI in a di\ufb00erent light \u2014 not only as a source of potential harms but through thelens of human rights-enabling technology.Current estimates suggest that there are not enough trained doctors and physicians globally to provide everyonein the world with a high level of medical care, a problem that is particularly true in low-income countries [68]. Thisshortfall could be addressed through the global redistribution of economic resources [14]. However, it is also somethingthat AI researchers are in a special position to in\ufb02uence, through the creation of diagnostic tools that can be deployeda\ufb00ordably at scale and the development of customized digital healthcare services. For example, ML-enabled technologyis now being used to detect diabetic retinopathy at scale in India [83]. A similar point, about the creation of potentiallylow-cost tools and services, holds true for the human right to education. AI-enabled services could, in principle, makeit possible for many more children to enjoy customised education in their own language, thereby improving globalliteracy and learning.In both cases it is important to steer clear of the pitfalls of technological solutionism [53]. As human rights advocatesnote, the exercise of human rights and impediments to them are frequently political in nature [29, 71]. Yet theseopportunities also ground a positive aspiration for AI: that it will expand the feasibility frontier so that people canenjoy a higher standard of human rights ful\ufb01lment around the world. Moreover, this goal dovetails with the SustainableDevelopment Goals [77] and would likely \ufb01nd widespread support among those who experience limited access toservices.",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "2f3beaee-fe40-481a-8f2a-1857024a2d0f",
                    "text": "The human right to science states that everyone has a right to \u201cshare in scienti\ufb01c advancement and its bene\ufb01ts\u201d (UDHR,Art. 27). Science is understood here to include: (1) knowledge, (2) the application of that knowledge, and (3) the methodof the knowledge production. Moreover, the right applies both to scienti\ufb01c knowledge itself and to the bene\ufb01ts it creates.This right has special relevance for AI both because it can be understood as a scienti\ufb01c practice and also because of theway in which AI is increasingly used to advance scienti\ufb01c progress, as with DeepMind\u2019s AlphaFold which successfullypredicted the structure of almost every known protein [72].The human right to science advances an ideal of inclusive science and prohibits discrimination both among thoseemployed in scienti\ufb01c pursuits and among its bene\ufb01ciaries. On this point Michelle Bachelet, UN High Commissionerfor Human Rights a\ufb03rms that \u201cthose participating in the global scienti\ufb01c e\ufb00ort should. . . take into account the needsand experiences of women, members of minority communities, Indigenous scholars, persons with disabilities, peopleliving in poverty and people living in less developed countries \u2013 among others. Only then will research fully addressall communities \u2013 and contribute to reducing the unequal access to scienti\ufb01c developments and capabilities acrossdi\ufb00erent countries and regions\u201d. This element of the right to science underscores the need to systematically promotediversity, equity and inclusion in AI research.Signi\ufb01cantly, the human right to science also bears upon the distribution of bene\ufb01ts enabled by science. The UDHRclearly states that the right is to be interpreted in a way that respects the intellectual property of researchers. However,it situates these \u201cmoral and material interests\u201d within the wider aspiration that science should be geared towardsful\ufb01lment of human rights. Speaking on behalf of CERN and the WHO among others, Bachelet states that \u201cthe bene\ufb01tsof scienti\ufb01c and medical progress were always meant to be shared. The great beauty of science is that it has no borders\u2013 and that, working together, every scientist and student of science can contribute to the shared knowledge and bene\ufb01t10 Vinodkumar Prabhakaran, Margaret Mitchell, Timnit Gebru, and Iason Gabrielof all\u201d. In the context of AI research, the human right to science asserts the importance of this technology ultimatelybene\ufb01ting a large section of humanity, including those historically excluded from the bene\ufb01ts of scienti\ufb01c advances.Without the assersion of this right, scienti\ufb01c progress may otherwise continue to exclude insights, ideas, and concernsfrom people who are historically excluded, pulling AI research farther from a trajectory that supports the global socialgood.",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "4db4f402-b1a2-49e5-a105-7a7d88740471",
                    "text": "In this paper, we have sought to show how greater attention to human rights can help ground the aspiration to buildmore ethical AI systems, anchored in values that have a measure of cross-cultural a\ufb03rmation. Orienting the researchin this space away from the machines and the risks of their biases, and towards humans and the risks to their rights,can help center conversation around the harms caused by a technology, including speci\ufb01c consideration of who isharmed, and how those harms may be mitigated. This reframing also has the potential to better align e\ufb00orts by theFATE community to improve AI systems with the wider global advocacy movement that is committed to securinghuman rights and their ful\ufb01lment.In support of these goals, future research on human rights-based approaches towards to AI ethics could help bridgethe gap in multiple ways. First, there is a need for translational research in this space that can address, more pre-cisely, how human rights principles map to the current ethics-based considerations in AI, such as fairness, consent,privacy, and ownership. Such research could be aimed at building a shared vocabulary of concerns, values, and expectedoutcomes as an important \ufb01rst step for meaningful bridging between computer science researchers and civil societyactivists working in this space. This should crucially include clarifying the needs of civil society that are overlookedand potentially easily addressed through technology, as well as the challenges of ensuring fairness of algorithmicpredictions at scale.Another line of work could look into the functional aspects of a human rights based inquiry into AI ethics. Forinstance, what does a human rights based inquiry into ML fairness reveal that existing methodologies do not. Theworked example in Appendix A in the context of online content moderation demonstrates some of these functionalaspects of a human rights based approach. In particular, this approach pushes us to identify the rights holders, toidentify which of their rights are at risk, and to map out how those risks interact with the claims of other right holders.Thus, it is essential for AI researchers to contend with various trade-o\ufb00s, when determining how to intervene. Forinstance, fairness researchers need to consider how bias mitigation measures, designed to mitigate risks to the humanrights of content creators, might create or increase risks to certain other rights of the audience.Finally, advances in AI have the potential to play an important role in enabling stronger human rights ful\ufb01lmentaround the world. As we lay out in Section 3, AI has already been shown promise when it comes to extending the scopeand content of various human rights (such as the right to health and right to scienti\ufb01c advancements) to marginalizedcommunities. Future research should consider what role AI can play in enhancing access to health and education forcommunities in lower-income countries. Similarly, AI-based technologies such as automatic captioning might play animportant role in increasing access to education for people with disabilities. There is more work to be done in this space,not only in employing AI in rights-enabling applications, but also in bringing the advancements in AI and technologyto communities around the world, rather than keeping it only within the reach of a select few.A Human Rights-Based Approach to Responsible AI 11",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                },
                {
                    "id": "22a38814-02ca-42c3-ba96-8e8222a36adc",
                    "text": "AI ethics does not always give due recognition to the idea that every human life has value and also that human lifeis fragile \u2013 considerations that ground a set of important moral claims on institutions, new technologies, and on oneanother. The notion that people have human rights builds upon this foundation, recognizing that we all share certainvulnerabilities, that we ought not to be harmed, and these considerations guide how AI should be developed. In thispaper we have suggested that human rights-based considerations can perform three valuable functions in the contextof AI research. First, human rights can serve as a partial basis for AI value alignment across a range of cultures anddi\ufb00erent contexts, due to the quali\ufb01ed but signi\ufb01cant cross-cultural validity that they evidence. Second, a humanrights framework can help us understand how ethical principles governing the design and deployment of AI systemstranslate into di\ufb00erent responsibilities for the actors that comprise di\ufb00erent parts of the AI ecosystem. Third, humanrights can function as a language that enables deeper collaboration between AI researchers, civil society groups andthe people impacted by these technologies. In this way it can help close the gap between technical research focusing onalgorithmic fairness, and the claims of those who interact with AI systems on the ground and in the public sphere. Takentogether, these elements of human rights doctrine make it an appealing set of guiding principles for AI researchersand practitioners to draw upon.ACKNOWLEDGEMENTSWe thank Roya Pakzad, Jamila Smith-Loud, Tan Zhi Xuan, and Ben Zevenbergen for helpful conversations on this topicand for useful feedback on early drafts of this paper. We also thank the anonymous reviewers for their constructivefeedback.",
                    "reference": "[1] Mitali Prabhakaran, Margaret Mitchell, Timothy Gebru, et al. 2022. A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667. Retrieved from https://arxiv.org/pdf/2210.02667"
                }
            ]
        },
        {
            "paper_title": "Responsible Artificial Intelligence---From Principles to Practice: A Keynote at TheWebConf 2022",
            "authors": "V Dignum",
            "publication_info": "ACM SIGIR Forum - dl.acm.org",
            "paper_url": "https://arxiv.org/pdf/2205.10785",
            "chunks": [
                {
                    "id": "34264216-2a21-4bbe-a8a0-1028213d4d51",
                    "text": "",
                    "reference": "[1] Virginia Dignum. 2023. Responsible Artificial Intelligence---From Principles to Practice: A Keynote at TheWebConf 2022. ACM SIGIR Forum. Retrieved from https://arxiv.org/pdf/2205.10785"
                },
                {
                    "id": "4a025656-6783-46e1-a640-903f6f655e7a",
                    "text": "Ensuring the responsible development and use of AI is becoming a main direction in AI researchand practice. Governments, corporations and international organisations alike are coming forwardwith proposals and declarations of their commitment to an accountable, responsible, transparentapproach to AI, where human values and ethical principles are leading.Currently, there are over 600 AI-related policy recommendations, guidelines or strategy re-ports, which have been released by prominent intergovernmental organisations, professional bod-ies, national-level committees and other public organisations, non-governmental, and private for-pro\ufb01t companies. A recent study of the global landscape of AI ethics guidelines shows that thereis a global convergence around \ufb01ve ethical principles: Transparency, Justice and Fairness, Non-Male\ufb01cence, Responsibility, and Privacy [Jobin et al., 2019]. These are much-needed e\ufb00orts, butstill much work is needed to ensure that all AI is developed and used in responsible ways thatcontribute to trust and well-being. Nevertheless, even though organisations agree on the need toconsider ethical, legal and societal principles, how these are interpreted and applied in practice,varies signi\ufb01cantly across the di\ufb00erent recommendation documents.At the same time, the growing hype around \u2018AI\u2019 is blurring its de\ufb01nition and shoving intothe same heap concepts and applications of many di\ufb00erent sorts. A hard needed \ufb01rst step in theACM SIGIR Forum 1 Vol. 56 No. 1 June 2022responsible development and use of AI is to ensure a proper AI narrative, one that demysti\ufb01es itscapabilities, minimises both overselling and underselling of AI-driven solutions, and that enableswide and inclusive participation in the discussion on the role of AI in society. Understandingthe capabilities and addressing the risks of AI, requires that all of us, from developers to policy-makers, from provides to end-users and bystanders, have a clear understanding of what AI is, howit is applied and what are the opportunities and risks involved.",
                    "reference": "[1] Virginia Dignum. 2023. Responsible Artificial Intelligence---From Principles to Practice: A Keynote at TheWebConf 2022. ACM SIGIR Forum. Retrieved from https://arxiv.org/pdf/2205.10785"
                },
                {
                    "id": "0892b622-4f98-49db-acfb-71532881d728",
                    "text": "Without a proper understanding of what AI is and what it can, and cannot, do, all the e\ufb00ortstowards governance, regulation and the responsible development and use of AI have the risk tobecome void. Current AI narratives bring forward bene\ufb01ts and risks and describe AI in manydi\ufb00erent ways, from the obvious next step in digitisation to some kind of magic. If the \u2018businessas usual\u2019 narrative is detrimental of innovation and contributes to the maintenance current powerstructures, the \u2018magic\u2019 narrative, well fed by science \ufb01ction and the popular press, often supportsa feeling that nothing can be done against such an all-knowing entity that rules over us in possiblyunexpected ways, either solving all our problems, or destroying the world in the process. In bothcases, the danger is that the message is that little can be done against the risks and challenges ofAI. Currently, AI is mostly associated with data-driven techniques that use statistical methodsto enable computers to perceive some characteristics of their environment. Such techniques areparticularly e\ufb03cient in perceiving images, written or spoken text, as well as the many applicationsof structured data. These techniques are extremely successful at pattern matching: By analysingmany thousands of examples (typically a few million), the system is able to identify commonalitiesin these examples, which then enable it to interpret data that it has never seen before, which isoften referred to as prediction. These results, however impressive and useful, are still far from anything that we would consider as \u2018intelligent\u2019. Moreover, data-driven approaches to AI have beenproven to be problematic in terms of bias, explanation, inclusion and transparency. Algorithmsare too complex for human inspection, and a over-reliance on data, condemns the future to repeatthe past. Indeed, data is always about the past, and decisions on which, how, when, why collectand maintain data fundamentally in\ufb02uence the availability and quality of data. Those that havethe power to decide on data, have the power to determine how AI system will be design, deployedand used.AI is based on algorithms, but then so is any computer program and most of the technologiesaround us. Nevertheless, the concept of \u2018algorithm\u2019 is achieving magical proportions, used rightand left to signify many things, de facto seen as a synonym to AI. The easiest way to understandan algorithm is as a recipe, a set of precise rules to achieve a certain result. Every time youmultiply two numbers, you are using an algorithm, as well as you are when you are baking anapple pie. However, by itself, the recipe has never turned into an apple pie; and, the end resultof your pie has as much to do with your baking skills and your choice of ingredients, as with thechoice for a speci\ufb01c recipe. The same applies to AI algorithms: for a large part the behaviour andresults of the system depends on its input data, and on the choices made by those that developed,trained and selected the algorithm. In the same way as we have the choice to use organic applesACM SIGIR Forum 2 Vol. 56 No. 1 June 2022to make our pie, in AI we also must consider our choices on which models, data to use, who toinclude in the design and considerations about impact, and how these choices respect and ensurefairness, privacy, transparency and all other values we hold dear.",
                    "reference": "[1] Virginia Dignum. 2023. Responsible Artificial Intelligence---From Principles to Practice: A Keynote at TheWebConf 2022. ACM SIGIR Forum. Retrieved from https://arxiv.org/pdf/2205.10785"
                },
                {
                    "id": "e4e0b464-b5d2-41cf-9a04-874d69c0d21e",
                    "text": "If AI is not intelligent, nor magic, nor business as usual, nor an algorithm, how best can we describeAI in order to take into account not only its capabilities but also its societal implications? AIis \ufb01rst and foremost technology that can automatise (simple, lesser) tasks and decision makingprocesses. At the present, AI systems are largely incapable of understanding meaning and thecontext of their operation and results. At the same time, considering its societal impact andneed for human contribution, AI is much more than an automation technique. When consideringe\ufb00ects and the governance thereof, the technology, or the artefact that embeds that technology,cannot be separated from the ecosystem of which it is a component. In this sense, AI can bestbe understood as a socio-technical ecosystem, recognising the interaction between people andtechnology, and how complex infrastructures a\ufb00ect and are a\ufb00ected by society and by humanbehaviour [Dignum, 2019]. AI is not just about the automation of decisions and actions, theadaptability to learn from the changes a\ufb00ected in the environment, and the interactivity requiredto be sensitive to the actions and aims of other agents in that environment, and decide when tocooperate or to compete. It is mostly about the structures of power, participation and accessto technology that determine who can in\ufb02uence which decisions or actions are being automated,which data, knowledge and resources are used to learn from, and how interactions between thosethat decide and those that are impacted are de\ufb01ned and maintained.Much has been said about the dangers of biased data, and discriminating applications. Min-imising or eliminating discriminatory bias or unfair outcomes is more than excluding the use oflow-quality data. The design of any artefact, such as an AI system, is in itself an accumulation ofchoices and choices are biased by nature as they involve selecting an option over another. Mostimportantly, it starts with the current reliance on data as a measure of what can be done. In-creasingly, we are seeing that the availability of that (or the possibility to access data) is takenas a guiding criteria to solving societal issues. If there is data, it is a problem we can address,but if there is no data, there is no problem. This is intrinsically related to power and to powerstructures. Those that can decide on which problems are worth address, are shaping not only howAI is being developed and used, which technologies to use and what values to prioritise. Those inpower are shaping the way we live with AI and how our future societies will look like.Nevertheless, attention for the societal, environmental and climate costs of AI systems isincreasing. All these must be included in any e\ufb00ort to ensure the responsible development anduse of AI. A responsible, ethical, approach to AI will ensure transparency about how adaptationis done, responsibility for the level of automation on which the system is able to reason, andaccountability for the results and the principles that guide its interactions with others, mostimportantly with people. In addition, and above all, a responsible approach to AI makes clearthat AI systems are artefacts manufactured by people for some purpose, and that those whichmake these have the power to decide on the use of AI. It is time to discuss how power structuresdetermine AI and how AI establishes and maintains power structures, and on the balance between,those who bene\ufb01t from, and those who are harmed by the use of AI [Crawford, 2021].ACM SIGIR Forum 3 Vol. 56 No. 1 June 2022",
                    "reference": "[1] Virginia Dignum. 2023. Responsible Artificial Intelligence---From Principles to Practice: A Keynote at TheWebConf 2022. ACM SIGIR Forum. Retrieved from https://arxiv.org/pdf/2205.10785"
                },
                {
                    "id": "8065cc42-8e08-4935-8bd7-e76d53ee1847",
                    "text": "Responsible AI (or Ethical AI, or Trustworthy AI) is not, as some may claim, a way to givemachines some kind of \u2018responsibility\u2019 for their actions and decisions, and in the process dischargepeople and organisations of their responsibility. On the contrary, responsible development anduse of AI requires more responsibility and more accountability from the people and organisationsinvolved: for the decisions and actions of the AI applications, and for their own decision of usingAI in a given application context [Dignum, 2022]. When considering e\ufb00ects and the governancethereof, the technology, or the artefact that embeds that technology, cannot be separated fromthe socio-technical ecosystem of which it is a component. Guidelines, principles and strategiesto ensure trust and responsibility in AI, must be directed towards the socio-technical ecosystemin which AI is developed and used. It is not the AI artefact or application that needs to beethical, trustworthy, or responsible. Rather, it is the social component of this ecosystem thatcan and should take responsibility and act in consideration of an ethical framework such that theoverall system can be trusted by the society. Having said this, governance can be achieved byseveral means, softer or harder. Currently several directions are being explored, the main ones arehighlighted in the remainder of this section. Future research and experience will identify whichapproaches are the most suitable, but given the complexity of the problem, it is very likely thata combination of approaches will be needed.Responsible AI is more than the ticking of some ethical \u2018boxes\u2019 or the development of someadd-on features in AI systems. Nevertheless, developers and users can bene\ufb01t from support andconcrete steps to understand the relevant legal and ethical standards and considerations whenmaking decisions on the use of AI applications. Impact assessment tools provide a step-by-stepevaluation of the impact of systems, methods or tools on aspects such as privacy, transparency,explanation, bias, or liability [Taddeo and Floridi, 2018].Inclusion and diversity are a broader societal challenge central to AI development. It is there-fore important that as broad a group of people as possible have a basic knowledge of AI, whatcan (and can\u2019t) be done with AI, and how AI impacts individual decisions and shapes society. Inparallel, research and development of AI systems must be informed by diversity, in all the mean-ings of diversity, and obviously including gender, cultural background, and ethnicity. Moreover,AI is not any longer an engineering discipline and at the same time there is growing evidence thatcognitive diversity contributes to better decision making. Therefore, it is important to diversifythe discipline background and expertise of those working on AI to include AI professionals withknowledge of, amongst others, philosophy, social science, law and economy.",
                    "reference": "[1] Virginia Dignum. 2023. Responsible Artificial Intelligence---From Principles to Practice: A Keynote at TheWebConf 2022. ACM SIGIR Forum. Retrieved from https://arxiv.org/pdf/2205.10785"
                },
                {
                    "id": "93ba4bad-5e75-4a1d-956f-67c06aa26915",
                    "text": "A multidisciplinary stance supporting understanding and critiquing the intended and unforeseen,positive and negative, and the socio-political consequences of AI for society, is core to the re-sponsible design of AI systems. This multidisciplinary approach is fundamental to understandgovernance, not only in terms of competences and responsibilities, but also in terms of power,trust and accountability; to analyse the societal, legal and economic functioning of socio-technicalsystems, providing value-based design approaches and ethical frameworks for inclusion and diver-sity in design, and how such strategies may inform processes and results.ACM SIGIR Forum 4 Vol. 56 No. 1 June 2022Achieving trustworthy AI systems is a multifaceted complex process, which requires both tech-nical and socio-legal initiatives and solutions to ensure that we always align an intelligent system\u2019sgoals with human values. Core values, as well as the processes used for value elicitation, must bemade explicit and that all stakeholders are involved in this process. Furthermore, the methodsused for the elicitation processes and the decisions of who is involved in the value identi\ufb01cationprocess must be clearly identi\ufb01ed and documented.Where it concerns the design process itself, responsibility includes the need to elicit and rep-resent stakeholders, their values and expectations, as well as ensuring transparency about howsuch values are interpreted and prioritised in the concrete functionalities of the AI system. Designfor Values methodologies [van den Hoven, 2005; Friedman et al., 2006] are often used for this end,providing a structured way for translation from abstract values into concrete norms comprehensiveenough so that ful\ufb01lling the norm will be considered as adhering to the value. Following a Designfor Values approach, the shift from abstract to concrete necessarily involves careful considerationof the context. Design for Values approach means that the process needs to include activities for(i) the identi\ufb01cation of societal values, (ii) deciding on a moral deliberation approach (e.g. throughalgorithms, user control or regulation), and (3) linking values to formal system requirements andconcrete functionalities.My research group is developing the Glass Box framework [Aler Tubella and Dignum, 2019]that is both an approach to software development, a veri\ufb01cation method and a source of high-leveltransparency for intelligent systems. It provides a modular approach integrating veri\ufb01cation withvalue-based design.",
                    "reference": "[1] Virginia Dignum. 2023. Responsible Artificial Intelligence---From Principles to Practice: A Keynote at TheWebConf 2022. ACM SIGIR Forum. Retrieved from https://arxiv.org/pdf/2205.10785"
                },
                {
                    "id": "d7908113-a658-43f6-9ec9-74fba00a105d",
                    "text": "Increasingly, AI systems will be taking decisions that a\ufb00ect our lives, in smaller or larger ways.In all areas of application, AI must be able to take into account societal values, moral and eth-ical considerations, weigh the respective priorities of values held by di\ufb00erent stakeholders andin multicultural contexts, explain its reasoning, and guarantee transparency. As the capabili-ties for autonomous decision-making grow, perhaps the most important issue to consider is theneed to rethink responsibility. Being fundamentally tools, AI systems are fully under the controland responsibility of their owners or users. However, their potential autonomy and capabilityto learn, require that design considers accountability, responsibility and transparency principlesin an explicit and systematic manner. The development of AI algorithms has so far been ledby the goal of improving performance, leading to opaque black boxes. Putting human values atthe core of AI systems calls for a mind-shift of researchers and developers towards the goal ofimproving transparency rather than performance, which will lead to novel and exciting techniquesand applications.Finally, it is crucial to understand responsibility, regulation and ethics as stepping-stone forinnovation, rather than the often referred hinder to innovation. True innovation is moving technol-ogy forward, not use existing technology \u2018as is\u2019. Taking responsibility and regulation as beaconspointing the direction to move, will not only lead to better technology but also ensure trust andpublic acceptance, serve as a drive for transformation and for business di\ufb00erentiation. E\ufb00orts infundamental research are part of this. Currently, much AI \u2018innovation\u2019 is based on brute force:when the main data analytics paradigm is correlation, better accuracy is achieved by increasedACM SIGIR Forum 5 Vol. 56 No. 1 June 2022amount of data and computational power. The e\ufb00ort/accuracy ratio is huge. However, humanintelligence is not based on correlation, and includes causality and abstraction. Responsibility inAI is not just about ethics, bias, and trolley problems. It is also about responsible innovation:ensuring the best tools for the job, minimize side e\ufb00ects. Responsible innovation in AI requires\u201ca shift from a perspective in which learning is more or less the only \ufb01rst-class citizen to one inwhich learning is a central member of a broader coalition that is more welcoming to variables,prior knowledge, reasoning, and rich cognitive models.\u201d[Marcus, 2020].AcknowledgementsThis work was partially supported by the Wallenberg AI, Autonomous Systems and SoftwareProgram (WASP), funded by the Knut and Alice Wallenberg Foundation and by the EuropeanCommission\u2019s Horizon2020 project HumaneAI-Net (grant 952026).",
                    "reference": "[1] Virginia Dignum. 2023. Responsible Artificial Intelligence---From Principles to Practice: A Keynote at TheWebConf 2022. ACM SIGIR Forum. Retrieved from https://arxiv.org/pdf/2205.10785"
                }
            ]
        },
        {
            "paper_title": "A pathway towards responsible ai generated content",
            "authors": "C Chen, J Fu, L Lyu",
            "publication_info": "arXiv preprint arXiv:2303.01325 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2303.01325",
            "chunks": [
                {
                    "id": "fd679bab-3b38-4704-8ac4-f783b5efc994",
                    "text": "AI Generated Content (AIGC) has received tremen-dous attention within the past few years, with con-tent generated in the format of image, text, au-dio, video, etc. Meanwhile, AIGC has becomea double-edged sword and recently received muchcriticism regarding its responsible usage. In this ar-ticle, we focus on 8 main concerns that may hinderthe healthy development and deployment of AIGCin practice, including risks from (1) privacy; (2)bias, toxicity, misinformation; (3) intellectual prop-erty (IP); (4) robustness; (5) open source and expla-nation; (6) technology abuse; (7) consent, credit,and compensation; (8) environment. Additionally,we provide insights into the promising directionsfor tackling these risks while constructing genera-tive models, enabling AIGC to be used more re-sponsibly to truly benefit society.including images,",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "5cad979c-03e9-4b44-ab17-66cd266e1723",
                    "text": ". The success of high-quality AI Generated Content (AIGC) is strongly corre-lated with the emergence and rapid advancement of largefoundation models. These models, with their vast capac-ity, enable the rapid development of domain-specific mod-els, which are commonly employed for the production ofvarious types of content, texts, audio,video, etc. For instance, many text generators are built onthe Generative Pre-trained Transformer (GPT) [Radford etal., 2018] or its derivatives, such as GPT-2 [Radford et al.,2019], GPT-3 [Brown et al., 2020], GPT-3.5, GPT-4, etc.Similarly, numerous text-to-image generators rely on vision-language models such as CLIP [Radford et al., 2021], Open-CLIP [Wortsman et al., 2022], etc.AIGC applications. In recent years, generative modelinghas made rapid advances and tremendous progress. Ope-nAI\u2019s DALL\u00b7E [Ramesh et al., 2021] was one of the first text-to-image models that had captured widespread public atten-tion. It is trained to generate digital images from text descrip-tions, referred to as \u201cprompts\u201d, using a dataset of text\u2013image pairs [Brown et al., 2020]. Its successor, DALL\u00b7E 2 [Rameshet al., 2022], which can generate more complex and realisticimages, was unveiled in April 2022, followed by Stable Dif-fusion [Rombach et al., 2022a], which was publicly releasedin August 2022. Google, as a rival to OpenAI, presented twotext-to-image models that can generate photorealistic images:the diffusion-based model Imagen [Saharia et al., 2022a], andthe Pathways Autoregressive Text-to-Image model (Parti) [Yuet al., 2022]. In addition to text-to-image tasks, diffusionmodels had been widely used for image-to-image [Sahariaet al., 2022b; Whang et al., 2022] and text-to-video models,such as Runway [Runway, 2022], Make-A-Video [Singer etal., 2022], Imagen Video [Ho et al., 2022], and Phenaki [Vil-legas et al., 2022]. Stable Diffusion has been adapted forvarious applications, from medical imaging [Chambon et al.,2022] to music generation [Agostinelli et al., 2023].In addition to image and video generation, text generationis a popular generative domain. OpenAI\u2019s GPT-3 [Brown etal., 2020] is a notable example of a large language model(LLM). With a simple text prompt, GPT-3 can produce apiece of writing or an entire essay. It can also assist pro-grammers in writing code. OpenAI has further developedGPT-3.5, an improved version which is better at generatingcomplex text and poetry. In 2022, OpenAI launched Chat-GPT [OpenAI, 2022], a 175 billion parameter natural lan-guage processing (NLP) model that can produce responses ina conversational style. This model combines two popular AItopics: chatbots and GPT-3.5. ChatGPT is a specific chatbotuse case wherein the chatbot interacts with a GPT informa-tion source. The most recent version of ChatGPT integratedGPT-4 [OpenAI, 2023] \u2013 OpenAI\u2019s most advanced system,which can produce safer and more useful responses.AIGC dispute. Despite its popularity, AIGC has raised arange of concerns such as privacy, bias, toxicity, misinfor-mation, intellectual property (IP), and potential misuse oftechnology. The recent release of ChatGPT has sparkedmuch conversation surrounding its capabilities and poten-tial risks, such as its ability to debug code or compose es-says for students [Elliot and DeLisi, 2022]. It is impor-tant to consider whether AIGC can be counted as uniquecreative works or simply replicate content from their train-ing sets. Ideally, AIGC should produce original and dis-tinct outputs, but the source and IP rights of the trainingdata are often unknown due to the use of uncurated web-scale data [Somepalli et al., 2022]. Furthermore, the power-ful memorization of large AIGC models [Carlini et al., 2022;Carlini et al., 2021] poses a risk of reproducing data directlyfrom the training data [Butterick, 2023], which potentiallyviolates privacy rights and raises legal concerns around copy-right infringement and ownership. In addition to the afore-mentioned privacy and IP issues, as most AIGC models relyon text encoders that are trained on large amounts of datafrom the internet, hence these learned models may inherentsocial biases, toxicity, and produce misinformation.Components in responsible AIGC. The essential compo-nents of responsible AIGC are summarized in Figure 1. Ta-ble 1 lists recent AIGC models and their associated issues re-lated to privacy, bias, toxicity, misinformation, and IP, notingwhich models have taken proactive actions.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "f4e40d9b-b6b9-43b0-bd2a-dd18e535f365",
                    "text": "Large foundation models are known to be vulnerable to pri-vacy risks, and it is possible that AIGC models that buildupon these models could also be subject to privacy leakage.Previous research has demonstrated that large language mod-els such as GPT-2 can be vulnerable to privacy attacks, asattackers can generate sequences from the trained model andidentify those memorized from the training set [Carlini et al.,2021]. Kandpal et al. [Kandpal et al., 2022] have attributedthe success of these privacy attacks to the presence of dupli-cated data in commonly used web-scraped training sets. Ithas been demonstrated that a sequence that appears multi-ple times in the training data is more likely to be generatedthan a sequence that occurred only once. This suggests thatdeduplication could be used as a potential countermeasure inprivacy-sensitive applications.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "0ee1add5-5e45-46b7-a0a5-0b470e20dc96",
                    "text": "The replication behavior in Generative Adversarial Networks(GANs) has been studied extensively [Meehan et al., 2020;Feng et al., 2021; Webster et al., 2021]. Due to the factthat AIGC models are trained on large-scale web-scrapeddata [Rombach et al., 2022a; Ramesh et al., 2022; Sahariaet al., 2022a], the issue of overfitting and privacy leakage be-comes especially relevant. For instance, the model card of Stable Diffusion recognized that it memorized duplicate im-ages in the training data [Rombach et al., 2022c]. Somepalliet al. [Somepalli et al., 2022] also demonstrated that Sta-ble Diffusion blatantly copies images from its training data,and the generated images are simple combinations of theforeground and background objects of the training dataset.Moreover, the system occasionally displays the ability to re-construct memories, producing objects that are semanticallyequivalent to the original without being identical in pixelform. The existence of such images raises concerns aboutdata memorization and the ownership of diffusion images.Similarly, Melissa Heikkil\u00a8a [Heikkil\u00a8a, 2023] reported thatGoogle\u2019s Imagen can leak photos of real people and copy-righted images. In Matthew Butterick\u2019s recent litigation [But-terick, 2023], he pointed out that because all visual infor-mation in the system is derived from copyrighted trainingimages, the images produced are necessarily works derivedfrom those training images, regardless of their outward ap-pearance. DALL\u00b7E 2 also encountered similar problems. Itcan sometimes reproduce images from its training data ratherthan creating new ones. OpenAI found that this image regur-gitation occurs due to images being replicated many times inthe dataset [Nichol, 2022]. Similarly, when we asked Chat-GPT \u201dWhat is the privacy risk of ChatGPT\u201d, it respondedwith multiple potential privacy risks, as illustrated in Figure 3.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "a1c974dc-c9cc-46fd-834f-c4238be87a35",
                    "text": "Although a complete resolution to the privacy issues men-tioned above has not been achieved, companies and re-searchers have taken proactive steps to address these issues,such as introducing warning messages and detecting repli-cated content.At the industry level, Stability AI has recognized the lim-itations of Stable Diffusion, such as the potential for mem-orization of replicated images in the training data. To ad-dress this, they provide a website [Beaumont, 2022] to sup-port the identification of such memorized images. In addition,art company Spawning AI has created a website called \u201dHaveI Been Trained\u201d to assist users in determining whether theirphotos or works have been used as AI training materials.OpenAI has taken steps to address privacy concerns by reduc-ing data duplication through deduplication [Nichol, 2022].Furthermore, companies such as Microsoft and Amazon haveimplemented measures to prevent employee breaches of con-fidentiality by banning the sharing of sensitive data with Chat-GPT, given that this information could be utilized for train-ing data for future versions of ChatGPT [Lopez, 2023]. Atthe academic level, researchers [Somepalli et al., 2022] havestudied image retrieval frameworks to identify content dupli-cation, while Dockhorn et al. [Dockhorn et al., 2022] haveproposed differentially private diffusion models to guaranteeprivacy in generative models. Zhuang et al. [Zhuang et al.,2023] proposed to adopt federated learning for the privacy-preserving and responsible development of foundation mod-els.Existing privacy measures may not be inadequate to meetthe demands of privacy. It is essential to explore more reliabledetection systems for data replication in generative models,and to further investigate memorization and generalization incurrent and future AIGC models. Designing more faithfulmetrics for the privacy assessment on the reconstructed orgenerated images is also worthwhile to explore [Sun et al.,2023c].",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "6e6e691f-ca6b-4677-beca-bc1fbe890f12",
                    "text": "Since the training data used in AI models are collected inthe real world, they can unintentionally reinforce harmfulstereotypes, exclude or marginalize certain groups, and con-tain toxic data sources, which can incite hate or violence andoffend individuals [Weidinger et al., 2021]. For example,the LAION dataset [Schuhmann et al., 2021], which is usedto train diffusion models, has been criticized for containingproblematic content related to social stereotyping, pornogra-phy, racist slurs, and violence.Although some AIGC models like Imagen [Saharia et al.,2022a] try to filter out undesirable data, such as pornographicimagery and toxic language, the filtered data can still con-tain sexually explicit or violent content. Moreover, recentresearch works [Prabhu and Birhane, 2020; Birhane et al.,2021] have pointed out that these unfiltered datasets uti-lized for training frequently encompass social biases, repres-sive perspectives, and derogatory connections towards under-represented communities. Google\u2019s Imagen Video [Ho etal., 2022] is trained on a combination of the LAION-400Mimage-text dataset and their internal dataset, and Google isconcerned that its Imagen tool could be used to generateharmful content. Meanwhile, this dataset inherits social bi-ases and stereotypes that are difficult to remove.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "cd90d473-4b39-4c0b-a544-89c1df6d76cd",
                    "text": "Models trained, learned, or fine-tuned on the aforementionedproblematic datasets without mitigation strategies can inheritharmful stereotypes, social biases, and toxicity, leading to un-fair discrimination and harm to certain social groups [Wei-dinger et al., 2021].For example, Stable Diffusion v1 was trained primarily onthe LAION-2B data set, which only contains images withEnglish descriptions [Rombach et al., 2022c]. As a result,the model was biased towards white, western cultures, andprompts in other languages may not be adequately repre-sented. Follow-up versions of the Stable Diffusion modelwere fine-tuned on the filtered versions of the LAION dataset, but the bias issue still occurs [Rombach et al., 2022b]. To il-lustrate the inherent bias in Stable Diffusion, we tested a toyexample on Stable Diffusion v2.1. As shown in Figure 4, im-ages generated with the prompt \u201cThree engineers running onthe grassland\u201d were all male and none of them belong to theneglected racial minorities, indicating a lack of diversity inthe generated images.Similarly, DALLA\u00b7E and DALLA\u00b7E 2 exhibited nega-tive stereotypes against minoritized groups [Johnson, 2022].Google\u2019s Imagen [Saharia et al., 2022a] also encoded severalsocial biases and stereotypes, such as generating images ofpeople with lighter skin tones and aligning with western gen-der stereotypes. These biases can lead to unfair discrimina-tion and harm to certain social groups. Even when generatingnon-human images, Imagen has been shown to encode socialand cultural biases [Miller, 2022]. Due to these issues, mostcompanies decided not to make their AIGC models availableto the public to avoid criticism and potential fine from gov-ernment.Beyond above issues, there is also a risk of misinformationwhen AIGC models provide inaccurate, false even harmfulanswers or responses [Sun et al., 2023a; Xie et al., 2023]. Forexample, ChatGPT and its derivatives are notorious for theirhallucination issues, i.e., the generated content may appear tobe accurate and authoritative, but it could be completely in-accurate. Therefore, it can be used for misleading purposesin schools, laws, medical domains, weather forecasting, andanywhere else. For example, the answer on medical dosagesthat ChatGPT provides could be inaccurate or incomplete,potentially leading to the user taking dangerous or even life-threatening actions [Bickmore et al., 2018]. Prompted mis-information on traffic laws could cause accidents and evendeath if drivers follow the false traffic rules. ChatGPT alsoexhibits verbosity and overuse of certain phrases. For in-stance, it repeatedly states that it is a language model trainedby OpenAI. These issues are due to biases inherent in train- ing data, as trainers tend to prefer longer answers that appearmore comprehensive [OpenAI, 2022].",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "011e47dc-c97a-4bd7-99d2-4ef851b50475",
                    "text": "The quality of the content generated by AIGC models is in-extricably linked to the quality of the training corpora. Onenoticeable point is that while problems such as biases, toxic-ity and stereotypes can be reduced in the source datasets, theycan still be propagated even exacerbated during the trainingand development of AIGC models. For example, althoughsome companies such as Google try to filter out undesirabledata before training Imagen [Saharia et al., 2022a], such aspornographic imagery and toxic language, the filtered datacan still contain sexually explicit or violent content. OpenAItook extra measures to ensure that any violent or sexual con-tent was removed from the training data for DALLA\u00b7E 2 bycarefully filtering the original training dataset. However, fil-tering can introduce biases into the training data that can thenbe propagated to the downstream models. To address thisissue, OpenAI developed pre-training techniques to mitigatethe consequent filter-induced biases [Nichol, 2022]. Over-all, it is crucial to evaluate the existence of bias and toxicitythroughout the entire lifecycle of data usage, rather than stay-ing solely at the data source level. Additionally, there is achallenge in defining a truly fair and non-toxic dataset. Theextent and nature of these issues within AIGC models havenot yet been comprehensively investigated.In terms of misinformation and hallucination prevention, itis vital to analyze the root reasons behind them. AI hallucina-tions can occur for several reasons [Alston, 2023], including:(1) Insufficient, outdated, or low-quality training data. An AImodel is only as good as the data it\u2019s trained on. If the AI tooldoesn\u2019t understand the input prompt or doesn\u2019t have sufficientinformation, it\u2019ll rely on the limited dataset it\u2019s been trainedon to generate a response\u2014even if it\u2019s inaccurate; (2) Over-fitting. When an AI model is trained on a limited dataset,it may memorize the inputs and appropriate outputs. Thisleaves it unable to effectively generalize new data, resultingin AI hallucinations; (3) Use of idioms or slang expressions.If a prompt contains an idiom or slang expression that the AImodel hasn\u2019t been trained on, it may lead to nonsensical out-puts; (4) Adversarial attacks. Prompts that are deliberatelydesigned to confuse the AI can cause it to produce AI hallu-cinations.To defend against misinformation and hallucination,Elena [Alston, 2023] recommended 6 ways to prevent AI hal-lucinations, including (1) Limit the possible outcomes; (2)Pack in relevant data and sources unique to you; (3) Cre-ate a data template for the model to follow; (4) Give theAI a specific role\u2014and tell it not to lie; (5) Tell it whatyou want\u2014and what you don\u2019t want; (6) Experiment withthe temperature which controls the randomness of model re-sults. Sun et al. [Sun et al., 2023a] recently adopted theself-verification strategy to address the hallucination issue ofLLMs. Xie et al. [Xie et al., 2023] proposed the psychologi-cally inspired self-reminder technique that can efficiently andeffectively mitigate against jailbreaks without further train-ing.It is also essential to regularly update the training corporaused by AIGC models with the most recent information toensure that AI-driven models reflect the current state of soci-ety, thus reduce misinformation and hallucination. This willhelp prevent information lag and hallucination, and ensurethat the models remain updated, relevant, and beneficial tosociety. Lazaridou et al. [Lazaridou et al., 2021] showed thattransformer models cannot accurately predict data that didnot fall into training data period. This is because test dataand training data come from different periods, and increasingmodel size does not improve performance. It is thus essentialto incorporate new training data and update the model regu-larly. Gathering user feedback is also an effective way to keepmodels updated. Companies such as OpenAI actively seekfeedbacks from users to identify harmful outputs that couldarise in real-world scenarios, as well as to uncover and mit-igate novel risks [OpenAI, 2022] in a timely manner. Actu-ally, GPT-4 had incorporated an additional safety reward sig-nal during Reinforcement Learning from Human Feedback(RLHF) training to reduce harmful outputs by training themodel to refuse requests for such content [OpenAI, 2023]. Byinvolving users in the feedback loop, AIGC developers canbetter understand the potential consequences of their modelsand take corrective actions to minimize any negative impacts.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "ab1f8a22-282d-478f-908c-e666d1edbf08",
                    "text": "As AIGC continues to advance in sophistication and popular-ity, it raises questions about the origin of content for copy-right purposes and whether AI-generated content should beentitled to the same intellectual property protections as con-tent created by humans.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "d1a68afd-9802-4528-bdec-1125e700de11",
                    "text": "Copyright lawgenerally protects original works of authorship that are cre-ated by human authors and are fixed in a tangible form [Of-fice, 2023]. For a work to be eligible for copyright protection,it needs to be expressed in a tangible form, either physical ordigital, such as a book, painting, or computer file.Difficulty of copyright definition in AIGC. The owner-ship and protection of generated content have raised a sig-nificant amount of concern and debate. It remains unclearwhether such generated content should be considered originalworks eligible for copyright protection under current laws.There are many different notions of replication from AIGC.Somepalli et al. [Somepalli et al., 2022] gave an (informal)definition as follows: An image is considered to contain repli-cated content if it includes an object that is identical to anobject in a training image, regardless of minor variations inappearance resulting from data augmentation, whether theobject is in the foreground or background.In fact, addressing AI copyright issues is a complex taskthat involves several factors, including: (1) unclear regula-tions on data collection, usage, rights confirmation, and com-mercial use of data; (2) the need for a fair benefit distributionmechanism for contributors; (3) the lack of a unified legalunderstanding of AIGC copyright worldwide, with disputesover ownership still unresolved; and (4) difficulties in identi-fying all original works used to train AIGC models, as these models can generate an unlimited amount of content, makingit impossible to test all of it.IP infringement examples4.2There is a risk of copyright infringement with the generatedcontent if it copies existing works, whether intentionally ornot, raising legal questions about IP infringement.In November 2022, Matthew Butterick filed a class ac-tion lawsuit against Microsoft\u2019s subsidiary GitHub, accusingthat their product Copilot, a code-generating service, violatedcopyright law [Butterick, 2022]. The lawsuit centers aroundCopilot\u2019s illegal use of licensed code sections from the in-ternet without attribution. Texas A&M professor Tim Davisalso provided examples of his code being copied verbatim byCopilot [Jennings, 2022]. Although Microsoft and OpenAIhave acknowledged that Copilot is trained on open-sourcesoftware in public GitHub repositories, Microsoft claims thatthe output of Copilot is merely a series of code \u201csuggestions\u201dand does not claim any rights in these suggestions. Microsoftalso does not make any guarantees regarding the correctness,security, or copyright of the generated code.In addition to code generation, text-to-image generativemodels like Stable Diffusion also faced accusations of in-fringing on the creative work of artists, as they are trainedon billions of images from the Internet without the approvalof the IP holders, which some argue is a violation of theirrights. This is evident in Stable Diffusion, which has gener-ated images with the Getty Images\u2019 watermark on them [Vin-cent, 2023]. Somepalli et al. [Somepalli et al., 2022] alsopresented evidence suggesting that Stable Diffusion copiesfrom the data on which it was trained on. While Stable Diffu-sion disclaims any ownership of generated images and allowsusers to use them freely as long as the image content is legaland non-harmful, this freedom raises questions about owner-ship ethics.IP problem mitigation4.3To mitigate IP concerns, many companies have started im-plementing measures to accommodate content creators. Mid-journey, for instance, has added a DMCA takedown policyto its terms of service, allowing artists to request the removalof their work from the dataset if they suspect copyright in-fringement [Midjourney, 2022]. Similarly, Stability AI plansto offer artists the option of excluding themselves from futureversions of Stable Diffusion [Heikkil\u00a8a, 2022a]. OpenAI hasreleased a classifier that can distinguish between text gener-ated by AI and that written by humans. However, this toolshould not be relied exclusively on for critical decisions.In addition to above attempts, watermarks can be extremelyuseful in tracking IP violations or detecting the origin ofthe generated content [He et al., 2022a; He et al., 2022b;Peng et al., 2023]. Wang et al. [Wang et al., 2023b;Wang et al., 2023a] recently utilized novel watermark tech-niques to conduct origin attribution of AI-generated imagesand detect the unauthorized data usages in text-to-image dif-fusion models. In light of the growing popularity of AIGC,the need for watermarking is becoming increasingly pressing.OpenAI is developing a watermark to identify text generatedby its GPT model. It could be a valuable tool for educatorsand professors to detect plagiarism in assignments generatedwith such tools. Google has already applied a Parti watermarkto all images it releases. John Kirchenbauer et al. [Kirchen-bauer et al., 2023] proposed a watermark to detect whetherthe text is generated by an AI model. Still, they only tested iton the smaller open-source language model OPT-6.7B fromMeta, leaving its performance on the larger and more widelyused ChatGPT model unknown.In general, the emergence of AIGC presents significant IPconcerns and challenges that demand immediate attention. Itis essential for technologists, lawyers, and policymakers torecognize these issues and work together to ensure that theintellectual property rights of human creators are protected.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "e6e7c7ba-1ea0-4b53-8daa-f39a89944856",
                    "text": "Previous studies have demonstrated that large models or foun-dation models trained on the unlabelled data can be back-doored [Pan et al., 2023; Shejwalkar et al., 2023]. Thispoisoning effect could cause catastrophic damage to down-stream applications that depend on the compromised founda-tion or generative models. For example, a diffusion modelwith a hidden \u201cbackdoor\u201d could carry out malicious ac-tions when it encounters a specific trigger pattern duringdata generation [Chou et al., 2022; Zhang et al., 2022;Sun et al., 2023b]. How to sift out clean data for train-ing [Zeng et al., 2022] matters a lot for model robustness.Beyond the poisoning attack during training phase, the re-cent emergence of jailbreak attacks [Daryanani, 2023; Albert,2023] which use adversarial prompts to bypass the deployedChatGPT\u2019s ethics safeguards and engender harmful responsesnotably threatens the responsible and secure use of Chat-GPT [Xie et al., 2023]. Xie et al. [Xie et al., 2023] proposedthe psychologically inspired self-reminder technique that canefficiently and effectively mitigate against jailbreaks withoutfurther training. Unfortunately, research on the robustness offoundational and fine-tuned models is still limited.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "bd50f0f0-f959-4913-961a-c27e29c1e0c5",
                    "text": "Lack of transparency of models behind AIGC can lead to aseries of unsatisfactory results. It is frequently challenging toexplain why and how the models generate different contentand determine the information used to generate a model\u2019s out-put. For example, social and cultural bias is introduced andpotentially amplified at multiple stages of model developmentand deployment. However, how the biases are propagatedthrough these models remain unclear. Similarly, while dedu-plication can be an effective method of preventing memoriza-tion, it does not completely explain why or how models likeDALL\u00b7E 2 memorize training data. As most of the code andmodels behind AIGC are not transparent to the public, andtheir downstream applications are diverse and may have com-plex societal impacts, it is challenging to determine the poten-tial harms they may cause. Therefore, the need for responsi-ble open source becomes critical in determining whether thebenefits of AIGC outweigh its potential risks in specific usecases. Open-sourcing can also facilitate explanation of thebehaviours of the models behind AIGC. Currently, most companies chose not to release their mod-els or open-source their code before solving all the potentialrisks associated with their models. OpenAI has been criti-cized for not sharing more about how the most recent GPT-4was created. Stable Diffusion [Rombach et al., 2022b] andMeta\u2019s LLAMA [Meta, 2023] are few generative AI modelsthat provide the source code and pretrained model (weights).The risk is that anyone can use these open-sourced models forfree, even for commercial or malicious purposes. To promotea healthy open-sourcing environment, communities have puta lot of joint efforts. In Dec, 2023, IBM And Meta Launchthe AI Alliance for safe and open AI with the belief thatopen and transparent innovation is crucial for harnessing AIadvancements in a way that prioritizes safety, diversity, andwidespread economic opportunity [Forbes, 2023].",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "caf23974-bf9e-46f1-89d7-3550edaa9729",
                    "text": "AIGC can be used for malicious purposes such as spreadingfake news, hoaxes, and harassment. The foundation mod-els that power AIGC have made it easier and cheaper to cre-ate deepfakes that are close to the original, posing additionalrisks and concerns. In fact, many models are still far from sat-isfactory and some of them have gained negative reputationsfor producing useless, biased, or harmful information.For example, on the 4chan online forum, there are numer-ous discussions about images of naked celebrities and otherforms of fake pornographic content generated by Stable Dif-fusion [Wiggers, 2022a]. The misuse of these technologiescould lead to the spread of misinformation, harm the reputa-tions of individuals, or even break the law. The potential neg-ative impact of ChatGPT on education is significant, as stu-dents could use it to write homework or solve math problems,thus compromising the integrity of their work. Moreover, asChatGPT is a chatbot, it lacks the necessary emotional con-nection that a human teacher can provide, which could lead toa diminished learning experience. In light of these concerns,New York City public schools have recently banned the useof ChatGPT [Rosenblatt, 2022]. Stack Overflow, a Q&A plat-form for coders and programmers, temporarily prohibited thesharing of ChatGPT information, acknowledging its potentialto cause significant harm to the site and users who rely on itfor accurate answers [Overflow, 2022]. Writing and editingtools that rely on ChatGPT also face the risk of losing cus-tomers if they inadvertently introduce errors into the output.Overall, the potential misuse of AIGC poses a threat to notonly the users but also the whole creative industry. Therefore,it is crucial to use AIGC only in situations where the risk canbe managed or corrected. To mitigate risks, it is also neces-sary to include governance mechanisms for AIGC models assoon as possible, such as establishing legal regulations. Themost recent deal on comprehensive rules for trustworthy AIfrom EU [EU-, 2023] reflects the urgency to deal with con-cerns on the misuse of AIGC technologies.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "a20f73a5-6a9e-45e7-b13f-56865e603d2f",
                    "text": "Many AIGC models are trained on datasets without obtain-ing consent or providing credit or compensation to the orig-inal data contributors. For example, Simon Willison andAndy Baio found that a large number of images in LAIONwere copied from DeviantArt and used to train Stable Dif-fusion [Willison and Baio, 2022]. This results in data con-tributors\u2019 works being learned by AI models and recreatedby other users for profit, without their knowledge or permis-sion. This practice damages the interests of the original datacontributors. To avoid negative impacts, AIGC companiesshould obtain consent from data contributors and take proac-tive measures before training their models on any original oraugmented works. Failure to do so could result in lawsuitsagainst AIGC. Therefore, AIGC companies must ensure thatdata collection and model training are conducted in an ethicaland responsible manner.A potential solution to the issue of using creators\u2019 worksfor AI training is to notify them from the beginning and givethem the option to benefit from subsequent creations basedon their works generated by the model. Additionally, cre-ators who give their consent for their data to be used can berewarded based on how their creations contribute to AIGCeach time the tool is queried. By incentivizing creators, com-panies can encourage creators to contribute more and accel-erate the development of AIGC. For example, a more user-friendly version of Copilot could allow voluntary participa-tion or compensate coders for contributing to the training cor-pus [Butterick, 2022].",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "2d7f0e37-1796-4127-ba34-c53d8e44f350",
                    "text": "The massive size of AIGC models, which can have billionseven trillions of parameters, results in high environmentalcosts for both model training and operation. For example,GPT-3 has 175 billion parameters and requires significantcomputing resources to train. Narayanan et al. [Narayananet al., 2021] estimated that training GPT-3 with A100s wouldrequire 1,024 GPUs, 34 days, and cost 4.6 million dollars,with an expected energy consumption of 936 MWh [Char-maine Lai and Maver, 2022]. This raises important ques-tions about how to reduce the energy consumption and carbonemission of AIGC models.The upcoming GPT-4, with even more parameters than itspredecessor, is expected to leave a more significant carbonemission. Failing to take appropriate steps to mitigate thesubstantial energy costs of AIGC could lead to irreparableIt is crucial to address these con-damage to our planet.cerns and explore sustainable alternatives. Communities havestarted to explore more slim alternatives with decent perfor-mance as much larger ones.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "55f09631-5fc1-4bbb-b6e8-9f519a145b07",
                    "text": "Many AIGC models are being uti-lized for art and graphic design commercially. For exam-ple, PromptBase [PromptBase, 2022] is an early marketplacefor DALL\u00b7E, Midjourney, Stable Diffusion & GPT-3 prompts.Microsoft is using DALL-E 2 to power a generative art fea-ture that will be available in Microsoft Edge. Microsoft andOpenAI are collaborating on ChatGPT-Powered Bing [Wig-gers, 2022b]. Moreover, Microsoft had integrated OpenAI\u2019s ChatGPT into Word, PowerPoint, Outlook, and other appli-cations to allow users to automatically generate text usingsimple prompts [Holmes and McLaughlin, 2023]. While us-ing the generated works for profit or commercial purposes isnot recommended, there are no mandatory legal restrictionsat this stage.The use of AIGC has faced criticism from those who fearthat it will replace human jobs. Insider has listed severaljobs that could potentially be replaced by ChatGPT, includingcoders, data analysts, journalists, legal assistants, traders, ac-countants, etc [Mok and Zinkula, 2023]. Some artists worrythat the wide use of image generation tools such as StableDiffusion could eventually make human artists, photogra-phers, models, cinematographers, and actors commerciallyuncompetitive [Heikkil\u00a8a, 2022b]. For example, the imagesgenerated by Stable Diffusion can be sold on the market.This creates direct competition and poses a significant threatto creators, such as writers, artists, and programmers, whocould suffer permanent damage to their businesses [Butter-ick, 2023]. Since Stable Diffusion can produce an unlimitednumber of infringing images, this threat is even more signifi-cant. However, David Holz, the founder of Midjourney, viewsartists as customers rather than competitors. Artists can useMidjourney to quickly prototype artistic concepts and showthem to clients before starting work themselves [Holz andClaburn, 2022].As AIGC models become more widespread, people maybecome too dependent on instant answers and less willing tothink critically on their own, which could ultimately diminishor destroy human creativity and increase the risk of AI exert-ing control over humans. Overreliance on AIGC could createopportunities for malicious attackers to exploit user trust andaccess their private information.Fairness of benefit distribution. It is important to recog-nize that AIGC models may have varying impacts on differ-ent groups of people depending on their environmental andindividual abilities, which could further exacerbate global in-equities [Weidinger et al., 2021]. Addressing the issue of howto fairly distribute the benefits of AIGC models is an area thatrequires further exploration and attention.Conflict among multiple goals. It is critical to ensure thatthe mitigation of one risk does not exacerbate another [Wei-dinger et al., 2021]. For example, approaches to mitigatethe use of toxic language in language models can introducebiases in model predictions against marginalized communi-ties [Welbl et al., 2021; Xu et al., 2021]. Therefore, it isessential to explore effective mitigation strategies that cantrade-off multiple goals.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                },
                {
                    "id": "fd26d8b7-8ea6-449a-8ddc-8dea59ceb81f",
                    "text": "Although AIGC is still in its infancy, it is rapidly expand-ing and will remain active for the foreseeable future. CurrentAIGC technologies only scratch the surface of what AI cancreate in the field of creativity. While AIGC offers many op-portunities, it also carries significant risks. To acquire a thor-ough comprehension of these risks, we provide a synopsis ofboth current and potential threats in recent AIGC models, sothat both the users and companies can be well aware of theserisks, and make the appropriate actions to mitigate them.In order to promote responsible usage of AIGC tools andmitigate associated risks, we propose several steps that com-panies and users can take. It is important for companies to in-corporate responsible AI practices throughout the whole lifecycles during development of AIGC products. For example,proactive measures should be taken to mitigate potential risksin data sources, models, and pre/post-processing steps. With-out proper safeguards, AIGC development may face signifi-cant challenges and regulatory hurdles. Note that this visionpaper is not exhaustive, and it is essential for the wider com-munity to contribute to the understanding and implementationof responsible AIGC. To facilitate this, it is necessary to buildcomprehensive benchmarks for measuring and evaluating therisks associated with different AIGC technologies.",
                    "reference": "[1] Carol Chen, Joanna Fu, and Lucas Lyu. 2023. A pathway towards responsible AI generated content. arXiv:2303.01325. Retrieved from https://arxiv.org/pdf/2303.01325"
                }
            ]
        },
        {
            "paper_title": "Responsible AI (RAI) Games and Ensembles",
            "authors": "Y Gupta, R Zhai, A Suggala\u2026",
            "publication_info": "Advances in Neural \u2026 - proceedings.neurips.cc",
            "paper_url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf",
            "chunks": [
                {
                    "id": "869bcfdf-144e-4e2d-9dd7-7f214a7732f4",
                    "text": "Several recent works have studied the societal effects of AI; these include issuessuch as fairness, robustness, and safety. In many of these objectives, a learner seeksto minimize its worst-case loss over a set of predefined distributions (known asuncertainty sets), with usual examples being perturbed versions of the empiricaldistribution. In other words, aforementioned problems can be written as min-maxproblems over these uncertainty sets. In this work, we provide a general frameworkfor studying these problems, which we refer to as Responsible AI (RAI) games. Weprovide two classes of algorithms for solving these games: (a) game-play basedalgorithms, and (b) greedy stagewise estimation algorithms. The former class ismotivated by online learning and game theory, whereas the latter class is motivatedby the classical statistical literature on boosting, and regression. We empiricallydemonstrate the applicability and competitive performance of our techniques forsolving several RAI problems, particularly around subpopulation shift.",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "24a4bad8-2faa-4aca-895c-f833b4070219",
                    "text": "In recent years, AI is increasingly being used in high-stakes decision-making contexts such as hiring,criminal justice, and healthcare. Given the impact these decisions can have on people\u2019s lives, it isimportant to ensure these AI systems have beneficial social effects. An emerging line of work hasattempted to formalize such desiderata ranging over ethics, fairness, train-time robustness, test-timeor adversarial robustness, and safety, among others. Each of these forms rich sub-fields with disparatedesiderata, which are sometimes collated under the umbrella of \u201cresponsible AI\u201d. Many organizationsare increasingly advocating the use of responsible AI models [Microsoft, 2021, Google, 2020].But how do we do so when the majority of recent work around these problems is fragmented andusually focuses on optimizing one of these aspects at a time (DRO [Namkoong and Duchi, 2017,Duchi and Namkoong, 2018], GDRO [Sagawa et al., 2019], CVaR [Zhai et al., 2021a], DistributionShift [Hashimoto et al., 2018, Zhai et al., 2021b])? Indeed optimizing for just one of these aspectshas even been shown to exhibit adverse effects on the other aspects [Roh et al., 2020]. To addressthis, we study a general framework that is broadly applicable across many of the settings above, andwhich we refer to as Responsible AI (RAI) games. Our starting point is the recent understanding of aunifying theme in many of these disparate problems, that a learner seeks to minimize its worst-caseloss over a set of predefined distributions. For example, in fairness, we seek to perform well on allsub-groups in the data. In robustness, we aim to design models that are robust to perturbations of thetraining data or the test distribution. This allows us to set up a zero-sum game between a learner thataims to learn a responsible model and an adversary that aims to prevent the learner from doing so. Inthe general RAI game setting, this is a computationally intractable game that need not even have aNash equilibrium. To address this computational issue, we study a relaxation of the single predictorRAI game, which we term the ensemble RAI game, which can also be motivated as a linearization ofthe original RAI game.We note that our framework encompasses not only the responsible AI settings but also the setting ofclassical boosting. Drawing upon the insights from boosting, we provide boosting-based algorithmsfor solving responsible AI games. We provide convergence guarantees of our algorithms by relyingon the connections between boosting and online convex optimization, two-player gameplay [Aroraet al., 2012, McMahan, 2011, Bubeck, 2011]. We also conduct empirical analyses to demonstrate theconvergence and utility of our proposed algorithms. Interestingly, the algorithms allow for plug-and-play convenience, with changes in the RAI settings requiring only simple changes to the algorithms.More importantly, we could consider intersections of different responsible AI considerations, whichin turn can simply be incorporated into our algorithms. Finally, we also study the population risks ofour algorithms in certain important settings. We show a surprising result that for the case of binaryclassification with the 0/1 loss, the optimal predictor for a large class of RAI games is the same as theBayes optimal predictor, thus generalizing an emerging line of results demonstrating this for certainspecific games [Hu et al., 2018]. Under such settings, solving the RAI game could nonetheless behelpful in finite sample settings (as also demonstrated in our experiments) since the RAI game servesto encode desiderata satisfied by the Bayes optimal classifier.",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "46a0aa51-0ed7-4b4f-a1a3-b3dea6b907ca",
                    "text": "We consider the standard supervised prediction setting, with input random variable X \u2208 X \u2286 R ,output random variable Y \u2208 Y, and samples S = {(x , y )} drawn from a distribution Pover X \u00d7 Y. Let (cid:98)P denote the empirical distribution over the samples. We also have a setH of hypothesis functions h : X (cid:55)\u2192 Y from which we wish to learn the best predictor. Weevaluate the goodness of a predictor via a loss function \u2113 : Y \u00d7 Y (cid:55)\u2192 R, which yields the empiricalrisk: (cid:98)R(h) = E f (x , y ). Apart from havinglow expected risk, most settings require h to have certain properties, for example, robustness todistribution shift, fairness w.r.t subpopulations, superior tail performance, resistance to adversarialattacks, robustness in the presence of outliers, etc. We cast all these subproblems into an umbrellaterm \u201cResponsible AI\u201d. Each of these properties has been studied extensively in recent works, albeitindividually. In this work, we attempt to provide a general framework to study these problems.\u2113(h(x), y) where E (f (x, y)) = (cid:80)",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "04461f05-ab4e-4fa7-a202-a4c22acde7de",
                    "text": "We draw our unified framework from seminal works over the past decade by responsible AI re-searchers on devising non-expected risk objectives, particularly min-max problems, to ensure MLmodels are responsible. These have resulted in a multitude of different objectives (even for a singleresponsible AI desideratum such as fairness), and also multiple different sub-communities (so thatfairness and multiple disparate robustness communities are relatively fractured), many (if not all)of which we combine within a single umbrella. There is emerging work on relating worst-caseperformance to invariance [B\u00fchlmann, 2018]; in other words, we might be able to get approximategroup invariance via minimizing an appropriately constructed worst-group risk and vice-versa.RAI aspects as constraints. Many prior works have enforced robustness as a constrained opti-mization [Shafieezadeh-Abadeh et al., 2015, Gao and Kleywegt, 2022, Namkoong and Duchi, 2016,Ben-Tal et al., 2011]. There have also been few prior works enforcing fairness constraints [Mandalet al., 2020]. To the best of our knowledge, there exists minimal prior work focusing on multipledesiderata at once in this regard.Multi Objective Optimization. Several works have considered a multi-objective view of ensuringfairness in classifiers [Martinez et al., 2020, Oneto et al., 2018]. If used for multiple RAI objectives,there is usually overhead in choosing a model that achieves a good trade-off between various losses.Also, it is difficult to guarantee that the solution is robust to any of the involved aspects. Ourframework guarantees a certain level of performance on each of the RAI aspects under consideration.Distribution shift. [Koh et al., 2021] classifies distribution shift problems into two categories:Domain generalization, and subpopulation shift. In this work, we focus on the subpopulation shiftproblem, where the target distribution is absolutely continuous to the source distribution. It has two2main applications: fairness [Hashimoto et al., 2018, Hu et al., 2018, Sagawa et al., 2019, Zhai et al.,2021b] and long-tail learning (i.e. learning on class-imbalanced datasets) [Cao et al., 2019, Menonet al., 2021, Kini et al., 2021].Distributionally Robust Optimization (DRO). In DRO one aims to study classifiers that are robustto deviations of the data distribution. DRO has been studied under various uncertainty sets includingf -divergence based uncertainty sets [Namkoong and Duchi, 2017, Duchi and Namkoong, 2018,Sagawa et al., 2019], Wasserstein uncertainty sets [Sinha et al., 2017, Gao et al., 2022], MaximumMean Discrepancy uncertainty sets [Staib and Jegelka, 2019], more general uncertainty sets in theRKHS space [Zhu et al., 2020]. [Li et al., 2021a] evaluate model performance under worst-casesubpopulations. Owing to its importance, several recent works have provided efficient algorithmsfor solving the DRO objective [Namkoong and Duchi, 2016, Qi et al., 2020, Kumar et al., 2023, Jinet al., 2021]. However, a lot of these techniques are specific to particular perturbation sets and arenot directly applicable to the more general framework we consider in our work. Furthermore, in ourwork, we aim to learn an ensemble of models instead of a single model.Boosting. Classical boosting aims to improve the performance of a weak learner by combiningmultiple weak classifiers to produce a strong classifier [Breiman, 1999, Friedman et al., 2000,Friedman, 2001, Freund and Schapire, 1995, Freund et al., 1996, Mason et al., 2000]. Over the years,a number of practical algorithms have been introduced such as AdaBoost [Schapire, 1999], LPBoost[Demiriz et al., 2002], gradient boosting [Mason et al., 1999], XGBoost [Chen and Guestrin, 2016],boosting for adversarial robustness [Zhang et al., 2022], [Meunier et al., 2021], [Balcan et al., 2023],and holistic robustness [Bennouna and Parys, 2022]. The algorithms we develop for RAI games areinspired by these algorithms.Fairness. There are a number of fairness notions for algorithmic fairness, ranging from individualfairness [Dwork et al., 2012, Zemel et al., 2013], group fairness [Hardt et al., 2016a, Zafar et al., 2017],counterfactual fairness [Kusner et al., 2017], Rawlsian max-min fairness [Rawls, 2020, Hashimotoet al., 2018] and others [Barocas et al., 2017, Chouldechova and Roth, 2018, Mehrabi et al., 2021].Our framework includes the popular notion of minimax group fairness. It doesn\u2019t capture othernotions of group fairness such as Demographic Parity, Equality of Odds, Equality of Opportunity.Population RAI Risks. Several recent works have studied properties of the population risks arisingin various responsible AI scenarios. Hu et al. [2018] showed that the minimizer of population DROrisk (under general f -divergences) is the classical Bayes optimal classifier. Li et al. [2021b], Duchiand Namkoong [2018], Sinha et al. [2017] provided generalization guarantees for DRO risk undervarious divergence families ranging from f -divergences to Wasserstein perturbations.",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "c287186b-f6a7-4d3d-9103-b5e7351b33c8",
                    "text": "In many cases, we do not wish to compute an unweighted average over training samples; due toreasons of noise, tail risk, robustness, and fairness, among many other \u201cresponsible AI\u201d considerations.Definition 1 (RAI Risks) Given a set of samples {(x , y )} , we define the class of empirical RAIE (h(x), y), where W \u2286 \u2206 , is some setrisks (for Responsible AI risks) as: (cid:98)R (h) = supof sample weights (a.k.a uncertainty set), and E (f (x, y)) = (cid:80)Various choices of W give rise to various RAI risks. Table 1 presents examples of RAI risks that arepopular in ML. Interestingly, classical problems such as boosting are special cases of RAI risks. Inthis work, we rely on this connection to design boosting-inspired algorithms for minimizing RAIrisks. More choices for W can be obtained by combining the one\u2019s specified in Table 1 using union,intersection, convex-combination operations. For example, if one wants models that are fair to certainpre-specified groups, and at the same-time achieve good tail-risk, then one could choose W to bethe intersection of Group-DRO and \u03b1-CVaR uncertainty sets. w f (x , y ).Given the empirical RAI risk (cid:98)R (h) of a hypothesis, and set of hypotheses H, we naturally wish toobtain the hypothesis that minimizes the empirical RAI risk: min (cid:98)R (h). This can be seen assolving a zero-sum game.Definition 2 (RAI Games) Given a set of hypothesis H, and a RAI sample weight set W , the classof RAI games is given as: min max E (h(x), y).We thus study RAI Games for the special cases above and for an arbitrary constraint set W .3Table 1:",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "4e04400f-5c92-4b55-99f8-c967e5b918c1",
                    "text": "In this section, we begin our discussion about ensembles. In general, a statistical caveat withDefinition 2 is that good worst-case performance over the sample weight set W is generally harder,and for a simpler set of hypotheses H, there may not exist h \u2208 H that can achieve such goodworst-case performance. Thus it is natural to consider deterministic ensemble models over H, whicheffectively gives us more powerful hypothesis classes. Let us first define RAI risk for such classifiers.Definition 3 (Deterministic Ensemble) Consider the problem of classification, where Y is a discreteset. Given a hypothesis class H, a deterministic ensemble is specified by some distribution Q \u2208 \u2206 ,and is given by: h (x) = arg max E I[h(x) = y]. Correspondingly, we can write thedeterministic ensemble RAI risk as (cid:98)R (h (x)) = max E \u2113(h (x), y).We discuss alternative definitions of deterministic ensembles in the Appendix. This admits a class ofdeterministic RAI games:Definition 4 (Deterministic Ensemble RAI Games) Given a set of hypothesis H, a RAI sampleweight set W , the class of RAI games for deterministic ensembles over H is given as:min max E \u2113(h (x), y).However, the aforementioned game is computationally less amenable because of the non-smoothnature of de-randomized predictions. Moreover, there are some broader challenges with RAI gamesgiven by Definitions 2 and 4. Firstly, they need not have a Nash Equilibrium (NE), and in general,their min-max and max-min game values need not coincide. This poses challenges in solving thegames efficiently. Next, in some cases, directly optimizing over the worst-case performance mightnot even be useful. For instance, [Hu et al., 2016, Zhai et al., 2021a] show the pessimistic result thatfor classification tasks where when models are evaluated by the zero-one loss, ERM achieves thelowest possible DRO loss defined by some f -divergence or the \u03b1-CVaR loss, given that the model isdeterministic. To this end, we consider the following randomized ensemble:Definition 5 (Randomized Ensemble) Given a hypothesis class H, a randomized ensemble isspecified by some distribution Q \u2208 \u2206 , and is given by: P[h (x) = y] = E I[h(x) =y]. Similarly, we can define its corresponding randomized ensemble RAI risk: (cid:98)R (Q) =max E E \u2113(h(x), y).We can then also define the class of ensemble RAI games:Definition 6 (Randomized Ensemble RAI Games) Given a set of hypothesis H, a RAI sampleweight set W , the class of mixed RAI games is given as:min max E E \u2113(h(x), y). (1)This is a much better class of zero-sum games: it is linear in both the hypothesis distribution P , aswell as the sample weights w, and if the sample weight set W is convex, is a convex-concave game.As shown below, under some mild conditions, this game has a Nash equilibrium which can be wellapproximated via efficient algorithms. 4Proposition 1 Let H be parameterized by \u03b8 \u2208 \u0398 \u2286 R ,and let W be a convex, compact set.max min E E \u2113(h(x), y) Then min max for convex, compact set \u0398E E \u2113(h(x), y) =The proposition follows as a direct consequence of well known minimax theorems (Appendix D.3).",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "b5e4ca6e-bf57-4a3d-91e6-1ccf39dadb1e",
                    "text": "To begin, we point out that what we want is a deterministic ensemble rather than a randomizedensemble. In fact, it can be seen that the deterministic ensemble in Definition 3 is a specific de-randomization of the randomized ensemble. It is such deterministic ensembles that we usually simplyrefer to as ensemble predictors. But the RAI risk for the ensemble predictor is NOT equal to theensemble RAI risk minimized by our desired game in Equation 1 above for randomized ensembles.Thus, the ensemble RAI game might not in general capture the ideal deterministic ensemble. In thissection, we study why and when might solving for a random ensemble is meaningful.Binary Classification. Interestingly, for the very specific case of binary classification, we can providesimple relationships between the risks of the randomized and deterministic ensemble.Proposition 2 Consider the setting with Y = {\u22121, 1}, the zero-one loss \u2113, and W = \u2206 . Then,(cid:98)R (h ) = I[ (cid:98)R (h ) \u2265 1/2].See Appendix E.2 for a simple proof. In this case, we can also relate the existence of a perfectdeterministic ensemble (\u201cboostability\u201d) to a weak learning condition on the set of hypotheses.Specifically, suppose H is boostable iff there exists Q \u2208 \u2206 s.t. (cid:98)R (h ) = 0. From the aboveproposition this is equivalent to requiring that (cid:98)R (h ) < 1/2. We thus obtain:E \u2113(h(x), y) < 1/2 \u21d0\u21d2 sup E \u2113(h(x), y) < 1/2supinf infwhere the equivalence follows from the min-max theorem and the linearity of the objective in P .The last statement says that for any sample weights w \u2208 W , there exists a hypothesis h \u2208 H thathas w-weighted loss at most 1/2. We can state this as a \u201cweak-learning\u201d condition on individualhypotheses in H. The above thus shows that for the specific case of Y = {\u22121, 1}, the zero-one loss\u2113(y, y ) = I[y \u0338= y ], and W = \u2206 , we can relate boostability of H to a weak learning condition onhypothesis within H.General Classification But in general, we do not have simple connections between (cid:98)R (h )and (cid:98)R (h ). All we can guarantee is the following upper bound:Proposition 3 Let \u03b3 = 1/ min max P [h(x ) = y]. Then,(cid:98)R (h ) \u2264 \u03b3 (cid:98)R (h ).See Appendix E.2 for a simple proof.Corollary 4 For binary classification, we have \u03b3 \u2264 2 and thus, we recover the well known bound(cid:98)R (h ) \u2264 2 (cid:98)R (h ) ifRemark 5 These bounds might be loose in practice.(cid:98)R (h ) \u2264 then we have (cid:98)R (h ) = 0. To this end, prior work [Lacasse et al., 2006,Germain et al., 2015, Masegosa et al., 2020] have developed tighter bounds using second-orderinequalities. We leave the analyses of these second-order RAI games to future work.for the binary case,Specifically,As such, we can cast minimizing randomized RAI risk as minimizing an upper bound on thedeterministic ensemble RAI risk. Thus, the corresponding randomized RAI game can be cast as arelaxation of the deterministic RAI game. In the sequel, we thus focus on this randomized ensembleRAI game, which we will then use to obtain a deterministic ensemble. Following the bounds above,the corresponding deterministic ensemble risk will be bounded by the randomized ensemble RAI risk",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "19f2e4a7-a208-4520-b187-eda5c37472ff",
                    "text": "In this section, we present two algorithms for solving the RAI game in Equation (1). Our firstalgorithm is motivated from online learning algorithms and the second algorithm is motivated from5greedy stepwise algorithms that have been popular for solving many statistical problems such asregression. For simplicity of presentation, we assume H is a finite set. However, our results in thesection extend to uncountable sets.",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "f79f6916-e568-490d-8f9c-0156448afd87",
                    "text": "In game play based algorithms, both the min and the max players are engaged in arepeated game against each other. Both players rely on no-regret algorithms to decide their nextaction. It is well known that such a procedure converges to a mixed NE of the game Cesa-Bianchiand Lugosi [2006]. In this work, we follow a similar strategy to solve the game in Equation (1)(see Algorithm 1 for the pseudocode). In the t round of our algorithm, the following distributionw \u2208 W is computed over the training data pointsw \u2190 argmax (cid:88) E \u2113(h (x), y) + \u03b7 Reg(w) (2)This update is called the Follow-The-Regularized-Leader (FTRL) update. Here, Reg(\u00b7) is a stronglyconcave regularizer and \u03b7 is the regularization strength. One popular choice for Reg(\u00b7) is thenegative entropy which is given by \u2212 (cid:80) w log w . This regularizer is also used by AdaBoost, whichis a popular boosting algorithm. In Appendix F.2, we provide analytical expressions for w forvarious choices of W , Reg(\u00b7). We note that the regularizer in the FTRL update ensures the stabilityof the updates; i.e., it ensures consecutive iterates do not vary too much. This stability is naturallyguaranteed when W is a strongly convex set (an example of a strongly convex set is the level set ofa strongly convex function. See Appendix for a formal definition and more details). Consequently,the regularization strength \u03b7 could be set to 0 in this case, and the algorithm still converges to aNE [Huang et al., 2017].Once we have w , a new classifier h is computed to minimize the weighted loss relative to w , andadded to the ensemble. This update is called the Best Response (BR) update. Learning h in this wayhelps us fix past classifiers\u2019 mistakes, eventually leading to an ensemble with good performance.Algorithm 1 Game play algorithm for solving Equation (1)Input: Training data {(x , y )}concave regularizer R over W , learning rates {\u03b7 }, loss function \u2113, constraint set W , hypothesis set H, stronglyFTRL: w \u2190 argmaxBR: h \u2190 argmin (cid:80)E \u2113(h(x), y)for t \u2190 1 to T doend forreturn P = (cid:80) w , Q = Unif{h , . . . h }E \u2113(h (x), y) + \u03b7 Reg(w)Greedy. We now take an optimization theoretic viewpoint to design algorithms for Equation (1).E E \u2113(h(x), y).Let L(Q) denote the inner maximization problem of (1): L(Q) := maxWhen L(Q) is smooth (this is the case when W is a strongly convex set), one could use Frank-Wolfe(FW) to minimize it. The updates of this algorithm are given byQ \u2190 (1 \u2212 \u03b1 )Q + \u03b1 G, where G = argmin (cid:10)Q, \u2207 L(Q )(cid:11) .E E \u2113(h(x), y). This algorithm is known to convergeHere, \u2207 L(Q ) = argmaxto a minimizer of L(Q) at O(1/t) rate [Jaggi, 2013]. When L(Q) is non-smooth, we first need tosmooth the objective before performing FW. In this work we perform Moreau smoothing [Parikhet al., 2014], which is given byL (Q) = max E E \u2113(h(x), y) + \u03b7Reg(w). (3)Here Reg(\u00b7) is a strongly concave regularizer. If Reg(\u00b7) is 1-strongly concave, it is well known thatL (Q) is O(1/\u03b7) smooth. Once we have the smoothed objective, we perform FW to find its optimizer(see Algorithm 2 for pseudocode). 6Relaxing the simplex constraint. We now derive a slightly different algorithm by relaxing thesimplex constraint on Q. Using Lagrangian duality we can rewrite min L (Q) as the followingproblem for some \u03bb \u2208 R min L (Q) + \u03bb Q(h).(cid:88)One interesting observation is that when W is the entire simplex and when \u03bb = \u22121/2, we recoverthe AdaBoost algorithm. Given the practical success of AdaBoost, we extend it to general W . Inparticular, we set \u03bb = \u22121/2 and solve the resulting objective using greedy coordinate-descent. Theupdates of this algorithm are given in Algorithm 2.Remark 6 Algorithm 2 takes the step sizes {\u03b1 }to figure out the optimal step-sizes, for better performance.as input. In practice, one could use line searchAlgorithm 2 Greedy algorithms for solving Equation (1)Input: Training data {(x , y )}concave regularizer R over W , regularization strength \u03b7, step sizes {\u03b1 }, loss function \u2113, constraint set W , hypothesis set H, stronglyfor t \u2190 1 to T doG = argminFW: Q \u2190 (1 \u2212 \u03b1 )Q + \u03b1 G / Gen-AdaBoost: Q \u2190 Q + \u03b1 Gend forreturn Q (cid:10)Q, \u2207 L (Q )(cid:11)We provide convergence rates for the algorithms below:Proposition 7 (Convergence Rates) Let l(h(x), y) \u2208 [0, 1] \u2200h \u2208 H, (x, y) \u2208 D and Reg : \u2206 \u2192 Rbe a 1-strongly concave function w.r.t norm \u2225.\u2225 . Let Q be the output returned from runningAlgorithm 1 or 2 for T iterations. Let D be a constant S.T. D = max |Reg(x) \u2212 Reg(y)|.1. (Gameplay) If \u03b7 = \u03b7, then Q satisfies L(Q ) \u2264 min L(Q) + + O( ).2. (Greedy) If line-search is performed for \u03b1 , then Q (FW or the Gen-AdaBoost update)+ O(satisfies L(Q ) \u2264 min L(Q) + \u03b7D ).We refer the reader to Appendix F.1 for a simple proof using existing theory on online convexoptimization [McMahan, 2011, Jaggi, 2013]. Another useful insight is that Algorithms 1 and 2 arerelated to each other under special settings as shown by Appendix H.1.Corollary 8 Consider Reg(w) = \u2212 (cid:80) w log w and l as the zero-one loss. Then, Algorithm 1and Algorithm 2 (line-search) achieve \u03f5\u2212approximate NE with \u03f5 as O (cid:18)(cid:113) (cid:19).It might not be practical for H-player to play BR (Step 3: AlgorithmWeak Learning Conditions1) or correspondingly, to find the best possible classifier at every round (Step 2: Algorithm 2). Underweak learning conditions, we can indeed achieve (approximate) convergence when we only solvethese problems approximately. See Appendix H.2 for more details.6 Generalization GuaranteesIn this section, we study the population RAI risk and present generalization bounds which quantifythe rates at which empirical RAI risk converges to its population counterpart.6.1 Population RAI GamesRecall, the empirical RAI risk optimizes over all sample re-weightings w \u2208 W that lie within theprobability simplex \u2206 . Thus it\u2019s population counterpart optimizes over distributions P that areabsolutely continuous with respect to the data distribution P :R (h) = E [\u2113(h(x), y)].sup7Following [Shapiro et al., 2021], we can rewrite this as follows. Suppose we use Z = (X, Y ) \u2208 Z :=X \u00d7 Y, so that P, P are distributions over Z. We then define \u2113 : Z (cid:55)\u2192 R as \u2113 (z) = \u2113(h(x), y).We can then write the population RAI risk as (see Appendix G for a proof):R (h) = sup E [r(z)\u2113 (z)]. (4)For classification, we define the RAI-Bayes optimal classifier as: Q = arg min R (Q). Here,the minimum is w.r.t the set of all measurable classifiers (both deterministic and random). This is the\u201ctarget\u201d classifier we wish to learn given finite samples. Note that this might not be the same as thevanilla Bayes optimal classifier: Q = arg min E[ (cid:98)R(Q)], which only minimizes the expected loss,and hence may not satisfactorily address RAI considerations.We now try to characterize the RAI-Bayes optimal classifier. However, doing this requires a bit morestructure on W . So, in the sequel, we consider constraint sets of the following form:(cid:26)W = r : Z (cid:55)\u2192 R : (cid:90) g (r(z))dP (z) \u2264 c , i \u2208 [m] , (5)(cid:27)where we assume that g : R (cid:55)\u2192 R, i \u2208 [m] are convex. Note that this choice of W encompasses abroad range of RAI games including DRO with f -divergence, CVaR, soft-margin uncertainty sets.Perhaps surprisingly, the following proposition shows that the minimizer of population RAI risk isnothing but the vanilla Bayes optimal classifier.Proposition 9 (Bayes optimal classifier) Consider the problem of binary classification whereY = {\u22121, +1}. Suppose \u2113(h(x), y) = \u03d5(yh(x)) for some \u03d5 : R \u2192 [0, \u221e) which is either the 0/1loss, or a convex loss function that is differentiable at 0 with \u03d5 (0) < 0. Suppose the uncertaintyset W is as specified in Equation (5). Moreover, suppose {g } are convex and differentiablefunctions. Then, the vanilla Bayes optimal classifier is also a RAI-Bayes optimal classifier.Remark 10 In the special case of m = 1 in Equation (5), we recover the result of [Hu et al., 2018].However, our proof is much more elegant than the proof of [Hu et al., 2018], and relies on the dualrepresentation of the population RAI risk.One perspective of the above result is that the vanilla Bayes optimal classifier is also \u201cresponsible\u201das specified by the RAI game. This is actually reasonable in many practical prediction problemswhere the label annotations are actually derived from humans, who presumably are also responsible.Why then might we be interested in the RAI risk? One advantage of the RAI risks is in finite samplesettings where the equivalence no longer holds, and the RAI risk could be construed as encoding priorknowledge about properties of the Bayes optimal classifier. We also note that the above equivalenceis specific for binary classification.6.2 Generalization GuaranteesOur generalization bounds rely on the following dual characterization of the RAI population risk.Proposition 11 Suppose the uncertainty set W is as specified in Equation (5). Then for any hypothesish, the population RAI risk can be equivalently written asR (h) = inf E G (\u2113 (z) \u2212 \u03c4 ) + (cid:88) \u03bb c + \u03c4, (6)where G is the Fenchel conjugate of G (t) = (cid:80) \u03bb g (t).We utilize the above expression for R (h) to derive the following deviation bound for (cid:98)R (h).Theorem 12 Consider the setting of Proposition 11. Suppose {g } are convex and differen-tiable functions. Suppose \u2113 (z) \u2208 [0, B] for all h \u2208 H, z \u2208 Z. Suppose, for any distribution P , theminimizers (\u03bb , \u03c4 ) of Equation (6) lie in the following set: E = {(\u03bb, \u03c4 ) : \u2225\u03bb \u2225 \u2264 \u00af\u039b, |\u03c4 | \u2264 T }.Moreover, let\u2019s suppose the optimal \u03bb for P is bounded away from 0 and satisfies min \u03bb .\u2265 \u039bLet G, L, be the range and Lipschitz constants of G :G := sup G (B \u2212 \u03c4 ) \u2212 G (\u2212\u03c4 ), L := sup (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) \u2202G (x \u2212 \u03c4 )\u2202(\u03bb, \u03c4 ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .8For any fixed h \u2208 H, with probability at least 1 \u2212 2e|R (h) \u2212 (cid:98)R (h)| \u2264 10n G((cid:112)t + m log(nL)).Given Theorem 12, one can take a union bound over the hypothesis class H to derive the followinguniform convergence bounds.Corollary 13 Let N (H, \u03f5, \u2225 \u00b7 \u2225 ) be the covering number of H in the sup-norm which isdefined as \u2225h\u2225 = sup |h(z)|. Then with probability at least 1 \u2212 N (H, \u03f5 , \u2225 \u00b7 \u2225 )e ,the following holds for any h \u2208 H: |R (h) \u2212 (cid:98)R (h)| \u2264 30n G((cid:112)t + m log(nL)). Here\u03f5 = n G(cid:112)t + m log(nL).The above bound depends on parameters (\u03bb , \u03c4 , G, L) which specific to the constraint set W . Toinstantiate it for any W one needs to bound these parameters. We note that our generalization\u2192 0. This is because the Lipschitz constant L could potentiallyguarantees become sub-optimal as \u039bget larger as \u03bb approaches the boundary. Improving these bounds is an interesting future direction.Remark 14 We note that aforementioned results results follow from relatively stringent assumptions.Exploring the impact of relaxing these assumptions is an interesting direction for future works.7 ExperimentsIn this section, we demonstrate the generality of proposed RAI methods by studying one of the mostwell-studied problems in RAI i.e. the case of subpopulation shift. Given a large number of possibleW , we acknowledge that this is not a complete analysis, even with respect to the problems that livewithin the minimax framework. Instead, we aim to display convergence, plug-and-play generality,and superior performance over some seminal baselines of this task. We conduct experiments on bothsynthetic and real-world datasets. Please refer to Appendix for details on synthetic experiments. Weconsider a number of responsible AI settings, including subpopulation shift, in the domain-oblivious(DO) setting where we do not know the sub-populations [Hashimoto et al., 2018, Lahoti et al., 2020,Zhai et al., 2021a], the domain-aware (DA) setting where we do [Sagawa et al., 2019], and thepartially domain-aware (PDA) setting where only some might be known.Datasets & Domain Definition. We use the following datasets: COMPAS [Angwin et al., 2016],CIFAR-10 (original, and with a class imbalanced split [Jin et al., 2021, Qi et al., 2021]) and CIFAR-100. See the Appendix for more details on our datasets. For COMPAS, we consider race (White vsOther) and biological gender (Male vs Female) as our sensitive attributes. This forms four disjointsubgroups defined by these attributes. In the PDA setting, we partition only across the attribute racewhile training, but still run tests for all four subgroups. On CIFAR-10, class labels define our 10subpopulations. Similarly as above, for the PDA setting, we make 5 super-groups of two classeseach. On CIFAR-100, class labels define our 100 subpopulations. For the PDA setting, we make 20super-groups, each consisting of five classes.Baselines. We compare our method against the following baselines: (a) Deterministic classifierstrained on empirical risk (ERM) and DRO risks, particularly the quasi-online algorithm for GroupDRO [Sagawa et al., 2019] (Online GDRO), and an ITLM-inspired SGD algorithm [Zhai et al.,2021b, Shen and Sanghavi, 2018] for \u03c7 DRO (SGD (\u03c7 )) (b) Ensemble models AdaBoost [Schapire,1999]. Note that the purpose of our experiments is to show that we can match baselines for a specificsingle desideratum (e.g. worst-case sub-population) while allowing for learning models that can solvemultiple responsible AI desiderata at the same time, for which we have no existing baselines.Proposed Methods. We focus on Algorithm 2 and refer to FW and Gen-AdaBoost updates asRAI-FW and RAI-GA, respectively. Moreover, our implementations include the following alterations:\u2022 We track the unregularized objective value from Equation 1 for the validation set, and whenever itincreases we double the regularization factor \u03b7, which we find can improve generalization. \u2022 We alsouse this objective w.r.t the normalized Q to perform a line search for the step size \u03b1. For the FWupdate, our search space is a ball around at round t, while for GA, we search within (0, 1).Base Learners & Training. Training time scales linearly with the number of base learners. Inference,though, can be parallelized if need be. We usually find training on 3-5 learners is good enough on allscenarios explored in the paper. We defer further details of our base learners and hyperparameterchoices to the Appendix. 9Constraint sets W . For RAI algorithms, we use the following constraint sets: \u2022 Domain Oblivious(DO): We use the \u03c7 -DRO constraint set to control for worst-case subpopulations. \u2022 Domain Aware(DA): We use the Group DRO constraint set as the domain definitions are known. \u2022 PartiallyDomain-Aware (PDA): We use a novel set W which is the intersection over Group DRO constraintsover the known domains and \u03c7 constraints to control for unknown group performance. For baselines,we use AdaBoost and SGD(\u03c7 ) for the DO setting. Online GDRO serves as our baseline for both DAand PDA settings, where the algorithm uses whatever domain definitions are available.Table 2:Results and Discussion. We run our methods and baselines under the settings described above andreport the results in Table 2. As such, we can make the following observations:1. RAI-FW and RAI-GA methods significantly improve the worst-case performance with only a fewbase learners across all datasets in all three settings, while maintaining average case performance.Moreover, For seemingly harder tasks i.e. a large gap between average and worst-case performance,the algorithms are able to improve significantly over the baselines. For example, we observe a 5%improvement in performance in the case of CIFAR-100.2. The plug-and-play framework allows for several different W to enhance various responsible AIqualities at once. We demonstrate this with the partial domain aware setting (PDA), where theperformance lead widens, indicating that RAI is able to jointly optimize effectively for both knownand unknown subpopulations while Online GDRO suffers from some of the group informationbeing unknown. In practice, one can construct many more novel sets W .3. Although bigger (complex) models exhibit stronger performance than RAI ensembles, there areseveral caveats to this observation. Firstly, these models are \u223c10-15 times larger than our basemodels. This limits their use w.r.t both training & inference compute required. However, RAIensembles utilize a small number of much smaller models which can be individually trained quiteeasily. Even with these large models as base learners, constructing ensembles exhibits a perfor-mance boost, indicating that our framework is able to \u201cboost\u201d models of varying complexities.8 ConclusionUnder the umbrella of \u201cresponsible AI\u201d, an emerging line of work has attempted to formalizedesiderata ranging over ethics, fairness, robustness, and safety, among others. Many of these settings(Table 1) can be written as min-max problems involving optimizing some worst-case loss under a setof predefined distributions. For all the problems that can be framed as above, we introduce and studya general framework, which we refer to as Responsible AI (RAI) games. Our framework extends toclassical boosting scenarios, offering boosting-based algorithms for RAI games alongside provenconvergence guarantees. We propose practical algorithms to solve these games, as well as statisticalanalyses of solutions of these games. We find that RAI can guarantee multiple responsible AI aspectsunder appropriate choices of uncertainty sets. 10AcknowledgementsWe acknowledge the support of DARPA via HR00112020006, and NSF via IIS-1909816.ReferencesMicrosoft. Microsoft. https://www.microsoft.com/en-us/ai/responsible-ai, 2021. Ac-cessed: Date Accessed.Google. Google. https://ai.google/responsibility/responsible-ai-practices/, 2020.Accessed: Date Accessed.Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives.Advances in neural information processing systems, 30, 2017.John Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionallyrobust optimization. arXiv preprint arXiv:1810.08750, 2018.Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robustneural networks for group shifts: On the importance of regularization for worst-case generalization.arXiv preprint arXiv:1911.08731, 2019.Runtian Zhai, Chen Dan, Arun Suggala, J Zico Kolter, and Pradeep Ravikumar. Boosted cvarclassification. Advances in Neural Information Processing Systems, 34:21860\u201321871, 2021a.Tatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness withoutdemographics in repeated loss minimization. International Conference On Machine Learning,2018.Runtian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. Doro: Distributional and outlier robustoptimization. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th InternationalConference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,pages 12345\u201312355. PMLR, 18-24 Jul 2021b. URL https://proceedings.mlr.press/v139/zhai21a.html.Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. Fr-train: A mutual information-based approach to fair and robust training. arXiv preprint arXiv: Arxiv-2002.10234, 2020.Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-algorithm and applications. Theory of computing, 8(1):121\u2013164, 2012.Brendan McMahan. Follow-the-regularized-leader and mirror descent: Equivalence theorems andl1 regularization. In Geoffrey Gordon, David Dunson, and Miroslav Dud\u00edk, editors, Proceedingsof the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 ofProceedings of Machine Learning Research, pages 525\u2013533, Fort Lauderdale, FL, USA, 11\u201313Apr 2011. PMLR.S\u00e9bastien Bubeck. Introduction to online optimization. Lecture notes, 2:1\u201386, 2011.Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervisedlearning give robust classifiers? In International Conference on Machine Learning, pages 2029\u20132037. PMLR, 2018.Peter B\u00fchlmann. Invariance, causality and robustness. arXiv preprint arXiv: 1812.08233, 2018.Soroosh Shafieezadeh-Abadeh, Peyman Mohajerin Esfahani, and D. Kuhn. Distributionally robustlogistic regression. Neural Information Processing Systems, 2015.Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with wassersteindistance. Mathematics of Operations Research, 2022.Hongseok Namkoong and John C. Duchi. Stochastic gradient methods for distributionally robustoptimization with f-divergences. In Neural Information Processing Systems, 2016. URL https://api.semanticscholar.org/CorpusID:7481496.11Aharon Ben-Tal, Dick den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.Robust solutions of optimization problems affected by uncertain probabilities. Advanced Risk &Portfolio Management\u00ae Research Paper Series, 2011. URL https://api.semanticscholar.org/CorpusID:761793.Debmalya Mandal, Samuel Deng, Suman Jana, Jeannette Wing, and Daniel J Hsu. Ensuring fairnessbeyond the training data. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,editors, Advances in Neural Information Processing Systems, volume 33, pages 18445\u201318456. Cur-ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/d6539d3b57159babf6a72e106beb45bd-Paper.pdf.Natalia Martinez, Martin Bertran, and Guillermo Sapiro. Minimax pareto fairness: A multi objectiveperspective. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th InternationalConference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,pages 6755\u20136764. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/martinez20a.html.L. Oneto, Michele Donini, Amon Elders, and M. Pontil. Taking advantage of multitask learning forfair classification. AAAI/ACM Conference on AI, Ethics, and Society, 2018. doi: 10.1145/3306618.3314255.Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal-subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: Abenchmark of in-the-wild distribution shifts. In International Conference on Machine Learning,pages 5637\u20135664. PMLR, 2021.Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanceddatasets with label-distribution-aware margin loss. Advances in Neural Information ProcessingSystems, 32:1567\u20131578, 2019.Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, andSanjiv Kumar. Long-tail learning via logit adjustment. In International Conference on LearningRepresentations, 2021.Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis. Label-imbalanced and group-sensitive classification under overparameterization. In Thirty-Fifth Confer-ence on Neural Information Processing Systems, 2021.Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributionalrobustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributionally robust optimization andvariation regularization. Operations Research, 2022.Matthew Staib and Stefanie Jegelka. Distributionally robust optimization and generalization in kernelmethods. Advances in Neural Information Processing Systems, 32, 2019.Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, and Bernhard Sch\u00f6lkopf. Kernel distributionallyrobust optimization. arXiv preprint arXiv:2006.06981, 2020.Mike Li, Hongseok Namkoong, and Shangzhou Xia. Evaluating model performance under worst-casesubpopulations. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, 2021a. URL https://openreview.net/forum?id=nehzxAdyJxF.Qi Qi, Yi Xu, Rong Jin, Wotao Yin, and Tianbao Yang. Attentional biased stochastic gradient forimbalanced classification. arXiv preprint arXiv:2012.06951, 2020.Ramnath Kumar, Kushal Majmundar, Dheeraj Nagaraj, and Arun Sai Suggala. Stochastic re-weightedgradient descent via distributionally robust optimization. arXiv preprint arXiv:2306.09222, 2023.Jikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang. Non-convex distributionally robustoptimization: Non-asymptotic analysis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. WortmanVaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=gZLhHMyxa-.12Leo Breiman. Prediction games and arcing algorithms. Neural computation, 11(7):1493\u20131517, 1999.Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. Additive logistic regression: a statisticalview of boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2):337\u2013407, 2000.Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals ofstatistics, pages 1189\u20131232, 2001.Yoav Freund and Robert E Schapire. A desicion-theoretic generalization of on-line learning and anapplication to boosting. In European conference on computational learning theory, pages 23\u201337.Springer, 1995.Yoav Freund, Robert E Schapire, et al. Experiments with a new boosting algorithm. In icml,volume 96, pages 148\u2013156. Citeseer, 1996.Llew Mason, Jonathan Baxter, Peter L Bartlett, and Marcus R Frean. Boosting algorithms as gradientdescent. In Advances in neural information processing systems, pages 512\u2013518, 2000.Robert E Schapire. The boosting approach to machine learning: An overview. In MSRI workshop onNonlinear Estimation and Classification, 1999.Ayhan Demiriz, Kristin P Bennett, and John Shawe-Taylor. Lpboost: A boosting algorithm withlinear programming. In International Conference on Machine Learning, pages 143\u2013150. AAAIPress, 2002.Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Boosting algorithms as gradientdescent. In S. Solla, T. Leen, and K. M\u00fcller, editors, Advances in Neural Information ProcessingSystems, volume 12. MIT Press, 1999. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Paper.pdf.Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. arXiv preprint arXiv:Arxiv-1603.02754, 2016.Dinghuai Zhang, Hongyang Zhang, Aaron Courville, Yoshua Bengio, Pradeep Ravikumar, andArun Sai Suggala. Building robust ensembles via margin boosting. In International Conference onMachine Learning, pages 26669\u201326692. PMLR, 2022.Laurent Meunier, Meyer Scetbon, Rafael B Pinot, Jamal Atif, and Yann Chevaleyre. Mixed nashequilibria in the adversarial examples game. In Marina Meila and Tong Zhang, editors, Proceedingsof the 38th International Conference on Machine Learning, volume 139 of Proceedings of MachineLearning Research, pages 7677\u20137687. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/meunier21a.html.Maria-Florina Balcan, Rattana Pukdee, Pradeep Ravikumar, and Hongyang Zhang. Nash equilibriaand pitfalls of adversarial training in adversarial robustness games. In International Conference onArtificial Intelligence and Statistics, pages 9607\u20139636. PMLR, 2023.M. A. Bennouna and Bart P. G. Van Parys. Holistic robust data-driven decisions. ARXIV.ORG, 2022.doi: 10.48550/arXiv.2207.09560.Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness throughawareness. In Proceedings of the 3rd innovations in theoretical computer science conference,pages 214\u2013226, 2012.Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.In International conference on machine learning, pages 325\u2013333. PMLR, 2013.Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advancesin neural information processing systems, 29, 2016a.Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairnessbeyond disparate treatment & disparate impact: Learning classification without disparate mistreat-ment. In Proceedings of the 26th international conference on world wide web, pages 1171\u20131180,2017. 13Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. Advances inneural information processing systems, 30, 2017.John Rawls. A theory of justice: Revised edition. Harvard university press, 2020.Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. Nips tutorial, 1:2017, 2017.Alexandra Chouldechova and Aaron Roth. The frontiers of fairness in machine learning. arXivpreprint arXiv:1810.08810, 2018.Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A surveyon bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1\u201335, 2021.Mike Li, Hongseok Namkoong, and Shangzhou Xia. Evaluating model performance under worst-casesubpopulations. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan,editors, Advances in Neural Information Processing Systems, volume 34, pages 17325\u201317334.Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/908075ea2c025c335f4865f7db427062-Paper.pdf.Manfred K Warmuth, Jun Liao, and Gunnar R\u00e4tsch. Totally corrective boosting algorithms thatmaximize the margin. In Proceedings of the 23rd international conference on Machine learning,pages 1001\u20131008, 2006.Peter Bartlett, Yoav Freund, Wee Sun Lee, and Robert E Schapire. Boosting the margin: A newexplanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651\u20131686,1998.Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Interna-tional Conference on Machine Learning, pages 4615\u20134625. PMLR, 2019.Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervisedlearning give robust classifiers? International Conference On Machine Learning, 2016.Alexandre Lacasse, Fran\u00e7ois Laviolette, Mario Marchand, Pascal Germain, and Nicolas Usunier.Pac-bayes bounds for the risk of the majority vote and the variance of the gibbs classifier. Advancesin Neural information processing systems, 19, 2006.Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario Marchand, and Jean-Francis Roy.Risk bounds for the majority vote: From a pac-bayesian analysis to a learning algorithm. arXivpreprint arXiv:1503.08329, 2015.Andr\u00e9s Masegosa, Stephan Lorenzen, Christian Igel, and Yevgeny Seldin. Second order pac-bayesianbounds for the weighted majority vote. Advances in Neural Information Processing Systems, 33:5263\u20135273, 2020.Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games. Cambridge universitypress, 2006.Ruitong Huang, Tor Lattimore, Andr\u00e1s Gy\u00f6rgy, and Csaba Szepesv\u00e1ri. Following the leader andfast rates in online linear prediction: Curved constraint sets and other regularities. The Journal ofMachine Learning Research, 18(1):5325\u20135355, 2017.Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In Internationalconference on machine learning, pages 427\u2013435. PMLR, 2013.Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and trends\u00ae in Optimization, 1(3):127\u2013239, 2014.Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on stochastic program-ming: modeling and theory. SIAM, 2021.Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, andEd Chi. Fairness without demographics through adversarially reweighted learning. Advances inneural information processing systems, 33:728\u2013740, 2020.14Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias: There\u2019sfuture criminals. and its biased againstURL https://www.propublica.org/article/software used across the country to predictProPublica, May 2016.blacks.machine-bias-risk-assessments-in-criminal-sentencing.Qi Qi, Zhishuai Guo, Yi Xu, Rong Jin, and Tianbao Yang. An online method for aclass of distributionally robust optimization with non-convex objectives. In M. Ranzato,A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances inNeural Information Processing Systems, volume 34, pages 10067\u201310080. Curran Associates,Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/533fa796b43291fc61a9e812a50c3fb6-Paper.pdf.Yanyao Shen and Sujay Sanghavi. Learning with bad training data via iterative trimmed lossminimization. International Conference On Machine Learning, 2018.Nidhi Kalra and Susan M. Paddock. Driving to safety: How many miles of driving would it take todemonstrate autonomous vehicle reliability? Transportation Research Part A: Policy and Practice,94:182\u2013193, 2016.Andreas Fuster, Paul Goldsmith-Pinkham, Tarun Ramadorai, and Alexander Walther. Predictablyunequal? the effects of machine learning on credit markets. Social Science Research Network,(3072038), 2018.Xinsong Ma, Zekai Wang, and Weiwei Liu. On the tradeoff between robustness and fairness. InS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances inNeural Information Processing Systems, volume 35, pages 26230\u201326241. Curran Associates,Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/a80ebbb4ec9e9b39789318a0a61e2e43-Paper-Conference.pdf.Anastasia Chan. Gpt-3 and instructgpt: technological dystopianism, utopianism, and \u201ccontextual\u201dperspectives in ai ethics and industry. AI and Ethics, 3(1):53\u201364, 2023.Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On thedangers of stochastic parrots: Can language models be too big. In Proceedings of the 2021 ACMconference on fairness, accountability, and transparency, pages 610\u2013623, 2021.Christos Louizos, Kevin Swersky, Yujia Li, M. Welling, and R. Zemel. The variational fair autoen-coder. International Conference on Learning Representations, 2015.Moritz Hardt, Eric Price, Eric Price, and Nati Srebro. Equality of opportunity in super-vised learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, edi-tors, Advances in Neural Information Processing Systems, volume 29. Curran Associates,Inc., 2016b. URL https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf.Haohan Wang, Zexue He, Zachary Chase Lipton, and E. Xing. Learning robust representations byprojecting superficial statistics out. International Conference on Learning Representations, 2018.Sravanti Addepalli, Anshul Nasery, R. Venkatesh Babu, Praneeth Netrapalli, and Prateek Jain.Learning an invertible output mapping can mitigate simplicity bias in neural networks. arXivpreprint arXiv: 2210.01360, 2022.Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun. Domain generalization by mutual-information regularization with pre-trained models. European Conference on Computer Vision,2022. doi: 10.48550/arXiv.2203.10789.Elad Hazan. Introduction to online convex optimization. Found. Trends Optim., 2016. doi: 10.1561/2400000013.Kartik Gupta, Arun Sai Suggala, Adarsh Prasad, Praneeth Netrapalli, and Pradeep Ravikumar.Learning minimax estimators via online learning. ARXIV.ORG, 2020.Jonathan Michael Borwein. A very complicated proof of the minimax theorem. 2016.15Abraham Wald. Generalization of a theorem by v. neumann concerning zero sum two person games.Annals of Mathematics, 46:281, 1945.Stephen Simons. Minimax theorems and their proofs. 1995.TES Raghavan. Zero-sum two-person games. Handbook of game theory with economic applications,2:735\u2013768, 1994.Andrew Cotter, Maya Gupta, and Harikrishna Narasimhan. On making stochastic classifiers deter-ministic. Advances in Neural Information Processing Systems, 32, 2019.Jimmy Wu, Yatong Chen, and Yang Liu. Metric-fair classifier derandomization. In InternationalConference on Machine Learning, pages 23999\u201324016. PMLR, 2022.H Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. The Journalof Machine Learning Research, 18(1):3117\u20133166, 2017.Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.R Tyrrell Rockafellar. Convex analysis. Number 28. Princeton university press, 1970.Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.Journal of the American Statistical Association, 101(473):138\u2013156, 2006.Ambuj Tewari and Peter L Bartlett. On the consistency of multiclass classification methods. Journalof Machine Learning Research, 8(5), 2007.Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-bridge university press, 2019.Robert M. Freund and Paul Grigas. New analysis and results for the frank\u2013wolfe method.Mathematical Programming, 155:199\u2013230, 2014. doi: 10.1007/s10107-014-0841-6. URLhttp://link.springer.com/article/10.1007/s10107-014-0841-6/fulltext.html.Sergey Zagoruyko and N. Komodakis. Wide residual networks. British Machine Vision Conference,2016. doi: 10.5244/C.30.87. 16A Broader ImpactB LimitationsC Terminology and NotationC.1 Terminology .C.2 Notation . . . .. .. .. .. .. ..D Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .D.1 Two Player Zero-sum Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . .D.2 Online Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .D.3 General Minimax Theorems (Proof of Proposition 1) . . . . . . . . . . . . . . . .E Ensemble RAI GamesE.1 Alternative Definitions: Deterministic Ensembles . . . . . . . . . . . . . . . . . .E.2 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .F AlgorithmsF.1 Convergence Rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .F.2 Closed Form Updates for different uncertainty sets . . . . . . . . . . . . . . . . .F.3 Proof of Corollary 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .G GeneralizationG.1 Population Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .G.2 Generalization Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .G.3 Proof of Corollary 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .H Algorithms: Further DiscussionH.1 Equivalence Conditions for RAI Algorithms (1 and 2) . . . . . . . . . . . . . . . .H.2 Weak Learning Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .I ExperimentsI.1I.2I.3I.4 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Further Details for Section 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Interesting Observation: Boosting Robust Learners . . . . . . . . . . . . . . . . .Synthetic Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1818191919191920212121212222232424242627282828292930303117A Broader ImpactResponsible AI has become an important topic as ML/AI systems increase in scale, and are beingdeployed in a variety of scenarios. Models not optimized for responsible facets can have disastrousconsequences [Kalra and Paddock, 2016, Angwin et al., 2016, Fuster et al., 2018].Our research promotes the principles of Responsible AI (RAI), encompassing ethics, fairness, andsafety considerations. By providing a framework to consider these simultaneously, this researchcould help prevent scenarios where optimizing for one aspect unintentionally compromises another[Ma et al., 2022], mitigating the risk of creating biases or vulnerabilities in AI systems.We also address the fragmentation in recent work around Responsible AI. The \u2019plug-and-play\u2019nature of our approach allows for the easy adaptation of algorithms for different Responsible AIconsiderations. This adaptability could lead to more practical and user-friendly tools for implementingRAI across different applications.Numerous ML models like Large language models, (GPT-3, GPT-4), are rapidly transformingnumerous domains, including natural language processing, data analysis, content creation, and more.Given their remarkable ability to generate human-like text, they can be used to construct narratives,answer queries, or even automate customer service. However, with such capabilities come significantresponsibilities, as these models can inadvertently perpetuate biases, misinformation, or harmfulcontent if not correctly regulated. Therefore, ensuring the responsible behavior of these models iscrucial [Chan, 2023, Bender et al., 2021]. Our research presents a general framework that can bepivotal in the responsible design of such large language models.B LimitationsWe now identify some limitations of our current work, along with corresponding future directions.\u2022 To more concretely establish the empirical superiority of our optimization techniques, moreexperiments involving large over-parametrized models need to be conducted.\u2022 The proposed generalization bounds are not tight for all risks. A more careful analysis would beneeded for such a generalization.\u2022 Our framework only handles uncertainty sets that are supported on the training data. It\u2019d be inter-esting to generalize our framework further to support other uncertainty sets based on Wassersteindivergences that are not necessarily supported on the training data.\u2022 Finally, the presence of outliers can often destabilize training of large models[Zhai et al., 2021b].However, our current framework assumes the training data is un-corrupted. In future, we aim toextend our framework to support corruptions in the training data.\u2022 We note that our framework can also be extended to the problem of adversarial test-time robustnesswhere there is an adversary corrupting the inputs sent to the model during inference. Let A(x)be the set of perturbations that the adversary can add to input x. The uncertainty set in this casecontains distributions supported on {(x , y ) : \u2203(x, y) \u2208 (cid:98)P such that x \u2208 x + A(x), y = y}.Our framework primarily covers RAI aspects which can written as minmax problems. Here weprovide some other notions of RAI which our framework does not directly cover.Fairness: Various notions of fairness have been studied by the ML community. While our frame-work captures minimax group fairness, it doesn\u2019t capture other notions of group fairness such asDemographic Parity [Louizos et al., 2015], Equality of Odds, Equality of Opportunity [Hardt et al.,2016b]. Our framework doesn\u2019t capture individual fairness notions.Robustness: While our framework covers group robustness and certain forms of distributionalrobustness, it doesn\u2019t cover robustness to Wasserstein perturbations and other (un)structured distribu-tion shifts often studied in the domain generalization community [Wang et al., 2018, Addepalli et al.,2022, Cha et al., 2022]. 18C Terminology and NotationC.1 TerminologyStrongly Convex Sets A set A is \u03bb-strongly convex w.r.t a norm \u2225\u00b7\u2225, if, for any x, y \u2208 A, \u03b3 \u2208 [0, 1],the \u2225 \u00b7 \u2225 norm ball with origin \u03b3x + (1 \u2212 \u03b3)y and radius \u03b3(1 \u2212 \u03b3)\u03bb\u2225x \u2212 y\u2225 /2 lies in A.f -divergence For any two probability distributions P, Q, f -divergence between P and Q is definedas D (Q||P ) = E [f (dQ/dP )]. Here f : R \u2192 R is a convex function such that f (1) = 0.C.2 Notation Table 3: NotationDescriptionInput Random VariableInput DomainOutput Random VariableOutput DomainSample Random Variable (X, Y )Sample Domain X \u00d7 YSample SetData Generating DistributionEmpirical Distribution (Uniform) over SSet of HypothesisDistribution over Hypothesis from HAny given hypothesisRandomized Ensemble given by QDe-randomized/Deterministic Classifier corresponding to hLoss FunctionEmpirical Risk of hSet of allowed sample weights (aka Uncertainty Set)Empirical RAI Risk of hPopulation RAI Risk of hSymbolXXYYZZSP(cid:98)PHQhhh l(cid:98)R(h)W(cid:98)R (h)R (h)(cid:98)R (Q) Randomized Ensemble RAI RiskKL(p||q)GReg KL-divergence Metric between p and qSubpopulation/Domain iAny given regularizer functionD BackgroundD.1 Two Player Zero-sum GamesConsider the following game between two players. One so-called \u201crow player\u201d playing actionsh \u2208 H, and the other \u201ccolumn player\u201d playing actions z \u2208 Z. Suppose that when the two playersplay actions h, z respectively, the row player incurs a loss of l(h, z) \u2208 R, while the column playerincurs a loss of \u2212l(h, z). The sum of the losses for the two players can be seen to be equal to zero sosuch a game is known as a two-player zero-sum game. It is common in such settings to refer to thegain l(h, z) of the column player, rather than its loss of \u2212l(h, z). Both players try to maximize theirgain/minimize their loss.It is common in game theory to consider a linearized game in the space of probability measures,which is in general better behaved. To set up some notation, for any probability distributions P overH, and P over Z, define: l(P , P ) = E l(h, z)19Nash Equilibrium A Nash Equilibrium (NE) is a stable state of a game where no player can gainby unilaterally changing their strategy while the other players keep theirs unchanged. In a two-playerzero-sum game, a Nash Equilibrium is a pair of mixed strategies (h , z ) satisfyingsup l(h , z) \u2264 l(h , z ) \u2264 inf l(h, z )Note that whenever a pure strategy NE exists, the minimax and maximin values of the game are equalto each other: inf sup l(h, z) = l(h , z ) = sup inf l(h, z)What often exists is a mixed strategy NE, which is precisely a pure strategy NE of the linearizedgame. That is, (P ) is called a mixed strategy NE of the zero-sum game, if, P, P ) \u2264 l(P l(P , P )) \u2264 inf, P l(PsupFor this paper, Q \u2261 P , w \u2261 P , \u2206 \u2261 P and W \u2261 P .\u03f5-Approximate Nash Equilibrium An \u03f5-approximate Nash Equilibrium is a relaxation of the NashEquilibrium, where each player\u2019s strategy may not be the best response but is still within \u03f5 of the bestresponse. Formally, a pair of mixed strategies (P , P ) is an \u03f5-approximate Nash Equilibrium ifinf l(P , P ) + \u03f5 \u2265 l(P , P ) \u2265 sup l(P , P ) \u2212 \u03f5 (7)No Regret Algorithms No-regret algorithms are a class of online algorithms used in repeatedgames. The regret of a player is defined as the difference between their cumulative payoff and thebest cumulative payoff they could have achieved by consistently playing a single strategy. A no-regretalgorithm guarantees that the average regret of a player goes to zero as the number of iterations(or rounds) tends to infinity. In the context of two-player zero-sum games, if both players followno-regret algorithms, their average strategy profiles converge to the set of Nash Equilibria.D.2 Online LearningA popular and widely used approach for solving min-max games is to rely on online learningalgorithms [Hazan, 2016, Cesa-Bianchi and Lugosi, 2006]. In this approach, the row (minimization)player and the column (maximization) player play a repeated game against each other. Both playersrely on online learning algorithms to choose their actions in each round of the game, with the objectiveof minimizing their respective regret. The following proposition shows that this repeated gameplayconverges to a NE.Proposition 15 ([Gupta et al., 2020]) Consider a repeated game between the minimization andmaximization players in the linearized game. Let (P ) be the actions chosen by the players initeration t. Suppose the actions are such that the regret of each player satisfies:, P(cid:88) l(P , P ) \u2212 inf (cid:88) l(h, P ) \u2264 \u03f5 (T )(cid:88)sup l(P , z) \u2212 (cid:88) l(P , P ) \u2264 \u03f5 (T )(cid:80)Let P , P denote the mixture distributions(P , P ) is an \u03f5-approximate mixed NE of the game with: P and (cid:80) P . Then\u03f5 = \u03f5 (T ) + \u03f5 (T )TThere exist several algorithms such as FTRL, FTPL, and Best Response (BR), which guaranteesub-linear regret. It is important to choose these algorithms appropriately, given the domains H, Z asour choices impact the rate of convergence to a NE and also the computational complexity of theresulting algorithm. 20D.3 General Minimax Theorems (Proof of Proposition 1)We first state the following convenient generalization of the original Von Neumann\u2019s minimaxtheorem.Proposition 16 (Von Neumann-Fan minimax theorem, [Borwein, 2016]) Let X and Y be Banachspaces. Let C \u2282 X be nonempty and convex, and let D \u2282 Y be nonempty, weakly compact, andconvex. Let g : X \u00d7 Y \u2192 R be convex with respect to x \u2208 C and concave and upper-semicontinuouswith respect to y \u2208 D, and weakly continuous in y when restricted to D. Then,sup inf g(x, y) = inf sup g(x, y)We now proceed to the proof of Proposition 1. Observe that a convex, compact W satisfies theconditions for D in the above proposition. Moreover, we have C = \u2206 i.e. the set of probabilitymeasures on \u0398. It is indeed nonempty and convex. Also, our g is bilinear in Q and w, and thus isconvex-concave. Thus, Proposition 1 directly follows from the above result.Relaxations We can relax the assumption that h is parameterized by a finite dimensional vector \u03b8.For simpler H, the minimax result directly holds with mild assumptions.\u2022 If H = {h , h , ...h } i.e. H is finite. Then the original minimax theorem by Neumannholds for arbitrary functions l.\u2022 If H = {h , h , ...} i.e. H is denumerable. We further assume l is a bounded loss function.Then from Theorem 3.1 from [Wald, 1945] to compact and convex W over n (finite)datapoints, we can conclude the relation holds.Hoever, minimax theorems for more general H require other conditions like the continuity of loss,compactness in function space, etc. See [Simons, 1995] and [Raghavan, 1994].E Ensemble RAI GamesE.1 Alternative Definitions: Deterministic EnsemblesAlternative definitions for deterministic ensembles could be considered. For example, one couldconsider h (x) = arg min E \u2113(h(x), y). [Cotter et al., 2019, Wu et al., 2022] designedother more sophisticated strategies, but these are largely domain dependent. For reasons that will beexplained later, we stick with Definition 3 in this work. For regression, a popular de-randomizationstrategy is to compute the expected prediction: h (x) = E [h(x)].E.2 ProofsE.2.1 Proposition 2Proof. supas required. (cid:98)E I[h (x) \u0338= y] = sup= I[ supI[y \u0338= arg max E [h(x ) = y]]E E I[h(x) \u0338= y] \u2265 1/2]= I[ (cid:98)R (h ) \u2265 1/2]21E.2.2 Proposition 3Proof. Denote y (x) = arg max E (h(x) = y). Then,(cid:98)R (h ) = sup E \u2113(y (x), y)\u2264 sup E \u2113(y (x), y)\u2264 \u03b3 sup E (cid:88) P (h(x) = y (x))1/\u03b3\u2113(y , y)P (h(x) = y )E E (cid:88) \u2113(y , y)I[h(x) = y ]= \u03b3 sup= \u03b3 sup= \u03b3 (cid:98)R (h ),E E \u2113(h(x), y)as required.F AlgorithmsF.1 Convergence RatesF.1.1 GameplayWe begin with the following lemma adapted from [McMahan, 2017] (Theorem-1)Lemma 17 Consider the setting of Algorithm 1, and further assume that \u03b7 \u2265 \u03b7 > 0, Reg(w) \u2265 0,\u03b7 Reg(w) is 1-strongly concave w.r.t. some norm \u2225.\u2225 . Then for any w \u2208 W and any T > 0, wehave: Regret \u2264 \u03b7 Reg(w ) + 12Consider \u03b7 = \u03b7 and \u2225.\u2225 = \u221a\u03b7\u2225.\u2225 , then (cid:88) \u2225l \u2225 (where l = l(h (x ), y ))Regret \u2264 \u03b7Reg(w ) + 12\u03b7 (cid:88) \u2225l \u2225 \u2264 \u03b7D + T2\u03b7Moreover, as H-player plays BR, Regret \u2264 0Using Proposition 15, we achieve \u03f5-approximate NE with:Regret + RegretT\u03f5 = \u03f5 \u2264 = \u03b7DT + O (cid:19)(cid:18) 1\u03b7Using definition in Equation 7 gives us the required result.F.1.2 GreedyFW Update Note that we are trying to minimize the objective L (Q) w.r.t Q by the FW update.Using properties of Fenchel conjugates, it is well known that L (Q) is smooth w.r.t. \u2225.\u2225 . Also,the diameter of the simplex \u2206 w.r.t. \u2225.\u2225 is \u2264 1. By [Jaggi, 2013] (Lemma 7), we have C \u2264 ,and thus by [Jaggi, 2013] Theorem 1, we have:L (Q ) \u2212 min L (Q) \u2264 2\u03b7(T + 2) = O (cid:19)(cid:18) 1\u03b7TUsing the definition of L (Q),L(Q ) \u2212 min L(Q) \u2264 \u03b7D + O (cid:19)(cid:18) 1\u03b7T22Gen-AdaBoost Update This update boils down to a standard coordinate descent update on convexand smooth L (Q) (w.r.t \u2225.\u2225 ). Following along the lines of analysis in [Boyd and Vandenberghe,2004] (Section 9.4.3), L (Q ) \u2264 L (Q ) \u2212 \u2225\u2207 L (Q )\u2225\u03b72Using this decent equation, we can follow standard gradient descent analysis to get:L (Q ) \u2212 min L (Q) \u2264 O (cid:19)(cid:18) 1\u03b7TThe rest of the argument will go through as above.F.2 Closed Form Updates for different uncertainty setsIn this section, we derive closed-form updates for Equation 2 for the entropic regularizer (alsomentioned below). We consider common settings mentioned in Table 1.w \u2190 argmax (cid:88) E \u2113(h (x), y) \u2212 \u03b7 (cid:88) w log(w)\u2022 W = { (cid:98)P } (Empirical Risk Minimization)w \u2190 (cid:98)P\u2022 W = \u2206 (Worst Case Margin)w \u2190 u\u2225u \u2225 (cid:32)where u \u2190 exp \u2212 (cid:80) l(h (x ), y )\u03b7 (cid:33)\u2022 W = {w : w \u2208 \u2206 , w \u2aaf } (\u03b1-CVaR)w \u2190 min (cid:32) 1\u03b1n (cid:32), exp \u2212 (cid:80) l(h (x ), y )\u03b7 (cid:33)(cid:33)\u2212 \u03bb for \u03bb S.T. w = 1(cid:88)Algorithm 3 describes a projection procedure to find such \u03bb in O(n log n) time.Algorithm 3 Projection for \u03b1-CVaR set(cid:17)\u2264 v \u2200i thenInput: l, \u03b7, \u03b1\u2212(cid:16)w \u2190return wy \u2190 expv \u2190ifelsey \u2190 sort{y }function C (m)S.T. y \u2265 y \u2200 i \u2264 jv1 + c y 1c \u2190S \u2190 (cid:80)return Send functionLet m \u2190 binary search for the smallest m such that C (m) \u2264 1c \u2190w \u2190 min(c y , v)return wend if 23\u2022 W = {w : D(w|| (cid:98)P ) \u2264 \u03c1 } (DRO) For general f -divergences, there do not exist closed formupdates for w . However, they can still be empirically solved using FW-like updates.\u2022 W = { (cid:98)P (G ), (cid:98)P (G ), . . . (cid:98)P (G )} (Group DRO)w \u2190 u\u2225u \u2225 (cid:32)where u \u2190 exp \u2212 (cid:80)F.3 Proof of Corollary 8 (cid:80) l(h (x ), y )\u03b7 s (cid:33) for i \u2208 G , s = |G |Proof. Note that Reg is 1-strongly concave w.r.t \u2225.\u2225 and \u2225.\u2225 . A conservative upper bound forD \u2264 log(n) for all W (\u2286 \u2206 ). Thus, we can thus take appropriate values of \u03b7 to get:L(Q ) \u2264 min L(Q) + O (cid:32)(cid:114) (cid:33)log(n)TThus, we have \u03f5 \u223c O (cid:18)(cid:113) (cid:19) from Proposition 15.G GeneralizationG.1 Population RiskWe first present a proposition which gives an equivalent characterization of the RAI population riskProposition 18 The following are equivalent characterizations of the population RAI risk1.2. R (h) = sup E [\u2113(h(x), y)].R (h) = sup E [r(z)\u2113 (z)].Proof. The equivalence between (1) and (2) follows by reparameterizing P in (1) as followsfor some r(z) \u2265 0 dP (z) = r(z)dP (z),Now suppose the uncertainty set W is as defined in Equation (5). The next proposition uses dualityto derive an equivalent characterization of the RAI risk in this setting.Proposition 19 Suppose the uncertainty set W is as specified in Equation (5). Then for any hypothesish, the population RAI risk can be equivalently written asR (h) = inf E G (\u2113 (z) \u2212 \u03c4 ) + (cid:88) \u03bb c + \u03c4, (8)where G is the Fenchel conjugate of G (t) = (cid:80) \u03bb g (t).Proof. We rely on duality to prove the proposition. First observe that the population RAI risk can berewritten asR (h) = sup E [r(z)\u2113 (z)]= sup inf E [r(z)\u2113 (z)] + \u03c4 (cid:90)(cid:18)1 \u2212 (cid:19)r(z)dP (z) + (cid:18)\u03bb c \u2212 (cid:90)(cid:88) g (r(z))dP (z)(cid:19)24Since the above objective is concave in r and linear in \u03bb, \u03c4 , we rely on Lagrangian duality to rewriteit as R (h) = inf sup L(r, \u03bb, \u03c4 ),where L(r, \u03bb, \u03c4 ) is defined as: (cid:34)L(r, \u03bb, \u03c4 ) = E r(z)(\u2113 (z) \u2212 \u03c4 ) \u2212 (cid:35)\u03bb g (r(z)) +(cid:88) (cid:88) \u03bb c + \u03c4.Recall the interchangeability theorem:(cid:90)inf F (r(z), z)p(z)dz = (cid:90) (cid:18)inf (cid:19)F (t, z) p(z)dz,so long as the space H is decomposable. Since in our case we are working with the set L (Z, P ) ={r : Z (cid:55)\u2192 R : (cid:82) r(z)p(z)dz = 1}, which is decomposable, we can apply the interchangeabilitytheorem to get: (cid:34)sup E r(z)(\u2113 (z) \u2212 \u03c4 ) \u2212 (cid:88) \u03bb g (r(z))(cid:35) (cid:34)= E sup= E G (\u2113 (z) \u2212 \u03c4 ),t(\u2113 (z) \u2212 \u03c4 ) \u2212 (cid:35)\u03bb g (t)(cid:88)where G (t) = (cid:80) \u03bb g (t), and G is its Fenchel conjugate. so that:R(\u2113 ) = inf (cid:88) \u03bb c + \u03c4 + E G (\u2113 (z) \u2212 \u03c4 ).We have the following properties of the Fenchel conjugate Gof Fenchel conjugates described in Rockafellar [1970].Lemma 20 Consider the setting of Proposition 19. The Fenchel conjugate Gand an increasing function that satisfies (t). These follow from the propertiesis convex, differentiabledG (x)dx \u2265 0, \u2200x \u2208 R.Proof of Proposition 9 For the sake of clarity, we first state Proposition 9 below.Proposition 21 (Bayes optimal classifier) Consider the problem of binary classification whereY = {\u22121, +1}. Suppose \u2113(h(x), y) = \u03d5(yh(x)) for some \u03d5 : R \u2192 [0, \u221e) which is either the 0/1loss, or a convex loss function that is differentiable at 0 with \u03d5 (0) < 0. Suppose the uncertaintyset W is as specified in Equation (5). Moreover, suppose {g } are convex and differentiablefunctions. Then, the vanilla Bayes optimal classifier is also a RAI-Bayes optimal classifier.Proof. Following Proposition 19 it is easy to see that the RAI Bayes optimal classifier is the minimizerof the following probleminf inf (cid:88) \u03bb c + \u03c4 + E G (\u03d5(yh(x)) \u2212 \u03c4 ),where the minimization over h is over the set of all classifiers. For any fixed (\u03bb, \u03c4 ), we now show thatthe classifier h that minimizes the above optimization problem is a vanilla Bayes optimal classifier.First note that the above optimization problem, for a fixed (\u03bb, \u03c4 ), can be rewritten asE G (\u03d5(yh(x)) \u2212 \u03c4 ).infUsing the interchangeability theorem, we can further rewrite this asE (cid:20) inf E [G (\u03d5(uy) \u2212 \u03c4 )] (cid:21)(cid:12)(cid:12)(cid:12)x .25Here, Ptioned on x.is the marginal distribution of P over x, and P (\u00b7|x) is the distribution of y condi-Now suppose \u03d5 is the 0/1 loss if x > 0otherwiseis an increasing function (see Lemma 20). So G\u03d5(x) = .(cid:26)0,1,Recall, Geasy to see that for any x, the following is a minimizer of inf E [G(\u2212\u03c4 ) \u2264 G (1 \u2212 \u03c4 ). Using this, it is(\u03d5(uy) \u2212 \u03c4 )]u = (cid:26)1,\u22121, if P (y = 1|x) \u2265otherwise .This shows that the vanilla Bayes optimal classifier is a minimizer of the population RAI risk.Now suppose \u03d5 : R \u2192 [0, \u221e) is convex, differentiable at 0 with \u03d5 (0) < 0. Moreover, supposeh : X \u2192 R is a real valued classifier. In this case, the RAI Bayes optimal classifier is a minimizer ofthe following objective (cid:20)E inf E [G (\u03d5(uy) \u2212 \u03c4 )] (cid:21)(cid:12)(cid:12)(cid:12)x .(\u03d5(x) \u2212 \u03c4 ). It is easy to see that \u03b9(x) is convex. This is because \u03b9 (x) = (G ) (\u03d5(x) \u2212Let \u03b9(x) = G\u03c4 )\u03d5 (x) is an increasing function; this follows from the fact that G is convex with non-negativegradients. Moreover, \u03b9 (0) = (G ) (\u03d5(0) \u2212 \u03c4 )\u03d5 (0) \u2264 0. Then, Bartlett et al. [2006], Tewari andBartlett [2007] show that for any x, the following u is a minimizer of the inner optimizatin problem:u > 0 if P (y = 1|x) \u2265 , u < 0 otherwise. This shows that vanilla Bayes optimal classifier isminimizer of the population RAI risk.G.2 Generalization GuaranteesG.2.1 Proof of Proposition 11Proposition 11 directly follows from Proposition 19.G.2.2 Proof of Theorem 12We first present a key concentration result we use in the proof.Lemma 22 (Hoeffding bound [Wainwright, 2019]) Suppose that the random variables {X }are independent with mean \u00b5 , and bounded between [a, b]. Then for any t \u2265 0, we have(cid:32)|P (cid:88) (cid:33) (cid:18)X \u2212 \u00b5 | \u2265 t \u2264 2 exp \u2212 2tn(b \u2212 a) (cid:19) .We now proceed to the proof of the Theorem. Following Proposition 11, we know that the populationand empirical RAI risk of a classifier h can be written asR (h) = inf(cid:98)R (h) = inf (cid:88)(cid:88) \u03bb c + \u03c4 + E G (\u2113 (z) \u2212 \u03c4 )\u03bb c + \u03c4 + E G (\u2113 (z) \u2212 \u03c4 )Our goal here is to bound the following quantity for any given h:|R (h) \u2212 (cid:98)R (h)| \u2264 sup (cid:12)(cid:12)(cid:12)E G (\u2113 (z) \u2212 \u03c4 ) \u2212 E G (cid:12)(cid:12)(cid:12).(\u2113 (z) \u2212 \u03c4 )The rest of the proof focuses on bounding the RHS of the above equation. The overall idea is to firstprovide a high probability bound of the RHS for any given \u03bb, \u03c4 . Next, we take a union bound over allfeasible (\u03bb, \u03c4 )\u2019s by constructing an appropriate \u03f5-net.26Fixed \u03bb, \u03c4 . Observe that G (\u2113 (z) \u2212 \u03c4 ) is bounded and satisfies(\u2212\u03c4 ) \u2264 GG (\u2113 (z) \u2212 \u03c4 ) \u2264 G (B \u2212 \u03c4 ).This follows from the fact that G is an increasing function (see Proposition 20), and \u2113 is boundedbetween 0 and B. From Heoffding bound we know that for any fixed h \u2208 H, the following holdswith probability at least 1 \u2212 2e(cid:12)(cid:12)(cid:12)E G (\u2113 (z) \u2212 \u03c4 ) \u2212 E G (cid:12)(cid:12)(cid:12) \u2264 G(\u2113 (z) \u2212 \u03c4 ) (cid:114) tn .Union bound over \u03bb, \u03c4 . Define set E asE := {(\u03bb, \u03c4 ) : \u03bb \u2265 0, min \u03bb \u2265 \u039b} \u2229 E. (9)Let N (E , \u03f5, \u2225 \u00b7 \u2225 ) be the \u03f5-net over E in \u2225 \u00b7 \u2225 norm. It is well known that there exists such a setwhose size is upper bounded by [Wainwright, 2019]|N (E , \u03f5, \u2225 \u00b7 \u2225 )| \u2264 O (cid:19)(cid:18) \u00af\u039b + T\u03f5. For any (\u03bb, \u03c4 ) \u2208 E , let (\u03bb , \u03c4 ) be an element in N (E , \u03f5, \u2225 \u00b7 \u2225 ) that is \u03f5-close to (\u03bb, \u03c4 ). Nowconsider the following (cid:12)(cid:12)(cid:12)sup E G (\u2113 (z) \u2212 \u03c4 ) \u2212 E G (cid:12)(cid:12)(\u2113 (z) \u2212 \u03c4 )(cid:12)\u2264 sup (cid:12)(cid:12)(cid:12)E G (\u2113 (z) \u2212 \u03c4 ) \u2212 E G (cid:12)(cid:12)(\u2113 (z) \u2212 \u03c4 )(cid:12)+ sup+ sup (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E G (\u2113 (z) \u2212 \u03c4 ) \u2212 E G (cid:12)(cid:12)(\u2113 (z) \u2212 \u03c4 )(cid:12)E G (\u2113 (z) \u2212 \u03c4 ) \u2212 E G (cid:12)(cid:12)(\u2113 (z) \u2212 \u03c4 )(cid:12)Since Ging this in the above equation, we getis L-Lipschitz, the last two terms in the RHS above can be upper bounded by L\u03f5. Substitut-(cid:12)E G(cid:12)(cid:12)sup (\u2113 (z) \u2212 \u03c4 ) \u2212 E G (\u2113 (z) \u2212 \u03c4 )(cid:12)(cid:12)(cid:12)E G (\u2113 (z) \u2212 \u03c4 ) \u2212 E G (cid:12)(cid:12)(cid:12) + 2L\u03f5(\u2113 (z) \u2212 \u03c4 )(cid:12)(cid:12)(cid:12)\u2264 sup\u2264 G(cid:114) tn + 2L\u03f5,where (a) follows from Equation (9), and holds with probability at least 1 \u2212Choosing \u03f5 = (cid:113) , we get the desired result.G.3 Proof of Corollary 13 (cid:17)(cid:16) e .The proof follows from a standard covering number argument. For any h \u2208 H, let h be the point inthe \u03f5-net that is closest to h. Then we have|R (h) \u2212 (cid:98)R (h)| \u2264sup sup |R (h) \u2212 (cid:98)R (h)|+ sup |R (h) \u2212 R (h )| + sup | (cid:98)R (h) \u2212 (cid:98)R (h )|Observe that the last two terms above are bounded by \u03f5 . Also observe that the first term in the RHScan be upper bounded by 10n G((cid:112)t + m log(nL)) with probability at least 1 \u2212 2N (H, \u03f5 , \u2225 \u00b7\u2225 )e . Combining these two and substituting the value of \u03f5 gives us the required result.27H Algorithms: Further DiscussionH.1 Equivalence Conditions for RAI Algorithms (1 and 2)Proposition 23 Assume that we set \u03b1 =(FW) i.e. G = argminAlgorithm 1 with \u03b7 = \u03b7t. and perform coordinate descent update in Algorithm 2(cid:10)h, \u2207 L (Q )(cid:11), then the update is equivalent to the update given byProof. From Equation 3, and using the fact that L (Q) can be written as a fenchel conjugate, weknow that \u2207 L (Q ) = argmax E E \u2113(h(x), y) + \u03b7Reg(w)= argmax (cid:88) E \u2113(h (x), y) + \u03b7tReg(w) (cid:18)as \u03b1 = (cid:19)1tThis matches Equation 2 with \u03b7 = \u03b7t. Moreover,G = argmin E E l(h(x), y) where w = \u2207 L (Q )This corresponds to the update for h in Algorithm 1. Thus, we have our equivalence.H.2 Weak Learning ConditionsFor the well-known scenario of binary classification and zero-one loss, we recover the quasi-AdaBoostweak learning condition:Proposition 24 Consider the scenario of binary classification and l as the zero-one loss. If theH-player only plays an approximate best response strategy i.e. h satisfies E \u2113(h (x), y) \u2264 1/2 \u2212 \u03b3for some \u03b3 > 0, then (cid:98)R (h ) = 0 for T > T for some large enough T .Proof. Since the D-player uses regret optimal strategy, we have that:1T (cid:88) E \u2113(h (x), y) \u2265 max 1T (cid:88) E \u2113(h (x), y) \u2212 \u03f5 ,while from the approximate-BR condition we have that:so that we have: 1T (cid:88) E \u2113(h (x), y) \u2264 1/2 \u2212 \u03b3,max 1T (cid:88) E \u2113(h (x), y) \u2264 1/2 \u2212 \u03b3 + \u03f5 ,so that for T > T large enough so that \u03f5 < \u03b3/2, we have that:max 1T (cid:88) E \u2113(h (x), y) < 1/2 \u2212 \u03b3/2.As Q assigns mass 1/T to each of {h } , we have:(cid:98)R (h ) = max E E \u2113(h (x), y) < 1/2 \u2212 \u03b3/2=\u21d2 (cid:98)R (h ) = 0 (from Proposition 2)Hence, we see that h incurs zero error.For the general setting, we have a slightly stronger weak learning condition, which follows from theanalysis of Frank-Wolfe update [Freund and Grigas, 2014, Jaggi, 2013].28Proposition 25 Consider Algorithm 2 with the FW update and Reg(.) as a 1-strongly concave(cid:10)Q, \u2207 L (Q )(cid:11) + \u03b4 , where {\u03b4 } is theIf G satisfies G \u2264 minregularize w.r.t. \u2225.\u2225 .sequence of approximation errors with \u03b4 \u2265 0, then:1. If \u03b4 \u22642. If \u03b4 \u2264 i.e. decaying errors, then L (Q ) \u2264 min L (Q) + +i.e. constant errors, then L (Q ) \u2264 min L (Q) +Proof. Note that we are trying to minimize the objective L (Q) w.r.t Q by the FW update. Usingproperties of Fenchel conjugates, it is well known that L (Q) is smooth w.r.t. \u2225.\u2225 . Also, thediameter of the simplex \u2206 w.r.t. \u2225.\u2225 is \u2264 1.1. By [Jaggi, 2013] (Lemma 7, Theorem 1), we have C \u2264 , and thus, we have:L (Q ) \u2212 min L (Q) \u2264 2(1 + \u03f5)\u03b7(T + 2)2. By [Freund and Grigas, 2014] (Theorem 5.1), in case of approximation errors, the FW/optimalitygap converges as before along with a convex combination of errors at each time step i.e. if weare able to solve the linear optimization problem within constant error , then these errors do notaccumulate. Moreover, the convex combination can be bound by the maximum error possible andwe get, L (Q ) \u2212 min L (Q) \u2264 2\u03b7(T + 2) + \u03f5\u03b7I ExperimentsRAI games constitute an optimization paradigm that goes beyond traditional approaches such asdistributionally robust optimization, fairness, and worst-case performance. We have seen that forspecific uncertainty sets W , RAI Games optimize over well-established robust optimization objectives.As such, the purpose of our experiments is to demonstrate the practicality and generality of ourproposed strategies, rather than establishing state-of-the-art over baselines. Given a large numberof possible W , we do not attempt an exhaustive empirical analysis. Instead, we underscore theplug-and-play nature of RAI Games.I.1 SetupSubpopulation Shift A prevalent scenario in machine learning involves subpopulation shift, neces-sitating a model that performs effectively on the data distribution of each subpopulation (or domain).We explore the following variations of this setting:\u2022 Domain Oblivious (DO). Recent work [Hashimoto et al., 2018], [Lahoti et al., 2020], [Zhaiet al., 2021a] studies the domain-oblivious setting, where the training algorithm lacks knowledgeof the domain definition. In this case, approaches like \u03b1-CVaR and \u03c7 -DRO aim to maximizeperformance over a general notion of the worst-off subpopulation.\u2022 Domain Aware (DA). Several prior works [Sagawa et al., 2019] have investigated the domain-awaresetting, in which all domain definitions and memberships are known during training.\u2022 Partially Domain-Aware (PDA). More realistically, in real-world applications, there usuallyexist multiple domain definitions. Moreover, some of these domain definitions may be knownduring training, while others remain unknown. The model must then perform well on instancesfrom all domains, regardless of whether their definition is known. This setting is challengingas it necessitates the model to learn both domain-invariant and domain-specific features and togeneralize well to new instances from unknown domains.29I.2 Further Details for Section 7Base Learners We use linear classifiers, WRN-28-1, and WRN-28-5 [Zagoruyko and Komodakis,2016] as base classifiers for COMPAS, CIFAR-10 and CIFAR-100 respectively. To get a sense ofperformance improvements, we also benchmark performance with larger models, namely a three-hidden-layer neural network for COMPAS and WRN-34-10 for CIFAR-10/100.Proposed Methods This paper introduces two categories of algorithms. We elect not to presentresults for Algorithm 1, which we notice has similar performance to Algorithm 2. Conversely,we provide in-depth experimental analyses for both updates of Algorithm 2, which warrant somespecial attention due to due to their relation to AdaBoost. In this section, we refer to the FW andGen-AdaBoost updates as RAI-FW and RAI-GA, respectively. Our implemented versions incorporatea few alterations: 1. We track the un-regularized objective value from Equation 1 for the validation set.If it increases at any round t, we increase the regularization factor \u03b7 by a fixed multiple (specifically,2). We notice that it leads to better generalization performance over the test set. 2. The sameun-regularized objective w.r.t normalized Q is also used to perform a line search for the step size\u03b1. For the FW update, our search space is a ball around at round t, while for the GA update, wesearch within the range (0, 1).Training. We use SGD with momentum = 0.9 for optimization. We first warm up the modelwith some predefined epochs of ERM (3 for COMPAS and 20 for CIFAR-10/100), followed by amaximum of T = 5 base models trained from the warm-up model with sample weights provided byour algorithms. Each base model is trained for 500 iterations on COMPAS and 2000 iterations onCIFAR-10/100. Each experiment is run three times with different random seeds. For evaluation, wereport the averaged expected and worst-case test loss from Equation 1.Datasets We conduct our experiments on three real-world datasets:\u2022 COMPAS [Angwin et al., 2016] pertains to recidivism prediction, with the target being whether anindividual will re-offend within two years. This dataset is extensively used in fairness research. Werandomly sample 70% of the instances for the training data (with a fixed random seed), and theremainder is used for validation/testing.\u2022 CIFAR-10 and CIFAR-100 are widely used image datasets. For CIFAR-10, we consider twosettings: the original set and an imbalanced split [Jin et al., 2021, Qi et al., 2021]. In the imbalancedsplit, we make worst-case performance more challenging by randomly sampling each categoryat different ratios. To be precise, we sample the ith class with a sampling ratio \u03c1 where \u03c1 ={0.804, 0.543, 0.997, 0.593, 0.390, 0.285, 0.959, 0.806, 0.967, 0.660}. For these datasets, we usethe standard training and testing splits, reserving 10% of the training samples as validation data.Hyperparameters For COMPAS, we warm up for 3 epochs and then train every base classifier for500 iterations. For CIFAR-10 and CIFAR-100, we warm up the models for 20 epochs and train baseclassifiers for 2000 iterations. The mini-batch size is set to 128. It should be noted that the primaryaim of our experiments is not hyperparameter tuning. The experiments in this paper are designed todemonstrate use cases and compare different algorithms. Hence, while we maintain consistency ofhyperparameters across all experiments, we do not extensively tune them for optimal performance.I.3 Interesting Observation: Boosting Robust LearnersTable 4:Boosting Robust Base Learners We conclude our results with one interesting observation. Untilnow, we have been comparing our ensembles with deterministic models. As such, we acknowledge30that given the inherent differences between the two, making a fair comparison is challenging. However,we find that our setup can \"boost\" not only ERM but also other robust base learners i.e. if we usethese robust optimization methods to find our base learners under analogous RAI constraints, we areable to further enhance the robust performance of these algorithms. The results are shown in Table 4.We hypothesize that individually robust base learners are able to help the ensemble generalize well,allowing our approach to further optimize through ensembles.I.4 Synthetic DatasetsIn this section, we use synthetic datasets to illustrate how our RAI algorithms converge, and howdifferent constraints on W translate into performance across various responsible metrics. We use thefollowing distributions to construct the datasets, and use class labels as the group labels.\u2022 Dataset-I: P (X|Y = 0) = N ((0, 0), I), P (X|Y = 1) = N ((\u22123, 1), I)) + N ((3, 0), I)) +N ((0, \u22123), I)), P (Y = 0) = 0.7, P (Y = 1) = 0.3.N ((\u22122, \u22122), 0.5I) +\u2022 Dataset-II: P (X|Y = 0) =P (X|Y = 1) = N ((\u22123, 0), 0.3I)) + N ((\u22122, \u22122), 0.5I) +N ((3, 0), 0.3I)), P (Y = 0) = 0.7, P (Y = 1) = 0.3N ((2, 2), 0.5I),We sample 1000 points each for both training and testing from both distributions. Note that thesedatasets deliberately exhibit: 1. Class imbalance (particularly in Dataset-I) 2. Multiple minoritysub-populations (within and between classes) 3. Varying noise levels in the sub-populations (predom-inantly in Dataset-II). Such characteristics are frequently encountered in real-world scenarios anddemand responsible classifiers.Models For base learners, we use linear classifiers for Dataset-I and neural network classifiers witha single hidden layer of size 4 and ReLU activations for Dataset-II. We find that base learners can bemodels with varying complexity.Hyperparameters Due to the limited size of the datasets, we forgo the warm-up stage. At everyround, we run 1000 iterations with a mini-batch size of 32. We run \u03b1-LPBoost with the default value\u03b7 = 1. For \u03b1-CVaR experiments, we take \u03b1 = 0.7 across all experiments. For lower values of \u03b1,we observe similar results and comparisons, albeit with a substantial reduction in average metrics.Consequently, we opt for conservative values to standardize average performance across all modelsand subsequently compare worst-case performance in responsible settings.I.4.1 Results and Discussion\u2022 Domain Oblivious (DO) To begin, we run ERM, AdaBoost, \u03b1-LPBoost, and RAI games onDataset-I. For RAI-GA and RAI-FW games, we use \u03b1-CVaR uncertainty set as W . Given the classimbalance, Y = 0 and Y = 1 represent good candidates for subpopulations of interest. The resultsare reported in Figure 1. We immediately observe the following:\u2022 Both proposed methods RAI-FW and RAI-GA effectively decrease the objective value and achievelower worst-class classification loss, as compared to both ERM and AdaBoost.\u2022 They closely follow the \u03b1\u2212LPBoost iterates. Intuitively, our quasi-boosting updates resemble\u03b1\u2212LPBoost for the CVaR objective, and that is reflected in similar objective values.\u2022 Domain Aware (DA) For this setting, we run ERM, AdaBoost, Online GDRO, RAI-GA, andRAI-FW on Dataset-II. We use Group DRO over the five gaussian groups as the uncertainty set W .Although Dataset-II was selected due to the presence of more pronounced subpopulation behavior,we get similar results for Dataset-I as well. The results are reported in Figure 2 and Table 5.\u2022 Partially Domain-Aware (PDA) For this setting, we run our algorithms for Dataset-II. Similar togaussian memberships, the class labels Y provide another secondary definition of implicit groupingin the dataset. We report the results in Table 5. A critical observation from the DA setting resultsis that Online GDRO, and RAI (Group) all exhibit inferior performance according to the secondaryclass definition i.e. although they optimize for the known groups (gaussian), they fail to optimize forunknown groups (class labels). Thus, a natural solution is to run RAI updates over the intersection31Figure 1:Figure 2:of \u03c7 (for unknown groups) and Group (for known groups) constraints. As seen in the Table, wesee that both RAI-GA and RAI-FW achieve a middle ground by significantly improving worst-caseperformance for both known and unknown groups.Table 5: 3233",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "14d488e3-858a-44a7-9dcc-fd9da2865966",
                    "text": "",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "ab1fa9f8-b374-4fb7-9be0-00ff384d0626",
                    "text": "In this section, we study the population RAI risk and present generalization bounds which quantifythe rates at which empirical RAI risk converges to its population counterpart.",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "38747c92-6fb3-49c0-a500-b246c197efed",
                    "text": "Recall, the empirical RAI risk optimizes over all sample re-weightings w \u2208 W that lie within theprobability simplex \u2206 . Thus it\u2019s population counterpart optimizes over distributions P that areabsolutely continuous with respect to the data distribution P :R (h) = E [\u2113(h(x), y)].sup7Following [Shapiro et al., 2021], we can rewrite this as follows. Suppose we use Z = (X, Y ) \u2208 Z :=X \u00d7 Y, so that P, P are distributions over Z. We then define \u2113 : Z (cid:55)\u2192 R as \u2113 (z) = \u2113(h(x), y).We can then write the population RAI risk as (see Appendix G for a proof):R (h) = sup E [r(z)\u2113 (z)]. (4)For classification, we define the RAI-Bayes optimal classifier as: Q = arg min R (Q). Here,the minimum is w.r.t the set of all measurable classifiers (both deterministic and random). This is the\u201ctarget\u201d classifier we wish to learn given finite samples. Note that this might not be the same as thevanilla Bayes optimal classifier: Q = arg min E[ (cid:98)R(Q)], which only minimizes the expected loss,and hence may not satisfactorily address RAI considerations.We now try to characterize the RAI-Bayes optimal classifier. However, doing this requires a bit morestructure on W . So, in the sequel, we consider constraint sets of the following form:(cid:26)W = r : Z (cid:55)\u2192 R : (cid:90) g (r(z))dP (z) \u2264 c , i \u2208 [m] , (5)(cid:27)where we assume that g : R (cid:55)\u2192 R, i \u2208 [m] are convex. Note that this choice of W encompasses abroad range of RAI games including DRO with f -divergence, CVaR, soft-margin uncertainty sets.Perhaps surprisingly, the following proposition shows that the minimizer of population RAI risk isnothing but the vanilla Bayes optimal classifier.Proposition 9 (Bayes optimal classifier) Consider the problem of binary classification whereY = {\u22121, +1}. Suppose \u2113(h(x), y) = \u03d5(yh(x)) for some \u03d5 : R \u2192 [0, \u221e) which is either the 0/1loss, or a convex loss function that is differentiable at 0 with \u03d5 (0) < 0. Suppose the uncertaintyset W is as specified in Equation (5). Moreover, suppose {g } are convex and differentiablefunctions. Then, the vanilla Bayes optimal classifier is also a RAI-Bayes optimal classifier.Remark 10 In the special case of m = 1 in Equation (5), we recover the result of [Hu et al., 2018].However, our proof is much more elegant than the proof of [Hu et al., 2018], and relies on the dualrepresentation of the population RAI risk.One perspective of the above result is that the vanilla Bayes optimal classifier is also \u201cresponsible\u201das specified by the RAI game. This is actually reasonable in many practical prediction problemswhere the label annotations are actually derived from humans, who presumably are also responsible.Why then might we be interested in the RAI risk? One advantage of the RAI risks is in finite samplesettings where the equivalence no longer holds, and the RAI risk could be construed as encoding priorknowledge about properties of the Bayes optimal classifier. We also note that the above equivalenceis specific for binary classification.",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "6543e3ad-db44-48a9-8b42-fa2a7021fde2",
                    "text": "Our generalization bounds rely on the following dual characterization of the RAI population risk.Proposition 11 Suppose the uncertainty set W is as specified in Equation (5). Then for any hypothesish, the population RAI risk can be equivalently written asR (h) = inf E G (\u2113 (z) \u2212 \u03c4 ) + (cid:88) \u03bb c + \u03c4, (6)where G is the Fenchel conjugate of G (t) = (cid:80) \u03bb g (t).We utilize the above expression for R (h) to derive the following deviation bound for (cid:98)R (h).Theorem 12 Consider the setting of Proposition 11. Suppose {g } are convex and differen-tiable functions. Suppose \u2113 (z) \u2208 [0, B] for all h \u2208 H, z \u2208 Z. Suppose, for any distribution P , theminimizers (\u03bb , \u03c4 ) of Equation (6) lie in the following set: E = {(\u03bb, \u03c4 ) : \u2225\u03bb \u2225 \u2264 \u00af\u039b, |\u03c4 | \u2264 T }.Moreover, let\u2019s suppose the optimal \u03bb for P is bounded away from 0 and satisfies min \u03bb .\u2265 \u039bLet G, L, be the range and Lipschitz constants of G :G := sup G (B \u2212 \u03c4 ) \u2212 G (\u2212\u03c4 ), L := sup (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) \u2202G (x \u2212 \u03c4 )\u2202(\u03bb, \u03c4 ) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .8For any fixed h \u2208 H, with probability at least 1 \u2212 2e|R (h) \u2212 (cid:98)R (h)| \u2264 10n G((cid:112)t + m log(nL)).Given Theorem 12, one can take a union bound over the hypothesis class H to derive the followinguniform convergence bounds.Corollary 13 Let N (H, \u03f5, \u2225 \u00b7 \u2225 ) be the covering number of H in the sup-norm which isdefined as \u2225h\u2225 = sup |h(z)|. Then with probability at least 1 \u2212 N (H, \u03f5 , \u2225 \u00b7 \u2225 )e ,the following holds for any h \u2208 H: |R (h) \u2212 (cid:98)R (h)| \u2264 30n G((cid:112)t + m log(nL)). Here\u03f5 = n G(cid:112)t + m log(nL).The above bound depends on parameters (\u03bb , \u03c4 , G, L) which specific to the constraint set W . Toinstantiate it for any W one needs to bound these parameters. We note that our generalization\u2192 0. This is because the Lipschitz constant L could potentiallyguarantees become sub-optimal as \u039bget larger as \u03bb approaches the boundary. Improving these bounds is an interesting future direction.Remark 14 We note that aforementioned results results follow from relatively stringent assumptions.Exploring the impact of relaxing these assumptions is an interesting direction for future works.",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "3b8f9259-0ec7-483c-a40f-69367d37d0d3",
                    "text": "In this section, we demonstrate the generality of proposed RAI methods by studying one of the mostwell-studied problems in RAI i.e. the case of subpopulation shift. Given a large number of possibleW , we acknowledge that this is not a complete analysis, even with respect to the problems that livewithin the minimax framework. Instead, we aim to display convergence, plug-and-play generality,and superior performance over some seminal baselines of this task. We conduct experiments on bothsynthetic and real-world datasets. Please refer to Appendix for details on synthetic experiments. Weconsider a number of responsible AI settings, including subpopulation shift, in the domain-oblivious(DO) setting where we do not know the sub-populations [Hashimoto et al., 2018, Lahoti et al., 2020,Zhai et al., 2021a], the domain-aware (DA) setting where we do [Sagawa et al., 2019], and thepartially domain-aware (PDA) setting where only some might be known.Datasets & Domain Definition. We use the following datasets: COMPAS [Angwin et al., 2016],CIFAR-10 (original, and with a class imbalanced split [Jin et al., 2021, Qi et al., 2021]) and CIFAR-100. See the Appendix for more details on our datasets. For COMPAS, we consider race (White vsOther) and biological gender (Male vs Female) as our sensitive attributes. This forms four disjointsubgroups defined by these attributes. In the PDA setting, we partition only across the attribute racewhile training, but still run tests for all four subgroups. On CIFAR-10, class labels define our 10subpopulations. Similarly as above, for the PDA setting, we make 5 super-groups of two classeseach. On CIFAR-100, class labels define our 100 subpopulations. For the PDA setting, we make 20super-groups, each consisting of five classes.Baselines. We compare our method against the following baselines: (a) Deterministic classifierstrained on empirical risk (ERM) and DRO risks, particularly the quasi-online algorithm for GroupDRO [Sagawa et al., 2019] (Online GDRO), and an ITLM-inspired SGD algorithm [Zhai et al.,2021b, Shen and Sanghavi, 2018] for \u03c7 DRO (SGD (\u03c7 )) (b) Ensemble models AdaBoost [Schapire,1999]. Note that the purpose of our experiments is to show that we can match baselines for a specificsingle desideratum (e.g. worst-case sub-population) while allowing for learning models that can solvemultiple responsible AI desiderata at the same time, for which we have no existing baselines.Proposed Methods. We focus on Algorithm 2 and refer to FW and Gen-AdaBoost updates asRAI-FW and RAI-GA, respectively. Moreover, our implementations include the following alterations:\u2022 We track the unregularized objective value from Equation 1 for the validation set, and whenever itincreases we double the regularization factor \u03b7, which we find can improve generalization. \u2022 We alsouse this objective w.r.t the normalized Q to perform a line search for the step size \u03b1. For the FWupdate, our search space is a ball around at round t, while for GA, we search within (0, 1).Base Learners & Training. Training time scales linearly with the number of base learners. Inference,though, can be parallelized if need be. We usually find training on 3-5 learners is good enough on allscenarios explored in the paper. We defer further details of our base learners and hyperparameterchoices to the Appendix. 9Constraint sets W . For RAI algorithms, we use the following constraint sets: \u2022 Domain Oblivious(DO): We use the \u03c7 -DRO constraint set to control for worst-case subpopulations. \u2022 Domain Aware(DA): We use the Group DRO constraint set as the domain definitions are known. \u2022 PartiallyDomain-Aware (PDA): We use a novel set W which is the intersection over Group DRO constraintsover the known domains and \u03c7 constraints to control for unknown group performance. For baselines,we use AdaBoost and SGD(\u03c7 ) for the DO setting. Online GDRO serves as our baseline for both DAand PDA settings, where the algorithm uses whatever domain definitions are available.Table 2:Results and Discussion. We run our methods and baselines under the settings described above andreport the results in Table 2. As such, we can make the following observations:1. RAI-FW and RAI-GA methods significantly improve the worst-case performance with only a fewbase learners across all datasets in all three settings, while maintaining average case performance.Moreover, For seemingly harder tasks i.e. a large gap between average and worst-case performance,the algorithms are able to improve significantly over the baselines. For example, we observe a 5%improvement in performance in the case of CIFAR-100.2. The plug-and-play framework allows for several different W to enhance various responsible AIqualities at once. We demonstrate this with the partial domain aware setting (PDA), where theperformance lead widens, indicating that RAI is able to jointly optimize effectively for both knownand unknown subpopulations while Online GDRO suffers from some of the group informationbeing unknown. In practice, one can construct many more novel sets W .3. Although bigger (complex) models exhibit stronger performance than RAI ensembles, there areseveral caveats to this observation. Firstly, these models are \u223c10-15 times larger than our basemodels. This limits their use w.r.t both training & inference compute required. However, RAIensembles utilize a small number of much smaller models which can be individually trained quiteeasily. Even with these large models as base learners, constructing ensembles exhibits a perfor-mance boost, indicating that our framework is able to \u201cboost\u201d models of varying complexities.",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                },
                {
                    "id": "c69065d5-d093-4e80-b31a-7f4270437a6d",
                    "text": "Under the umbrella of \u201cresponsible AI\u201d, an emerging line of work has attempted to formalizedesiderata ranging over ethics, fairness, robustness, and safety, among others. Many of these settings(Table 1) can be written as min-max problems involving optimizing some worst-case loss under a setof predefined distributions. For all the problems that can be framed as above, we introduce and studya general framework, which we refer to as Responsible AI (RAI) games. Our framework extends toclassical boosting scenarios, offering boosting-based algorithms for RAI games alongside provenconvergence guarantees. We propose practical algorithms to solve these games, as well as statisticalanalyses of solutions of these games. We find that RAI can guarantee multiple responsible AI aspectsunder appropriate choices of uncertainty sets. 10AcknowledgementsWe acknowledge the support of DARPA via HR00112020006, and NSF via IIS-1909816.",
                    "reference": "[1] Yash Gupta, Ruishan Zhai, Alexander Suggala, and others. 2024. Responsible AI (RAI) Games and Ensembles. In Advances in Neural Information Processing Systems 2024. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2023/file/e6057bf047bcc5f86ebf4e8db6e24a1f-Paper-Conference.pdf"
                }
            ]
        },
        {
            "paper_title": "Progressing towards responsible AI",
            "authors": "T Scantamburlo, A Cort\u00e9s, M Schacht",
            "publication_info": "arXiv preprint arXiv:2008.07326 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2008.07326",
            "chunks": [
                {
                    "id": "f2264e72-d923-4cb1-b0b3-4ff8419548f1",
                    "text": ". The \ufb01eld of Arti\ufb01cial Intelligence (AI) and, in particular,the Machine Learning area, counts on a wide range of performancemetrics and benchmark data sets to assess the problem-solving effec-tiveness of its solutions. However, the appearance of research centres,projects or institutions addressing AI solutions from a multidisci-plinary and multi-stakeholder perspective suggests a new approach toassessment comprising ethical guidelines, reports or tools and frame-works to help both academia and business to move towards a re-sponsible conceptualisation of AI. They all highlight the relevanceof three key aspects: (i) enhancing cooperation among the differentstakeholders involved in the design, deployment and use of AI; (ii)promoting multidisciplinary dialogue, including different domains ofexpertise in this process; and (iii) fostering public engagement tomaximise a trusted relation with new technologies and practitioners.In this paper, we introduce the Observatory on Society and Arti\ufb01-cial Intelligence (OSAI), an initiative grew out of the project AI4EUaimed at stimulating re\ufb02ection on a broad spectrum of issues of AI(ethical, legal, social, economic and cultural). In particular, we de-scribe our work in progress around OSAI and suggest how this andsimilar initiatives can promote a wider appraisal of progress in AI.This will give us the opportunity to present our vision and our modusoperandi to enhance the implementation of these three fundamentaldimensions.",
                    "reference": "[1] Antonella Scantamburlo, Amanda Cort\u00e9s, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
                },
                {
                    "id": "34e79e88-7f53-4192-a455-6e0d1c0ae43e",
                    "text": "In science, nothing has been more controversial than the notion ofprogress. Debates on what progress is and if it does really existabound in the philosophy of science and are closely related to ques-tions about the goals and the methods of a scienti\ufb01c discipline [54].Although between the 1960s and 1970s philosophers of science putforward thought-provoking views, the idea of progress is commonlyassociated with the incremental acquisition of knowledge in a par-ticular domain. The same idea is still prevalent in toady\u2019s researchpractice, including Arti\ufb01cial Intelligence (AI). For instance, a studydiscussing the \ufb02aws in Google Flu Trends claimed that \u201cscience isa cumulative endeavour, and to stand on the shoulders of giants re-quires that scientists be able to continually assess work on which theyare building\u201d [47, p 1205].In this paper, we commit to the view that the progress of a scien-ti\ufb01c discipline can be measured by the problem-solving effective-ness of its theories [46]. This view applies to, not only science, but also to different intellectual endeavours, including areas where solu-tions consist of technical artefacts such as algorithms and computingsystems.Typically, the problem-solving effectiveness of AI solutions is amatter of performance testing. For example, in supervised learning,we assess algorithms based on the number of errors that they make onnew, unseen data. Performance lies at the very heart of any learningalgorithm, which is, by de\ufb01nition, a computer program that improvesthrough experience.Intuitively, a Machine Learning (ML) technique is progressive as itperforms well along with distinct criteria, including the task at hand,the benchmark data set and the computed performance measure. Al-though there is no hint of absolute progress in the \ufb01eld - in that noalgorithm has proved to be the best at any possible task or condition-, deep learning methods have nevertheless hit the mark in multipledomains ranging from diagnosing eye diseases [37] to playing game[58]. Other signs of progress regard the time of processing [55] whichrelies on the evolution of CPU capabilities.However, the introduction of AI algorithms into large portions ofhuman life has suggested that technical performance is not enough.The adequacy of AI solutions depends on a broader set of considera-tions accounting for the behaviour of AI systems within the environ-ment they are embedded. Repeated facts of algorithmic discrimina-tion and lack of transparency shifted the focus from performance toaccountability, from advances in accuracy and speed of computationto the protection of human rights and democratic values. In otherwords, the appraisal of AI progress is moving away from a purelytechnical assessment and becoming a multi-factorial affair which in-tegrates aspects of privacy, fairness and transparency, among others.The transition towards a broader notion of AI assessment is themain focus of the present work. As we will see, a new ethical turnprompted the rise of centres and networks addressing AI solutionsfrom a multidisciplinary and multi-stakeholder perspective.Our maim claim is that these initiatives have contributed to movingtowards a different notion of progress in AI that goes beyond techni-cal performance to foster responsible development and disseminationof AI, acting on three main elements: (i) promoting an interdisci-plinary dialogue around AI; (ii) involving diverse stakeholders and(iii) engaging the public. In particular, we would like to present theObservatory on Society and AI as an example of these multidisci-plinary and multi-stakeholder initiatives.The paper is structured as follows. In Section 2, we describe tra-ditional practices for the assessment of AI, focusing in particular onML, and the attempts to change them. Section 3 introduces the Ob-servatory for Society and AI as an example of an initiative which maycontribute to the ethical turn of AI. In Section 4, we survey some re-sponsible practices which will be part of the inventory of resources tobe explored by the Observatory. We will conclude in Section 5 withsome \ufb01nal remarks.",
                    "reference": "[1] Antonella Scantamburlo, Amanda Cort\u00e9s, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
                },
                {
                    "id": "8c8a3eb2-b670-4345-b47a-d343b43c93f4",
                    "text": "One of the keys to understanding the progress of a scienti\ufb01c disci-pline is to assess its ability to solve problems. Disciplines have de-veloped several methodologies to assess their own problem-solvingeffectiveness, but what it means for a problem to be solved can varyacross the \ufb01elds. For example, in logic, a problem can be viewed assolved when a theorem has been proved, in philosophy when a thesisis well-argued, in medicine when a treatment cures a disease, and soforth. Each discipline may develop different standards and processesfor accepting solutions. The ef\ufb01cacy of a philosophical argument isthus assessed differently from the ef\ufb01cacy of an experimental result.The \ufb01eld of ML has developed various practices to assess the ef-fectiveness of learning techniques. Most of them re\ufb02ect the ideal ofexperimentation in natural science, a model recommended particu-larly in the early days of ML research [45], then taken as a sign ofmaturity and objectivity of the \ufb01eld [35]. Usually, testing ML algo-rithms involve the use of benchmark data sets, the selection of per-formance measures, multiple tests and comparisons with competingmethods. In a classi\ufb01cation task, for example, a typical performancemetric is accuracy, a scalar value which represents the fraction ofpredictions that an algorithm got right. More sophisticated methodslook at the optimal trade-off between bene\ufb01ts (the true positive rate)and costs (the false-negative rate) such as ROC analysis.The choice of the performance measure is a crucial task as theavailable metrics have a distinct meaning which depends on the con-text of an application. In other words, the measured value representssomething we care about [35]. For example, in a system predictingfraud attempts by loan applicants, testing for precision might be suf-\ufb01cient and more informative than other metrics such as speci\ufb01city.Things would change if the outcome referred to cancer detection,where misclassi\ufb01cation comes at different cost.The creation and maintenance of large data repositories is an-other in\ufb02uential component of ML testing practice. Since the ap-pearance of UCI collection [28], the \ufb01eld has mostly committed to abenchmark-oriented attitude where data sets from disparate domainsbecome the reference point for comparing algorithms\u2019 performance.In recent times, the activity of data collection has witnessed a mas-sive surge and new large-scale databases, as well as more productivedata annotation practices, have come to the surface.The ImageNet project [17] generated more than 14 million imagesannotated by thousands of crowdworkers and structured around theWordNet hierarchy [29]. ImageNet has also inspired contests (the so-called ImageNet Large Scale Visual Recognition Challenge ) whereML practitioners can compete and test their models in different spe-ci\ufb01c tasks, such as object detection and image segmentation. Bench-marks and competitions abound also in natural language processingwhere we count challenges for question-answering, reasoning andsentiment analysis .In recent times, the assessment of progress in AI was felicitatedby initiatives tracking algorithms\u2019 performance during competitions,open repositories and code platforms. These include, for example, theAI index initiative [55] and the AI Watch methodology for the mon-itoring of AI progress [50]. Thanks to them, one can get a glimpseof the signi\ufb01cant breakthroughs achieved by the \ufb01eld. For example,in large-scale object classi\ufb01cation tasks, the classi\ufb01cation error of best-performing algorithms fell from 0.28 to 0.023 (see the resultspresented at CVPR Workshop 2017).While the systematic analysis of technical performance tells usthat the \ufb01eld is progressing at a fast pace, there were AI researcherscasting doubts on the robustness of standard assessment approachand claiming that, in reality, what we call progress could be only anillusion [41].",
                    "reference": "[1] Antonella Scantamburlo, Amanda Cort\u00e9s, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
                },
                {
                    "id": "5001c3bc-bcb5-4991-aa33-16c35db34bbe",
                    "text": "The discontent with standard testing practice in ML research is nota recent phenomenon. As early as 2006 Chris Drummond [35] criti-cised the standard testing approach raising three key points:1. Performance measures: he observes that there are other factorsin\ufb02uencing a performance measure like accuracy and these mayinclude misclassi\ufb01cation costs, the stability of the error rate, andthe needs of the end users, among others.2. Statistical tests: following criticism in Psychology, he highlightsthat statistical tests are frequently misinterpreted, for example, asa con\ufb01rmation of the alternative to the null hypothesis . Whiletheir employment in ML experimental practice is often taken as asign of rigor and objectivity, he contends that they do not give thedegree of evidence that many people believe.3. Benchmark data sets: while the use of benchmark data sets allowus to easily compare algorithms\u2019 performance, they suffer fromserious limitations. For example, drawing on previous analysis, heraises concerns as to whether benchmark data sets are really rep-resentative of reality and how much the data collection and con-struction account for differences in class distribution.Further impulse to renovate the experimental practice in ML camefrom the movement of reproducible research requiring the publica-tion of code and data to reproduce the results reported in scienti\ufb01carticles[59]. In a similar spirit, important ML conferences and jour-nals encourage authors to adhere to best practices by making avail-able data and software tools . Other relevant initiatives include opensource repositories and platforms allowing AI practitioners to sharedata and AI models, such as Papers with Code [26] and Open ML[25]. Another recent work proposed a more comprehensive view ofassessment integrating classical performance measures with oftenneglected costs connected to the development and deployment of anAI system [49].While these efforts operated within the edges of traditional scien-ti\ufb01c principles such as transparency and reproducibility, other assess-ment criteria emerged as a consequence of numerous debates aroundthe impact of AI on society along with plans for action, what we callthe ethical turn of AI. The new wave of optimism prompted by thesuccessful application of AI to big data [40] was followed by criti-cal analyses raising issues for culture [34, 44] and human decision-making [31]. Other failures and overstatements in the application ofAI to transport and healthcare stimulated many initiatives around theworld. These comprise research projects, journalistic investigation,centres and networks focused on the impact of AI on our lives.In the few last years, we have witnessed more than one hundreddeclarations of AI principles from governments, organizations andmulti-stakeholder initiatives, aimed at providing normative guidancefor ethical, rights-respecting, and socially bene\ufb01cial developmentand use of AI technologies. Most of those guidelines are alignedon the following eight key themes: Privacy, Accountability, Safetyand Security, Transparency and Explainability, Fairness and Non-discrimination, Human Control of Technology, Professional Respon-sibility, and Promotion of Human Values [38].Note that, while ethical considerations are not new in the \ufb01eld ofAI and notable scholars, such as Norbert Wiener, had already warnedof the possible misuse of intelligent and control systems [60], thescale and the number of present efforts have no precedents in the his-tory of the \ufb01eld. For this reason, we acknowledge all these initiativesas a whole movement with the potential to widen future assessmentpractices.A \ufb01nal, interesting remark regards the variety of actors who arecontributing to this ethical wave. The European Commission, for in-stance, based on the Trustworthy AI Guidelines (TAIG) [42] pub-lished by the High Level expert Group on AI (HLEG-AI), proposeda regulatory framework for high-risk AI applications with a view tobuild an \u201cecosystem of trust\u201d [33]. Big companies, likewise, pub-lished new design principles and audit frameworks [56], also moti-vated by practical needs which are usually attenuated in a researchcontext (think of company\u2019s liability and reputation). Finally, thelandscape of ethical activities comprises a large number of centres(see Table 1) which scrutinize AI systems through the lens of le-gal principles and social values, study the impact on economy andhuman labour, and engage lay people with educational material orworks of art.",
                    "reference": "[1] Antonella Scantamburlo, Amanda Cort\u00e9s, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
                },
                {
                    "id": "2bf9e6e6-44d3-4aaf-ac39-81bdc04fa614",
                    "text": "The Observatory on Society and Arti\ufb01cial Intelligence (OSAI or sim-ply \u201cObservatory\u201d hereafter) was set up in 2019 within the H2020EU funded project AI4EU [5], whose objective is to build the \ufb01rstEuropean AI on-demand web platform and ecosystem. The OSAI isan example of this vast array of initiatives animating the ethical turnof AI outlined in the previous section. Though at its infancy, it givesus the opportunity to explore how this and similar activities can con-tribute to stretch the assessment of AI and turns progress towardsethical principles.The OSAI\u2019s aim is to support discussion and to facilitate the distri-bution of information about the Ethical, Legal, Socio-Economic andCultural issues of AI (ELSEC-AI) within Europe. Speci\ufb01cally, theOSAI has the following objectives:\u2022 To stimulate re\ufb02ection, discussion and due consideration ofELSEC-AI issues within the project through a series of workinggroups (see Section 4). OSAI is attracting a network of experts indifferent domains of ELSEC-AI that will contribute to bridge theknowledge gap existing today within AI practitioners and users.\u2022 To provide resources to educate the general EU public more accu-rately about AI and ELSEC-AI issues by generating weekly con-tent in the form of articles, reports, cultural announcements withthe objective to promote discussion and awareness on these topics. The Observatory evolves in a complex scenario: the \ufb01eld of AI isgaining momentum, and many public and private agencies have be-gun to consider the opportunities and the risks that lie behind thisexciting trend. The OSAI seeks to carve out its own identity and roleneither in contrast nor competition with other existing European ini-tiatives (e.g., HLEG-AI). It aims to increase connections among theserelated projects and make accessible a broad range of articles to theEuropean public at large. The OSAI\u2019s approach can be described bythree verbs: 1) Observe facts and events occurring within Europe bymonitoring newspapers, online bulletins, scienti\ufb01c literature, etc.; 2)Re\ufb02ect on particular events or issues through to the contribution ofELSEC-AI experts and, in particular, thanks to the activities of theworking groups; 3) Report to the general public by using a simple(but not simplistic) language in a way to support mutual understand-ing among experts and educate lay people.",
                    "reference": "[1] Antonella Scantamburlo, Amanda Cort\u00e9s, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
                },
                {
                    "id": "c7014c6d-baed-4f4c-8630-47cc0b50ffc5",
                    "text": "As we said, the creation of the Observatory takes place in a complexand dynamic context where an imprecise number of AI-related eventspopulate the European calendar. Table 1 includes a collection of Eu-ropean centres that are speci\ufb01cally dedicated to the research aroundAI and its impact on Society. These were selected form a larger setbased on a search of simple keywords on Google engine (such as\u201cAI\u201d, \u201cethics\u201d, and \u201csociety\u201d). The common aim among these insti-tutions is to promote designs and developments of technologies thatput upfront concepts such as social responsibility, trust or fairness.Some are dedicated to the creation of guides, others to de\ufb01ne evalu-ation methods, but all have in common the will to create spaces formultidisciplinary dialogue.While the abundance of centres and projects dealing with AI andits social and ethical impact is a sign of cultural awareness and asource of knowledge, all these positive undertakings run the risk ofisolation and self-referentiality. Therefore, OSAI should try to bridgethis gap and promote cooperation and mutual knowledge. In addition,it will focus on areas that extend beyond the ethical and legal aspects,including also socio-economic and cultural elements (e.g., how AI isperceived among European citizens, how the arts are presenting orusing AI).The Observatory differs from these initiatives in several respects.In the \ufb01rst place, the OSAI focuses not only on articles and news, butalso on people. Indeed, one of the motivating ideas behind the Obser-vatory is the creation of a community of people who can contributeto the discussion of ELSEC-AI. Such a community can combine var-ious types of subjects such as AI experts (e.g., AI researchers andpractitioners), specialists in any ELSEC-related \ufb01eld (ethicists, soci-ologists, lawyers, policy makers, artists, etc) and lay people. In thesecond place, the OSAI will approach ELSEC-AI in the context ofEurope so as to foster the dialogue among European countries.",
                    "reference": "[1] Antonella Scantamburlo, Amanda Cort\u00e9s, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
                },
                {
                    "id": "84822a58-43b7-48c6-b522-0ab58e85ada2",
                    "text": "To revise how AI systems are evaluated and complement perfor-mance metrics with ELSEC-AI considerations, we need a change inthe background of the process. A \ufb01rst step is to promote diversity inthe teams that are both designing and assessing such systems. Notethat when we talk about diversity, we do not only refer to gender,ethnicity or functional capabilities, but also to include professionalsfrom multiple disciplines and domains of expertise.A second step is to foster a multidisciplinary dialogue among ex-perts to promote re\ufb02ection on ELSEC-AI and identify shared strate-NameHumanE AI [16] CountryEurope TypeH2020 EU ProjectAI Watch [4]AI4People [6] EuropeanCommissionEuropeanCommission Public InstitutionMulti-stakeholderForumOECD.AI [24] Inter-governmental International Organi-sationKnowledge CentreData & Society [22] Belgium Research CentreDataEthics [10] Denmark ThinkDoTankDATALAB - Centerfor Digital Social Re-search [12] Denmark Research CentreImpactAI [18] FranceAlgorithm Watch [7] Germany Non-pro\ufb01t Associa-tionNon-pro\ufb01t Organisa-tionAI & Society Lab [1] Germany Research LaboratoryInstitute for Ethics inAI [21] Germany Research CentreSustainabilityAICentre [2] Sweden ConsultancyAI Transparency In-stitute [3] Switzerland Non-pro\ufb01ttion associa-Digital Ethics Lab[13]Institute for EthicalAI & ML [19]Institute for EthicalAI in Education [20]Leverhulme Centrefor the Future ofIntelligence [23]Centre for DataEthics and Innova-tion [8] UKUKUKUK Research CentreResearch CentreResearch CentreResearch CentreUK Public Institution ObjectiveTo create the foundations for AI systems that empower people and so-ciety, with special focus on Collaborative Humane Computer Inter-action based on a convergence of HCI with ML.An initiative to monitor the development, uptake and impact of AI forEuropeTo bring together all actors interested in shaping the social im-pact of new applications of AI, including the European Parliament,civil society organisations, industry and the media. They published theAI4People\u2019s Ethical Framework [39] which inspired the TAIG [42].The OECD AI Policy Observatory combines resources from across theOECD, its partners and all stakeholder groups to facilitate dialoguebetween stakeholders while providing multidisciplinary, evidence-based policy analysis in the areas where AI has the most impact.Funded by the Flemish Department on Economy, Science and Innova-tion, it enables socially responsible, ethical and legally appropriateimplementations of AI in Flanders.To ensure primacy of the human being in a world of data, based ona European legal and value-based framework. It has a core focus onAI as the evolution of complex data processing extended in humandecision-making within politics, economics, identity and culture.Conducts research in many different aspects of behavioural data withinseveral areas. A special focus is brought to the social effects of auto-mated data processing as well as to the social adaptation of automateddata systems.Think&Do Tank for Ethics and Responsible AI aiming to promote thedevelopment of trusted AI, support innovative projects and publishannual reports.Based on research and advocacy to evaluate algorithmic decision-making processes, raise ethical con\ufb02icts and explain its features togeneral audience.Interface and translator between academia on one side and industry andcivil society on the other, it functions as experimental space for newformats to advance knowledge generation and knowledge transferto AI.To generate of global, egalitarian and interdisciplinary guidelines forthe ethical development and implementation of AI and to integrate ofethical and societal priorities into the development of fundamen-tally integrative AI technologies.Creation of AI Sustainability Framework for identifying, measuringand governing the ethical implications of AI and assist organisationsfrom a legal, technical and societal perspective.Dedicated to AI governance and human trust in AI, they addresskey challenges in digital ethics, AI safety, transparency, fairness andprivacy.To tackle the ethical challenges of digital innovation from a multi-disciplinary perspective, with the aim to identify bene\ufb01ts and positiveopportunities while avoiding risks and shortcomings.Highly-technical, practical and cross-functional research across 8 Ma-chine Learning Principles and Explainable AI FrameworkAs a response to the TAIG, IEAIE works to develop frameworks andmechanisms to help ensure that the use of AI across education is de-signed and deployed ethically.To build an interdisciplinary community of researchers with stronglinks to technologists and the policy world to study the impact of AIin society with a focus on trust, fairness, accountability and democ-racy.Part of Department for Digital, Culture, Media & Sport, they connectpolicymakers, industry, civil society, and the public to develop theright governance regime for data-driven technologies.gies to consider ELSEC issues within the AI life cycle. OSAI is try-ing to ful\ufb01ll these tasks (diversity and multidisciplinary discussion)by creating a set of working groups, i.e., semi-organised groups ofexperts working on ELSEC-AI topics.The experience of the working groups is a laboratory to exploreways to address ELSEC-AI from a diversity of perspectives. Theyhave an experimental character in that there is no systematic knowl-edge and expertise in dealing with ELSEC-AI in real-world with amultidisciplinary approach. At the beginning, working groups willemphasize two perspectives with a view to incorporating further as-pects in future:\u2022 Legal AI: to study existing laws and regulations, how applicablethey are to AI systems and identify possible gaps.\u2022 Ethical AI: to promote the design and development of AI systemsthat respect fundamental rights.The working groups will be formed by experts in different areas,from academic, business, media or other backgrounds: lawyers anddata protection of\ufb01cers, philosophers, software engineers, journal-ists, sociologists, etc. With this variety of pro\ufb01les we expect to gen-erate a wide range of opinions and experiences around our topics ofinterest. Moreover, these working groups are supposed to grow witha bottom-up approach engaging participants from the very beginning.Participants will work in smaller groups and in a limited span time .This activity aims to encourage the participation of experts with anopen and transparent methodology and engage them in the commu-nication and sharing of knowledge in this new area of interest. Themain objectives of this activity are the following:\u2022 To \ufb01ll the gap between the ethical debate and the engineering prac-tice in European organizations.\u2022 To help researchers and practitioners navigate the ethical chal-lenges that arise in different real-world AI applications.\u2022 To support interdisciplinary dialogue engaging people from dif-ferent backgrounds.\u2022 To promote cross-fertilisation among different sectors (e.g.,academia, companies, public institutions).\u2022 To inspire future responsible practices in the \ufb01eld of AI.\u2022 To create ELSEC-AI literacy understandable for different types ofaudiences and domains of expertise.To ful\ufb01ll the last two points, we expect to generate a set of goodAI practices with a focus on how to implement guidelines such asthe HLEG-AI Trustworthy AI Guidelienes, especially for SMEs andstart-ups. We will also adapt part of the content into educational ma-terial that will be shared through the Observatory and the AI4EUcommunication channels.",
                    "reference": "[1] Antonella Scantamburlo, Amanda Cort\u00e9s, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
                },
                {
                    "id": "eb517707-4fc2-44d2-9bc4-3d8036505288",
                    "text": "In the second half of 2019, the authors participated in the pilot-ing of the Trustworthy AI assessment List (TAIL) that was pub-lished along with the Trustworthy AI Guidelines by the HLEG-AI,were 50 companies from different sectors and European countrieswere interviewed. The result of that investigation suggested the needto promote operational resources (e.g., tools, frameworks, proce-dures/methodologies, etc.) to support organisations to take the Trust-worthy AI requirements into account. Other studies have stressed thevalue to share knowledge and experiences (e.g., best practices, case studies) to help practitioners navigate complex ethical issues and in-terventions [43].To build upon the piloting of the TAIL, we propose to form smallteams that can collaborate with companies (but also public organisa-tions) that are implementing AI products with a view to experimenthow to tackle Trustworthy AI requirements, also drawing on respon-sible practices (see section 4.2), generate and share a list of goodpractices that can inspire other organisations. We hope in this wayto help \ufb01ll the gap of knowledge and language between the differ-ent stakeholders, i.e., from regulations, to recommendations to the\ufb01nal translation to software engineering methods and other sorts ofprocesses (Human-Computer Interaction methods, ML approaches,management strategies, etc.). Also we will try to disseminate the re-sult of working groups through the Observatory web-site so as toinclude the Society in this process of gaining trust in new technolo-gies.At present, the working groups are not intended to generate a com-prehensive methodology or a new standard but identify good prac-tices that help integrate ethical and legal requirements into the as-sessment of AI systems. In concrete, the teams of experts will inter-act with organisations to analyse speci\ufb01c case studies where they canapply and test one or more responsible AI practices. Interactions cantake the form of interviews, design thinking sessions, algorithmic au-dits, among others, depending on the operational resources adoptedby the team (see the responsible practices in Section 4.2). This wouldhelp organisations to check whether their AI products meet Trustwor-thy AI requirements, and possibly re-frame their objectives and theirKey Performance Indicator (KPIs) in the light of ethical and legalconstraints.A series of training sessions with invited speakers (either externalor internal to the working groups) will help the members of workinggroups to achieve a shared knowledge about responsible AI practices(questionnaires and checklists, frameworks, strategy guides and can-vases, etc). Also, to test and re\ufb01ne our proposal, we plan to pilotworking groups with an internal activity involving a few experts andpartners (from research and industry) of the AI4EU consortium.",
                    "reference": "[1] Antonella Scantamburlo, Amanda Cort\u00e9s, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
                },
                {
                    "id": "93d7ed9a-4cad-45e5-8f70-423808d1f289",
                    "text": "While many AI principles have been published, the translation ofthese into practices and processes is still at the very beginning. Themain reason relies on their abstract nature, which makes them dif-\ufb01cult for practitioners to operationalise. Even though awareness ofthe potential ethical issues is increasing at a fast rate, the AI commu-nity\u2019s ability to take action to mitigate the associated risks is still atits infancy [52]. Operationalisation so far strongly depends on moralcompass of individual since there are no legal binds to the existingethical guidelines. Thus, a framework for ethical decision makingand responsible practices is required.We are classifying the responsible practices by their nature andintended use into 5 groups:\u2022 Assessments, questionnaires and checklists raise questions, en-courage re\ufb02ection and inspire potential action, e.g., the HLEG-AI TAIL with 131 questions to operationalise the seven key re-quirements declared in the AI guidelines [42], or the ConsequenceScanning, an agile practice to consider the potential consequencesof a product or service on people, communities and the planet [9].\u2022 End-to-end frameworks address each stage of the entire processwith appropriate activities and involve multiple audiences, e.g.,the End-to-End Framework for Internal Algorithmic Auditing tohelp companies and their engineering teams audit AI systems be-fore deploying them [56], or the People + AI Guidebook to helpuser experience (UX) professionals and product managers followa human-centered approach to AI [27].\u2022 Strategy guides and canvases are thinking frameworks and diag-nostic tools, that help break down and work through complex chal-lenges, e.g., the Data Ethics Canvas helps identify and managedata ethics considerations [11], or the Ethical Operating System tohelp inform the design and development process, provide strate-gies to mitigate risks and take action [15].\u2022 Design guides are sets of recommendations towards responsiblegood practice in design, e.g., the Guidelines for Human-AI Inter-action recommend best practices for how AI systems should be-have [30]; AI Ethics Cards are a set of four design principles andten activities that help guide an ethically responsible, culturallyconsiderate, and humanistic approach to designing with data [14].\u2022 Software toolkits provide metrics and algorithms to support theethical development of AI-powered software, e.g., the AI Fairness360 Toolkit to help examine, report, and mitigate discriminationand bias in ML models throughout the AI application life cycle[32]; Aequitas bias audit toolkit to audit machine learning modelsfor discrimination and bias [57].The landscape of responsible practices is wide, but insuf\ufb01cient,inef\ufb01cient and scattered. Many of these practices do not adequatelyaddress the challenges of context-dependency, they lack ease of use[36] and completeness, they either address only silo disciplines, orsingle process steps or particular problems. Furthermore, tickingboxes on fairness checklists, mitigating bias with algorithms and an-ticipating consequences with ethics cards is by far not enough. Asdepicted in Figure 1, in order to progress towards responsible out-comes, it needs \ufb01rst and foremost close collaboration and a diverserange of perspective, guidance derived from values, principles andpolicies, a curated set of responsible practices, throughout the entireprocess, ensured by governance procedures, such as audit services,certi\ufb01cations and AI labels, or company-internal self-check tools, redteams, and ethics Objectives and Key Results (OKRs) and KPIs [51].As companies start to envision procedures to operationalise AIprinciples, new professional roles, summarised as \u201cethics owners\u201dby [51], are being created in order to cover the lack of attentionthat ethics had in \ufb01rst place. They own responsibility for ethics prac-tices across an organization, and engage to transform principles, val-ues, ethical stances, and often legal and regulatory imperatives intoconcrete practices within their organization. To be most effective at achieving these goals, new responsible practices must be alignedwith teams existing work\ufb02ows and supported by organizational cul-ture [48].",
                    "reference": "[1] Antonella Scantamburlo, Amanda Cort\u00e9s, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
                },
                {
                    "id": "e17a9e85-38ad-47c2-8620-d31e8be9e451",
                    "text": "This paper presents an overview on the existing initiatives that wecan \ufb01nd today to enhance the progress of AI towards responsibledesign, development and deployment. This review puts special atten-tion on the European centres and networks involved in this processsince this is the main scope of the Observatory and the objectiveswithin AI4EU. Although it is not exhaustive, it suggests that we arewitnessing an ethical turn promoting a wider discussion about theassessment of AI systems. In addition, the study on the different re-sponsible practices shows an interest from a diversity of stakeholdersto promote a different approach to the assessment of AI, although itis clear that there are still some gaps to cover in order to integrate therequired ecosystem.In his book Progress and its Problems, Larry Laudan recommendsto cast the nets of appraisal suf\ufb01ciently widely so as to include allthe cognitively relevant factors which are actually present in the his-torical situation [46]. The OSAI aims to contribute to this change ofparadigm by creating a space for multidisciplinary gathering, wheredifferent actors will be able to discuss and create literacy related toELSEC-AI. We expect to study in depth the key pillars for progress-ing towards responsible outcomes of AI and their relation with theTrustworthy AI Guidelines and Assessment List in order to \ufb01nd ex-isting relations, but also possible opportunities to complement them.Our \ufb01nal objective is to generate a set of good responsible practicesthat could help AI practitioners to implement the HLEG-AI docu-ments and align with the European Commission\u2019s vision of Trust-worthy AI. We strongly believe that this step is necessary to bringclose all the stakeholders involved in the design, deployment and useof new technologies such as AI, which may have great bene\ufb01ts forthe Society but are still in a process of being trusted.ACKNOWLEDGEMENTSTeresa Scantamburlo and Atia Corts are partially supported bythe project A European AI On Demand Platform and Ecosystem(AI4EU) H2020-ICT-26 #825619. The views expressed in this pa-per are not necessarily those of the consortium AI4EU.",
                    "reference": "[1] Antonella Scantamburlo, Amanda Cort\u00e9s, and Michael Schacht. 2020. Progressing towards responsible AI. arXiv:2008.07326. Retrieved from https://arxiv.org/pdf/2008.07326"
                }
            ]
        },
        {
            "paper_title": "Responsible ai pattern catalogue: a multivocal literature review",
            "authors": "Q Lu, L Zhu, X Xu, J Whittle, D Zowghi\u2026",
            "publication_info": "arXiv preprint arXiv \u2026 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2209.04963",
            "chunks": [
                {
                    "id": "0dac0304-43e3-406f-ad22-22f3ab2f5bce",
                    "text": "Responsible AI is widely considered as one of the greatest scientific challenges of our time and iskey to increase the adoption of AI. Recently, a number of AI ethics principles frameworks have beenpublished. However, without further guidance on best practices, practitioners are left with nothingmuch beyond truisms. Also, significant efforts have been placed at algorithm-level rather than system-level, mainly focusing on a subset of mathematics-amenable ethical principles, such as fairness.Nevertheless, ethical issues can arise at any step of the development lifecycle, cutting across many AIand non-AI components of systems beyond AI algorithms and models. To operationalize responsibleAI from a system perspective, in this paper, we present a Responsible AI Pattern Catalogue based onthe results of a Multivocal Literature Review (MLR). Rather than staying at the principle or algorithmlevel, we focus on patterns that AI system stakeholders can undertake in practice to ensure that thedeveloped AI systems are responsible throughout the entire governance and engineering lifecycle.The Responsible AI Pattern Catalogue classifies the patterns into three groups: multi-level governancepatterns, trustworthy process patterns, and responsible-AI-by-design product patterns. These patternsprovide systematic and actionable guidance for stakeholders to implement responsible AI.Responsible AI, ethical AI, trustworthy AI, AI governance, AI engineering, MLOps, software engineering, softwarearchitecture, pattern, best practice",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "eee2ee9e-5f20-48a8-91ff-6c1f6af74d09",
                    "text": "Artificial Intelligence (AI) has been transforming our society and listed as the top strategic technology in manyorganizations. Although AI has huge potential to solve real-world challenges, there are serious concerns about its abilityto behave ethically and make decisions in a responsible way. Compared to traditional software systems, AI systemsinvolve higher degree of uncertainty and more ethical risk due to their dynamic, autonomous and opaque decisionmaking and historical-data-dependent behaviors. Responsible AI refers to the ethical development of AI systems tobenefit the humans, society, and environment. The concept of responsible AI has attracted significant attention fromgovernments, organizations, companies and societies. According to the 2022 Gartner CIO and Technology ExecutiveSurvey, 48% of organizations have already adopted or plan to adopt AI technologies within the next 12 months while21% of organizations have already deployed or plan to deploy responsible AI technologies within the next 12 months .Responsible AI has been widely considered as one of the greatest scientific challenges of our time and the key to unlockthe market and increase the adoption of AI.To address the responsible AI challenges, a number of AI ethics principles frameworks have been published recently[1], which AI systems are supposed to conform to. There has been a consensus made around the AI ethics principles [2].A principle-based approach allows technology-neutral, future-proof and context-specific interpretations and operational-ization. However, without further best practice guidance, practitioners are left with nothing much beyond truisms. Forexample, it is a very challenging and complex task to operationalize the the human-centered value principle regardingA - S 29, 2023Figure 1: Overview of RAI pattern catalogue.how it can be designed for, implemented and monitored throughout the entire lifecycle of AI systems. In addition,significant efforts have been put on algorithm-level solutions which mainly focus on a subset of mathematics-amenableethical principles (such as privacy and fairness). However, issues (including ethical issues) can occur at any step ofthe development lifecycle crosscutting many AI, non-AI and data components of systems beyond AI algorithms andmodels. To try to fill the principle-algorithmic gap, further guidance such as guidebooks , questions to generatediscussions [3, 4], checklists [5, 6] and documentation templates [7, 8, 9, 10, 11, 12] have started to appear. Thoseefforts tend to be ad-hoc sets of more detailed prompts for practitioners to think about all the issues and come up withtheir own solutions.In this paper, we therefore adopt a pattern-oriented approach and present a Responsible AI Pattern Catalogue foroperationalizing responsible AI from a system perspective. In software engineering, a pattern is a reusable solutionto a problem that occurs commonly within a given context in software development [13]. Rather than staying at theethical principle level or algorithm level, we focus on patterns that practitioners can utilize in practice to ensure thatthe developed AI systems are responsible throughout the entire software development lifecycle. As shown in Fig. 1,the Responsible AI Pattern Catalogue classifies patterns into three groups: 1) governance patterns for establishingmulti-level governance for responsible AI; 2) process patterns for setting up trustworthy development processes; 3)product patterns for building responsible-AI-by-design paradigm into AI systems. These patterns are identified throughconducting a systematic Multivocal Literature Review. The full version of our Responsible AI Pattern Catalogue can beaccessed online .The remainder of the paper is organized as follows. Section 2 introduces the methodology for building up the patterncatalogue. Section 3 presents the AI system stakeholders and governance patterns. Section 4 discusses the processpatterns for each stage of the development lifecycle. Section 5 introduces the project patterns. Section 6 discusses therelated work. Section 5 concludes the paper.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "7b2ca3ff-0370-4ba5-bba4-7d0ffb5b0404",
                    "text": "To build up a Responsible AI (RAI) Pattern Catalogue, we performed a systematic Multivocal Literature Review (MLR)to collect patterns. Fig. 2 presents the research design and methodology. The high level research question that hasguided this research is: \u201dWhat responsible AI solutions can be identified?\u201d. The research question focuses on identifyingthe reusable patterns for responsible AI. 2 A - S 29, 2023",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "a5135d4b-d42c-487b-8f16-6f775dca1fab",
                    "text": "Figure 2: Methodology.The benefit of an MLR is to cover both academic literature and grey literature in the study. Grey literature iswritten by practitioners (such as governments, organizations, companies) and not published in books or scientificjournals/conferences. However, grey literature can provide valuable insights on the state of practice and may includemany industry solutions that are not discussed in academic papers. Given the nature of patterns, we decided to alsoreview grey literature to understand the state of the practice in the field of Responsible AI and collect patterns fromindustry. In our MLR, we identify: (1) relevant academic peer-reviewed academic literature and (2) relevant greyliterature for this study.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "d6a62874-ad24-4838-baf6-9f04ddf07f0e",
                    "text": "The study has been carried out separately for academic literature and grey literature. We adopted Kitchenham\u2019s System-atic Literature Review (SLR) guideline [14] to review the academic literature and used Garousi et al\u2019s guideline [15] toperform the grey literature review. The complete MLR protocol is available as online material . Overall, we first testeddifferent search strings in the five well-known search engines and evaluated the total number of studies retrieved, aswell as their relevance. The evaluation involved cross-checking the inclusion of known relevant literature. Once wedetermined the most effective search string, we proceeded to perform searches on Google Scholar. From the results, werandomly selected 20 papers and extracted the relevant answers for each of the research questions.We determined the search strings by deriving relevant keywords from the research question. Before conducting thesystematic search, we did a pilot study by experimenting with the search terms to compare the results. We used \u201cAI\u201d,\u201cResponsible\u201d, \u201cSolution\u201d as the key terms and included synonyms and abbreviations as supplementary terms to increasethe search results. We designed the search strings for each primary source to check the title. After completing the firstdraft of search strings, we examined the results of each search string against each database to check the effectivenessof the search strings. The finalised search terms are shown in Table 1. We use Australia\u2019s AI ethics principles [16] toidentify the supplementary terms for \u201cResponsible\u201d as a close-enough representation of the many similar ones [1, 2]around the world. The eight AI ethics principles include human, societal and environmental wellbeing, human-centredvalues, fairness, privacy protection and security, reliability and safety, transparency and explainability, contestability,accountability. We mapped each individual term in the eight principles to its corresponding noun term and adjective3 A - S 29, 2023forms. Furthermore, in order to encompass the relevant terms related to responsible AI, we expanded the mapping toinclude \u201cresponsible\u201d as well as its variations, such as ethics, ethical, responsibility, trust, trusted, trustworthiness, andtrustworthy. By doing so, we ensure comprehensive coverage of the terms relevant to responsible AI. The search stringsand the respective paper quantities of the initial search for each primary source are listed in our MLR protocol. Weapplied the search string to both scholar search engines for academic literature and Google Search Engines for greyliterature. The scholar search engines include: ACM Digital Library, IEEE Xplore, Science Direct, Springer Link, andGoogle Scholar. The search period is until 31 July 2022.Table 1: Key and supplementary search termsWe screened the initial results against inclusion and exclusion criteria. The inclusion criteria include: (1) A paper/articlethat presents a governance or process or design solution for responsible AI. (2) A paper/article that presents a tool ortoolkit for developing responsible AI systems. (3) A paper/article that is in the form of a published scientific paper orindustry article. The exclusion criteria are: (1) A paper/article that only discusses high-level principles or frameworks.(2) A paper/article that only focuses on algorithm-level techniques. (3) A paper/article that is not written in English. (4)Conference version of a study that has an extended journal version. (5) PhD/Master\u2019s dissertations, tutorials, editorials,books.The snowballing technique has been recommended and used in place of database searches in systematic literaturereviews. For the academic literature, we identified a set of papers that serve as the starting point (i.e., seed set) forsnowballing. The seed set papers were selected based on the source databases and Australia\u2019s AI ethics principles tocover various communities. For each of the 5 source databases, we selected 1 top cited paper for responsible AI ingeneral and each of the 8 principles respectively. For some principles, there is no paper found in one particular database.We only collected the grey literature from the first 10 Google pages. For the grey literature, snowballing is conducted ifrelated responsible AI solutions are mentioned on the webpage. We finally identified 205 academic items and 69 greyitems for the MLR. For the grey literature, we organized the responsible AI solutions according to the correspondingcompanies. For example, we found 13 responsible AI tools/solutions on Microsoft\u2019s website but only counted Microsoftas one grey item in our data extraction sheet and recorded a few patterns extracted from Microsoft\u2019s tools/solutions.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "4a621ec9-20ca-498c-ac79-673a77ca385c",
                    "text": "To realise responsible AI from a software engineering perspective, we need to make both AI products and theirdevelopment processes trustworthy and responsible. Additionally, compliance with AI standards and laws from agovernance perspective is necessary. Thus, we classify the patterns into three categories: governance, process, andproduct. Not only should you use product patterns to enforce responsible AI principles directly in the product andverify/validate the product, but you should also use process and governance patterns to complement it further.We extracted data and summarized findings from the selected academic and grey items based on the pre-defined researchquestion. Based on the answers extracted for the research question, we identified different types of patterns. Forexample, there are a few papers using federated learning to deal with data privacy issues, thus \u201cfederated learner\u201d isidentified as a product pattern that can be built into the architecture of AI systems for continuous learning. Someof the solutions can be mapped to multiple levels. For example, software bill of materials can be identified as anorganisation-level governance pattern which is interconnected with and supported by product pattern \u201cBill of materialsregistry\u201d. After identifying a pattern, we documented its details on our RAI Pattern Catalogue website according tothe traditional pattern structure [17]: context, problem, solution, benefits, drawbacks, related patterns, known uses.The known uses were found through data extraction and additional manual search. The pattern users and impactedstakeholders are identified based on 1) AI software supply chain and ecosystem, 2) AI standards, 3) our expertise andknowledge.We also extract some general information, e.g. authors name, organization, publication venue, publication year. Weperformed a pilot study on 20 items to test the research question and the way to extract the required data. We stored allthe extracted data in a spreadsheet for analysis. 4 A - S 29, 2023",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "cf783b59-ebd8-493f-bedf-2c77b7496e83",
                    "text": "Figure 3: Stakeholders for RAI governance.The governance for responsible AI systems can be defined as the structures and processes that are employed to ensure thedevelopment and use of AI systems meet AI ethics principles. According to Shneiderman\u2019s structure [18], governancecan be built at three levels: industry-level, organization-level, and team-level. As illustrated in Fig. 3, we identified thestakeholders for RAI governance and classified them into three groups:\u2022 Industry-level stakeholders\u2013 AI technology producers: develop AI technologies for others to build on top to produce AI solutions,e.g., parts of Google, Microsoft, IBM. AI technology producers may embed RAI in their technologiesand/or provide additional RAI tools.\u2013 AI technology procurers: procure AI technologies to build their in-house AI solutions, e.g., companiesor government agencies buying/using AI platform/tools. AI technology procurers may care about RAIissues and embed RAI into their AI technology procurement process.\u2013 AI solution producers: develop in-house/blended unique solutions on top of technology solutions andneed to make sure the solutions adhere to RAI principles/standards/regulations, e.g., parts of MS/Googleproviding Office/Gmail \u201csolutions\u201d. They may offer the solutions to AI consumers directly or sell toothers. They may use RAI tools (provided by AI technology producers or RAI tool producers) and RAIprocesses during their solution development.\u2013 AI solution procurers: procure complete AI solutions (with some further configuration and instantiation)to use internally or offer to external AI consumers, e.g., a government agency buying from a completesolution from vendors. They may care about RAI issues and embed RAI into their AI solution procurementprocess.\u2013 AI users: who use an AI solution to make decisions that may impact on a subject, e.g., a loan officer or agov employee. AI users may exercise additional RAI oversight as the human-in-the-loop.\u2013 AI impacted subjects: who are impacted by some AI-human dyad decisions, e.g., a loan applicant or atax payer. AI impacted subjects may care about RAI issues and contest the decision on dyad AI ground.\u2013 AI consumers: who consume AI solutions (e.g., voice assistants, search engines, recommender engines)for their personal use (not affecting 3rd parties). AI consumers may care about RAI issues and the dyadAI aspects of AI solutions.\u2013 RAI governors: those that set and enable RAI policies and controls within their culture. RAI governorscould be functions within an organization in the above list or external (regulators, consumer advocacygroups, community).\u2013 RAI tool producers: technology vendors and dedicated companies offering RAI features integrated intoAI platforms or AIOps/MLOps tools.\u2013 RAI tool procurers: any of the above stakeholders who may purchase or use RAI tools to improve orcheck solutions/technology\u2019s RAI aspects. 5 A - S 29, 2023Figure 4: Governance patterns for responsible AI.\u2022 Organization-level stakeholders\u2013 Management teams: individuals at the higher level of an organization who are responsible for es-tablishing RAI governance structure in the organization and achieving RAI at the organization-level.The management teams include board members, executives, and (middle-level) managers for legal,compliance, privacy, security, risk, and sustainability.\u2013 Employees: individuals who are hired by an organization to perform work for the organization andexpected to adhere to RAI principles in their work.\u2022 Team-level stakeholders\u2013 Development teams: those who are responsible for developing and deploying AI systems, includingproduct managers, project managers, team leaders, business analysts, architects, UX/UI designers, datascientists, developers, testers, and operators. The development teams are expected to implement RAI intheir development process and embed RAI into the product design of AI systems.As shown in Fig. 4, we identify a set of governance patterns and classify them into industry-level governance patterns,organization-level governance patterns, and team-level governance patterns based on Shneiderman\u2019s governancestructure [18]. The target users of industry-level governance patterns are RAI governors, while the impacted stakeholdersinclude AI technology producers and procurers, AI solution producers and procurers, RAI tool producers and procurers.For the organization-level patterns, the target users are the management teams and the impacted stakeholders areemployees, AI users, AI consumers, and AI impacted subjects. The target users of team-level patterns are thedevelopment team, whilst the impacted stakeholders are AI users, AI consumers, and AI impacted subjects.3.1 Industry-level governance patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "52d1448f-2bea-40d5-a3a3-898ee9ece7ea",
                    "text": "Laws already apply to AI systems, however the processes/requirements to ensure compliance are not always certain,also some regulations may need to be updated e.g. administrative law. There is an urgent need for clear guidanceto ensure that AI systems are developed and used responsibly in compliance with existing and upcoming laws, e.g.,discrimination law. RAI regulations are developed by governments in their jurisdiction to enable the trustworthydevelopment of AI systems by industry [18, 19, 20, 21, 22, 23, 24]. Organisations will be required to ensure that theycomply with the requirements of the EU AI Act when the applications fall into the high risk category . In US, theAlgorithmic Accountability Act of 2022 was introduced in the Senate and House of Representatives, while an AI6 A - S 29, 2023Bill of Rights is under development by the White House Office of Science and Technology Policy. The aim of RAIregulations is to prevent illegal or negligent, malicious use of AI systems. However, there are many regulations indevelopments in each jurisdictions, which may cause an interoperability challenge for organisations. Also, it usuallytakes a long time to enact AI regulations due to the lengthy consultation and approval process.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "c8a51198-723a-4277-82fc-79742ac0491a",
                    "text": "To enable the trial of the innovative AI products in the market, a regulatory sandbox can be designed to allow testing theinnovative AI products in the real-world under relaxed regulatory requirements but with appropriate safeguards in placeon a time-limited and small-scale basis [21]. An AI Regulatory Sandbox is introduced in the EU\u2019s AI Act proposalsubmitted in 2021. The UK Information Commissioner\u2019s Office advised a Regulatory Sandbox for utilising personaldata. The Australian Government released the Enhanced Regulatory Sandbox for innovative financial services. AIproducts can enter the market under more flexible regulatory requirements in a faster pace and be tested in the real-worldmarket to ensure they are designed ethically. However, it might incur extra cost to apply for a regulatory sandbox. Also,the AI products might not work well with large scale deployment in different context.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "ffc0f5b1-79ce-4c6b-a1c3-7930548761bf",
                    "text": "AI systems may have various degrees of risk depending on the design and application domains. To ensure that AIsystems are trustworthy and meet certain minimum standards, building code can be designed to provide mandatoryregulatory rules for authority parties (e.g., independent oversight and advisory committee) to assess the complianceof AI systems before they are allowed to launch [18]. For example, IEEE has released a set of building codes fordeveloping smart cities , Medical Device Software Security , and Power System Software Security . Buildingcode sets out clear compulsory regulatory requirements for developing AI systems. AI systems cannot be sold in themarket until an approval is issued by the assessment authority.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "b6c41d7b-aee4-4e03-a214-af3cad89a054",
                    "text": "An AI system may use data or components from multiple jurisdictions where may have conflicting regulatory re-quirements on their usage. To enable interoperability between jurisdictions, RAI standards are developed to describerepeatable processes to develop and use AI systems responsibly that are recognised internationally and can be eithermandated by law or by contract [18, 19, 23]. ISO/IEC JTC 1/SC42 AI Technical Committee is developing ISO/IEC42001 IT-AI-Management System Standard that provides a pathway for the certification of AI systems and WG3trustworthiness that covers risk management and bias . IEEE has released Guide for Architectural Framework andApplication of Federated Learning , Standard for Technical Framework and Requirements of Trusted ExecutionEnvironment based Shared Machine Learning , and IEEE p7000 IEEE Standards for Model Process for AddressingEthical Concerns During System Design . Those AI standards provide repeatable processes and guidance for the useand development of AI systems that are recognised internationally.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "1e56ed11-3ce0-4c81-b7f1-0aa422f41b0c",
                    "text": "Organizations can face challenges that can hurt their business if they are not aware of their RAI maturity. RAI maturitymodel can be used to assess an organization\u2019s RAI capabilities and the degree of readiness to take advantage of AI7 A - S 29, 2023based on a set of dimensions [18, 25, 26, 27]. RAI maturity model can guide organizations on how to increase theirRAI capabilities. The assessment results depend on the model quality, e.g., assessment dimensions and rating methods.There have been a few AI maturity models developed in industry, such as Gartner\u2019s AI Maturity Model , Microsoft\u2019sAI Maturity Model , and IBM\u2019s AI Maturity Framework .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "d3add88b-2c82-4fdf-bed5-bd1c0b818782",
                    "text": "AI is a high-stake technology that requires evidence to prove AI products\u2019 compliance with AI standards or regulationsin order to operate in the society. RAI certification can be designed to recognize that an organization or a personhas the ability to develop or use an AI system in a way that is compliant with standards or regulations [18, 19, 24,28, 29, 30, 31, 32, 33]. Malta AI-ITA certification is the world\u2019s first AI certification scheme for RAI systems.DO-178C Certification has been used to approve commercial software-based aerospace systems. Queen\u2019s Universityoffers an executive education program on Principles of AI Implementation . The evidence of compliance can beprovided through RAI certification to improve human trust in AI systems. However, like other types of certificates, RAIcertificates may be forged, which makes the verification of authenticity of certificates challenging. The certificationprocess is usually complex, costly and time consuming.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "b6188598-1643-4d88-9f6e-3d50b81af83d",
                    "text": "Consumers in the market usually do not have professional knowledge about AI. To improve public confidence on AI anddispel their ethical concerns, trust mark, a seal of endorsement, is easy to understand by all consumers and can be usedto inform consumers about AI system. Trust mark is important for small companies which are often not well-known inthe AI market. However, consumers may not trust the AI systems with trust mark performing more responsible thanthose without one. There have been several trust marks designed for responsible use of data, such as Australian Dataand Insights Association Trust Mark , New Zealand Privacy Commissioner\u2019s Privacy Trust Mark , and Singapore\u2019sData Protection Trustmark (DPTM) .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "b2f75087-8315-4b03-94c0-96d80f37e499",
                    "text": "Decisions made by AI systems may lead to severe failures due to its autonomous decision-making process. To auditAI systems and investigate failures in a trusted way, independent oversight can be conducted by the independentoversight boards that consist of experts who are knowledgeable to perform the review and has no conflict of interestwith the reviewed organizations[18, 19, 34, 35]. The U.S. National Transportation Safety Board investigates everycivil aviation accident. The U.S. National Artificial Intelligence Advisory Committee advises on AI-related issues.Independent oversight provides a trusted review infrastructure to gain public confidence. Planning oversight providesearly feedback on the new development proposals. Failures of independent oversight could happen due to lack ofsufficient independence.3.2 Organization-level governance patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "d44f870c-8d88-4728-b7e7-c91a62476321",
                    "text": "The management teams need to understand the values, cost and risk for adopting AI in an the organization. Commitment8 A - S 29, 2023needs to be made by the management team to build RAI culture within an organization [18]. Leadership commitment isachieved by the management team dedicating their time and efforts on establishing ethics principles and governancestructure (e.g., appointment of chief RAI officer, RAI advisory boards) [36], as well as incorporating RAI intoorganization\u2019s values, vision, mission [19], board\u2019s strategy planning, executives\u2019 performance reviews[21], audit andrisk committee\u2019s scope [20], and ESG commitments. Leadership commitment enables organizational culture on RAIand visible sponsorship to build RAI capability. IBM has established an AI ethics board to support a culture of RAIthroughout IBM. Axon has assembled an independent AI ethics board to provide guidance on AI system development.Schneider Electric has appointed its first Chief AI Officer to advance its AI strategy.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "582bb84d-c48c-41db-8c5f-95fcd90aedd8",
                    "text": "Organizations need to build capability incorporating multiple areas of expertise to address RAI issues. An AI ethicscommittee is an AI governance body that is established to develop standard processes for decision-making, as wellas to approve and monitor AI projects [29, 32]. Accenture released a report on how to build AI ethics committees .Adobe has created an AI ethics committee which includes experts with different background. Sony has establishedAI ethics committee to ensure the ethically development of AI systems. The ethics committee provides feedbackand guidance to the project team after reviewing the proposal. However, the committee might not have the expertise toreview a particular case, which might cause bias issues.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "e0f498b6-a738-4cd8-821a-69c679678510",
                    "text": "AI may make wrong decisions or behave inappropriately, e.g. impact human lives or buy the wrong product. To guideAI related activities in an organization, a code of ethics is a set of rules that employees should uphold when developingan AI system [18, 23, 29, 37]. AAAI has issued Code of Professional Ethics and Conduct for all members. Boschsets out code of ethics to establish guidelines for the development of AI. BMW has released a code of ethics forAI . A code of ethics provides employees with the same concrete rules on developing AI systems, but it relies onindividuals to do the right thing with limited monitoring and enforcement.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "6d025343-eaca-472b-9bf6-60520567cae9",
                    "text": "Although there are increasing concerns on AI ethics, RAI regulation is still at a very early stage. To assess the ethicalrisks associated with AI systems, an organization needs to extend the existing IT risk framework or design a new one tocover AI ethics [7, 9, 18, 20, 29, 38, 39, 40, 41]. ISO/IEC JTC 1/SC 42 committee is developing ISO/IEC 23894 onArtificial Intelligence and Risk Management . NIST released the initial draft of AI Risk Management Frameworkthat provides a standard process for managing risks of AI systems . The Canadian government has released theAlgorithmic Impact Assessment tool to identify the risks associated with automated decision-making systems . TheAustralian NSW government is mandating all its agencies that are developing AI systems to go through the NSW AIAssurance Framework . Singapore launches AI Verify Toolkit to test responsible AI . UK ICO released ai and data9 A - S 29, 2023protection risk toolkit which is built up on their guidance for organizations using AI systems. Although ethical riskassessment has the potential to prevent majority of incidents and increase awareness of RAI, it is often a one-off type ofrisk assessment with subjective judgement on measurement [42].",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "0c7dd202-9352-49d4-8043-279e5bb75911",
                    "text": "Standardized reporting is essential to address the opaque black box issue of AI systems. Organizations should setup standarized processes and templates for informing the development process and product design of AI systems todifferent stakeholders (e.g., AI governors, users, consumers) [43]. RAI regulations may request such obligations toensure transparency and explainability of AI systems. The Cyberspace Administration of China published transparentdisclosure requirements for online service providers . The service providers are requested to file with the regulators(i.e., AI governors) for impact assessment when realising new services. In addition, the online services must informusers when AI is being used to recommend contents to them and explain the purposes and design of recommendedsystems. In EU\u2019s AI Act , the incidents of AI systems are required to be reported and disclosed by AI system providers(i.e., AI technology or solution producers). Helsinki and Amsterdam released AI registers describing where andhow the two cities are using AI, how AI is built, which data and algorithms are used, how the applications impact thecitizens\u2019 daily lives, and development team\u2019s contact information.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "2ad46283-71dd-4781-a8d5-ab6d78d313f9",
                    "text": "It is necessary that organizations have an appropriate approach to enable accountability throughout the entire lifecycle ofAI systems. Role-level accountability can be established through formal contracts to define the boundary of responsibilityand identify who should be held accountable when an AI system misbehaves [35]. For example, Australia\u2019s NationalData Commissioner creates a data sharing agreement template for using Australian Government data . Developersprimarily focus on the technique aspects of AI systems and may not be familiar with the ethical principles. Role-levelaccountability contracts make the developers keep ethics in mind at every step, but may create stress for employees atall levels within an organization.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "785013a3-9793-40d6-b32c-050cca2b46ff",
                    "text": "From a software supply chain angle, development of AI systems involves complex and dynamic software supply chain.Many organizations procure AI technologies/solutions to build their AI systems. The AI systems are often assembledby using commercial or open-source AI and/or non-AI components from third parties. Despite cost efficiency, theunderlying security and integrity issues of the third party components have attracted significant attentions. Accordingto Sonatype\u2019s report on 2021 state of the software supply chain , the software supply chain attacks increased 650%in 2021, while it was 430% in 2020. RAI software bill of materials keeps a list of components used to create an AIsoftware product, which can be used by AI solution procurers and consumers to check the supply chain details of eachcomponent of interest and make buying decisions [44]. The supply chain details should at least include componentname, version, supplier, dependency relationship, author of software bill of materials data, and timestamp [45]. Thisprovides traceability and transparency about components and allows AI solution procurers and consumers to easilycheck component information (such as supply chain details and context information) and track ethical issues. RAIsoftware bill of materials enables faster vulnerability identification but may need to be updated frequently since AIsystems may evolve over time. Dependency-Track is widely used by practitioners to track components\u2019 supply chaininformation and identify known vulnerabilities. Software Package Data Exchange (SPDX) and CycloneDX aretwo standards for exchanging software bill of material information for security analysis.10 A - S 29, 2023",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "3ae5e8f7-184e-4177-bf47-c09405825c46",
                    "text": "It is urgent that the employees of an organizations begin to think through the potential implications of AI on their workand make ethical choices during the development and use of AI systems. Ethics training provides employees withknowledge on how to deal with ethical issues during development [18, 19, 21, 23, 35, 40, 46, 47, 48]. MIT offers a3-day course \u201dEthics of AI: Safeguarding Humanity\u201d introducing the ethics of AI development and deployment.The University of Technology Sydney (UTS) designed a short course \u201dEthical AI: from Principles to Practice\u201d forbusiness executives. The University of Helsinki created a free online course \u201dThe Ethics of AI\u201d for anyone who isinterested in AI ethics. Halmstad University provides a short course on critical design and practical ethics for AI .Ethics training can improve organizational awareness on RAI and sharpen the employees\u2019 RAI skills. However, RAIcovers a broad range of knowledge and skills and ethics training may only offer a subset of knowledge and skills withinlimited time.3.3 Team-level governance patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "f57c0684-2640-4dd3-bb30-dbe3c14b1972",
                    "text": "Agile development has been increasingly adopted by organizations to incrementally and iteratively develop softwaresystems, including AI systems. However, the existing agile development methods mainly focus on business valueand largely neglect the AI ethics principles. To address ethical issues in the AI system development process, agilemethods need to be extended and customized to allow consideration of ethics principles. Extension points could beartefacts, roles, ceremonies, practices, and culture [49]. Microsoft\u2019s Azure DevOps allows the customization of inheritedprocesses . Atola Technology provides customized agile methodology that contains different development practices .Apptio Targetprocess is a web-based visual tool for managing projects with flexibility at various levels .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "a40c4b27-2703-4801-9c99-cdc827d417e2",
                    "text": "AI system development involves the development of both AI and non-AI components with rapid iterations. This requiresmore frequent integration of AI and non-AI components. Compared with non-AI components, the development ofAI components that support the AI model pipeline is more experimental with still limited methodological supportand mostly done by data scientists and data engineers who are not familiar with software engineering. To bridge themethodological gap between AI and non-AI development, both AI team and non-AI team need to be clear about whatexactly is being delivered by a project and share the same sprints and use a common co-versioning registry to track theprogress [50]. The close coupling of AI and non-AI development results in improved trust within the project team andbetter communication on both system-level and model-level ethical requirements. The challenge for the tight couplingmight be that the non-AI component development is application centric while the AI component development is mostlydata centric. There have been a few attempts in industry on continuously integrating AI components/models into thesoftware, such as Microsoft Team Data Science Process , Amazon SageMaker Pipelines , Azure Pipelines .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "3ae2eec7-c172-4511-9aa5-dcef3050cabc",
                    "text": "AI pipelines are built by humans and thus may imply bias (such as racism and sexism) and produce discriminatingresults. Also, the code of AI systems is written by developers who are primarily focused on technical aspects. Buildinga diverse project team can effectively eliminate bias and improve diversity and inclusion in AI systems [33, 40, 51]. The11 A - S 29, 2023diversity can be across gender, race, age, sextual orientation, expertise, etc. A diverse team can drive creative thinkingfor greater innovation, but communication could become challenging due to different background and preference .Google published 2022 Diversity Annual Report which introduces the actions to build an inclusive workplace.Microsoft aims to integrate diversity and inclusion principles into their organization . Meta has been working oncreating diverse and inclusive work communities .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "7955716d-8b38-4d13-b68a-ba2322948a16",
                    "text": "Stakeholders may have various ethical concerns about the development and use of AI systems. Keeping stakeholderengagement throughout the AI project is essential to building AI systems responsibly. Stakeholder engagement allowsAI systems to better reflect their stakeholders\u2019 needs and expectations [18, 51, 52, 53]. There are various manners toengage stakeholders: interviews, online and offline meetings, project planning/review, participatory design workshops,crowd sourcing etc. Stakeholders may help the project team identify potential ethical risks before they become threats,but there maybe conflicting opinions from different stakeholders. Association for project management publishedten stakeholder engagement principles . Australian Public Service Commission released stakeholder engagementguidelines . Deloitte published a report on stakeholder engagement .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "0b386723-0cbe-4118-bdf0-2006d115cc49",
                    "text": "Developers primarily focus on the code and often neglect updating the documentation during rapid iterations. Theproject teams need to create and continuously update documentations for the key artifacts of AI systems that may leadto ethical issues, such as data and models. Continuous documentation using templates helps track the evolution ofartifacts and clarify the context in which AI systems are trustworthy [7, 8, 9, 10, 11, 12]. Google\u2019s model cardsenables transparent model reporting on model provenance and ethical evaluation [54, 55]. Microsoft\u2019s datasheets fordatasets allows every dataset to be accompanied with a datasheet document [56]. IBM\u2019s AI service factsheetsmaintains AI services\u2019 performance, safety, security, and provenance information [57]. Meta\u2019s method cards provides aprescriptive model specification templates that provides guidance on how to mitigate potential issues [12, 58].",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "4d27952d-afba-40a4-9b32-db4d78c9047b",
                    "text": "Ethical defects in AI systems are often detected through extensive simulation and testing in the later stages ofdevelopment. However, this may lead to significant delays to timelines and additional development cost. FMEA is abottom-up risk assessment method that can be used to identify ethical risks and calculate their priorities at the beginningof the development process [59]. FMEA was originally proposed in US Armed Forces Military Procedures documentMIL-P-1629 in 1949 . Ford Motor Company firstly introduced FMEA to the automotive industry since mid 1970s .FMEA has been extended and adopted by Toyota\u2019s Design Review Based on Failure Modes (DRBFM) for assessingpotential risk and reliability for Automotive and Non-Automotive applications. FMEA replies on experts to apply theirprofessional knowledge and experience to the ethical risk assessment process. Also, FMEA is better suited for bottomup analysis and not able to detect system-level complex ethical failures.12 A - S 29, 2023",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "39cc6d2d-dc8d-40fc-a1ad-63c2bfd59c27",
                    "text": "Undesired system behaviors or decisions could lead to serious consequence and even cause loss of human lives.FTA [59] can be used to describe how system-level ethical failures are led by small ethical failure events through ananalytical graph, i.e., fault tree. The development team can easily capture how ethical failures propagate in the AIsystem. Fault tree analysis can be done during the design or operation stage to anticipate the potential ethical risks andto recommend mitigation actions. FTA was firstly introduced by Bell Laboratories in 1962 to assess the safety of amissile launch control system . Boeing started using FTA to design civil aircrafts from 1966 . FTA was included inU.S. Army Materiel Command\u2019s Engineering Design Handbook on Design for Reliability . FTA assists in analyzingthe ethical issues related to AI system artifacts and prioritizes the issues to address that contribute to an ethical risk.Hoever, it is complex to use for large system analysis, which may involve many ethical events and gates. Also, time canhardly be captured in FTA.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "0fd216fa-f042-4b83-bb3c-66706d6a8e1a",
                    "text": "The potential users of AI systems need methods for assessing an AI system\u2019s ethical properties and compare thesystem to other systems. A verifiable claim platform can be built to support developers in making claims on ethicalproperties [60] and conducting the verification [30]. Such platform must consider the disparity of the stakeholder\u2019sviews. For example, developers might focus on reliability, while users might be interested in fairness. A verifiableclaim is a statement about an AI system or an artifact (such as model or dataset) that is substantiated by a verificationmechanism. The platform itself provides management capabilities such as claim creation and verification, accesscontrol, and dispute management. W3C Verifiable Claims Working Group aims to make expressing and exchangingclaims . The Open Web Application Security Project has published a Verifiable Claims documentation . TheEthereum Verifiable Claims is a method for off-chain variable claims .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "18fbbdc3-9f34-45b3-a482-681c86c6d23c",
                    "text": "In this section, we discuss the process patterns that can be incorporated into responsible AI system developmentprocesses. The process patterns are reusable methods and best practices which can be used by the development teamduring the development process. Fig. 5 illustrates the process patterns collected for each stage of the developmentprocess.4.1 Requirement engineering",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "30b2e891-9264-49ab-8e8e-c7fb3454d090",
                    "text": "AI has a huge potential to provide effective solutions to tackle critical problems. However, it does not necessarily addvalue to every software system. Before starting to build a software system with AI, the development team first needs toidentify the right problem to solve and the corresponding user needs. Once the problem is found and the environmentwhere the system will be situated fully explored, the development team needs to analyse whether the system and theusers benefit from AI or are they potentially degraded by AI [61]. It is essential to make sure AI adds value to the design.Oftentimes, a heuristic-based design may be easier and cheaper to develop and may work better than an AI-baseddesign in terms of predictability and transparency. AI suitability assessment can help the development team understandwhether AI can add unique value to the design but may incur additional cost and require extra resources.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "271956b2-da4a-4d6b-88a0-2eebadb63b79",
                    "text": "The development of AI systems needs to adhere to AI ethics principles which are generally abstract and domain-agnostic. Ethical requirements need to be derived from the AI ethics principles to fit into a specific domain and system13 A - S 29, 2023Figure 5: Process patterns for responsible AI system development.context [35, 62, 63, 64, 65]. Every ethical requirement specified in a requirements specification document should beput into a verifiable form (i.e., with acceptance criteria). This means a person or machine can later check that theAI system meets the ethical requirements that are derived from AI ethics principles and grounded in users\u2019 needs.Vague or unverifiable statements should be avoided [66]. If there is no way to determine whether the AI systemmeets a particular ethical requirement, then this ethical requirement should be revised or removed. Ethical risk can bereduced via considering ethical requirements from the beginning of the development process and explicitly verifyingethical requirements. Some ethical principles/requirements may not be easily quantitatively validated [35], such ashuman-centered values. There may be trade-offs between some ethical principles or requirements. The current practiceto deal with the trade-offs is usually the developers following one principle while overwriting the others rather thanbuilding balanced trade-offs through patterns.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "fdf6c363-d3f9-4d9a-9ab5-2b5598a40136",
                    "text": "The quality of an AI model is largely dependent on the quality of the data used to train or evaluate. The lifecycle ofdata consists of several phases, including data collection, cleaning, preparation, validation, analysis, and termination.Unfortunately, the scope of data requirements [18, 62] often focuses on the data analysis phase and largely neglectsthe other key phases in the data lifecycle. This may lead to downstream ethical concerns such as AI model reliability,accountability, and fairness. AI systems can hardly be trusted when the data lifecycle is poorly managed. Datarequirements need to be listed explicitly and specified throughout the data lifecycle (i.e., collection, cleaning, preparation,validation, analysis, and termination) taking into account ethical principles and involved stakeholders (i.e., dataproviders, data engineers, data scientists, data consumers, data auditors). Data requirements can be managed throughdata requirements specification. The specification could include detailed requirements for each phase in the datalifecycle, e.g., data collection requirements including data sources and collection methods. Google has created atemplate for dataset requirements specification [10].",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "9ef11b78-b2ce-49d4-a8c2-8a0a3ff49a06",
                    "text": "Requirements elicitation methods are needed to collect detailed ethical requirements from stakeholders to captureAI ethics principles. In agile processes, ethical user stories [65, 67] can help the development team elicit ethicalrequirements for AI systems and implement AI ethics principles from the early stage of development. Ethical userstories are created to serve as items of the product backlog which is to be worked on by the development team initerations (i.e., sprints). Card-based toolkits can be used to list questions related to AI ethics principles. The answers tothose questions are integrated into ethical user stories to be included in sprint backlogs. The development team or userscan write ethical user stories on cards or notes using predefined template and assign them to different sprints based onthe priority. Ethical user stories make ethical requirements traceable both backward and forward, but they are difficult14 A - S 29, 2023to scale for larger projects. Guide for Artificial Intelligence Ethical Requirements Elicitation consists of 25 cardswhich are used by the development team to answer questions related to ethical principles. The answers are used tocreate ethical requirements in the form of ethical user stories which are included in sprint backlogs. ECCOLA [67]consists of 21 cards which are divided into 8 themes and with questions to be answered by the development team.4.2 Design",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "2d5b332e-1459-4229-8441-9af67ee9fa75",
                    "text": "Compared with traditional software, the architecture of AI systems is more complex due to different levels of integration.On the one hand, AI models are developed by data scientists/engineers via an AI model pipeline. The AI model pipelineusually comprises of a sequence of automatic steps including data collection, data cleaning, feature engineering, modeltraining, and model evaluation. These steps can be viewed as software components for producing AI models froma software architecture perspective. On the other hand, the produced AI models cannot work alone and need to beintegrated into software systems that are to be deployed in the real-world [68]. The decisions made by the AI modelneed to be executed as actions via other software components. The architecture of an AI ecosystem consists of threelayers: AI software supply chain, AI system, and operation infrastructure. The focus of AI software supply chain layer isabout developing and managing AI and non-AI components [69], including AI model pipeline components, deploymentcomponents, co-versioning components, provenance tracking components, credential management components, etc.The AI system layer comprises AI components that embed AI models and non-AI components that use the outputs of AIcomponents for overall system functionalities [70]. The operation infrastructure layer is mainly about monitoring andfeedback components. Multi-level co-architecting is required to ensure the seamless integration of different components,including co-architecting AI components and non-AI components and co-architecting of different AI model pipelinecomponents. Multi-level co-architecting allows both system and model level requirements to be considered in designdecision making.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "10327426-deae-44b9-81a6-4b7c82e29b02",
                    "text": "AI ethics principles, including the human-centred values principles, are too high-level for developers who often lack thetechnical means to assure human values and ethics. Envisioning cards [52, 71] are designed to help the developmentteam operationalize human values during design processes of AI systems. The design of envisioning cards is based onfour envisioning criteria, including stakeholder, time, value, and pervasiveness. The stakeholder criterion helps thedevelopment team takes into account the effects of an AI system on both direct stakeholders and indirect stakeholders.The time criterion emphasizes the long-term implication of AI systems on human, society, and environments. Thevalue criterion guides the development team to consider the impact of AI systems on human values. The pervasivenesscriterion discusses the challenges encountered if an AI system is widely adopted in terms of geography, culture,demographics, etc. The adoption of envisioning cards comes at a relatively low cost, in terms of both money and time.However, envisioning cards are hard to scale when the number of participants are large or the AI systems are complex.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "e18e0f58-a1c8-4c8e-87b3-271415f3ed57",
                    "text": "To reduce ethical risks, AI ethics principles need to be adhered to during the design process. Design modelling methodscan be extended and used to support the modelling of AI components and the ethical aspects, including: using UML todescribe the architecture of AI systems and represent their ethical aspects [72], designing formal models taking intoaccount human values [73], using ontologies to model the AI system artifacts for accountability [74, 75], establishingRAI knowledge bases for making design decisions considering ethical concerns [76], using logic programming toimplement ethical principles [77]. UML is an option to describe the AI systems and represent their ethical aspects [72].UML extension could be a declarative graphic notation for AI system architecture. Additional stereotypes/metamodelelements can be added for responsible-AI-by-design reference architecture (e.g., to describe AI pipeline components).Use case diagrams can help define the stakeholders and explain the functions they use, which are valuable for achievingaccountability. State diagrams are useful to analyse the system states and identify the states that may cause ethicalfailures. Design patterns like AI mode switcher can take effect to change the state of an AI system to a more humancontrolled state. Sequence diagrams describe the human-AI interactions to ensure all the required explanations areprovided. Using design modelling methods are helpful to capture and analyse ethical principles in design. One15 A - S 29, 2023disadvantage when using modelling languages is the time to create and manage the models. Also, the modellinglanguages do not scale up for large and complex systems.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "9c3dbcc7-3063-4e18-ad83-66aa4106c4eb",
                    "text": "To avoid ethical disasters and gain public trust, it is necessary to model the real-world situations of AI systems withoutethical risk. System-level simulation (e.g. [59, 78, 79, 80]), is a cost-effective way to imitate real-world situations andassess the behaviors of AI systems before deploying the AI systems in real-world. A simulation model needs to bebuilt to mimic the possible behaviors and decisions of the AI system and assess the ethical impacts. The assessmentresults can be sent to the development team or potential users before the AI systems are deployed in the real-world.System-level simulation can predict potential ethical risks and avoid serious ethical disasters before deploying the AIsystems in the real-world. However, the simulation model cannot represent all the behaviors and ethical impacts of AIsystems in the real-world. The accuracy of assessment results is limited by the quality of the simulation model.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "0caf1579-1a67-462f-ad71-971b4157f082",
                    "text": "The end users often do not understand how decisions are made by AI systems and are not aware of the capabilitiesor limitations of the AI systems. The missing explainability may lead to a lack of trust and has been identified asone of the most urgent challenges of RAI to be addressed. Explainable AI (XAI) can be viewed as a human-AIinteraction problem and achieved by effective human-centered interface design. Checklists (such as question bank)are often used to help design the explainable user interfaces [3, 4, 5] and understand the user needs, choices of XAItechniques, and XAI design factors [4]. For example, the checklist questions could consider the following aspects [3]for different stakeholders: input, output, how, performance (can be extended to ethical performance), why and whynot, what if, etc. The design of conversational interfaces can be experimented via a Wizard of Oz study [81], in whichusers interact with a system that they believe to be autonomous but is actually being operated by a hidden human,called the Wizard. The conversation data is collected and analysed to understand requirements for a self-explanatoryconversational interface. There can be several ways to increase human trust in AI systems through human-centered userinterface, including anthropomorphism [33], proactive informing (such as capability/limitation of AI systems, ethicscredentials, explanations of decisions/behaviors, potential outcomes, data use information).4.3 Implementation",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "fdcbeeeb-7c7d-465f-b561-165a6db643e2",
                    "text": "APIs allow developers to solve problems more efficiently and can effectively reduce the development cost and time of AIsystems. However, there may be ethical quality issues with APIs (e.g., data privacy breaches or fairness issues). Ethicalcompliance checking for APIs is needed to detect if any ethics violation exists [82]. A knowledge-driven approachcan be adopted to detect ethics issues through ethical knowledge graphs. Ethical knowledge graphs make meaningfulentities and concepts, and their relationships in development of AI systems. With the ethical knowledge graph, therich semantic relationships between entities are explicit and traceable across heterogeneous high-level documents andvarious AI systems artifacts. Ethical knowledge graphs can be built based on the ethical principles and guidelines (e.g.,privacy knowledge graph based on GDPR [83, 84]) and technical documents (e.g., API documentation) to support theethical compliance checking for APIs.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "c5ce1e30-d2c3-4f34-bfec-3b70f128f1bf",
                    "text": "Some AI systems may provide high risk capabilities, which can be used or modified to implement harmful tasks. Toavoid harmful dual uses in AI systems [85], developers should carefully design how their AI systems can be directlyused and indirectly used (i.e., potential ways their systems can be adapted). Developers must restrict the way AIsystems are used and preventing the users from getting around of restrictions by unauthorized reverse engineering ormodification to the system design. Rather than fully opening the access to AI systems by allowing AI systems to runlocally, developers could provide AI services on cloud and control the interactions with the AI services via APIs [86].16 A - S 29, 2023For example, OpenAI\u2019s language model GPT-3 can be only integrated with AI systems through an API by approvedusers . Google Vision AI limit its facial recognition feature to a few celebrities through API .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "ef054c4e-ad57-4e64-88d3-e328fa4087b9",
                    "text": "Building AI systems from scratch can be very complex and time consuming. Very big companies usually have massiveAI investments and large volumes of data to compete in the market, while smaller companies may only have a coupleof data scientist and can hardly keep up with larger companies. To speed up the development and reduce cost, itis highly desirable and valuable to reuse the AI artifacts (i.e., AI components and/or AI pipeline artifacts) acrossdifferent applications. However, there might be ethical quality issues with the reused AI artifacts, which requiresfurther assurance mechanisms. Ethical construction with reuse means to develop responsible AI systems with theuse of existing AI artifacts that are compliant with AI ethics principles, e.g., from an organizational repository or anopen-source platform. A marketplace can be built up to trade the reusable AI artifacts, including component code,models, and datasets. Blockchain can be adopted to design an immutable and transparent marketplace enabling theauction-based trading for AI artifacts and material assets (e.g., cloud resources) [87]. Ethics credentials might berequired to be attached to the traded AI artifacts. Also, tooling support might be needed, such as model migration toolpytorch2keras ) and glue code for compatibility . Low/no code tools can also help to achieve ethical constructionwith reuse.4.4 Testing",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "3a2d17ef-880b-454f-8cc2-722ea1d09eb3",
                    "text": "Since the AI Ethical principles are very high-level, they need to be captured through ethical requirements, whichcan be viewed as the agreed commitments by the development team and customers. Ethical acceptance testing (e.g.,bias testing) is designed to detect the ethics-related design flaws and verify the ethical requirements (e.g. whetherthe data pipeline has appropriate privacy control, fairness testing for training/validation data) [88, 89, 90]. In agileprocess, the ethical requirements can be framed as ethical user stories and associated with ethical acceptance tests. Theethical acceptance tests are a contract between the customer and development team. The behavior of the AI systemshould be quantified by the acceptance tests and the acceptance criteria for each of the ethical principles should bedefined in a testable way. The history of ethical acceptance testing should be recorded and tracked, such as how and bywhom the ethical issues were fixed. A testing leader may be appointed to lead the ethical acceptance testing for eachethics principle. For example, when bias detected at runtime, the monitoring reports are returned to the bias testingleader [18, 91]. Ethical acceptance tests capture the ethical requirements and measure how well the AI system meetsethical requirements, but may need to be amended frequently as ethical requirements change.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "8e6cef22-7336-4603-976b-2a8c124db4da",
                    "text": "The ethical quality assurance for AI systems is heavily dependent on ethical acceptance testing which is aimed atdetecting and solving ethical issues in the AI system. A collection of test cases with expected results should begenerated [92] and maintained for to detect possible ethical failures in a variety of extreme situations [93]. However,there might be ethical issues within the test cases. For example, the test data may introduce fairness or privacyissues [94]. Preparing quality test cases is an integral part of ethical acceptance testing. A test case usually is composedof ID, description, preconditions, test steps, test data, expected results, actual results, status, creator name, creation date,executor name, execution date. All the test cases for verification and validation should pass the ethics assessment. Thisincludes ethical risk assessment for test steps and test data. The creation and execution information are essential to trackthe accountability of ethical issues with test cases. Ethical assessment for test cases improves the ethical quality of thedevelopment process of AI systems, but new test cases need to be continually added and assessed when there is a newethical requirement added or the operation context changes.17 A - S 29, 20234.5 Operation",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "f5519530-76d4-47fd-b8c4-e1335d9e1f77",
                    "text": "AI systems may frequently evolve due to their data dependency. When ethical performance degradation occurs over time,AI models need to be retrained with new data or features and reintegrated into AI components. The non-AI componentmay also need to be upgraded to meet new requirements or changing context. New versions of AI systems need to befrequently and continuously deployed into production environments. On the other hand, AI systems involve higherdegree of uncertainty and risks associated with the autonomy of the AI systems. Thus, there is a strong desire for variousdeployment strategies to support continuous deployment [35, 95]. There are various deployment strategies for AIsystems. Phased deployment means deploying AI systems for a subset group of users initially to reduce ethical risk [96].The new version of AI systems rollouts incrementally and serves alongside the old version. Phased deployment canbe also about automating decisions in phases to better supervise and control automation. This usually depends on thestakes of the situations and the level of confidence that users may have with automatic decisions made by AI systems.Further, A/B testing deployment[97] is a common deployment strategy undertaken in industry, where different versionsof the AI model deployed to production. The models are compared and selected based on their ethical performance.In addition, the existing reliability practices, like redundancy, are also applicable to AI components in an AI system.Multiple AI models work independently to improve the ethical performance of the AI components. Applying variousdeployment strategies helps to reduce the ethical risk. Users can be quickly redirected to the older version or the otherversion of AI systems/models. However, it is complex and expensive to adopt different deployment strategies duringoperations.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "e6a2a9e7-2a56-4dd0-956a-95b4d75fabe8",
                    "text": "The current risk-based approach to ethical principles is often a done-once-and-forget type of algorithm-level riskassessment [18, 29, 98, 99, 100] and mitigation for a subset of ethical principles (such as privacy or fairness ) at aparticular development step (such as Canada\u2019s Algorithmic Impact Assessment Tool ), which is not sufficient for thehighly uncertain and continual learning AI systems. In addition, the context of AI systems varies with the applicationdomains, organizations, culture, and regions. It is essential to perform continuous risk assessment and mitigation ofresponsible AI systems [38, 101]. The ethical risk assessment framework can be built with guided extension points fordifferent context (e.g. culture context). The risk mitigation can be designed from three aspects: reducing frequencyoccurrence, consequence size, and consequence response. Extensible, adaptive and dynamic risk assessment caneffectively ensure an AI system adheres to AI ethics principles throughout the whole lifecycle, but it might be hard tomeasure some of the ethical principles, e.g., human-centered values.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "4d04f644-4b04-4bed-9e96-1b5d7308c063",
                    "text": "AI systems involve two levels of relationships and dependencies across various AI artifacts, including supply chainlevel and system level. At the system level, there are multiple versions of AI components and non-AI components. Atthe supply chain level, there are different versions of data, model, code, and configuration, which are used to producedifferent versions of AI components [70]. At the system level, the AI components that embed AI models are integratedinto AI systems and interact with non-AI components. On the other hand, the retraining of AI models introduces newversions of data, code and configuration parameters. If federated learning is adopted, for each round of training, aglobal model is ensembled based on local models sent from participating clients [102]. It is important to capture allthese dependencies during the development process. Multi-level co-versioning provides end-to-end traceability andaccountability throughout the whole lifecycle of AI systems, but the collection and documentation of co-versioninginformation incur additional development cost. There have been many version control tools in industry focusing onsupply chain level co-versioning, e.g., MLflow Model Registry on Databricks and Amazon provenance tool , andData Version Control (DVC) . 18 A - S 29, 2023Figure 6: Product patterns for responsible-AI-by-design architecture of an AI system.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "42be40d4-1325-4ffc-ada7-501101e5e81e",
                    "text": "This section provides a system-level guidance on how to design the architecture of responsible AI systems. We presenta collection of product patterns (i.e., design patterns) for building responsible-AI-by-design into AI systems. Broadly,an AI system is comprised by three layers: (1) the supply chain layer that generates the software components whichcompose the AI system, (2) the system layer which is deployed AI system, and (3) the operation infrastructure layerthat provides auxiliary functions to the AI system. Fig. 7 presents the identified products patterns for each of the threelayers. Those product patterns can be embedded into the AI ecosystems as product features. Fig. 7 illustrates a statediagram of a provisioned AI system and highlights the patterns associating with relevant states or transitions, whichshow when the product patterns could take effect.5.1 Supply chain patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "3e4b90aa-a136-4adb-95bd-e897731084ee",
                    "text": "Bill of materials registry [44, 103] can be designed to keep a formal machine-readable record of the supply chaindetails of the components used in building an AI system, including component name, version, supplier, dependencyrelationship, author, and timestamp. In addition to supply chain details of the components, context documents (likemodel cards [54] for reporting AI models, and datasheets for the datasets [56] used to train AI models) can also beintegrated to the bill of materials registry. The main purpose of bill of materials registry is to provide traceability andtransparency into the components within AI systems so that ethical issues can be tracked and addressed [104]. Someplatforms manage bill of materials registry, such as OpenBOM , Codenotary , Snorkel Flow . Immutable datainfrastructure can store the bill of materials to enable integrity. For example, the manufacturers of autonomous vehiclescould maintain a material registry contract on blockchain to track their components\u2019 supply chain information, e.g., theversion and supplier of the third-party navigation component. Stakeholders can access the supply chain details of eachcomponent of interest in AI systems via bill of materials registry. As AI systems evolve over time, the bill of materialsmay need to be updated frequently. The cost of managing the bill of materials of all the components depends on thecomplexity of the AI system.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "ba207bfd-0acf-4847-a36c-25ac2843560b",
                    "text": "Verifiable ethical credentials can be used as evidence of ethical compliance for AI systems, components, models,developers, operators , users, organizations, and development processes [33, 105, 106]. Verifiable credentials are19 A - S 29, 2023Figure 7: Product patterns for responsible-AI-by-design.data that could be cryptographically verified and be presented with strong proofs [107]. Publicly accessible datainfrastructure needs to be built to support the generation and verification of the ethical credentials on a neutral platform.Before using AI systems, users may verify the systems\u2019 ethical credential to check if the systems are compliant with AIethics principles or regulations [105]. On the other hand, the users may be required to provide the ethical credentials touse and operate the AI systems, e.g., to ensure the flight safety of drones. Verifiable ethical credential helps increaseuser trust towards an AI system through conferring the trust that the user has with the authority that issues the credentialto AI systems, organizations that develop AI systems and the operators that operate AI systems. Such transitive trustrelationship is critical in the efficient functioning of the AI system. With an ethical credential, an AI system couldprovide proof of compliance as an incentive for the users to use the AI system, thus increase AI adoption. Ethicalcredential may be forged, which makes the verification of authenticity of the ethical credentials becomes challenging.Blockchain could be adopted to build the credential infrastructure to ensure data integrity. For example, Securekey isa blockchain-based infrastructure for ID management with support of verifiable credential.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "867e4f30-d3d2-4d0f-a70f-74a05f8a3a99",
                    "text": "Compared with traditional software, AI systems involve different levels of dependencies and may evolve more frequentlydue to their data-dependent behaviors. From the viewpoint of the AI system, it is important to know the version of the AIcomponent integrated into the system. From the viewpoint of the AI component, it is important to know what datasetsand parameters were used to train the AI model and what data was used to evaluate the AI model. Co-versioning of thecomponents or AI artifacts of AI systems provides end-to-end provenance guarantees across the entire lifecycle of AIsystems. Co-versioning registry can track the co-evolution of components or AI artifacts [70, 102]. There are differentlevels of co-versioning: co-versioning of AI components and non-AI components, co-versioning of the artifacts withinthe AI components (i.e., co-versioning of data, model, code, configurations, and co-versioning of local models andglobal models in federated learning). Co-versioning enables effective maintenance and evolution of AI componentbecause the deployed model or code can be traced to the exact set of artifacts, parameters and metadata that were used to20 A - S 29, 2023develop the model and code. MLflow Model Registry is a model repository and set of APIs that enable managementof the full lifecycle of MLflow Models, including model lineage and versioning.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "de4e75fe-a218-41cb-96b9-b5ed0974f6eb",
                    "text": "Despite the widely deployed mobile or IoT devices generating massive amounts of data, lack of training data is still achallenge for AI systems given the increasing concern in data privacy. Federated learner trains an AI model acrossmultiple edge devices or servers with local data samples. Federated learner [69, 102, 108, 109, 110, 111, 112, 113]preserves the data privacy by training models locally on the client devices and formulating a global model on a centralserver based on the local model updates, e.g., train the visual perception model locally in each vehicle. Decentralizedlearning is a variant of federated learning, which could use blockchain to remove the single point of failure andcoordinate the learning process in a fully decentralized way [114]. TensorFlow Federated is an open-sourceframework for machine learning on decentralized data sources. FATE is an open-source project that support thefederated AI ecosystem.5.2 System patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "8e5eb4c7-f0f5-4210-8796-775acc0e8ac0",
                    "text": "When to use AI at decision making points can be a major architectural design decision when designing an AI system.Adding an AI mode switcher to the AI system offers users efficient invocation and dismissal mechanisms for activatingand deactivating the AI component whenever needed, thus, defer the architectural decision to the execution time whichis decided by the end user or the operator of the AI system. AI mode switcher is like a kill switch of AI system thatcould immediately shut down the AI component and thus, stop its negative effects [61, 115, 116], e.g., turning offthe automated driving system and disconnecting it from the internet. The decisions made by the AI component canbe executed automatically or reviewed by a human expert before being executed in critical situations. The humanexpert serves to approve or override the decisions (e.g., skipping the path generated by the navigation system). Humanintervention can also happen after acting the AI decision through the fallback mechanism that reverses the system backto the state before executing the AI decision. A built-in guard can be used to ensure that the AI component is onlyactivated within the predefined conditions (such as domain of use, boundaries of competence). The end users or theoperators can ask questions or report complaints/failures/near misses through a recourse channel after observing a baddecision from AI component. Tesla autopilot has multiple driver assistance features that can be enabled or disabledduring the driving. Users maintain control of the vehicles and can override the operations by these features at runtime.Baidu autonomous mini-bus Robobus requires a staff in the seat to supervise the self-driving operations, and the buscan be switched to manual driving mode by braking.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "1abe0db6-0161-4ff4-bbeb-5c5781a98020",
                    "text": "In reliability community of software system, traditional architecture-based software reliability is based on softwarecomponent. The existing reliability practices, like redundancy, are also applicable to AI components in an AI system.In addition, reasonable combination of multiple AI models that are normally work independently could improve theperformance (e.g., accuracy) of the AI component. Multi-model decision-maker employs different models to performthe same task or enable a single decision, e.g., deploying different algorithms for visual perception. It improves thereliability by deploying different models under different context (e.g., different geo-location regions) and enablingfault-tolerance by cross-validating ethical requirements for a single decision [117, 118]. Different consensus protocolscould be defined to make the final decision, for example, taking the majority decision. Another strategy is to only acceptthe same results from the employed models. In addition, the end user or the operator could step in to review the outputfrom the multiple models and make a final decision based on human\u2019s expertise. Scikit-learn is a Python packagethat supports using multiple learning al-gorithms to obtain better performance through ensemble learning. AWS FraudDetection Using Machine Learning solution trains an unsupervised anomaly detection model in addition to a supervised21 A - S 29, 2023model, to augment the prediction results . IBM Watson Natural Language Understanding uses an ensemble learningframework to include predictions from multiple emotion detection models .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "b7bfe620-b002-47f7-99ef-6b3be960ee51",
                    "text": "N-version programming is a software design pattern to ensure fault tolerance of software [119]. Similarly, deployingmultiple redundant and identical AI components (e.g., two brake control components) can be a solution to toleratethe individual AI component with high uncertainty that may make unethical decisions or the individual adversaryhardware component that produces malicious data or behaves unethically [118]. A cross-check can be conducted forthe outputs provided by multiple components of a single type. The results are accepted only there is a consensus amongthe redundant components. The results that are not accepted automatically according to a consensus protocol can befurther reviewed by the end user or the operator of the AI system. Waymo contains multiple redundant componentsat various levels, including redundant braking, steering, and inertial measurement systems for vehicle positioning.5.3 Operation infrastructure patterns",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "b0cbdf7a-65c0-477f-9223-4d2f7ad9a37c",
                    "text": "AI components of an AI system often require continual learning based on new data collected during operation of the AIsystem. Continuous ethical validator deployed in an AI system continuously monitors and validates the outcomes of AIcomponents (e.g., the path recommended by the navigation system) against the ethical requirements [97, 101]. Theoutcomes of AI systems are about whether the AI system provides the intended benefits and behaves appropriatelygiven the situation. The time and frequency of validation can be configured. Version-based feedback and rebuild alertare sent when the predefined conditions regarding the ethical requirement are met. AWS SageMaker Model Monitorcontinuously monitors the bias drift of the AI models in production. Qualdo is an AI monitoring solution thatmonitors data quality and model drift. Azure Machine Learning uses Azure Monitor to create monitoring data.Azure Monitor is a full stack monitoring service.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "ea8379db-d3d2-4b54-9352-520397b9f870",
                    "text": "Given AI systems are of high stake, it is risky to run the entire system in the same execution environment. Ethicalsandbox can be applied to isolate an AI component from other AI components and non-AI components by running theAI component separately in a safe environment [120], e.g. sandboxing the unverified visual perception component.Thus, the AI component could execute without affecting other components and the output of the AI system. Ethicalsandbox is an emulated environment with no access to the rest of the AI system. An emulation environment duplicatesall the hardware and software functionality of an AI system. Thus, developers could run an AI component safely todetermine how it works and whether it is responsible before widely deploying the AI component. Maximal tolerableprobability of violating the ethical requirements should be defined as ethical margin for the sandbox. A watch dogcan be used to limit the execution time of the AI component to reduce the ethical risk, e.g., only activating the visualperception component for 5 mins on the bridges built especially for autonomous vehicles. Fastcase AI Sandboxprovides a secure platform for the users to upload dataset and do data analysis in a safe environment. AI Sandboxprovides an AI execution and RESTful interface that could be used by modern programming languages.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "5bd2f7ab-f27c-44c0-958f-671dcf6ace5d",
                    "text": "22 A - S 29, 2023The ecosystem of AI systems involves broad ethical knowledge, such as AI ethics principles, regulations, and guidelines.Such ethical knowledge is scattered and is usually implicit or abstract to end users or even developers and datascientists who primarily without legal background and focus more on the technical aspects of AI systems. Ethicalknowledge base, such as a knowledge graph, makes meaningful entities and concepts, and their relationships in design,implementation, deployment, and operation of AI systems [74, 76, 121]. With the ethical knowledge based, the richsemantic relationships between entities are explicit and traceable across heterogeneous high-level documents on onehand, and different artifacts across the AI system lifecycle on the other hand. Thus, ethical requirements of the AIsystem can be systematically accessed and analyzed using the ethical knowledge base. Awesome AI guidelinesaims to provide a mapping between ecosystem of guidelines, principles, codes of ethics, standards and regulationaround artificial intelligence. The responsible AI community portal is provided by AI Global, which is an evolvingrepository of reports, standards, models, government policies, datasets, and open-source software to inform and supportresponsible AI development. Responsible AI Knowledge-base is a knowledge base of different areas of using anddeveloping AI in a responsible way.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "15a29c51-ca8e-42d0-8c78-2cc3cc0d9778",
                    "text": "Simulation is designed to imitate a real-world situation. Before running AI system in real-world, it is important toperform system-level simulation through an ethical digital twin running on a simulation infrastructure to understand thebehaviors of the AI system and assess ethical risks in a cost-effective way. Digital twin [122] is introduced by NASA asa digital representation of a real system used in lab-testing activities. The digital twin of an AI system could be usedto represent the behaviors of the AI system and forecast change impacts. Ethical digital twin can also be used duringoperation of the AI system to assess the system\u2019s runtime behaviors and decisions based on the simulation model usingthe real-time data. The assessment results can be sent back to alert the system or user before the unethical behavior ordecision takes effect [79]. Vehicle manufacturers can use the ethical digital twin to explore the limits of autonomousvehicles based on the collected real-time data, such as NVIDIA DRIVE Sim and rfPro .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "cfd19321-a59d-4af0-8d66-b4beb0cbaf78",
                    "text": "Incentive mechanisms are effective treatments in motivating AI systems and encouraging the stakeholders involved in theAI system ecosystem to execute tasks in a responsible manner. An incentive registry records the rewards that correspondto the AI system\u2019s ethical behavior and outcome of decisions [123, 124], e.g., rewards for path planning withoutethical risks. There are various ways to formulate the incentive mechanism, for example, using reinforcement learning,or building the incentive mechanism on a publicly accessible data infrastructure like blockchain [124]. Traditionalincentive mechanisms for human participants include reputation-based and payment-based. However, it is challengingto formulate the form of rewards in the context of responsible AI as the ethical impact of AI systems\u2019 decisions andbehaviors might hardly to be measured for some of the ethical principles (such as human values). Furthermore, theincentive mechanism needs to be agreed by all the stakeholders who may have different views on the ethical impact.In addition, there may be trade-offs between different principles, which makes the design harder. The Open ScienceRewards and Incentives Registry incentivizes the development of an academic career structure that fosters outputs,practices and behaviors to maximize contributions to a shared research knowledge system. FLoBC is a tool forfederated learning over blockchain which utilizes a reward/punishment policy to incentivize legitimate training, and topunish and hinder malicious trainers.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "499b3add-a972-4b4b-8572-53d3e2969bef",
                    "text": "Black box was introduced initially for aircraft several decades ago for recording critical flight data. The intention ofadding a black box to aircrafts is to collect evidence of the actions of system and the surrounding context informationfor analysis after near misses and failures. The near misses and failures are specific to the use cases. Although theprimary usage of a black box is accident investigation, black boxes are useful for other purposes. Data collection and23 A - S 29, 2023Figure 8: Top 5 major industry players on responsible AI according to the number of tools.24 A - S 29, 2023the analysis could support improvement of the system. The purpose of embedding an ethical black box in an AI systemis to investigate why and how an AI system caused an accident or a near miss. The ethical black box continuouslyrecords sensor data, internal status data, decisions, behaviors (both system and operator) and effects [125, 126, 127].For example, an ethical black box could be built into the automated driving system to record the behaviors of the systemand driver and their effects [125]. All these data need to be kept as evidence with the timestamp and location data.Designing the ethical black box is challenging as the ethical metrics need to be identified for data collection. Also,design decisions need to be made on what data should be recorded and where the data should be stored (e.g. using ablockchain-based immutable log or a cloud-based data storage). RoBoTIPS aims to develop an ethical black box forsocial robots, to enable the explainability of their behavior.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "5301e0e3-c48c-422f-afae-61132ff5b5d8",
                    "text": "When an accident happens, there might be more than one AI systems or multiple AI components within on AIsystem involved (e.g., multiple autonomous vehicles in an accident). The data collected from each involved AIsystems/components might conflict with each other since the individual AI system/component may have their ownperception. Global-view auditor is a component that collects information from multiple AI components/AI systems,process the information to identify discrepancies among the information collected [128]. Based on the result, theglobal-view auditor may alert the AI system/component with wrong perception, thus, avoid negative impacts or identifyliability when negative events occur. This pattern can be also used to improve the decision-making of an AI systemby taking the knowledge from other systems. For example, an autonomous vehicle may increase their visibility usingthe perceptions of others to make better decisions at runtime. Global-view auditor enables accountability that coverdifferent perceptions of AI components/systems that are involved and redresses the conflicting information collectedfrom multiple AI components/ systems.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "9fcbb3da-2884-452b-a568-06283c3f9613",
                    "text": "The challenge of responsible AI has attracted significant attention in both industry and academia. To achieve responsibleAI, there have been nearly 100 high-level AI ethics principles and guidelines issued by governments, organizations,and companies [1]. Some degree of consensus around AI ethics principles has been achieved [2]. A principle-basedapproach allows technology-independent operationalization of responsible AI. However, these principles are veryabstract and high-level for stakeholders of AI systems to use in practice.Significant efforts have been put on algorithm-level solutions which mainly focus on a subset of principles. Fig. 8 liststhe top 5 major industry players on responsible AI according to their number of responsible AI tools based on theresults of our MLR study. Most of these tools focus on privacy, security, reliability, safety, fairness, and explainabilityfrom an AI model perspective. More work is needed on transparency, accountability, contestability, human-centeredvalues, and human, societal, and environmental wellbeing, particularly from a system perspective.Overall, AI ethics principles need to be operationalized in the form of concrete patterns and best practices that are usableby AI developers and other stakeholders to build up responsible AI systems. Some add-hoc sets of guidebooks, questionbanks, checklists and templates have started to appear. Microsoft\u2019s Human AI Interaction (HAX) Toolkit providesa set of HAX guidelines and patterns . However, those guidelines and patterns only focus on interaction designand do not provide any guidance on development and governance. Google\u2019s People AI Guidebook (PAIR) summaries23 design patterns which mainly address some of the AI ethics principles for AI models, including explainability,privacy, reliability. Process and governance guidelines are not discussed in Google\u2019s PAIR. Although OECD providesa framework of tools for trustworthy AI [129], the framework largely contains categorised but disjointed softwaretools, lacking process-related linkages. Thus, a systematic and operationalized guidance for AI system stakeholders isrequired throughout the entire lifecycle of AI systems.There have been a few survey papers on operationalizing responsible AI [98, 130, 131]. However, the findings andinsights in these papers are still around principles and do not provide concrete and actionable guidelines for stakeholdersto use in practice. Our previous roadmap paper [50] discusses the current state and identify the critical researchchallenges in the area of software engineering for responsible AI based on an initial systematic literature review. Thispattern catalogue paper is built on top of the published roadmap and provides a comprehensive list of concrete patternsfrom multi-level governance patterns to process and product patterns based on the results of a multivocal literature25 A - S 29, 2023review. In the Responsible AI Pattern Catalogue, we present structured knowledge about the patterns, including context,problem, solution, benefits, drawbacks, and known uses. The full version of the Responsible AI Pattern Catalogue isavailable online .",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "ad7ab3e4-1730-4d49-b913-9fc38e87ff7c",
                    "text": "External Validity - First, \u201dresponsible AI\u201d is loosely defined with many terms that refer to emerging technologies in thisarea. There is a set of terms currently being used in the community to mean largely the same thing: responsible AI, AIethics, ethical AI, trustworthy AI, and trust in AI. This issue has been addressed by including search terms that arebeing used interchangeably in the search string to ensure that all the relevant work were covered. Another issue is thatmany solutions were initially designed only for addressing one of the AI ethics principles but could be identified as apattern and extended to implement responsible AI. To mitigate this threat, we included all the AI ethics principles in thesearch string as supplementary terms.Internal Validity - To mitigate the threat of not finding all relevant studies, we performed a rigorous search using definedkeywords and executed snowballing that allows us to recover the missing studies from the literature. To address thebias, one researcher performed the screening of titles, abstracts and full-texts. The other researcher evaluated a randomsample of the selected studies after screening to check the consistency of their inclusion/exclusion decisions.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                },
                {
                    "id": "e98d534b-88fa-400d-bb64-99edb6de8948",
                    "text": "To operationalize responsible AI, this paper adopts a pattern-oriented approach and presents a comprehensive responsibleAI pattern catalogue that AI system stakeholders can utilise to ensure the developed AI systems are trustworthythroughout the entire governance and engineering lifecycle, from multi-level governance patterns to concrete processand product patterns. These patterns offer a systematic and actionable system-level guidance with consequence analysisand well-known uses for AI system stakeholders to reference during the governance and development processes. Weare currently building a Question Bank and a software tool for AI risk assessment, which will use the responsible AIpattern catalogue as one of the knowledge sources to recommend mitigation strategies. We also plan to validate theutility, usability and effectiveness of the pattern catalogue and the supporting tools in industrial projects.",
                    "reference": "[1] Qinghua Lu, Liming Zhu, Xuyen Xu, Jon Whittle, and Didar Zowghi. 2022. Responsible AI pattern catalogue: a multivocal literature review. arXiv:2209.04963. Retrieved from https://arxiv.org/pdf/2209.04963"
                }
            ]
        },
        {
            "paper_title": "Towards implementing responsible AI",
            "authors": "C Sanderson, Q Lu, D Douglas, X Xu\u2026",
            "publication_info": "\u2026 Conference on Big \u2026 - ieeexplore.ieee.org",
            "paper_url": "https://arxiv.org/pdf/2205.04358",
            "chunks": [
                {
                    "id": "fe890dd2-3d7c-416e-a66e-1bd7a5058963",
                    "text": "",
                    "reference": "[1] Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big \u2026 - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
                },
                {
                    "id": "b8c3d58d-2123-4f4c-b8d4-62d99929c81f",
                    "text": "",
                    "reference": "[1] Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big \u2026 - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
                },
                {
                    "id": "03148591-b914-45c6-9b9a-d94da04dc66a",
                    "text": "",
                    "reference": "[1] Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big \u2026 - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
                },
                {
                    "id": "281f6b45-6622-4110-b788-b227e05e552d",
                    "text": "",
                    "reference": "[1] Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big \u2026 - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
                },
                {
                    "id": "cd2ac550-729d-4958-ae02-b4a2744cc070",
                    "text": "Assessment of ethical risk. Throughout the interviews,many types of ethical risks were discussed and approachedin various ways. As an example, for ensuring fairness theincomplete data problem was noted: \u201cyou can be limited inwhat data [you have] available to use in the \ufb01rst place\u201d (X01).However, ethical risks were typically considered and checkedin isolation, and were mostly around data and ML models. Wefurther observed that while rudimentary ethical risk assessmentframeworks were used in practice, they were not speci\ufb01callytailored to AI system development: \u201cthere was a privacyimpact assessment; we went through a lengthy process to un-derstand the privacy concerns and built in provisions to enableprivacy controls\u201d (X10). These types of risk assessments gen-erally follow a do-once-and-forget approach, which does nottake into account AI systems that may continually learn andadapt. It was also argued that adherence to the Transparency &Explainability principle can be merely an interim target relatedto operational risk: \u201conce I know that [the system] works mostof the time I don\u2019t need explainability [and] transparency. It\u2019sjust temporary to establish the risk pro\ufb01le\u201d (X11). Overall,it appears there is a lack of a comprehensive system-levelchecklist that covers all relevant ethics aspects throughout thefull lifecycle of AI systems.Trust. Many interviewees acknowledged the importance ofhuman trust in AI; for example: \u201ca lot of the work that wedo trust comes as an important factor here, that a user [...]who takes that information, wants to be able to trust it\u201d(X09). Gaining and then maintaining trust from the providersof data that is used to train the AI system was identi\ufb01edas an important factor (and potentially an obstacle) for thedevelopment of reliable AI systems (X02). Contestability cancontribute to trust: \u201cit can be very hard to get people to trustan analytical system that is just telling them to do somethingand does not give them the choice to disagree with the system\u201d(X15). Furthermore, evidence needs to be presented to enabletrust by humans (X12, X18). The evidence can be in formssuch as demonstrated reliable operation, and explainability ofthe results produced by an AI system (X21).Credentials. Several interviewees suggested that by attach-ing ethics credentials to AI components and products canenable a degree of responsible AI; for example: \u201cGetting those certi\ufb01cates, it always helps. As long as there is standardisationaround it.\u201d (X13). Even partial certi\ufb01cation can be useful,related to the underlying hardware used by AI systems: \u201cA lotof hardware is actually certi\ufb01ed. [In] full size aviation, youhave at least a certi\ufb01cation. So when you buy something youget some sort of guarantees\u201d (X12).Development type. Two development types were of-ten mentioned in the interviews: requirements-driven andoutcome-driven, as well as a mixture of the two [3]. An itera-tive approach to outcome-driven development was mentioned:\u201c[development] is a continual and [iterative] process: humansneed to continually evaluate the performance, identify [gaps]and provide insight into what\u2019s missing. Then go back toconnect data and re\ufb01ne the model\u201d (X02).System-level development tools. The signi\ufb01cance of asystem-level approach to AI development was recognised byseveral interviewees; for example: \u201c[the] AI was designed anddeployed as an end-to-end solution, it wasn\u2019t that AI sat in themiddle [...] it actually had to sit within the system\u201d (X14).Lack of tools to help with addressing ethics principles wasmentioned. For example, manual work is currently requiredavoid accidental collection and use of sensitive data: \u201cwe hadto go through a lot of data and make sure that there was nota single frame with a person in it\u201d (X13).",
                    "reference": "[1] Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big \u2026 - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
                },
                {
                    "id": "53429e46-0ef1-4a6e-aa38-9625534f686c",
                    "text": "Ethics requirements. Privacy and security were the mostdiscussed requirements across the interviews. Some principles(eg. HSE Wellbeing), were often expressed only as indirectobjectives instead of veri\ufb01able requirements and outcomes:\u201cthe project leader might frame the project with we\u2019re workingon improving [grass species] yield forecasting using machinelearning. You do feel good about working on projects thatprovide environmental bene\ufb01t\u201d (X09).Rather than relying purely on software engineers, ethicsrequirements may need to be analysed and veri\ufb01ed by arange of specialists and domain experts. It was noted that AIsystem developers may opt to seek legal advice to con\ufb01rmthat an AI system is following existing legal rules in agiven application domain (X06). In some cases, clients wereunaware of privacy requirements regarding use of personallyidenti\ufb01able information: \u201cwe had to contact our privacy of\ufb01certo [...] con\ufb01rm that\u2019s the case, then we had to escalate thatto the client, to let them know of that potential issue\u201d (X10).To address the Reliability & Safety principle, the approachof fail-safe by design was recommended, although with thecaveat that \u201cthere\u2019s only so much you can think ahead aboutwhat those failure modes might be\u201d (X16).Responsibility scope. There were various meanings andinterpretations of responsible AI. The interviewees consideredthe following three interpretations [28] as important: norma-tive (ie. behaving in positive and socially acceptable forms),possessive (ie. duty and obligation), descriptive (ie. worthyof response/answerable). The exact meaning of responsibilityin the context of autonomous aerial systems was unclear:\u201cwhat happens if [the] remote pilot is really there, \ufb02icks the[disable] switch and the system doesn\u2019t react? The remotepilot is not always in full control of [the UAV] because oftechnical reasons [like a failed communications link]\u201d (X12).Moreover, the temporal span of responsibility may also needto be taken into account: \u201cwhether the stuff works in 10 years,it\u2019s not under our control\u201d (X11).",
                    "reference": "[1] Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big \u2026 - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
                },
                {
                    "id": "c1f9cb0f-9f04-4861-9c19-419cfa9b3dab",
                    "text": "Incorporation of AI. Incorporating AI into a system canbe a major architectural decision during the system designprocess. A closely related design decision is whether thejudgements, rathersystem allows users to make the \ufb01nalthan purely relying on the AI component. This may involveallowing the AI component to be optionally disabled, orchanged from decision mode to suggestion mode. For example,overriding AI provided decisions in medical contexts is seenas important: \u201cthere was actually a de\ufb01ned process where ifa patient was not \ufb02agged as being high risk, [...] clinicianswere still allowed to include the patient into the next step clin-ical review\u201d (X18). Arguments against incorporating AI intosystems include reduced interpretability due to the complexityand/or vastness of machine learning models: \u201ctraditionally, instatistics, people have used simpler linear models and thatkind of thing. They really worry about the parameters andthey assign meaning to the parameters\u201d (X21).Trade-offs. Many tensions and trade-offs were noted be-tween various ethics principles. For example, reliability is intension with fairness: \u201cwe are in the spot where by design werestrict the variance as much as possible to make it easierto \ufb01nd a signal\u201d (X11). Reliability is also in tension withprivacy: \u201cif you [have] other ways of protecting privacy thatdon\u2019t involve aggregating, then you can be actually gettingbetter distributional properties\u201d (X01). The reliability of AIcan greatly depend on the quantity and quality of the trainingdata: \u201cif you\u2019re training a model without a lot of data, youcan actually get some really weird results\u201d (X09). Obtaininga suf\ufb01cient number of samples to ensure reliability can bechallenging, as in some contexts (such as genomics) acquiringeven one sample can be high in terms of \ufb01nancial and/or timecosts, and may also involve privacy issues (X03).In many trade-off cases, one principle was chosen in favourof other principles, in contrast to building balanced trade-offsbetween principles, where a cohort of stakeholders collectivelyevaluates value and risk [30]. In cases involving tensionsbetween privacy and reliability, federated learning was sug-gested: \u201c[various] research institutions from around the worldcan collaborate, because they don\u2019t have to give up their data;they don\u2019t have to share their data\u201d (X03).Reuse. The reuse of trained AI models and related com-ponents was desired, since training models and building com-ponents from scratch can be costly and/or time-consuming.Furthermore, there was also a desire to reuse and/or itera-tively adapt the overall design and architecture of existing AIsystems, in order to allow training with new datasets (X13).The downside of the such reuse and adaptation includesaccumulation of technical debt over time, leading to increasedmaintenance issues [23], which in turn may affect reliability. Explainability. Interviewees considered practical aspectsof explainability and interpretability by adopting human-centered approaches that take into account the background,culture, and preferences of users [2]. Users are more likelyto trust the recommendations made by AI systems if there issupporting evidence for a given prediction/recommendation:\u201cthere have been instances where we\u2019ve chosen an explainablemodel which has slightly [lower] performance [than] a non-explainable model which has higher performance but would beharder to convey the reasoning behind the prediction\u201d (X18).Explainability was also seen as a waypoint to establish trust:\u201c[explainability] is just a temporary thing until people knowit works\u201d (X11). Explainability was often discussed in termsof the interface design: \u201c[...] nobody seems to ask about,what\u2019s the predictive performance of the algorithm [in theinitial stakeholder meeting]? [Instead] can I look at yourinterface and [...] see a couple of patient risk pro\ufb01les andthen understand that\u201d (X18).",
                    "reference": "[1] Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big \u2026 - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
                },
                {
                    "id": "475fc459-c369-410f-82f2-29aa3e097647",
                    "text": "Continuous validation. To ensure adherence to ethicsrequirements, continuous monitoring and validation of AIsystems post-deployment was seen as necessary: \u201cit\u2019s up tous to come with technology that makes it acceptable forthem to implement measurements [...] and being able toprove compliance\u201d (X07). Furthermore, awareness of potentialmismatches between training data and data seen in operationis necessary to ensure AI models are used for their intendedpurpose (X04). For maintaining the reliability of AI systemsover the long term, model updates and retraining on newerand/or more comprehensive data were noted as important:\u201cIf you build a model on 10 year old data, then you\u2019re notrepresenting the current state of risks for certain disease. Asa minimum, [recalibration] on new data would probably bemore meaningful\u201d (X18).Traceability of artefacts. Two main approaches were oftenidenti\ufb01ed related to traceability, provenance and reproducibil-ity: (i) tracking the use of AI systems, and (ii) keeping track ofinformation related to model provenance (eg. code and trainingdata) [10], [16]. Both aspects were also seen as useful forimproving transparency and accountability, which in turn canbe useful for building trust.Usage tracking was additionally seen as helpful for evalu-ating the effect of user interventions on system performance:\u201c[the system] suggested doing one scenario, we chose to doanother, this is the result we got [...] did we do the job thatwe expected? Or did we do the job that the system expected?\u201d(X15).Keeping logs and previous versions of data/models/systemswas suggested: \u201cWhen the system gets complex, you have tokeep more evidence along the way. Version control, and theimmutable log. You don\u2019t want people to tamper this [...] afterthings went wrong\u201d (X02). Many interviewees used estab-lished software development management tools to explicitlykeep previous revisions; for example: \u201cAny software we aredeveloping is in Bitbucket, internal con\ufb01guration managementsystem\u201d (X17).IV. L RThe interviewees for this investigation were selected throughsolicitation emails and recommendations, which may pose athreat to internal validity. While selection bias is a possibilitywhen the interviewees are not randomly selected, the threatis partially alleviated as the interviewers had no contact withthe interviewees beforehand. Moreover, the interviewees hadvarious backgrounds, roles, and genders.A saturation of \ufb01ndings was reached after interviewing 21participants. To reduce the risk of missing information andinterviewer subjectivity, each interview included three inter-viewers with diverse research backgrounds. The interviewersworked jointly to pose questions, which can aid in increasingthe range and depth of inquiry, as well as reducing thelikelihood of subjective bias on the stopping point of questions.This investigation was conducted within one organisation,which may pose a threat to external validity; the opinionsprovided by the interviewees may not be representative of thelarger AI development community. To reduce this threat, weensured that the interviewees had various roles and degreesof expertise, and worked on a variety of research areas andprojects (for both internal and external customers).V. C RExisting AI ethics principles are typically high-level anddo not provide tangible advice on how to design and developresponsible AI systems. In this work we have presented anempirical investigation with the aim of increasing the under-standing of practitioners\u2019 perceptions of AI ethics principles,as well as their possible implementation.A recent interview study involving high-level organisationalleaders provides empirical evidence that the following four setsof practices are likely to be required for the implementationof AI ethics principles [24]: (i) governance, (ii) AI systemdesign and development, (iii) competence and knowledgedevelopment, and (iv) stakeholder communication.The investigation presented here speci\ufb01cally focuses on theproblem of implementing high-level AI principles through thelens of AI system design and development, adapting processesused in software engineering. The salient \ufb01ndings cover fouraspects: (i) high-level view, (ii) requirements engineering,(iii) design and implementation, (iv) deployment and opera-tion.The observations and comments given by the intervieweesfor this investigation provide insights into the challenges thatpractitioners are facing when dealing with AI ethics issuesduring research, development and deployment. The sugges-tions presented in this work are necessarily not exhaustive, asthey re\ufb02ect the content of the interviews and the surroundingdiscussions. However, this work complements recent literaturesuch as [15], [18], [21], [25], [26], [27], which allows a morecomplete picture to be obtained on the translation of AI ethicsprinciples into practice.",
                    "reference": "[1] Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big \u2026 - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
                },
                {
                    "id": "7700707e-287e-4033-8eab-a9978daeaeb6",
                    "text": "",
                    "reference": "[1] Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big \u2026 - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
                },
                {
                    "id": "a4a3485d-224f-4870-9a3c-68e3e7ee2f40",
                    "text": "",
                    "reference": "[1] Christopher Sanderson, Qing Lu, Dhruv Douglas, and Xiaohong Xu. 2022. Towards implementing responsible AI. In Conference on Big \u2026 - IEEE Xplore. Retrieved from https://arxiv.org/pdf/2205.04358"
                }
            ]
        },
        {
            "paper_title": "Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations",
            "authors": "M Madaio, S Kapania, R Qadri, D Wang\u2026",
            "publication_info": "The 2024 ACM \u2026 - dl.acm.org",
            "paper_url": "https://dl.acm.org/doi/pdf/10.1145/3630106.3658988",
            "chunks": [
                {
                    "id": "e71bcaa7-a92a-4411-9a45-5584e4e9c431",
                    "text": "Prior work has developed responsible AI (RAI) toolkits and studiedhow AI practitioners use such resources when practicing RAI. How-ever, AI practitioners may not have the relevant skills or knowledgeto effectively use RAI resources\u2014particularly as pre-trained AI mod-els have enabled more people to develop AI-based applications. Inthis paper, we explore current practices and aspirations for learningabout RAI on-the-job, by interviewing 16 AI practitioners and 24RAI educators across 16 organizations. We identify AI practition-ers\u2019 learning pathways for RAI, including information foraging andinterpersonal learning; the orientations of RAI learning resources to-wards computational and procedural approaches to RAI; and aspira-tions for RAI learning, including desires for more sociotechnical ap-proaches to understand potential harms of AI systems\u2014aspirationsthat can be in tension with organizational priorities. We contributeempirical evidence of what and how AI practitioners are learningabout RAI, and we suggest opportunities for the field to better sup-port sociotechnical approaches to learning about RAI on-the-job.CCS CONCEPTS\u2022 Social and professional topics \u2192 Computing education; Codesof ethics; \u2022 Human-centered computing \u2192 Empirical studiesin HCI.KEYWORDSResponsible AI, sociotechnical AI, on-the-job learning, training",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "476e65b9-83d1-4d41-8584-a7de2af03f82",
                    "text": "Artificial intelligence (AI) systems are increasingly integratedinto public life, where they have led to harms, particularly foralready marginalized communities [85, 115]. To address this, re-searchers, policymakers, civil society organizations, and many oth-ers have developed principles [70], toolkits [29, 42, 91, 141], play-books [100, 133], documentation tools [32, 50, 90, 100, 105, 106, 122],and other interventions to lead to more responsible AI (RAI) de-velopment practices [23, 37, 54, 93, 130]. Policymakers have begunto formalize such processes as guidelines, standards, or require-ments for AI design and deployment [1, 46, 67, 68, 126]. However,recent work suggests that AI practitioners may lack the skills andknowledge needed to incorporate RAI practices into their AI designand development workflows [9, 30, 135, 142]. University courseson ethics in technology and computer science [39, 40, 48, 119] mayhelp train future AI practitioners in potentially relevant topicsfor addressing RAI issues, but many working practitioners maynot have had the opportunity to take university ethics courses [cf.72, 76]. Although prior research investigated how AI practitionersare engaging in responsible AI [e.g., 6, 64, 81, 102, 132, 135] and howethics is incorporated into universities\u2019 computer science courses[e.g., 39, 40, 48, 101], this paper instead asks how AI practitionersare learning about RAI on-the-job. In this study, we explore:RQ1: What and how are AI practitioners currently learningabout responsible AI?RQ2: What are AI practitioners\u2019 and RAI educators\u2019 challengesand aspirations for learning about RAI?To investigate these questions, we interviewed AI practitioners(across job roles, application types, and domains) with experiencewith responsible AI (n=16), and people with experience developinglearning resources for AI practitioners (n=24), whether in a formaleducational role or not. Participants were from 16 organizationsof varying sizes and types, including technology companies andnonprofits. In this paper, we identify AI practitioners\u2019 learning path-ways for RAI, including information foraging and interpersonallearning; we highlight how the orientations of RAI learning re-sources tend to reinforce computational and procedural approachesto RAI; and we identify practitioners\u2019 and educators\u2019 aspirations forRAI learning that draws on sociotechnical ways of understandingpotential harms and helps learners apply RAI in the workplace. Wecontribute empirical evidence of AI practitioners\u2019 and educators\u2019current practices, challenges, and aspirations for learning about RAI;and we contribute implications for the design of RAI learning oppor-tunities that emphasize the sociotechnical nature of algorithmic im-pacts and that open space for critical reflection in AI development.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "915ae959-6220-4390-8737-2e045e33e1da",
                    "text": "Recent calls have urged computer scientists to anticipate and proac-tively address the potential negative impacts of computing [e.g.,33, 45, 73, 74]. In response, computer science (CS) departmentsin higher education are embedding tech ethics in their curricula[56, 63, 66, 69, 96, 97, 113, 143] either through standalone coursesor modules in standard CS courses [40, 48]. Fiesler et al. [40] re-viewed syllabi from hundreds of tech courses with ethics contentto identify the topics covered, the instructors, and the hosting de-partments. A related analysis revealed that only 12% of nearly 200AI/ML courses included any mention of an ethics-related topic onthe syllabus, and among those that did, these topics were coveredin the last few weeks of the course \u201cas time allows\u201d [48]. Moreover,Raji et al. [101] reported a lack of support in tech ethics coursesfor cross-disciplinary work, where the language of the syllabi mayreinforce a hierarchy of knowledge [cf. 49] that implicitly values\u201chard\u201d or \u201cpractical\u201d skills from computer science over \u201csoft\u201d skillsfrom humanities disciplines. A complementary study explored CSeducators\u2019 perspectives on tech ethics, finding that while manyCS educators felt it was important to teach ethics, they found itdifficult to make time to teach it alongside other topics [119].Some courses focus on ethics in machine learning (ML) and AI,as opposed to CS more generally [e.g., 14, 62, 80, 94, 103, 107, 114,116, 137]. Weerts and Pechenizkiy [137] discuss the challenges ofencouraging engineering students to link engineering and model-ing choices to real-world outcomes and impacts. To address this gap,Rea et al. [103] and Orchard and Radke [94] demonstrate how usingscenarios and case studies may help ML students better understandand identify the social implications of AI systems. Others, like Lewisand Stoyanovich [80] and Shapiro et al. [114] use stages in a typicalAI development lifecycle to foster reflection on ethical issues, usingstudents\u2019 personal data [114] and transparency tools as \u201cobjects-to-think-with\u201d [80]. Meanwhile, Shen et al. [116] and Hod et al. [62]foster dialogue and reflection among students using \u201cvalue cards\u201d[116] and case studies from law and data science to encourage mul-tidisciplinary dialogue [62]. However, formal university coursesare not the only pathway for learning about CS, data science, or AI[41, 72, 76, 109, 144]. Thus, in this paper, we investigate workingAI professionals\u2019 ongoing learning about RAI on-the-job.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "0abae163-4cbd-4325-87f1-3d2c3de8a979",
                    "text": "Recent surveys reveal most data scientists and ML engineers ac-quire and refine their skills on-the-job [71, 72]. Many transitioninto AI from other roles, learning from online courses [72], from\u201cpractitioner\u2013instructors\u201d [76], or from other self-directed learningmethods [26]. However, this prior work on data scientists\u2019 learn-ing pathways has not focused on the ways that data scientists andML engineers learn about topics related to ethics or responsibleAI. Substantial prior work has empirically studied how AI practi-tioners (including data scientists, ML and software engineers, userexperience (UX) researchers and designers, and others involved inbuilding AI products) engage in the work of responsible AI in theworkplace, including the organizational dynamics and incentivesthat shape that work [6, 64, 81, 82, 102, 132, 135]. For instance, AIpractitioners are conducting assessments of the fairness of models[e.g., 81, 136], documenting datasets and models [e.g., 100], explor-ing how AI models might lead to harms during UX prototypingand evaluation processes [e.g., 135], and leading adversarial testingof potential model failures [38, 47, 95].Research suggests that the work of responsible AI entails newforms of work practices that may be outside the norm for tradi-tional machine learning and AI development [9, 13]. For instance,empirical studies of how AI teams are adopting responsible AI prac-tices suggests that members of AI teams are informally taking oneducator roles to support their peers\u2019 learning, as in AI teams usingresources such as the People+AI Research guidebook to learn (andteach others on their team) about human-centered AI [142], cross-functional teams creating their own educational resources aboutRAI to bridge disciplinary boundaries [30], and UX professionalsleading \u201cresponsibility lifts\u201d at the start of a new project to fosterlearning about RAI [135]. Recognizing this need for additional learn-ing about RAI topics, researchers have identified trainings as a keydimension of organizational maturity for RAI [60, 131, 134]. At thesame time, some companies have developed some resources for in-formal learning or training about responsible AI [27, 112]; however,it is unclear to what extent these resources are meeting the needs ofAI practitioners to effectively engage in the work of responsible AI.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "0a3ec739-3a31-45aa-af34-cff500acae8c",
                    "text": "To investigate our research questions, we conducted semi-structuredinterviews with two groups of participants, for a total of 40 par-ticipants across 16 organizations. We recruited both groups of par-ticipants using a combination of direct emails to contacts in ourprofessional networks, recruitment messages on email lists andsocial media, and snowball sampling. We recruited n=16 industrypractitioners working on teams designing or developing AI prod-ucts or services at four technology companies of various sizes, whowe refer to in this paper as \u201cAI practitioners.\u201d Our inclusion criteriawas that they reported some prior experience engaging with RAIin their work (e.g., contributing to evaluations of fairness of mod-els, adversarial testing, privacy for generative models, etc), thoughlearning about RAI on-the-job was not a recruitment criterion.These participants had various roles including software engineers,data scientists, program/product managers, UX researchers and de-signers, and more, and they worked on AI systems across multipleapplication areas, such as finance, education, and healthcare. Then,to gain a complementary perspective on learning about responsibleAI, we interviewed n=24 people across 13 technology companies,universities, or nonprofits who had developed RAI trainings orlearning resources for AI practitioners, whom we refer to in thispaper as \u201cRAI educators.\u201d We recruited people who had developedresources or led trainings for industry AI practitioners\u2014either intheir own organization or elsewhere\u2014rather than university stu-dents. Participants held a variety of formal roles, some explicitlyrelated to education or RAI, such as technical writer, head of cur-riculum, or responsible AI lead, while others were AI practitionerstaking on educational responsibilities in more or less formal ways.See Table 1 for a summary of participants\u2019 roles.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "b0bd1713-6319-4f65-8982-3ffa8b454a4b",
                    "text": "We conducted semi-structured interviews with both groups of prac-titioners, using different protocols for each group. Interviews werean average of 60 minutes, and participants were compensated anaverage of $54 USD, in either gift cards or donations to a charity,based on participants\u2019 choice. For both groups, we started the in-terview by asking them to describe their role and what responsibleAI means to them in their work. We asked AI practitioners abouttheir learning process for RAI, including the context(s) for learning(e.g., university, bootcamps, online courses, on-the-job training,etc), their motivations for learning about RAI, the skills and con-cepts they felt were the most important, and their specific processes,modalities, or pedagogies for learning. We then asked about howthey applied what they learned in their current work (including chal-lenges to that application). Finally, we asked about their aspirationsfor ideal learning experiences for RAI, and which skills or conceptsthey wish they had learned (and how). For RAI educators, we askedthem to give an overview of the learning resources or trainings theydeveloped about RAI. We asked them to describe a single resourcein depth, including their motivation for creating it, the intendedaudience, learning goals, and specific skills or concepts taught, andhow they were assessed. We asked about their design process for RAIlearning resources, such as how they decided on the specific topics,the topics they felt were most important, and which were easieror harder to teach or assess. We asked about their aspirations forlearning resources for RAI, including the skills and concepts theyfeel future RAI learning resources should focus on and what maybe preventing them from realizing those aspirations. See AppendixA for the interview protocols. When RAI educators were also AIpractitioners, we asked about their learning process for RAI.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "6b0a32b1-a7b9-47dd-8ed0-ab89c79a04c9",
                    "text": "We took a reflexive thematic analysis approach to analyze the in-terview data, following Braun and Clarke [21, 22]. We started bydiscussing epistemological trade-offs of different approaches to the-matic analysis, deciding to take a reflexive approach [21], as all ofthe authors are or were employed as researchers at an industryresearch group (see Section 7.2), and we thus wanted to grapplewith and reflect on how our position as industry researchers whohave variously contributed to the design of responsible AI tools andresources may impact our approach to data collection and analysis.We first coded the 40 transcripts from the interviews, with all sevenauthors coding at least two transcripts each, meeting regularly todiscuss and reflect on our codes (and the assumptions underpinningthem) throughout the coding process. Following the initial coding,we met regularly as a group to inductively generate themes that cap-tured patterns of shared meaning across the interviews. We used thedigital whiteboard Mural to iteratively cluster the codes into largerthemes, discussing the relationship between codes and themes aswe went and resolving any disagreements in synchronous groupdiscussions. For instance, one of the themes that we developed inthis process was \u201cquantifiable and technical solutions are prioritizedover other forms of knowledge.\u201d After several rounds of iterating onthe themes and their relationship over several weeks, we generatedthe final set of themes, which we report on in the following sections.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "db7b946c-504b-49c9-8c67-5556bccc1d76",
                    "text": "In this section, we describe three high-level themes in our findings.First, we identify three primary pathways by which AI practitionerslearn about RAI on-the-job\u2014by applying ethics knowledge fromprevious experiences; by foraging for learning resources; and bylearning from coworkers and impacted communities. Second, wethen identify several orientations towards RAI that participantsidentified and critiqued, including computational orientations toRAI and a procedural orientation that focuses on teaching learnershow to use RAI toolkits or how to comply with their company\u2019sRAI processes. Finally, we present RAI educators\u2019 and learners\u2019aspirations to teach and learn about RAI in more sociotechnical andrelational ways, and the organizational pressures that can comeinto tension with these aspirations.4.1 Learning Pathways for Responsible AI",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "d7b735aa-fdf5-4c5d-8129-a70a31e88e5e",
                    "text": "Some participants reported applying their priorknowledge of ethics from their university training, which includeddesign, information schools, philosophy of technology, social psy-chology, and medical ethics\u2014but, with few exceptions, participantsdid not report learning about ethics in their CS courses, eitherbecause they came from different disciplines or their CS coursesdid not include ethics. Some participants described how they ap-plied skills and knowledge from their previous work experiences\u2014typically in other industries. For instance, participants describedgrappling with ethical issues in a wide range of contexts: workingon data privacy in city government, data ethics in education, med-ical ethics, and more. Because the focus of this paper is learningon-the-job, we do not discuss details of what these participantsTable 1: Summary of participants\u2019 groups and roles.learned in their university programs, but we highlight that apply-ing such knowledge to their current roles is a challenge (which wediscuss more in section 4.3).",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "fc8d72d9-30e9-4f37-bca7-c7a22f9249e5",
                    "text": "Themajority of our participants reported learning about responsibleAI on-the-job. Although some participants were required by theiremployer to complete RAI trainings, many practitioners describedbeing self-motivated to learn about RAI. AI practitioners engagedin self-driven learning by adopting information foraging tactics [98]to find, compile, and share resources on RAI from different sources.Participants told us: \u201ceverything I know about [R]AI, I just learned onthe job...but it was all self-learning...\u201d (P14), and \u201cit was much moregrassroots, you have to go out and find information\u201d (E31). Partici-pants foraged for resources either within their company\u2019s internalrepositories or by searching for external resources. Participantsdescribed searching within their company for training videos orother professional development courses, talks, best practice guides,or educational games (P2, P5, P24, E33). Externally, some partici-pants found online courses or other semi-structured resources likebootcamps (P7, P15, E18). Participants also described reading bookson algorithmic bias, such as Algorithms of Oppression and Race afterTechnology (P11, P12, E40). Participants also described looking forresearch papers related to fairness, interpretability, or other RAItopics (P6, P7, E10, P23, P24). Finally, participants reported usingsocial media to search for articles or testimonials about harms orfailures of algorithmic systems (P4, P9, E10, P27).Although foraging can be a useful part of sense-making in a newdomain [98], participants pointed out several drawbacks and chal-lenges with this approach for RAI. Many participants expressedanxiety about the quality of the information and the reliabilityof the content they found. There was an aspiration to learn fromwhat some referred to as \u201cauthoritative\u201d sources of information,although it was not clear what such authority might look like. OnePM described wanting to \u201cknow who are folks that regularly publishdigestible updates or information as things develop that I can learnfrom... an authorized dealer of information... to not do a course or readsomething that is not actually grounded in research, is not groundedin facts\u201d (P14). Participants\u2019 access to information was often de-pendent on \u201cinfluencers\u201d (P14) whose posts on social media wereshared widely: \u201cpart of my job was tapping into the sort of academicinfluencer community online. . . and most of it was just following folkson Twitter\u201d (P14). Thus, their searches were shaped by who they fol-lowed or the content that was amplified on their social media feeds.However, social media content that goes viral may not include themost critical or comprehensive topics in responsible AI. Participantsalso noted how the learning resources they found may depend on the search terms they use, which may require background knowl-edge that some learners may not have. For instance, one participantdescribed searching for their company\u2019s RAI learning resources:\u201cI suspect that maybe someone newer to [the company], might notknow that we use the term responsible AI. I know fairness was a termthat was used before. I think human-centered ML has been used... butI\u2019m not sure what someone who\u2019s completely new to this space mightsearch\u201d (E21). This may be exacerbated by disciplinary divisionsthat lead people to search for\u2014and find\u2014resources that appeal totheir disciplinary identity. We return to this in Section 5.3.Interpersonal learning about responsible AI. Participants also4.1.3described learning from (and in some cases educating) other peopleabout RAI. This includes learning from coworkers in the form ofinformal discussions, via \u201ccasual conversations with collaboratorsor friends\u201d (P2), \u201cteam chats\u201d (P9), and \u201cword of mouth talking withother people\u201d (P7). Participants also learned about RAI from users,impacted communities, or other stakeholders. For example, a prod-uct manager described how social media exposed them to advocacyfrom artists and impacted communities:\u201cThe first time I experienced [responsible AI] was whenLensa launched and everyone was posting their personalportraits on Instagram stories... There was a huge AIstrike of a lot of these artists whose image was usedto train a model without being acknowledged... andhaving a family member who worked in that field andhas spoke out so clearly against it on social platformswas what first got me thinking of... there\u2019s people thathelped build this that didn\u2019t consent to their art beingused.\u201d (P9)Other participants (particularly in user-facing roles like UX), de-scribed learning about impacts on users via \u201cconversations withreal people who are telling me what their issues with a certain prod-uct are\u201d (P34). Other participants described a similar user-focused(P4) or customer-focused (P8) approach by \u201ctalking to the clini-cians that are part of our pilot program and seeing how they willuse it\u201d (P6). However, this may privilege issues surfaced by thelargest or highest-paying groups of customers or users, rather thancommunities most severely impacted by a given technology [cf. 81].Participants also described taking on roles as informal educatorsfor their peers\u2014or even their managers or organizational leaders\u2014while they were learning themselves. Thus, many of the participantswho signed up for the study based on their experience creatingor delivering RAI learning experiences (i.e., RAI educators) werethemselves AI practitioners. As one participant described: \u201cI wasalso educating based on what we learned... and everybody just startedturning to me basically. And so I was happy to consult as best as Ican\u201d (P29). Other participants created reading groups or discus-sion groups, shared newsletters, or created Slack groups to discussRAI topics with their teams (P4, P6, E11, P14, P25, P37), while oneparticipant described how they \u201cstart[ed] a community of practice\u201d(P27) with others interested in RAI within their company. Mean-while, others found opportunities to educate \u201csenior leadership\u201d (P2),where they \u201chave to engage with the founders and educate them onlike, \u2018Hey, this is why this [model output] is actually racist\u201d\u2019 (P37).Although interpersonal learning was a common learning path-way, participants brought up tensions with this pathway that mayimpact what and how practitioners are learning about RAI, includ-ing differences in values among co-workers that shape what re-sources or guidance they give or receive from their peers, demandson practitioners\u2019 time and other organizational (dis)incentives thatmay impede whether and how they help others learn [cf. 30, 82,102, 135, 139], such as power dynamics involved when people arelearning from (or trying to teach) their managers or other organiza-tional leaders\u2014particularly if \u201cyou have a team lead who just shutseveryone down\u201d (E32). As one participant told us: \u201cat the end of theday, responsible AI is really values-laden and people will have differ-ent values and different ideas of what\u2019s normatively ideal\u201d (E18). Thispotential difference in values and normative ideals may prohibit AIpractitioners from engaging in this type of informal, interpersonallearning. As they went on to describe: \u201cworking in a technical role inindustry, you learn so much from the people... who\u2019ve been there fordecades... and so from a software engineering perspective, that\u2019s howyou learn how to become a better software engineer\u201d (E18). However,exclusively learning from mentors who align with one\u2019s values [cf.16, 49, 101] may inadvertently reproduce the current status quo inAI\u2014we return to this in Section 5.4.2 Orientations Towards Responsible AI inLearning Resources4.2.1 Computational orientation to RAI. Because of the differencesin values in an interdisciplinary field like RAI [cf. 16, 34, 101], wefocus here on the framing, or orientation, of RAI learning resources,which may implicitly communicate to practitioners (particularlythose who are foraging for resources) what is important to learn.We find that many RAI learning resources focus on computationalimplementations of RAI concepts, or what one participant referredto as \u201ca pure technical approach, [despite] also knowing that [they\u2019ll]have to eventually go back and look at how this thing actually affectedend users or measure the impacts\u201d (E19). Some of the most commonlytaught RAI concepts and skills involved computational evaluationsof a model\u2019s performance for different demographic groups usingquantifiable fairness metrics and assessment processes (sometimesreferred to as dis-aggregated evaluations [e.g., 12, 81]). Similarly,RAI educators are teaching adversarial testing [e.g., 44, 95] to eval-uate potential harms of generative models (P2, E10, E16). Learninggoals for adversarial testing were often oriented around how to con-duct adversarial testing rather than interrogating who is involvedin such testing, and for what purpose [cf. 44]. RAI educators described why they took a computational ori-entation to the topics and pedagogical approaches: to appeal toengineers\u2019 disciplinary backgrounds, via teaching with computa-tional notebooks (e.g., Jupyter Notebooks) or framing fairness \u201casan optimization problem\u201d (E20). However, this computational ori-entation was prevalent even when the target audience for RAIlearning was \u201cnon-technical people\u201d (E30), with numerous partic-ipants describing how a pre-requisite to learning about RAI wasfirst knowing how AI systems work, saying: \u201cthe foundational pieceof being involved in responsible AI is you have to understand at avery basic level how the technology works\u201d (E31).Despite this perceived need to appeal to engineers by translatingsocial or philosophical concepts into computational or quantifiableforms, many participants revealed misgivings that a computationalorientation to RAI might unintentionally reinforce technosolution-ism, or the belief that technical interventions are able to solvefundamental societal challenges [92]. One RAI educator describedan exercise they used with learners, to re-implement a researchpaper on gender de-biasing in word embeddings, but they sharedmisgivings that learners might come away thinking that \u201c\u2018we cansolve gender bias by just doing some linear algebra,\u2019 and is that thetakeaway that we want people to have? But the counterfactual is, ifwe didn\u2019t have this kind of [training], then people probably wouldn\u2019teven care about it at all\u201d (E18). As several participants identified,choices about who is testing systems and for what types of po-tential harms are never value-neutral [e.g., 31, 44]. Shying awayfrom teaching a socio-political analysis, however, meant that evenmeasuring differences in group-level metrics was difficult to teach,as such evaluations may rely on practitioners asking sociopoliticalquestions like \u201cwhat does the hierarchy of social favorability... thehierarchy of privilege look like in this community?\u201d (E16).This primarily computational orientation to RAI impacted peda-gogical decisions, such as the choice of learning goals, instructionalformats, and ways of demonstrating mastery over the material. Weheard about \u201cthe pedagogical challenges of creating content for en-gineers in areas that are outside the areas that they\u2019ve been trained\u201d\u2019(E20). Many RAI educators described reservations about adoptingassessments from CS courses: \u201cfor the more technical things, we canjust borrow from how other technical projects are assessed...like calcu-late the group conditional true positive rate or whatever and then youcan check if they implemented that correctly. So stuff that\u2019s technical ismuch easier to assess...if you\u2019re talking about more qualitative [meth-ods]...it\u2019s not a math test where you test whether or not someone gotthe right answer\u201d (E18). Others felt pressure to develop quantifiableassessments, reflecting on how \u201cthere\u2019s more that we could be doingin the evaluation space, but it\u2019s just really hard to figure out how youcan quantify and assess that\u201d (E20). In the absence of readily avail-able approaches to evaluating less computational or quantifiableskills, RAI educators drew on \u201canecdotal\u201d course evaluations (E20),or \u201ctestimonials\u201d of learners\u2019 takeaways from the course (E22).This desire for a tighter integration of ethics content into ML-focused trainings [cf. 39, 48] was accompanied by language that re-vealed anxieties about the values that were prioritized in the coursedesign: \u201c[ideally] incorporating [ethical content] more naturally intoeverything, not kind of shoving it into people\u2019s faces, right? And hav-ing it overtake the actual technical concepts that students are learning\u201d(E11). Other RAI educators reflected on the difficulty of these ped-agogical decisions: \u201chow do you think about whether something isnormatively good or not, right? That\u2019s just really hard to do withinthe bounds of the CS discipline\u201d (E18). However, left unsaid here isthe more difficult normative question of what should be within thebounds of the CS discipline, which we return to in the Discussion.4.2.2 Procedural orientation to RAI: corporate processes and toolkits.In addition to a computational orientation, participants describedhow RAI learning experiences are oriented around procedures\u2014teaching practitioners how to use RAI toolkits or how to complywith their companies\u2019 RAI policies. This included how to use RAItoolkits [see 9, 29, 79, 141, for a review], including fairness toolkitssuch as Fairlearn [136], AI Fairness 360 [4], as well as transparencytools like Datasheets [50], Data Cards [100], and Model Cards [90].The focus on toolkits was important for some RAI educators: \u201cnot[just] to teach people what is fairness? What is transparency?... [but]how to actually practice it, here\u2019s how to actually implement it\u201d (E22).Analogizing RAI to agile development, one educator noted: \u201cit\u2019s sim-ilar to when you\u2019re doing a daily stand-up, the objective is a processobjective, it\u2019s a \u2018did we do the process?\u2019 \u2018Yes.\u2019 As opposed to an outcomeobjective... like did the process produce like X number of actionableitems or Y number of mitigations\u201d (E32). Similarly, many RAI ed-ucators oriented their trainings around their companies\u2019 internalRAI principles and RAI review processes. For instance, one RAIeducator \u201csocialized the [company]\u2019s responsible AI principles andcreated all sorts of games and challenges that incentivize employeesto complete them to try and build awareness. One thing that [mycolleague] found was that if you had asked employees if they hadheard of the RAI principles, they would say, yes, I know there\u2019s AIprinciples. If you ask them to name just one of them or explain it,they could not do it\u201d (E20). For them, building awareness of theircompanies\u2019 RAI principles among employees\u2014including namingor explaining what those principles were\u2014was a critical precursorto enacting broader organizational change [cf. 60, 134]. AnotherRAI educator justified this theory of change: \u201cpractices are the mosttangible thing that exist inside of a business in terms of how peopleexperience a business, its values and its decision making\u201d (E32).However, a procedural orientation to teaching about RAI toolkitsand corporate AI policies may limit AI practitioners to learningonly those aspects of RAI that the creators of RAI tools and policiesdeem relevant (or the types of algorithmic harms that such toolkitsand policies are able to address\u2014potentially acting as a \u201ctechnologyof de-politicization\u201d [cf. 53, 61, 141]), and not, for instance, focusingon \u201cmaking systems more accountable to the public in the form oftransparency or having public input into how the systems operate\u201d(E28). Similarly, participants reflected on the tensions inherent indeveloping educational resources to teach corporate principles andpractices, perhaps at the expense of a focus on values or prioritiesthat may not be aligned with corporate business imperatives [cf.81, 141], referring to companies\u2019 trainings on RAI principles as \u201cthisvery weird sanitized version of ethics\u201d (E18). however, that partici-pant expressed ambivalence, voicing a theory of change that corpo-rations developing AI products did need to get their AI developersto align with some set of values or practices, because \u2018\u201ccompaniesare the ones that are [developing AI] that we need them to [develop re-sponsibly]. I kind of feel extremely ambivalent about that\u201d (E18). This ambivalence was echoed by others who acknowledged that corpo-rations\u2019 RAI processes may be \u201cPR, but it also makes sense\u201d (E30).4.3 Aspirations for Responsible AI LearningResources4.3.1 Understanding harms and impacts. RAI educators and AIpractitioners shared aspirations for sociotechnical approaches toRAI learning that could integrate RAI topics \u201cacross disciplinarydivides\u201d (E28) and enable practitioners to identify potential socialimpacts of algorithmic systems early in the design process. Partici-pants wanted to \u201cshift away from the technical components and moreon the social, cultural components\u201d of RAI (E13), or \u201cnot just the tech-nical angles, but the sociological and anthropological angles... [whichis] outside the scope of what [AI practitioners] typically do\u201d (E20).Some described how they used case studies of AI harms acrossdomains to help learners \u201cbe able to foresee potential harm of an AIcase... to understand the ramification and the impact of deploying anAI model within a larger system\u201d (E30). Others tried to foster theskill of identifying potential harms, either via consulting with prod-uct teams, using approaches like value-sensitive design (E28), ortrying to make time early in an ideation phase to \u201cthink more widelyabout all the potential harms as well as the opportunities\u201d (E32).Participants also wanted learning resources to help incorpo-rate perspectives from external stakeholders, such as members ofcommunities impacted by AI systems, into RAI design and evalu-ation approaches, but they felt this was not typically covered byRAI learning resources. Participants reflected on how helping AIpractitioners learn about community engagement or participatoryapproaches [cf. 15, 28, 59] could involve learning theories, methods,and skills for how to, for instance, establish relationships with com-munity groups and \u201cbeing involved with the community, listeningto these communities and, and just sitting at the same table basically\u201d(E13)\u2014but this was not a part of typical RAI learning resources. Tothe extent that current RAI learning resources do discuss engagingwith stakeholders, participants shared that it is often via adversarialtesting or more traditional user research paradigms where the goalis to identify \u201cissues with a certain product\u201d (P34), which orientsthe matters of concern around product improvements, rather thansystemic harms or impacts.4.3.2 Building capacity to engage in RAI in the workplace. Practi-tioners described feeling unprepared to apply what they learnedabout RAI to their day-to-day work, due in large part to the value-laden and highly contextual nature of RAI. Some desired moresupport to be able to have potentially difficult, value-laden conver-sations with coworkers. For instance: \u201cI don\u2019t know if I feel preparedto go into that conversation [with other AI practitioners] and breakingdown some misconceptions that could be harmful. How do you starta conversation about responsible AI?\u201d (P14). Educators reflected onthe ability to identify one\u2019s values and how they manifest in designchoices for algorithmic systems:\u201cYou should know about bias and that\u2019s important, butthere\u2019s a difference between that and knowing, \u2018I am asoftware engineer at this company and I know how toarticulate how I feel about this thing I\u2019m being asked tobuild,\u2019 or \u2018I know how to engage with my coworkers ina way that makes them feel safe and respected\u2019... [or]what should we choose as a target variable, and doesit have value-related implications? People might havedifferent beliefs about that.\u201d (E18)These tensions in values were common, and many educatorswanted to help learners develop the skill of recognizing that one\u2019svalues may be different from others on a product team\u2014and howto negotiate (and ideally resolve) the tensions in those values [cf.78, 84, 89]. Many participants described how they wanted to learnhow to raise issues or concerns about potential harms to theirmanager or other leadership, but these conversations about valuesmay be difficult due to power dynamics within tech companies [cf.82, 102, 139, 140]. RAI educators struggled to teach learners how toshare concerns with their manager (E36), while learners felt therewas \u201ca business case to be made [for RAI]. It\u2019s not just about doingthe right thing, which is super important...\u201d, but they struggled toknow \u201c...how can I justify this to stakeholders?\u201d (P37).Participants also described the gap they felt between learningabout RAI, and being able to apply this knowledge in their devel-opment practices, leading to desires for prescriptive guidance thatwould reduce the \u201cburden for [practitioners] to interpret it\u201d (E21). Yet,both practitioners and educators aspired to build capacity to applylearned RAI concepts and skills to new use cases, domains, contexts,applications. To close this gap, practitioners wanted resources thatwere situated in real-world examples of harms or tailored to differ-ent use cases or domains, via case studies or scenarios of RAI issues.Others wanted customized trainings for different geographic orcultural contexts [cf. 108] to help support AI practitioners who are\u201clooking [at RAI issues] in this specific country, here\u2019s a process forfairness or how to test your models\u201d (E22).4.3.3 Organizational tensions in pedagogical aspirations. Finally,RAI educators\u2019 aspirations for what they saw as pedagogically bene-ficial approaches to RAI were often in tension with the incentives orrequirements from their organizations. RAI educators described thepedagogical benefits of live instruction, especially in synchronous,small-group learning settings. This included the accountability ofshowing up for a course led by an instructor, as well as having theability to ask questions or get help from the instructor for particulartopics (P2, E3, E36). Some pointed out the value of being able to\u201cgo off script and bring some of their own personal experiences to theclass\u201d (E20). Participants noted that sociotechnical topics in partic-ular were easier to learn in collaborative, conversational learningsettings instead of, e.g., remote, asynchronous learning such aswatching a training video or working through a Jupyter notebook:\u201cthe sociotechnical concepts are easier to do in person,right? Because you can talk to people about why dis-criminating by gender is bad... It\u2019s a little bit harder todo that in a one-way reading text-based delivery versusthe sort of Socratic conversation that can help bringpeople to the table better.\u201d (E19)However, numerous RAI educators felt that organizational con-straints (e.g., a lack of budget, time, personnel) impacted their de-cisions about the design of learning resources, making it difficultfor them to achieve their aspirations [for similar organizationalimpacts on RAI work practices, see 6, 64, 81, 82, 86, 102, 132]. RAI educators described how shifting organizational priorities made itdifficult to allocate time to create trainings, including teams thatcreated tutorials and trainings being \u201cre-orged\u201d into other teams,while others were laid off, or felt that creating educational resourceswas \u201cnot my job anymore\u201d (E16) [cf. 6]. Educators noted a tension be-tween their pedagogical aspirations for instructor-led, synchronouslearning opportunities, and their organization\u2019s pressure to developRAI trainings for large numbers of employees (i.e., \u201cscalability\u201d[cf. 58, 127])\u2014\u201cmost people would prefer some sort of instructor-ledexperience.... but self-study scales, that\u2019s the main asset of it\u201d (E20).Similarly, despite some educators\u2019 aspirations to create a pro-gression of learning resources from basic to more advanced topicsin RAI (e.g., weighing trade-offs between different dimensions ofresponsible AI, such as fairness evaluations and privacy [7]), RAI ed-ucators felt organizational pressure to create \u201clightweight\u201d trainingsthat could be quickly completed. For instance, \u201cthe first [require-ment] is that it was really important to create something that waslightweight, which meant that it did not require a lot of prep and itdidn\u2019t take a lot of time and it was very easy to understand how todo. So anyone could just take like 45 minutes to an hour and just doconsequence scanning\u201d (E32). This was a common theme across RAIeducators, who told us how they \u201coptimized for speed\u201d (E21), andhow this shaped the types of resources they created: \u201cmaybe it\u2019s ashort video like a five minute tech talk or some way of synthesizingthis rich stuff into a quick way that [they] can absorb it and moveon to [their] job?\u201d (E21). Some RAI educators discussed how theybalanced this tension by providing multiple formats for learningresources, in varying lengths and complexity (E35, E40).Such organizational pressures may have similarly shaped AIpractitioners\u2019 aspirations for learning resources. AI practitionerstold us how they wanted learning resources to give them practi-cal, actionable guidance that they could use immediately, a prag-matic desire that was at odds with the desire of many participants(both practitioners and educators) to develop reflexive mindsets andvalue-driven ways of conceptualizing and designing AI systems,as discussed in Section 4.3.2. RAI educators described how practi-tioners taking their RAI trainings wanted prescriptive guidance tomeet their companies\u2019 RAI requirements: \u201cI\u2019m seeing people say, \u2018Iknow we have an [AI review process] that you have to make sure thatyou do. So just tell me what the thing is so that I can do it... if you canmake it clearer, then you\u2019re removing some of that, that burden for meto like interpret it\u201d (E21). However, RAI educators noted that despitethis desire from practitioners, there was no automated process orsingle solution to anticipating and avoiding harms of algorithmicsystems. Participants told us how there was no \u201croadmap\u201d (E20)or \u201cquick guide\u201d (E26) for RAI, or in some cases \u201cthere isn\u2019t really aright answer\u201d (P9) at all. RAI educators described how they wantedto foster critical thinking, to shape new ways of thinking towards a\u201ccultural shift\u201d in AI development (E17), to enable \u201cpeople to makebetter decisions in the long term\u201d (E18). They raised concerns thatproviding guidance that was overly prescriptive (E13) would en-courage a mindset of \u201ca tick box [approach]\u201d (E19) [cf. 9, 141]. Asothers told us, \u201cwhat [learners] need is not what they want. Becausethere\u2019s no one answer. It depends on who\u2019s your audience, what isyour product, what is the risk tolerance of your executives?\u201d (E16).5 DISCUSSION5.1 Implications of learning environments forresponsible AIThe environments or sites in which learning about RAI occurs shapethe learning process in crucial ways. In our study, we found RAItrainings are often oriented around companies\u2019 AI policies or or-ganizational processes for RAI review, which can be understood asone element of organizational cultural change or RAI organizational\u201cmaturity models\u201d [60, 102, 134]. While such trainings may providelearners with opportunities to directly apply what they learn in theirwork, corporate sites for learning may also have mixed incentives:trainings may be required by their employers [e.g., 10, 11, 24, 134],but AI practitioners may face challenges applying what they learn,due to the organizational pressures for speed and scale that impedeRAI work practices [6, 64, 81, 82, 102]. In our study we see similarincentives impacting opportunities for learning about RAI, via con-straints on aspirations for RAI learning resources. On the one hand,learners and educators described aspirations to foster reflexivemindsets [cf. 20] and build capacity to apply learned RAI conceptsand skills to new use cases, domains, contexts, applications. Yet afast-paced development environment contributed to a desire for pre-scriptive guidance that removes the burden of high-stakes decision-making from practitioners [cf. 139]. The pedagogical approachesRAI educators adopt are also shaped by organizational pressures.For example, participants described desires for curricula of increas-ing depth and instructor-led collaborative learning. Yet, organiza-tional pressures to develop scalable learning opportunities droveeducators towards developing self-study resources and training.We also found that AI practitioners are learning about RAI inunstructured, self-directed ways outside of their companies\u2014frombooks, documentaries, blogs, social media, or larger communities ofAI practitioners. Much like prior work has found for self-directedlearning of web developers [36] and data scientists [72], designchoices in learning environments implicitly communicate the ob-jects of concern for the AI community and how such topics shouldbe approached. Prior work on self-directed learning for ML hasidentified learners\u2019 challenges finding the right resources for theirlearning goals [e.g., 26]\u2014however, given the risk of epistemologicalbifurcation in learning about responsible AI (i.e., into social andtechnical goals), these challenges become even more salient. AsRAI researchers, we can support informal learning opportunities bydeveloping learning resources that are able to reach a much wideraudience of people developing AI models or AI-infused applica-tions in the open-source community. However, our findings raisequestions about precisely how such informal learning resourcesmight be designed\u2014if integrated into existing toolkits [e.g., 136],how might they avoid the de-politicization or technosolutionismof that genre [cf. 141]? If integrated into online leaderboards orcommunities such as Kaggle competitions or HuggingFace [cf. 3],how might such learning resources resist the technical orientationof such approaches, rather than inadvertently reinforcing them?Finally, interpersonal learning is one key pathway by which AIpractitioners learn about RAI on-the-job. Some RAI educators in ourstudy had formal roles related to education (e.g., technical writers),though many participants who developed learning resources didso as side projects or in informal educator roles. This echoes prior work that found practitioner\u2013educator roles are common amongstdata scientists [76] and AI practitioners [30, 135, 142]. While thepredominance of practitioner\u2013educator roles may speak to under-resourcing and under-investment in RAI education, it also suggestsan opportunity to leverage RAI expertise that may be distributedacross a company. Our findings suggest opportunities for organi-zations to support interpersonal RAI learning\u2014e.g., via mentoringprograms for RAI, providing support for informal or formal con-versations with peers about RAI, or more broadly fostering a com-munity of practice for RAI. However, prior research on corporate\u201csafety cultures\u201d [cf. 117] identifies risks of relying on approaches tosafety (here, RAI) that put the onus for cultural change on workers,given the organizational pressures that may disincentivize workersfrom raising concerns about harms that may pose threats to theircompanies\u2019 business models [6, 82, 117, 132, 139, 141].5.2 Designing sociotechnical learningopportunities for responsible AIThroughout the interviews, we heard RAI educators grapplingwith underlying disciplinary tensions via pedagogical decisionsabout learning objectives, instructional approaches, and methodsof assessing learners\u2019 understanding. Despite an acknowledgmentfrom many participants that interdisciplinary, sociotechnical ap-proaches to responsible AI were important [cf. 34, 55, 101], thelearning design choices that RAI educators in our study describedmay reinforce a bifurcation between approaches that teach so-called\u201ctechnical\u201d AI concepts and approaches from the social sciences orhumanities that grapple with social, historical, and political forcesthat may shape or be shaped by algorithmic systems.We saw this disciplinary bifurcation reproduced when RAI learn-ing materials taught sociotechnical concepts (e.g., fairness [34] )in primarily computational ways, to appeal to learners with MLexpertise. This includes teaching concepts such as fairness in ML inways that were removed from any social context [cf. 111], treatingfairness as a metric for algorithmic optimization rather than, e.g.,understanding the historical specificity of marginalization in thecontext(s) in which AI systems are deployed [108], in which theirdata was collected or annotated [87, 88], or in which AI develop-ment teams were located [cf. 123, 138]. RAI educators\u2019 relianceon computational approaches may be a response to the existingdisciplinary norms of AI development more generally\u2014and yet,such appeals may smuggle in the positivist, technical values ofmachine learning [e.g., 16, 55, 101], rather than the aspirations ourparticipants had for more integrated, sociotechnical approaches toRAI. We also found that in cases where resources were designedwith explicit learning objectives, those objectives often emphasizedtechnical goals or made a distinction between social and technicalgoals. Given the self-directed foraging our study found, the lackof integrated sociotechnical learning objectives may lead learnersto discover or complete only those learning resources that alignwith their disciplinary identity, further reinforcing a disciplinarydivision. Instead, we suggest designing RAI resources around ex-plicitly sociotechnical learning objectives and adopting pedagogicalapproaches that involve case studies, scenarios, problem-basedlearning, or other ways of understanding how harms are situatedwithin particular historical contexts [34, 63, 69, 73, 75, 80, 113, 115].When developing assessments, or ways for learners to demon-strate mastery of RAI skills and concepts, RAI educators in our studyreported tensions between what they felt were \u201cscalable\u201d methodsfor learners to demonstrate mastery, like multiple choice questionsor code notebooks (where learners could compare their code to ahidden code block with answers), with modes of assessment per-haps better suited to sociotechnical concepts, such as reflectivewriting, presentations, group discussions, but which were seen asless scalable. However, our participants were uncertain how toadopt such approaches to demonstrating mastery of RAI concepts,in part due to either their own or their learners\u2019 disciplinary train-ing in computer science, or due to pressure from their employersto develop trainings that could be deployed to large numbers oflearners across their company. One participant voiced their concernabout the challenge of assessing mastery of sociotechnical conceptsas \u201cthere\u2019s no right answer.\u201d Although this reflects the normativequestions at the heart of contested concepts such as fairness or RAI[55], this is a challenge that other fields (e.g., the humanities) havelong since grappled with when designing assessments. Our findingssuggest that more work is needed to develop modes of assessmentthat are appropriate to an interdisciplinary, sociotechnical approachto RAI [e.g., 5, 75, 113].5.3 Resisting hierarchies of knowledge inlearning about responsible AIOur findings echo others\u2019 calls to resist and destabilize existinghierarchies of knowledge in AI [cf. 49, 55, 101], to lead to a moresociotechnical approach to learning about responsible AI. As nu-merous educational philosophers have argued, choices about whatand how to teach are inherently choices about values [43, 51, 52,65]\u2014indeed, curricular decisions and educational standards havelong been sites of public contestation and negotiation [77, 110].In AI, in addition to tech ethics courses in higher education [e.g.,39, 40, 48, 119], professional training and on-the-job learning arepart of what Bourdieu and others refer to as socialization into anoccupational community [8, 18, 19]. In other words, choices aboutdesigning learning opportunities for RAI (or AI more generally)communicate to members of the AI field what is important for themto know and be able to do as an AI practitioner. As we discuss inSection 5.2, many approaches to learning about RAI implicitly sug-gest to AI practitioners of all roles and disciplinary backgroundsthat fairness and other RAI goals are a technical problem that canbe subsumed under model optimization goals like accuracy [cf. 16]or usability, rather than socio-political forces that shape every stepof the development, deployment, and use of AI systems [cf. 55].Thus, technosolutionist orientations to learning may implicitly sug-gest to practitioners that critical, reflexive work is somebody else\u2019sproblem [e.g., 6, 20, 55, 61, 84, 132, 141]\u2014as part of the separation ofconcerns or dis-located accountability that cultures of abstractionand modularity in computer science may reinforce [83, 138]. Toresolve this, prior work has suggested integrative, interdisciplinaryapproaches to AI, including Agre\u2019s call for \u201ccritical technical practice\u201d[2], Turkle and Papert\u2019s call for epistemological pluralism [129],among many others [e.g., 49, 55, 73, 84, 101]. It is thus worth asking why, decades after such provocations,the AI field continues to reproduce dominant technical values inthe occupational socialization of future AI researchers. We thus askhow the FAccT and RAI community might resist this when devel-oping new ways for practitioners to learn about social impacts ofalgorithmic systems. How might we as a field shift the professionalnorms and identity of AI development\u2014or what anthropologistKarin Knorr-Cetina refers to as the \u201cepistemic culture\u201d [25] of sci-entific practice in AI? As one approach, we may look to theoriesof learning as a subversive activity [43, 65, 99] for inspiration forpedagogical approaches that push learners out of their comfortzones or actively draw on disciplinary or epistemic discomfort asa generative way to support learners\u2019 growth [cf. 124].Radical education philosophers have long argued that formalsystems of schooling are likely to reproduce dominant, hegemonicviews, rather than leading to more liberatory social change [e.g.,43, 52, 65]. In AI, we see signs of what historians have referred to inother fields as \u201cnormative centering\u201d [57]\u2014wherein formal RAI train-ing may center technosolutionist, computational approaches. Tocounter this normative centering of technosolutionism, we call forpedagogical provocations that destabilize dominant, hegemonic val-ues in RAI (or AI more broadly). This may entail drawing on Freire\u2019sproblem-posing approach to learning that is situated in learners\u2019contexts [cf. 43]\u2014rather than centralized, abstracted, homogenizedapproaches to teaching a single set of ratified concepts, in waysthat may reproduce technosolutionist ideologies about (responsible)AI. Along these lines, Malik and Malik [84] draw on Freire\u2019s workon critical consciousness to call for technologists to support oneanothers\u2019 critical technical awakening via learning communities[cf. 2]. However, radical, liberatory educational philosophies andpedagogies may not fit neatly within corporate sites for learningon-the-job, which may prioritize corporate values and desideratasuch as scalability. We do not provide easy answers here, but insteadcall for a proliferation of radical approaches to fostering criticaltechnical awakenings [84] and the development of sociotechnicalRAI knowledge and skills as a core part of the AI discipline.5.4 LimitationsOur participants were primarily tech workers\u2014future work shouldexplore learning with people outside the technology industry, in-cluding nonprofits and civil society, policymakers, and the public,including communities impacted or harmed by AI [cf. 35]. Ourinclusion criteria for AI practitioners were those who had someprior experiences with RAI, but this may have led to self-selectionof participants already interested in the topic, or who saw theirwork as RAI. In addition, 35 of 40 of our participants were locatedin the US, and as such, our findings may be skewed towards theperspectives of US-based AI practitioners; future research shouldexplore cross-cultural perspectives on learning about RAI. Finally,future work might analyze the content of RAI learning resources,which we did not include here in part due to access restrictions.6 CONCLUSIONAs technology companies increasingly integrate AI systems intomore facets of public life and AI practitioners attempt to identify,evaluate, and mitigate potential harms of AI systems, it is criticalto understand what and how AI practitioners are learning about re-sponsible AI on-the-job. Via interviews with 16 AI practitioners and24 RAI educators from 16 organizations, we identify AI practition-ers\u2019 learning pathways for RAI; the primarily computational andprocedural orientations of many RAI learning resources; and prac-titioners\u2019 and educators\u2019 aspirations for sociotechnical approachesto RAI learning, impacted by organizational pressures. We closewith implications of our findings for learning environments forRAI, implications for the design of sociotechnical approaches tolearning about RAI on-the-job, and broader questions for the fieldabout how to foster critical reflection among AI practitioners andresist hierarchies of knowledge in RAI.7 RESEARCH ETHICS AND SOCIAL IMPACT7.1 Ethical ConsiderationsBefore recruiting participants, we reviewed participant gratuityamounts, data management plans, and study design documents,including consent forms, with experts in the RAI community withmany years of human participant research experience, to reviewsocial and ethical implications of our choices. We also aligned thesechoices with standards of human subjects research within ourinstitution. We followed strict protocols to ensure the confiden-tiality, anonymity, and privacy of our participants. We collectedpersonally-identifiable information only for the purpose of provid-ing gratuities, and kept that information separate from the studydata. After transcribing the interviews, we replaced participants\u2019names with unique identifiers and removed any other potentiallyde-anonymizing information (e.g., the name of their company). Par-ticipation in our study was voluntary. At the start of each session,we walked each participant through an informed consent processand form, sharing the study\u2019s purpose and intended use of the data,and suggesting they not share confidential information. We told par-ticipants they were free to ask us to move on to a different questionif they weren\u2019t comfortable responding, and they were able to with-draw from the study at any time with no penalty for them (i.e., theywould still receive the gratuity), and we would delete their studydata if so, although none of the participants asked to withdraw.7.2 PositionalityAll authors are, or were at the time of conducting the research,employed as researchers at a large technology company. Manyof the authors have contributed to the design, development, andimplementation of RAI tools, frameworks, playbooks, or otherRAI resources. Several authors have partnered with or advisedproduct teams on RAI, such as conducting fairness evaluations,data collection best practices, and more. Multiple authors have cre-ated RAI learning resources or conducted RAI training sessionswith AI practitioners. We have backgrounds in machine learning,human\u2013computer interaction, critical computing, and tend towardspost-positivist or interpretivist traditions in qualitative research[120]. These backgrounds and experiences have shaped our re-search, through choices about the research questions to explore(e.g., focusing on learning for working professionals, rather thanlearning in academic settings) and our data analysis (e.g., choicesabout specific codes, themes, or how to interpret them in light of ourepistemological orientations and experiences with RAI in industry). Additionally, our positionality\u2014e.g., all authors were employed ata technology company\u2014may have shaped our approach to recruit-ment (and thus the set of participants). Of the 40 participants, onlytwo were employed at non-profits and two were employed at auniversity (but spoke about prior experiences in industry).7.3 Adverse/Unintended ImpactsAlthough we intend this paper to open a critical conversation inFAccT and RAI about the implications of learning design choicesfor practitioners learning about RAI, we acknowledge that thiswork may have unintended impacts. First, we focus in this paperon on-the-job learning, but we do not mean to suggest that for-mal learning pathways such as ethics courses in higher educationshould be de-prioritized or under-invested in. On the contrary, wesee informal learning about RAI as a complementary approach tointervening in the occupational socialization of (responsible) AIpractitioners. Indeed, we believe there is much that RAI and FAccTresearchers can learn from prior research on integrating ethics andcritical computing into formal computer science education [e.g.,40, 48, 73, 101, 119, and see Section 2.1 for more detail]. Second, al-though we identify participants\u2019 concerns for disciplinary divisionsof RAI work into social and technical, and we argue in the Discus-sion for integrated, interdisciplinary approaches to learning aboutRAI, we acknowledge the risk that this paper may unintentionallyfurther reinforce disciplinary divisions within the AI field.ACKNOWLEDGMENTSWe want to thank our participants for sharing their experiences andthe anonymous reviewers for their helpful feedback on the draft.We also want to thank Daniel J. Barrett, Alicia Chang, SamanthaFinkelstein, Andrew Smart, Tom Stepleton, Bogdana Rakova, ZijieJay Wang, Elizabeth Anne Watkins, Hilde Weerts, David Widder,and Richmond Wong for helpful feedback that greatly improvedthis work.REFERENCESA INTERVIEW PROTOCOLSSee this section for a high-level version of the interview proto-cols used for both groups of participants. Note, however, that forsemi-structured interviews, the actual questions asked may differin various ways from the protocol [e.g., 118], such as follow-upquestions to probe deeper on specific topics that became salientlater in the interview process.A.1 Interview protocol for AI practitioners withRAI experienceA.1.1 Overview.\u2022 Can you tell me about your role and your team?\u2022 What does RAI mean to you in the work that you do?\u2022 Can you walk me through a specific project where you ad-dressed responsible AI considerations? Which aspects ofresponsible AI did you focus on, and why?A.1.2 Reflections on their RAI learning process.\u2022 How did you learn about RAI? Talk me through your learning \u2022 What skills or knowledge do you think RAI trainings orprocess.\u2022 Why did you learn about responsible AI?\u2022 What were more or less effective ways of learning about RAIthat you\u2019ve experienced? resources should focus on?\u2022 Are there certain topics or skills you wish you could teachor develop content for, but aren\u2019t sure how?\u2022 Are there certain formats or pedagogical approaches you\u2022 What were the easiest and hardest RAI skills or concepts to think RAI learning resources should adopt?B DEMOGRAPHIC QUESTIONS ANDRESPONSESThis questionnaire was based on work from Spiel et al. [121] ondesigning better survey questions about gender.B.1 What is your gender?\u2022 Woman\u2022 Man\u2022 Non-binary\u2022 Prefer to self-describe\u2022 Prefer not to say\u2022 If you would prefer to self-describe your gender, please doso here:B.2 With which racial or ethnic groups do youidentify?Mark all boxes that apply.\u2022 White\u2022 Hispanic, Latino, or Spanish origin\u2022 Black or African American\u2022 Asian\u2022 American Indian or Alaska Native\u2022 Middle Eastern or North African\u2022 Native Hawaiian or other Pacific Islander\u2022 Prefer not to answer\u2022 Other:learn?A.1.3 Applying RAI learning in practice.\u2022 Is what you learned from your RAI learning process relevantin your current work? How?\u2022 Can you give an example of how you applied what youlearned about RAI on your project?\u2022 What were some challenges you faced when applying whatyou learned about RAI to your current work?A.1.4 Aspirations for RAI learning.\u2022 What would you want out of an ideal RAI learning experi-ence?\u2022 What RAI skills or knowledge do you wish you had learnedor been taught?\u2022 What kinds of resources or trainings for learning RAI wouldyou want to have?A.2 Interview protocol for RAI educatorsA.2.1 Overview.\u2022 Can you tell me about your role and your team?\u2022 What does RAI mean to you in the work that you do?A.2.2 Reflection on RAI training or learning resources.\u2022 Can you walk me through a recent RAI training or learningresource you\u2019ve developed? What was the focus of it?\u2022 Why did you develop that RAI training or learning resource?\u2022 Who is the intended audience?\u2022 What RAI topics did you include in your training or learningresource?\u2022 How did you decide what topics or goals to develop contentfor?\u2022 Are there easier or harder topics to develop educationalcontent for? What are they?\u2022 What were the specific learning goals or objectives you hadfor that content, and how did you determine them?\u2022 How did you assess students\u2019 understanding or competenceabout the topic?A.2.3 Reflections on their RAI learning process.\u2022 How did you learn about RAI? Talk me through your learningprocess.\u2022 Why did you learn about responsible AI?\u2022 What were more or less effective ways of learning about RAIthat you\u2019ve experienced?\u2022 What were the easiest and hardest RAI skills or concepts tolearn?A.2.4 Aspirations for RAI learning design.\u2022 If you could go back, and create that training or resourceagain, what would you do differently?\u2022 What would you want out of an ideal RAI learning resource?Table 2: Summary of participants\u2019 groups, roles, and demographics. Participants could select multiple options for race/ethnicity.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "ba2378a3-6450-4770-a76c-d9b492c14fd9",
                    "text": "",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "d1050244-39c9-45ed-9877-45ff77b8cdaa",
                    "text": "In addition to a computational orientation, participants describedhow RAI learning experiences are oriented around procedures\u2014teaching practitioners how to use RAI toolkits or how to complywith their companies\u2019 RAI policies. This included how to use RAItoolkits [see 9, 29, 79, 141, for a review], including fairness toolkitssuch as Fairlearn [136], AI Fairness 360 [4], as well as transparencytools like Datasheets [50], Data Cards [100], and Model Cards [90].The focus on toolkits was important for some RAI educators: \u201cnot[just] to teach people what is fairness? What is transparency?... [but]how to actually practice it, here\u2019s how to actually implement it\u201d (E22).Analogizing RAI to agile development, one educator noted: \u201cit\u2019s sim-ilar to when you\u2019re doing a daily stand-up, the objective is a processobjective, it\u2019s a \u2018did we do the process?\u2019 \u2018Yes.\u2019 As opposed to an outcomeobjective... like did the process produce like X number of actionableitems or Y number of mitigations\u201d (E32). Similarly, many RAI ed-ucators oriented their trainings around their companies\u2019 internalRAI principles and RAI review processes. For instance, one RAIeducator \u201csocialized the [company]\u2019s responsible AI principles andcreated all sorts of games and challenges that incentivize employeesto complete them to try and build awareness. One thing that [mycolleague] found was that if you had asked employees if they hadheard of the RAI principles, they would say, yes, I know there\u2019s AIprinciples. If you ask them to name just one of them or explain it,they could not do it\u201d (E20). For them, building awareness of theircompanies\u2019 RAI principles among employees\u2014including namingor explaining what those principles were\u2014was a critical precursorto enacting broader organizational change [cf. 60, 134]. AnotherRAI educator justified this theory of change: \u201cpractices are the mosttangible thing that exist inside of a business in terms of how peopleexperience a business, its values and its decision making\u201d (E32).However, a procedural orientation to teaching about RAI toolkitsand corporate AI policies may limit AI practitioners to learningonly those aspects of RAI that the creators of RAI tools and policiesdeem relevant (or the types of algorithmic harms that such toolkitsand policies are able to address\u2014potentially acting as a \u201ctechnologyof de-politicization\u201d [cf. 53, 61, 141]), and not, for instance, focusingon \u201cmaking systems more accountable to the public in the form oftransparency or having public input into how the systems operate\u201d(E28). Similarly, participants reflected on the tensions inherent indeveloping educational resources to teach corporate principles andpractices, perhaps at the expense of a focus on values or prioritiesthat may not be aligned with corporate business imperatives [cf.81, 141], referring to companies\u2019 trainings on RAI principles as \u201cthisvery weird sanitized version of ethics\u201d (E18). however, that partici-pant expressed ambivalence, voicing a theory of change that corpo-rations developing AI products did need to get their AI developersto align with some set of values or practices, because \u2018\u201ccompaniesare the ones that are [developing AI] that we need them to [develop re-sponsibly]. I kind of feel extremely ambivalent about that\u201d (E18). This ambivalence was echoed by others who acknowledged that corpo-rations\u2019 RAI processes may be \u201cPR, but it also makes sense\u201d (E30).4.3 Aspirations for Responsible AI LearningResources",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "55f04b18-e3b2-4119-b104-7b63ee3ffd69",
                    "text": "RAI educators and AIpractitioners shared aspirations for sociotechnical approaches toRAI learning that could integrate RAI topics \u201cacross disciplinarydivides\u201d (E28) and enable practitioners to identify potential socialimpacts of algorithmic systems early in the design process. Partici-pants wanted to \u201cshift away from the technical components and moreon the social, cultural components\u201d of RAI (E13), or \u201cnot just the tech-nical angles, but the sociological and anthropological angles... [whichis] outside the scope of what [AI practitioners] typically do\u201d (E20).Some described how they used case studies of AI harms acrossdomains to help learners \u201cbe able to foresee potential harm of an AIcase... to understand the ramification and the impact of deploying anAI model within a larger system\u201d (E30). Others tried to foster theskill of identifying potential harms, either via consulting with prod-uct teams, using approaches like value-sensitive design (E28), ortrying to make time early in an ideation phase to \u201cthink more widelyabout all the potential harms as well as the opportunities\u201d (E32).Participants also wanted learning resources to help incorpo-rate perspectives from external stakeholders, such as members ofcommunities impacted by AI systems, into RAI design and evalu-ation approaches, but they felt this was not typically covered byRAI learning resources. Participants reflected on how helping AIpractitioners learn about community engagement or participatoryapproaches [cf. 15, 28, 59] could involve learning theories, methods,and skills for how to, for instance, establish relationships with com-munity groups and \u201cbeing involved with the community, listeningto these communities and, and just sitting at the same table basically\u201d(E13)\u2014but this was not a part of typical RAI learning resources. Tothe extent that current RAI learning resources do discuss engagingwith stakeholders, participants shared that it is often via adversarialtesting or more traditional user research paradigms where the goalis to identify \u201cissues with a certain product\u201d (P34), which orientsthe matters of concern around product improvements, rather thansystemic harms or impacts.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "cfc36a58-d727-4757-bf44-ee58d459f866",
                    "text": "Practi-tioners described feeling unprepared to apply what they learnedabout RAI to their day-to-day work, due in large part to the value-laden and highly contextual nature of RAI. Some desired moresupport to be able to have potentially difficult, value-laden conver-sations with coworkers. For instance: \u201cI don\u2019t know if I feel preparedto go into that conversation [with other AI practitioners] and breakingdown some misconceptions that could be harmful. How do you starta conversation about responsible AI?\u201d (P14). Educators reflected onthe ability to identify one\u2019s values and how they manifest in designchoices for algorithmic systems:\u201cYou should know about bias and that\u2019s important, butthere\u2019s a difference between that and knowing, \u2018I am asoftware engineer at this company and I know how toarticulate how I feel about this thing I\u2019m being asked tobuild,\u2019 or \u2018I know how to engage with my coworkers ina way that makes them feel safe and respected\u2019... [or]what should we choose as a target variable, and doesit have value-related implications? People might havedifferent beliefs about that.\u201d (E18)These tensions in values were common, and many educatorswanted to help learners develop the skill of recognizing that one\u2019svalues may be different from others on a product team\u2014and howto negotiate (and ideally resolve) the tensions in those values [cf.78, 84, 89]. Many participants described how they wanted to learnhow to raise issues or concerns about potential harms to theirmanager or other leadership, but these conversations about valuesmay be difficult due to power dynamics within tech companies [cf.82, 102, 139, 140]. RAI educators struggled to teach learners how toshare concerns with their manager (E36), while learners felt therewas \u201ca business case to be made [for RAI]. It\u2019s not just about doingthe right thing, which is super important...\u201d, but they struggled toknow \u201c...how can I justify this to stakeholders?\u201d (P37).Participants also described the gap they felt between learningabout RAI, and being able to apply this knowledge in their devel-opment practices, leading to desires for prescriptive guidance thatwould reduce the \u201cburden for [practitioners] to interpret it\u201d (E21). Yet,both practitioners and educators aspired to build capacity to applylearned RAI concepts and skills to new use cases, domains, contexts,applications. To close this gap, practitioners wanted resources thatwere situated in real-world examples of harms or tailored to differ-ent use cases or domains, via case studies or scenarios of RAI issues.Others wanted customized trainings for different geographic orcultural contexts [cf. 108] to help support AI practitioners who are\u201clooking [at RAI issues] in this specific country, here\u2019s a process forfairness or how to test your models\u201d (E22).",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "65c99592-d3ea-4a92-9a7e-08fa564c9160",
                    "text": "Finally,RAI educators\u2019 aspirations for what they saw as pedagogically bene-ficial approaches to RAI were often in tension with the incentives orrequirements from their organizations. RAI educators described thepedagogical benefits of live instruction, especially in synchronous,small-group learning settings. This included the accountability ofshowing up for a course led by an instructor, as well as having theability to ask questions or get help from the instructor for particulartopics (P2, E3, E36). Some pointed out the value of being able to\u201cgo off script and bring some of their own personal experiences to theclass\u201d (E20). Participants noted that sociotechnical topics in partic-ular were easier to learn in collaborative, conversational learningsettings instead of, e.g., remote, asynchronous learning such aswatching a training video or working through a Jupyter notebook:\u201cthe sociotechnical concepts are easier to do in person,right? Because you can talk to people about why dis-criminating by gender is bad... It\u2019s a little bit harder todo that in a one-way reading text-based delivery versusthe sort of Socratic conversation that can help bringpeople to the table better.\u201d (E19)However, numerous RAI educators felt that organizational con-straints (e.g., a lack of budget, time, personnel) impacted their de-cisions about the design of learning resources, making it difficultfor them to achieve their aspirations [for similar organizationalimpacts on RAI work practices, see 6, 64, 81, 82, 86, 102, 132]. RAI educators described how shifting organizational priorities made itdifficult to allocate time to create trainings, including teams thatcreated tutorials and trainings being \u201cre-orged\u201d into other teams,while others were laid off, or felt that creating educational resourceswas \u201cnot my job anymore\u201d (E16) [cf. 6]. Educators noted a tension be-tween their pedagogical aspirations for instructor-led, synchronouslearning opportunities, and their organization\u2019s pressure to developRAI trainings for large numbers of employees (i.e., \u201cscalability\u201d[cf. 58, 127])\u2014\u201cmost people would prefer some sort of instructor-ledexperience.... but self-study scales, that\u2019s the main asset of it\u201d (E20).Similarly, despite some educators\u2019 aspirations to create a pro-gression of learning resources from basic to more advanced topicsin RAI (e.g., weighing trade-offs between different dimensions ofresponsible AI, such as fairness evaluations and privacy [7]), RAI ed-ucators felt organizational pressure to create \u201clightweight\u201d trainingsthat could be quickly completed. For instance, \u201cthe first [require-ment] is that it was really important to create something that waslightweight, which meant that it did not require a lot of prep and itdidn\u2019t take a lot of time and it was very easy to understand how todo. So anyone could just take like 45 minutes to an hour and just doconsequence scanning\u201d (E32). This was a common theme across RAIeducators, who told us how they \u201coptimized for speed\u201d (E21), andhow this shaped the types of resources they created: \u201cmaybe it\u2019s ashort video like a five minute tech talk or some way of synthesizingthis rich stuff into a quick way that [they] can absorb it and moveon to [their] job?\u201d (E21). Some RAI educators discussed how theybalanced this tension by providing multiple formats for learningresources, in varying lengths and complexity (E35, E40).Such organizational pressures may have similarly shaped AIpractitioners\u2019 aspirations for learning resources. AI practitionerstold us how they wanted learning resources to give them practi-cal, actionable guidance that they could use immediately, a prag-matic desire that was at odds with the desire of many participants(both practitioners and educators) to develop reflexive mindsets andvalue-driven ways of conceptualizing and designing AI systems,as discussed in Section 4.3.2. RAI educators described how practi-tioners taking their RAI trainings wanted prescriptive guidance tomeet their companies\u2019 RAI requirements: \u201cI\u2019m seeing people say, \u2018Iknow we have an [AI review process] that you have to make sure thatyou do. So just tell me what the thing is so that I can do it... if you canmake it clearer, then you\u2019re removing some of that, that burden for meto like interpret it\u201d (E21). However, RAI educators noted that despitethis desire from practitioners, there was no automated process orsingle solution to anticipating and avoiding harms of algorithmicsystems. Participants told us how there was no \u201croadmap\u201d (E20)or \u201cquick guide\u201d (E26) for RAI, or in some cases \u201cthere isn\u2019t really aright answer\u201d (P9) at all. RAI educators described how they wantedto foster critical thinking, to shape new ways of thinking towards a\u201ccultural shift\u201d in AI development (E17), to enable \u201cpeople to makebetter decisions in the long term\u201d (E18). They raised concerns thatproviding guidance that was overly prescriptive (E13) would en-courage a mindset of \u201ca tick box [approach]\u201d (E19) [cf. 9, 141]. Asothers told us, \u201cwhat [learners] need is not what they want. Becausethere\u2019s no one answer. It depends on who\u2019s your audience, what isyour product, what is the risk tolerance of your executives?\u201d (E16).",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "dff8a88c-8b07-4c1a-b6eb-8547131e76b2",
                    "text": "The environments or sites in which learning about RAI occurs shapethe learning process in crucial ways. In our study, we found RAItrainings are often oriented around companies\u2019 AI policies or or-ganizational processes for RAI review, which can be understood asone element of organizational cultural change or RAI organizational\u201cmaturity models\u201d [60, 102, 134]. While such trainings may providelearners with opportunities to directly apply what they learn in theirwork, corporate sites for learning may also have mixed incentives:trainings may be required by their employers [e.g., 10, 11, 24, 134],but AI practitioners may face challenges applying what they learn,due to the organizational pressures for speed and scale that impedeRAI work practices [6, 64, 81, 82, 102]. In our study we see similarincentives impacting opportunities for learning about RAI, via con-straints on aspirations for RAI learning resources. On the one hand,learners and educators described aspirations to foster reflexivemindsets [cf. 20] and build capacity to apply learned RAI conceptsand skills to new use cases, domains, contexts, applications. Yet afast-paced development environment contributed to a desire for pre-scriptive guidance that removes the burden of high-stakes decision-making from practitioners [cf. 139]. The pedagogical approachesRAI educators adopt are also shaped by organizational pressures.For example, participants described desires for curricula of increas-ing depth and instructor-led collaborative learning. Yet, organiza-tional pressures to develop scalable learning opportunities droveeducators towards developing self-study resources and training.We also found that AI practitioners are learning about RAI inunstructured, self-directed ways outside of their companies\u2014frombooks, documentaries, blogs, social media, or larger communities ofAI practitioners. Much like prior work has found for self-directedlearning of web developers [36] and data scientists [72], designchoices in learning environments implicitly communicate the ob-jects of concern for the AI community and how such topics shouldbe approached. Prior work on self-directed learning for ML hasidentified learners\u2019 challenges finding the right resources for theirlearning goals [e.g., 26]\u2014however, given the risk of epistemologicalbifurcation in learning about responsible AI (i.e., into social andtechnical goals), these challenges become even more salient. AsRAI researchers, we can support informal learning opportunities bydeveloping learning resources that are able to reach a much wideraudience of people developing AI models or AI-infused applica-tions in the open-source community. However, our findings raisequestions about precisely how such informal learning resourcesmight be designed\u2014if integrated into existing toolkits [e.g., 136],how might they avoid the de-politicization or technosolutionismof that genre [cf. 141]? If integrated into online leaderboards orcommunities such as Kaggle competitions or HuggingFace [cf. 3],how might such learning resources resist the technical orientationof such approaches, rather than inadvertently reinforcing them?Finally, interpersonal learning is one key pathway by which AIpractitioners learn about RAI on-the-job. Some RAI educators in ourstudy had formal roles related to education (e.g., technical writers),though many participants who developed learning resources didso as side projects or in informal educator roles. This echoes prior work that found practitioner\u2013educator roles are common amongstdata scientists [76] and AI practitioners [30, 135, 142]. While thepredominance of practitioner\u2013educator roles may speak to under-resourcing and under-investment in RAI education, it also suggestsan opportunity to leverage RAI expertise that may be distributedacross a company. Our findings suggest opportunities for organi-zations to support interpersonal RAI learning\u2014e.g., via mentoringprograms for RAI, providing support for informal or formal con-versations with peers about RAI, or more broadly fostering a com-munity of practice for RAI. However, prior research on corporate\u201csafety cultures\u201d [cf. 117] identifies risks of relying on approaches tosafety (here, RAI) that put the onus for cultural change on workers,given the organizational pressures that may disincentivize workersfrom raising concerns about harms that may pose threats to theircompanies\u2019 business models [6, 82, 117, 132, 139, 141].",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "1e49a4cb-4459-4207-82e4-6395eb6bb9b6",
                    "text": "Throughout the interviews, we heard RAI educators grapplingwith underlying disciplinary tensions via pedagogical decisionsabout learning objectives, instructional approaches, and methodsof assessing learners\u2019 understanding. Despite an acknowledgmentfrom many participants that interdisciplinary, sociotechnical ap-proaches to responsible AI were important [cf. 34, 55, 101], thelearning design choices that RAI educators in our study describedmay reinforce a bifurcation between approaches that teach so-called\u201ctechnical\u201d AI concepts and approaches from the social sciences orhumanities that grapple with social, historical, and political forcesthat may shape or be shaped by algorithmic systems.We saw this disciplinary bifurcation reproduced when RAI learn-ing materials taught sociotechnical concepts (e.g., fairness [34] )in primarily computational ways, to appeal to learners with MLexpertise. This includes teaching concepts such as fairness in ML inways that were removed from any social context [cf. 111], treatingfairness as a metric for algorithmic optimization rather than, e.g.,understanding the historical specificity of marginalization in thecontext(s) in which AI systems are deployed [108], in which theirdata was collected or annotated [87, 88], or in which AI develop-ment teams were located [cf. 123, 138]. RAI educators\u2019 relianceon computational approaches may be a response to the existingdisciplinary norms of AI development more generally\u2014and yet,such appeals may smuggle in the positivist, technical values ofmachine learning [e.g., 16, 55, 101], rather than the aspirations ourparticipants had for more integrated, sociotechnical approaches toRAI. We also found that in cases where resources were designedwith explicit learning objectives, those objectives often emphasizedtechnical goals or made a distinction between social and technicalgoals. Given the self-directed foraging our study found, the lackof integrated sociotechnical learning objectives may lead learnersto discover or complete only those learning resources that alignwith their disciplinary identity, further reinforcing a disciplinarydivision. Instead, we suggest designing RAI resources around ex-plicitly sociotechnical learning objectives and adopting pedagogicalapproaches that involve case studies, scenarios, problem-basedlearning, or other ways of understanding how harms are situatedwithin particular historical contexts [34, 63, 69, 73, 75, 80, 113, 115].When developing assessments, or ways for learners to demon-strate mastery of RAI skills and concepts, RAI educators in our studyreported tensions between what they felt were \u201cscalable\u201d methodsfor learners to demonstrate mastery, like multiple choice questionsor code notebooks (where learners could compare their code to ahidden code block with answers), with modes of assessment per-haps better suited to sociotechnical concepts, such as reflectivewriting, presentations, group discussions, but which were seen asless scalable. However, our participants were uncertain how toadopt such approaches to demonstrating mastery of RAI concepts,in part due to either their own or their learners\u2019 disciplinary train-ing in computer science, or due to pressure from their employersto develop trainings that could be deployed to large numbers oflearners across their company. One participant voiced their concernabout the challenge of assessing mastery of sociotechnical conceptsas \u201cthere\u2019s no right answer.\u201d Although this reflects the normativequestions at the heart of contested concepts such as fairness or RAI[55], this is a challenge that other fields (e.g., the humanities) havelong since grappled with when designing assessments. Our findingssuggest that more work is needed to develop modes of assessmentthat are appropriate to an interdisciplinary, sociotechnical approachto RAI [e.g., 5, 75, 113].",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "f4e6c75b-ae86-40bc-a099-895ec43869f0",
                    "text": "Our findings echo others\u2019 calls to resist and destabilize existinghierarchies of knowledge in AI [cf. 49, 55, 101], to lead to a moresociotechnical approach to learning about responsible AI. As nu-merous educational philosophers have argued, choices about whatand how to teach are inherently choices about values [43, 51, 52,65]\u2014indeed, curricular decisions and educational standards havelong been sites of public contestation and negotiation [77, 110].In AI, in addition to tech ethics courses in higher education [e.g.,39, 40, 48, 119], professional training and on-the-job learning arepart of what Bourdieu and others refer to as socialization into anoccupational community [8, 18, 19]. In other words, choices aboutdesigning learning opportunities for RAI (or AI more generally)communicate to members of the AI field what is important for themto know and be able to do as an AI practitioner. As we discuss inSection 5.2, many approaches to learning about RAI implicitly sug-gest to AI practitioners of all roles and disciplinary backgroundsthat fairness and other RAI goals are a technical problem that canbe subsumed under model optimization goals like accuracy [cf. 16]or usability, rather than socio-political forces that shape every stepof the development, deployment, and use of AI systems [cf. 55].Thus, technosolutionist orientations to learning may implicitly sug-gest to practitioners that critical, reflexive work is somebody else\u2019sproblem [e.g., 6, 20, 55, 61, 84, 132, 141]\u2014as part of the separation ofconcerns or dis-located accountability that cultures of abstractionand modularity in computer science may reinforce [83, 138]. Toresolve this, prior work has suggested integrative, interdisciplinaryapproaches to AI, including Agre\u2019s call for \u201ccritical technical practice\u201d[2], Turkle and Papert\u2019s call for epistemological pluralism [129],among many others [e.g., 49, 55, 73, 84, 101]. It is thus worth asking why, decades after such provocations,the AI field continues to reproduce dominant technical values inthe occupational socialization of future AI researchers. We thus askhow the FAccT and RAI community might resist this when devel-oping new ways for practitioners to learn about social impacts ofalgorithmic systems. How might we as a field shift the professionalnorms and identity of AI development\u2014or what anthropologistKarin Knorr-Cetina refers to as the \u201cepistemic culture\u201d [25] of sci-entific practice in AI? As one approach, we may look to theoriesof learning as a subversive activity [43, 65, 99] for inspiration forpedagogical approaches that push learners out of their comfortzones or actively draw on disciplinary or epistemic discomfort asa generative way to support learners\u2019 growth [cf. 124].Radical education philosophers have long argued that formalsystems of schooling are likely to reproduce dominant, hegemonicviews, rather than leading to more liberatory social change [e.g.,43, 52, 65]. In AI, we see signs of what historians have referred to inother fields as \u201cnormative centering\u201d [57]\u2014wherein formal RAI train-ing may center technosolutionist, computational approaches. Tocounter this normative centering of technosolutionism, we call forpedagogical provocations that destabilize dominant, hegemonic val-ues in RAI (or AI more broadly). This may entail drawing on Freire\u2019sproblem-posing approach to learning that is situated in learners\u2019contexts [cf. 43]\u2014rather than centralized, abstracted, homogenizedapproaches to teaching a single set of ratified concepts, in waysthat may reproduce technosolutionist ideologies about (responsible)AI. Along these lines, Malik and Malik [84] draw on Freire\u2019s workon critical consciousness to call for technologists to support oneanothers\u2019 critical technical awakening via learning communities[cf. 2]. However, radical, liberatory educational philosophies andpedagogies may not fit neatly within corporate sites for learningon-the-job, which may prioritize corporate values and desideratasuch as scalability. We do not provide easy answers here, but insteadcall for a proliferation of radical approaches to fostering criticaltechnical awakenings [84] and the development of sociotechnicalRAI knowledge and skills as a core part of the AI discipline.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "92857761-b980-496c-be89-85b826b43d06",
                    "text": "Our participants were primarily tech workers\u2014future work shouldexplore learning with people outside the technology industry, in-cluding nonprofits and civil society, policymakers, and the public,including communities impacted or harmed by AI [cf. 35]. Ourinclusion criteria for AI practitioners were those who had someprior experiences with RAI, but this may have led to self-selectionof participants already interested in the topic, or who saw theirwork as RAI. In addition, 35 of 40 of our participants were locatedin the US, and as such, our findings may be skewed towards theperspectives of US-based AI practitioners; future research shouldexplore cross-cultural perspectives on learning about RAI. Finally,future work might analyze the content of RAI learning resources,which we did not include here in part due to access restrictions.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "4bcadc27-a59a-4886-9af9-865860ddb0c8",
                    "text": "As technology companies increasingly integrate AI systems intomore facets of public life and AI practitioners attempt to identify,evaluate, and mitigate potential harms of AI systems, it is criticalto understand what and how AI practitioners are learning about re-sponsible AI on-the-job. Via interviews with 16 AI practitioners and24 RAI educators from 16 organizations, we identify AI practition-ers\u2019 learning pathways for RAI; the primarily computational andprocedural orientations of many RAI learning resources; and prac-titioners\u2019 and educators\u2019 aspirations for sociotechnical approachesto RAI learning, impacted by organizational pressures. We closewith implications of our findings for learning environments forRAI, implications for the design of sociotechnical approaches tolearning about RAI on-the-job, and broader questions for the fieldabout how to foster critical reflection among AI practitioners andresist hierarchies of knowledge in RAI.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "9ba78370-4246-4038-9554-bc4cf960a956",
                    "text": "Before recruiting participants, we reviewed participant gratuityamounts, data management plans, and study design documents,including consent forms, with experts in the RAI community withmany years of human participant research experience, to reviewsocial and ethical implications of our choices. We also aligned thesechoices with standards of human subjects research within ourinstitution. We followed strict protocols to ensure the confiden-tiality, anonymity, and privacy of our participants. We collectedpersonally-identifiable information only for the purpose of provid-ing gratuities, and kept that information separate from the studydata. After transcribing the interviews, we replaced participants\u2019names with unique identifiers and removed any other potentiallyde-anonymizing information (e.g., the name of their company). Par-ticipation in our study was voluntary. At the start of each session,we walked each participant through an informed consent processand form, sharing the study\u2019s purpose and intended use of the data,and suggesting they not share confidential information. We told par-ticipants they were free to ask us to move on to a different questionif they weren\u2019t comfortable responding, and they were able to with-draw from the study at any time with no penalty for them (i.e., theywould still receive the gratuity), and we would delete their studydata if so, although none of the participants asked to withdraw.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "c155a720-2089-44be-8678-e9a139a5b7f1",
                    "text": "All authors are, or were at the time of conducting the research,employed as researchers at a large technology company. Manyof the authors have contributed to the design, development, andimplementation of RAI tools, frameworks, playbooks, or otherRAI resources. Several authors have partnered with or advisedproduct teams on RAI, such as conducting fairness evaluations,data collection best practices, and more. Multiple authors have cre-ated RAI learning resources or conducted RAI training sessionswith AI practitioners. We have backgrounds in machine learning,human\u2013computer interaction, critical computing, and tend towardspost-positivist or interpretivist traditions in qualitative research[120]. These backgrounds and experiences have shaped our re-search, through choices about the research questions to explore(e.g., focusing on learning for working professionals, rather thanlearning in academic settings) and our data analysis (e.g., choicesabout specific codes, themes, or how to interpret them in light of ourepistemological orientations and experiences with RAI in industry). Additionally, our positionality\u2014e.g., all authors were employed ata technology company\u2014may have shaped our approach to recruit-ment (and thus the set of participants). Of the 40 participants, onlytwo were employed at non-profits and two were employed at auniversity (but spoke about prior experiences in industry).",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                },
                {
                    "id": "0a3457f9-46b8-4b42-9d93-63244c27665d",
                    "text": "Although we intend this paper to open a critical conversation inFAccT and RAI about the implications of learning design choicesfor practitioners learning about RAI, we acknowledge that thiswork may have unintended impacts. First, we focus in this paperon on-the-job learning, but we do not mean to suggest that for-mal learning pathways such as ethics courses in higher educationshould be de-prioritized or under-invested in. On the contrary, wesee informal learning about RAI as a complementary approach tointervening in the occupational socialization of (responsible) AIpractitioners. Indeed, we believe there is much that RAI and FAccTresearchers can learn from prior research on integrating ethics andcritical computing into formal computer science education [e.g.,40, 48, 73, 101, 119, and see Section 2.1 for more detail]. Second, al-though we identify participants\u2019 concerns for disciplinary divisionsof RAI work into social and technical, and we argue in the Discus-sion for integrated, interdisciplinary approaches to learning aboutRAI, we acknowledge the risk that this paper may unintentionallyfurther reinforce disciplinary divisions within the AI field.ACKNOWLEDGMENTSWe want to thank our participants for sharing their experiences andthe anonymous reviewers for their helpful feedback on the draft.We also want to thank Daniel J. Barrett, Alicia Chang, SamanthaFinkelstein, Andrew Smart, Tom Stepleton, Bogdana Rakova, ZijieJay Wang, Elizabeth Anne Watkins, Hilde Weerts, David Widder,and Richmond Wong for helpful feedback that greatly improvedthis work.",
                    "reference": "[1] Michael Madaio, Shreya Kapania, Rabee Qadri, and Diyi Wang. 2024. Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations. In Proceedings of the 2024 ACM Conference. ACM, New York, NY, USA. https://doi.org/10.1145/3630106.3658988"
                }
            ]
        },
        {
            "paper_title": "Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness",
            "authors": "Y Nakao, L Strappelli, S Stumpf, A Naseer\u2026",
            "publication_info": "\u2026 Journal of Human \u2026 - Taylor & Francis",
            "paper_url": "https://arxiv.org/pdf/2206.00474",
            "chunks": [
                {
                    "id": "60935a29-d659-4922-9694-7f726ff767a9",
                    "text": "",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "afd95c7a-b1fb-4a4f-a3b0-24ad7f93bf6e",
                    "text": "Arti\ufb01cial intelligence (AI) to aid or automate decision-making is advancing rapidly, andthere are now many systems in use in jurisprudence, medicine and \ufb01nance (Barocas,Hardt, & Narayanan, 2019; Barocas & Selbst, 2016; Chouldechova, 2017; Corbett-Davies, Pierson, Feller, Goel, & Huq, 2017). However, there have been increasing callsfor a human-centred arti\ufb01cial intelligence (HCAI) design approach that is a part ofa responsible AI development process to address concerns that automated decision-making is not very reliable, safe or trustworthy because they can ignore human-valuesor contexts that changes depending on the situations (Shneiderman, 2020, 2021).HCAI proposes that considerations are made for how to integrate user control intotasks, how to involve stakeholders in responsible AI development, and how to designuser interfaces (UIs) that allow users to interact with AI systems.In recent years, a number of UIs and technologies have been proposed that aimto involve stakeholders in investigating fairness of machine learning (ML) models bymaking them more transparent, such as AI Fairness 360 (Bellamy et al., 2018a), theWhat-If Tool (Wexler et al., 2020), FairSight (Ahn & Lin, 2020), FairVis (Cabreraet al., 2019), SILVA (Yan, Gu, Lin, & Rzeszotarski, 2020), and Fairlearn (Bird et al.,2020). However, most of this strand of HCAI research has focused on enabling datascientists or ML experts to inspect and assess their models, rather than involving otherstakeholders such as domain experts or end-users. This is especially important for AIfairness, as stakeholder groups may vary in human values underlying fairness (Saxenaet al., 2020; Veale, Van Kleek, & Binns, 2018), information needs, practices, andtechnical abilities, and these di\ufb00erences need to be re\ufb02ected in the design of UIs thatallow users to inspect fairness of AI (Lee, Kim, & Lizarondo, 2017; Veale et al.,2018).In this work, we provide a design space exploration of UI components that allowboth domain experts and data scientists to investigate fairness. We investigated this,1) by conducting a series of workshops to better understand domain experts\u2019 and datascientists\u2019 practices and information needs for investigating fairness and produce a setof requirements, 2) by designing FairHIL which instantiates a suite of 7 UI componentsbased on requirements that can support these stakeholders in investigating fairnessthrough a human-in-the-loop approach, and 3) by evaluating FairHIL through a userstudy. We situated our work in the loan application domain, working directly withloan o\ufb03cers and data scientists employed by <anonymized>, a bank who partneredwith us in these studies.Our aims were as follows:\u2022 to understand how these di\ufb00erent groups of users assess AI fairness in loanapplications in terms of process, criteria, informational needs, and transparency,and develop requirements for UI design;\u2022 to design a suite of UI components to support data scientists as well as loano\ufb03cers in assessing fairness and \ufb01nding potential fairness issues in datasets thatunderlie ML models and also ML models developed from these datasets. Weshow how these UI components could be generalized for other domains;\u2022 to evaluate the usefulness and usability of these UIs through a user study in-volving data scientists and loan o\ufb03cers. We re\ufb02ect on what these results meanfor implementing these UIs into an AI development process.We contribute to an evolving space of HCAI design with a focus on developingresponsible AI. Our work concentrates speci\ufb01cally on fairness which is of great concernin increasingly automated decision-making. We advance the current state-of-the-art by\u2022 identifying the requirements of domain experts and data scientists for investi-gating fairness;\u2022 providing a set of UI components which support the processes and practices ofthese stakeholder groups;\u2022 clarifying the space of design options for developing UIs that allow fairness tobe investigated in other domains,\u2022 evaluating an instantiation of these UI components in the loan application do-main.Our paper is structured as follows. We \ufb01rst review the related work in ML fairness,paying particular attention to UIs that have been developed to interactively explorefairness in ML models. We then describe our \ufb01rst study with data scientists and loan2o\ufb03cers and show their UI requirements for investigating fairness. We then present indetail UI components that we developed in order to help data scientists and domainexperts to investigate fairness and \ufb01nd potential fairness issues. We provide the \ufb01ndingsof a second user study that evaluates these UI components with data scientists andloan o\ufb03cers. We conclude with a discussion of the implications and limitations of ourwork and future research that is warranted.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "388ec017-7961-4735-8702-eff1183afc23",
                    "text": "Fairness is often equated with justice which comprises two main principles, libertyand equality, in which all people should be entitled to basic rights and freedoms,and should have an equal opportunity (Rawls, 1958). Scholars have attempted todi\ufb00erentiate and taxonomize notions of justice (Binns et al., 2018; Cook & Hegtvedt,1983; Eckho\ufb00, 1974; Lee et al., 2017), including equity (i.e. equal distribution),procedural (i.e. relating to a fair decision-making process), interactional (i.e. relatingto people\u2019s treatment), and informational (i.e. that a decision is well-justi\ufb01ed) fairness.A common way of perceiving fairness is at the group and individual level (Hegtvedt,2005). For example, in US law, group fairness is embodied through the concepts ofdisparate treatment and disparate impact (Barocas & Selbst, 2016). The former is anintentional discriminatory treatment of groups or members of groups, and the latter,the process that causes a dissimilar impact on groups. These groups are often de-\ufb01ned using \u2018protected attributes\u2019 which are enshrined in law. For example, the UnitedKingdom Equality Act 2010 states that it is illegal to discriminate against someonebecause of their age, disability, gender reassignment, marriage and civil partnership,pregnancy and maternity, race, religion or belief, sex and sexual orientation (Equality& Commission, 2020). Other countries have also moved to protect certain groups fromunfair treatment. However, discrimination against a group can be less obvious. Even ifprotected attributes are not explicitly used in decision-making, discrimination mightbe indirect through the link of a protected attribute (e.g. race) with non-protectedattributes (e.g. zip code). A di\ufb00erent way of judging fairness is through individualfairness, which relates to the similar treatment or outcomes of two similar individu-als (Dwork, Hardt, Pitassi, Reingold, & Zemel, 2012). Often, one di\ufb03culty that arisesin individual fairness is de\ufb01ning how two people are similar. Additionally, the trade-o\ufb00between individual fairness and the group fairness widely used in legal situations alsohave been considered as a di\ufb03cult issue to solve (Dwork et al., 2012).Unfairness can manifest in ML systems through biases. The ML development processpresents numerous opportunities for bias to in\ufb01ltrate, and biases can eventually playa role in making a decision (Barocas & Selbst, 2016; Caliskan, Bryson, & Narayanan,2017; Dodge, Liao, Zhang, Bellamy, & Dugan, 2019; Friedman & Nissenbaum,1996; Hajian, Bonchi, & Castillo, 2016; Olteanu, Castillo, Diaz, & K\u0131c\u0131man, 2019),for example, through the choice and characteristics of the dataset, feature engineeringand selection, or choice of learning model. A well-known study underscores this point:COMPAS (Correctional O\ufb00ender Management Pro\ufb01ling for Alternative Sanctions), asystem used in the US justice system, was found to predict Black defendants as a muchhigher risk of violent recidivism than their White counterparts (Je\ufb00 Larson & Angwin,2016). Because a lot of these ML systems are not accountable or transparent, there isa strong need to investigate and then potentially mitigate fairness issues in ML.3To measure fairness in AI systems, a number of fairness measures have beenproposed. In the last few years over twenty di\ufb00erent types have been identi\ufb01ed(Narayanan, 2018; Verma & Rubin, 2018), for example:\u2022 Individual fairness: similar outcomes for similar individuals (Dwork et al., 2012).\u2022 Group fairness (statistical/demographic parity): people in both protected andunprotected groups have equal probability of having the positive outcome (Kami-ran & Calders, 2009; Verma & Rubin, 2018).\u2022 Subgroup fairness (intersectional bias): a combination of multiple sensitive fea-tures lead to an unfair result, which would have otherwise been considered fairwhen looking at them individually (Kearns, Neel, Roth, & Wu, 2018; Yang,Cisse, & Koyejo, 2020).\u2022 Counterfactual fairness: a comparison between predictions concerning an indi-vidual and its \u201ccounterfactual self\u201d with di\ufb00erent values for protected attributes(Kusner, Loftus, Russell, & Silva, 2017).\u2022 Equalized Odds: same error rates (false positive rate and false negative rate)in groups with di\ufb00erent values of protected attributes (Hardt, Price, & Srebro,2016).\u2022 Fairness Through Unawareness: sensitive features are not explicitely employedto make decisions (Kusner et al., 2017).\u2022 Predictive Parity: same precision for groups with di\ufb00erent values of protectedattributes (Verma & Rubin, 2018).These fairness measures can then be used to develop ways to mitigate or removebias or unfairness. There are a number of toolkits available to investigate and miti-gate fairness issues, for example, IBM\u2019s AI Fairness 360 (Bellamy et al., 2018b) andFairlearn (Bird et al., 2020). There are various approaches for bias mitigation, forexample, manipulating or cleaning observed data, reweighting observations, suppress-ing features correlated with protected attributes, or trying to remove from the dataall the information of protected attributes while trying to keep as much informationas possible from other variables (Calmon, Wei, Vinzamuri, Ramamurthy, & Varshney,2017; Kamiran & Calders, 2009; Zemel, Wu, Swersky, Pitassi, & Dwork, 2013).Bias mitigation can also include adopting training approaches such as adversarialtraining (Zhang, Lemoine, & Mitchell, 2018), regularization (Kamishima, Akaho, &Sakuma, 2011), or reductions approach (Agarwal, Beygelzimer, Dud\u00b4\u0131k, Langford, &Wallach, 2018). Finally, it is also possible to tweak the \ufb01nal outcomes in order tosatisfy the desired fairness metric (Barocas et al., 2019; Hardt et al., 2016).Despite the exploration of fairness-aware ML, there has been little agreement whichmeasures to use (Verma & Rubin, 2018) because of trade-o\ufb00s between them (Barocaset al., 2019; Barocas & Selbst, 2016; Chouldechova, 2017; Verma & Rubin,2018). It has also been argued that fairness measures are poor proxies for identifyingdiscrimination (Corbett-Davies & Goel, 2018; Narayanan, 2018; Veale et al., 2018),and that therefore statistical bias, based on fairness measures, and societal bias, basedon human values, need to be di\ufb00erentiated (Mitchell, Potash, Barocas, D\u2019Amour, &Lum, 2018; Narayanan, 2018).Whether a decision or a system is judged as fair depends very much on humanvalues embedded in individuals and society (Saxena et al., 2020), and there havebeen increasing research e\ufb00orts to investigate human fairness values empirically. Forexample, it has been found that while fairness outputs were perceived by participantsas statistically fair, some participants were not comfortable with an AI system makinghigh-stakes decisions without a human in the loop (Binns et al., 2018). In addition,4stakeholders can vary in what they perceive as fair (Narayanan, 2018). A spectrum ofvariables has been identi\ufb01ed that in\ufb02uence fairness judgements in individuals. Theseinclude education, computer literacy (Wang, Harper, & Zhu, 2020), gender (Mallari etal., 2020; Wang et al., 2020), culture and region (Berman, Murphy-Berman, & Singh,1985; Blake et al., 2015; Bolton, Keh, & Alba, 2010; Hofstede, 2011; Kim & Leung,2007; Mattila & Choi, 2006), and environment and stakeholder role (Lee et al., 2017;Veale et al., 2018). Hence, it is very important to study how stakeholders investigatefairness in di\ufb00erent domains, how stakeholder groups di\ufb00er within a domain, and alsohow to better involve them through human-in-the-loop UIs, as we do in this work.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "d8725411-37b0-46ee-9af0-a84fcac8c7c2",
                    "text": "There has been a recent in\ufb02ux of fairness UIs that look to address these questions byintroducing a human-in-the-loop approach in the development of ML models. Many ofthese e\ufb00orts are based on work in Explainable AI (XAI) to communicate models anddecisions to users, for example, through model-agnostic tools such as LIME (Ribeiro,Singh, & Guestrin, 2016) and SHAP (Lundberg & Lee, 2017). LIME looks to explainmodel predictions on an local level, identifying the features that are driving the pre-diction (Ribeiro et al., 2016). SHAP can explain the contribution of features to themodel (Lundberg & Lee, 2017). Coupled with data visualisation, XAI approaches areoften used to identify potential bias, for example, Dodge et al. (Dodge et al., 2019)investigated global and local explanation styles in fairness assessments.There are also now a number of toolkits being developed to support interactiveexploration of fairness in datasets and ML models. We review \ufb01ve of the most well-known and recent ones in detail.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "255ec469-6742-40a4-a6ef-da2f9200fdf7",
                    "text": "The What-If Tool (Wexler et al., 2020) has been developed for data scientists tovisualise fairness in ML (Figure 1 a). The UI is comprised of three screens: datapointeditor, performance and fairness, and features. The datapoint editor facilitates on-the-\ufb02y visualizations of the dataset or model. Users are easily able to bin, position,color and label data points. Features can be sliced to investigate group fairness andcombined for subgroup fairness. Data points can be directly investigated and comparedthrough counterfactual reasoning; by allowing users to select a data point, visualise itscounterfactual and adjust its values, they are able to see \u2018what could have happened\u2019and subsequently infer a reason to an outcome. Through color encoding, users are alsoable to see decision boundaries between the (red and blue) points. The performanceand fairness screen provides the user with a selection of six performance and fairnessmetrics. The features screen provides users with more descriptive statistics of thefeatures in the dataset, such as their count, mean, median, and standard deviation.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "e1fe68d2-dd7f-42b9-9421-b77865fc0f00",
                    "text": "FairVis (Cabrera et al., 2019) (Figure 1 b) is a UI developed for data scientists toaudit their models before deployment. In addition to comparing fairness between twoor more groups, it also allows users to explore intersectional bias, i.e., identifyingsubgroup discrimination. The UI is made up of four interconnected panels. The \ufb01rstpanel enables users to see all features (and their distributions), select them, or drilldown further to select speci\ufb01c target classes. The second panel provides users with the5ability to apply a number of prede\ufb01ned metrics such as accuracy, precision and recallfrom a drop-down list. Strip plots for each metric are displayed below. By clicking orhovering on subgroups, the user is able to see, through color encoding, the subgroup\u2019sposition across the selected metrics and the four panels, therefore, comparing andcontrasting the subgroup across the dataset. To explore further, the user is able toclick and pin their subgroups of interest and compare metric outputs in the thirdpanel. The fourth and last panel facilitates further exploration through a carouselof suggested subgroups, which showcases features and their classes (sorted by chosenmetric) as percentile bar charts.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "f18bfd80-76e7-4323-87c9-6f817201f240",
                    "text": "FairSight (Ahn & Lin, 2020) provides users familiar with ML with a UI that embedsa work\ufb02ow to ensure fair decision-making. The FairSight UI (Figure 1 c) is made upof six panels; the \ufb01rst, \u2018generator\u2019, supports the user in setting up a model by de\ufb01ningsensitive features, features (including showing their distribution and correlation withthe target), target and the selecting a prediction algorithm from a pre-de\ufb01ned list.After running the model, group and instance-based performance and fairness metricsare shown in the Ranking View, Rankings, and Global Inspector panels. The LocalInspector and Feature Inspector panels facilitate the investigation of a speci\ufb01c datapoint and its nearest neighbor, and the features that might contribute to bias.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "a8347018-dde0-4d29-8722-0859ea5dc582",
                    "text": "Silva (Yan et al., 2020) is a UI to support the AI development work\ufb02ow. Crucially itis claimed that this UI is useful for both data scientists and \u2019novices\u2019 unfamiliar withfairness analysis. Its main feature is a causal graph generated by structural learningalgorithms (Figure 1 d). The graph enables exploration of the features (nodes) andthe relationships (edges) between them to identify potential bias in a classi\ufb01er. Theinterface is made up of four panels; the \ufb01rst enables users to select and group featuresof interest and toggle their sensitiveness. The second panel shows the causal graph.Groups and their fairness metrics are shown in the third panel. The user is also ableto access the dataset. The \ufb01nal panel displays the fairness metrics scores of each groupfor each of the machine learning algorithms.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "7ab234ac-13e5-4cb3-a650-133d3c508db1",
                    "text": "A UI, coupled with an interview protocol, has been proposed to elicit fairness notionsfrom stakeholders who have no technical background in ML (Cheng et al., 2021) (Fig-ure 1 e).The investigations for this UI were focused on predicting child maltreatmentcases, and involved social workers and parents. The UI consists of a group view whichshows di\ufb00erences between variable groups along fairness metrics and allows divisioninto subgroup by a further variable; a case-by-case view which shows a pair of cases andtheir prediction and AI model features; and a similarity comparison view which showsa scatterplot that compares a selected case with all other cases, calculated throughfeature similarity using weighted Euclidean distance. While this UI is encouraging asit targeted at stakeholders other than data scientists, its aim is only to function as asupportive tool to elicit fairness notions instead of providing comprehensive supportto investigate fairness of an AI model.[Figure 1 near here] 6",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "997fa31a-b5fe-43c7-8ca5-bb329334a73f",
                    "text": "While every tool works in di\ufb00erent ways to support a user to explore the fairnessof an AI model, some common UI functionality is emerging designed to cover variousfunctional and informational needs. For example, all tools allow the data to be groupedin some way. Many tools either display performance or fairness metrics, or both. Table1 shows a mapping of functionality to the tools we have described in 2.2.[Table 1 near here]Our work explores a design space of UI component and associated functionality toinvestigate fairness. We aim to support a variety of stakeholder groups that di\ufb00er intheir technical ability with respect to AI, and experience with formalized notions offairness. In order to ground our exploration, we started with an empirical investigationof stakeholder functional and informational needs to investigate fairness in the loanapplications domain.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "78abdebd-695f-47c0-991d-bb81350a390e",
                    "text": "To explore user needs, we worked with two stakeholder groups: loan o\ufb03cers and datascientists employed in a partner bank, <anonymised for review> based in Italy. Loano\ufb03cers act as intermediaries between the bank and the customer, and are domainexperts in this area but typically have little formal knowledge of AI or fairness metrics.Data scientists have experience in developing ML models for banking systems andprocesses and will typically have come across some fairness metrics.We conducted two workshops with each stakeholder group to inform the explorationof this design space and uncover functional and informational needs. Due to Covid-19restrictions, all research was conducted remotely using online meeting software.Workshop 1 focused on how stakeholders investigate fairness, and what informationthat they consider important and need. It also exposed them to some initial potentialUI components, based on our literature review, as design provocations. Workshop1 was analyzed, and the insights informed the development of an initial prototype.This prototype was then discussed in Workshop 2, in order to obtain early feedbackon more detailed designs while also being open to extending functionality that wehad missed previously or needs that we had misunderstood in Workshop 1. Basedon the requirements and feedback from the workshops, we produced a \ufb01nal set of UIcomponents (section 4) for the evaluation study (section 5).",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "93e736d9-c621-49d4-bba9-1f66028662e3",
                    "text": "We recruited 6 loan o\ufb03cers (5 men, 1 woman, mean age = 36.5, stdev = 3.39) and6 data scientists (3 men, 3 women, mean age = 29.7, stdev = 4.13). For furtherbackground details, see Table 2. All participants were over 18 years old and \ufb02uent inEnglish. We excluded participants with any severe visual or hearing impairments, asit would have been di\ufb03cult to make accommodations for these impairments in thestudy setup. Five of the 6 loan o\ufb03cers had previous experience with granting retailloans in bank branches across Italy, one of which had served as branch manager. Theremaining participant had previously worked in Zagreb, Croatia in another role butmoved to Italy where he worked in retail and wealth management for a year. Four ofthe 6 data scientists were very involved in considering AI fairness as part of their role.[Table 2 near here] 7No incentives were o\ufb00ered for participation. This study was approved by the<anonymized> Research Ethics Committee.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "2cc51092-a8c9-4a77-9961-4569d2e407dd",
                    "text": "Separate workshops were held for data scientists and loan o\ufb03cers. All 6 data scientistswere in one online workshop, and the 6 loan o\ufb03cers were in another. The sessionswere run as semi-structured discussions, with the \ufb02exibility to explore relevant areasin more detail. This meant that data scientists and loan o\ufb03cers worked together intheir respective groups and discussed their approaches and views for investigatingfairness.3.2.1. Workshop 1The aim of workshop 1 was to conduct user research into how fairness was investigatedby these data scientists and loan o\ufb03cers, their functional and informational require-ments, and to expose them to initial conceptual designs. Workshop 1 took place inSeptember 2020 for each group and lasted 2 hours. These 2 hours were structured into3 parts: 1) a general discussion about fairness in AI, 2) a detailed investigation of thefairness of decisions in a given dataset, and 3) initial reactions to causal reasoninggraphs.In part 1 (approx. 30 minutes), participants had a general discussion about fairness,for example, what makes decisions in loan applications fair or unfair, and the role ofML in loan decision-making and fairness. This was to familiarize participants to thetopic at hand, and to discover any general fairness values that they paid attention to.In part 2 (approx. 1 hour), participants were asked to investigate fairness for adataset we provided to them using their own preferred way. The dataset was de-veloped by <anonymized for review>, a partner bank in this project. The datasetcontained 1000 anonymized but real loan applications and their decisions, with 26features. These features include demographic information of the applicant (age, gen-der, nationality, etc), \ufb01nancial information (household income, insurance, etc), loaninformation (amount of loan requested, purpose of loan, loan duration, monthly pay-ments, etc), as well as some information of their \ufb01nancial and banking history (yearsof service with the bank, etc). There were also some features that related to inter-nal bank procedures, such as a money laundering check and a credit score developedby the bank. The dataset was sent ahead of the workshop so that participants hadtime to familiarize themselves with it (they could but did not have to have a lookat the dataset before the workshop). We created a data visualization tool, providingthe ability to slice the features and present them using various chart types such ashistograms, scatter plots, bar graphs and a strip plot, in order to answer questionsthat participants had about the data on-the-\ufb02y. We screenshared this dataset and alsoprovided a visualisation tool so that any questions they had could be answered onthe \ufb02y, in order to further investigate the fairness of the dataset. They were asked todiscuss their individual approaches of how they would examine this dataset. As such,there was no set \ufb02ow; it was all directed by the participants.In part 3 (approx. 30 minutes), we aimed to understand how these users would reactto some initial design provocations. Based on our literature review, casual graphs, asused in (Yan et al., 2020), seemed an intuitive and well-received yet much less commonway to explore fairness that could be useful for both data scientists and loan o\ufb03cers.We wanted to see the reactions of our participants to this. Hence, we introduced8the concept of a causal graph to support fairness investigations. We only provided asimpli\ufb01ed example (Figure 2) to gauge their initial reactions.[Figure 2 near here]After the workshop, all data was transcribed and de-identi\ufb01ed. Thematic coding(Clarke, Braun, & Hay\ufb01eld, 2015) identi\ufb01ed themes around:\u2022 Fairness in AI: Opinions on the limitations and bene\ufb01ts of AI and human in-volvement in loan application decision making and fairness.\u2022 Fairness Criteria: The criterion that participants use to judge and decide fairnesswithin the dataset.\u2022 Fairness Assessments: How participants make fairness assessments within thedataset. This includes what they look at, how they manipulate data to exploreand troubleshoot, and the decision points they encounter along their journey.\u2022 Causal Graph: Comments on the limitations, bene\ufb01ts and improvements of thecausal graph presented during the workshop.The insights from workshop 1 guided the design of an initial prototype which wasdiscussed in Workshop 2.3.2.2. Workshop 2The second workshop aimed to gain early feedback on an initial prototype that wasdeveloped based on insights from workshop 1, to deepen our understanding of theinformational and functional requirements needed to support stakeholders to explorefairness, and to extend and change our designs based on requirements that we hadmissed in workshop 1 or needs that we had misunderstood. These workshops tookplace with the same groups and participants in November 2020, two months afterWorkshop 1, lasting 2 hours each.The initial prototype contained several di\ufb00erent UI components (Figure 3): a systemoverview (A), a causal graph showing the relationships between \ufb01ve features (B), adataset view (C), comparing nearest application with a di\ufb00erent outcome (D). Therewas also an opportunity to see information on subgroups. Components within theUI worked in conjunction with one another, for example, selecting an application incomponent C highlighted this application in component D.[Figure 3 near here]We took participants through the UI to gather feedback. The prototype was screen-shared; participants did not have access to it directly, instead the researcher clickedthrough it and described what would happen in a fully developed prototype.As in workshop 1, data was collected through screen and audio recordings, tran-scribed and analysed through thematic coding (Clarke et al., 2015). Themes relatingto participants\u2019 reactions were developed around the screens and interface areas, andthe clarity and usefulness of information and functionality o\ufb00ered in the prototype UI.3.3. Insights into Stakeholder Requirements",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "3745711f-2d06-46b2-95fb-0076aa23cbba",
                    "text": "There were some di\ufb00erences between data scientists and loan o\ufb03cers in perceptionsabout the role that ML should play and how human-in-the-loop ML fairness assess-ments need to be situated in the decision-making process. Both stakeholder groupsstated that ML systems do not currently provide the ability to explain their decisions.9Participants were aware of the impact biased ML systems could have on individualsand that human intervention is needed to ensure that outputs were fair. However, theyalso stated that it is di\ufb03cult for users to control the ML system\u2019s decision-making.Loan o\ufb03cers saw ML systems as additional support for human decision-making.Rather than replacing human expertise, they thought that a ML system does not seethe whole picture of the decision-making. Loan o\ufb03cer thought that the informationthat the ML system is based on might sometimes be subjective, or that importantinformation is not available to the system at all. For example:In their view, this inability of the ML system to work with tacit information andtake into account \u2018grey areas\u2019 may then lead to unfairness. This re\ufb02ects earlier \ufb01ndings(Lee et al., 2017) that pointed out that some tasks are seen to require human skill.Data scientists on the other hand leaned towards a process where the ML modelwas checked by humans before adoption.What this means for user interfaces: The context in which human-in-the-loopfairness tools are used will matter for their design. Because data scientists would usethese tools for model fairness assurance, we expect that they focus more on fairnessmeasures of the model and relationships across attributes. Loan o\ufb03cers, on the otherhand, want support for fair decision-making where humans have high control Shnei-derman (2020), and hence they would focus more on information and functions thatapply to individual applications. We do not suggest that these speci\ufb01c foci are exclu-sive, i.e. information about model fairness might also be useful to loan o\ufb03cers. Whatwe take away from this though is that the UI should be adaptable to the needs ofthese user groups, or be equally usable by both.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "8d07355f-5f3f-488e-ab6b-3a67a71e67d2",
                    "text": "Our investigation found that loan o\ufb03cers and data scientists use di\ufb00erent fairnesscriteria.Loan o\ufb03cers agreed that group fairness was important:However, they focused mainly on individual fairness, often wrestling with unin-tended bias creeping in. For example:Perhaps because of the danger of bias based on the \u2018worthiness\u2019 of loan purpose,loan o\ufb03cers focused most frequently on a\ufb00ordability, by trying to understand howincome and out-goings contributed to the application decisions. For example:10In contrast, for data scientists group fairness played a signi\ufb01cant role, while theydid not mention individual fairness frequently. In fact, it seemed that they shied awayfrom using individual fairness, possibly because it is more subjective, for example:Data scientists preferred to use metrics that captured group fairness objectively.There was also an expectation that a range of di\ufb00erent metrics should be available inthe UI, to cover di\ufb00erent ways of assessing fairness.What this means for user interfaces: Designs should support a range of popularfairness metrics, and support adding new ones if they are not already included. Aspreviously discussed, there are quite a number of ways to measure fairness and it issometimes unclear what measures to apply, therefore, it might also be necessary todescribe these metrics and how they are derived in more detail.While there are readily available metrics for capturing fairness that can be appliedin AI systems and which would be useful for data scientists, there is currently a lackof ways to support the fairness assessments of loan o\ufb03cers. This could be overcome byallowing loan o\ufb03cers to specify \u2018custom\u2019 attributes, for example, to capture the notionof a\ufb00ordability. These attributes can then be used in ensuring fairer decision-making.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "b91ee41a-62e8-4995-98e7-7a3b13774f56",
                    "text": "To make fairness assessments, both groups looked in detail at what they consideredsensitive features. Gender and citizenship were discussed the most in both groups andboth considered them unfair to be used in AI decision-making. However, age as asensitive feature proved to be more contentious. Loan O\ufb03cers in particular pointedout that young people often could not provide a long credit history, so su\ufb00ered from a\u2018cold start\u2019 problem in loan applications. Similarly, older people might be deemed toomuch of a risk if their repayment terms were long.Both groups requested information on the acceptance rates for loan applicationsrelated to feature values, particularly for the ones they considered sensitive. In general,investigating relationships between attributes was very important to them, throughexploring the distribution of attribute values to the target values. This allowed them tocompare whether there was a bias against certain applicant groups, and to determineif there was a cause for concern in terms of group fairness:Data scientist also wanted to explore subgroup fairness, in addition to simple groupfairness. This meant combining a number of feature values and looking at the accep-tance rates.As already highlighted, participants identi\ufb01ed that outcomes might be related tofeatures and feature combinations, however the stakeholders were also interested ininvestigating relationships between any features to see if they were linked to eachother, to see how bias might have crept in unintentionally:11Loan o\ufb03cers often leveraged their domain knowledge to explore fairness. For exam-ple, they were already aware how credit risk could impact the outcome, and broughtin the citizenship feature in their exploration:In our workshops, we provided Sankey diagrams (Figure 4) as design provoca-tions for communicating relationships between features values. While both stakeholdergroups stated that visualizing relationships between features is useful, it became clearthat this choice of visualization was too complex for participants to grasp, especiallyfor continuous, binned feature values. Instead, they requested simpli\ufb01ed visualizationsor numerical information.[Figure 4 near here]We also showed causal relationships underlying the data, showing these in a simplenode-edge graph (Figure 2), to allow an alternative and easy way for stakeholder toinvestigate relationships between all features. Both stakeholder groups saw the valuein having causal relationships shown:They also appreciated how these graphs could be integrated in the process ofdecision-making in loan applications and mitigating fairness issues:What this means for user interfaces: Although protected features by law willneed to be taken into account, there is considerable variability between the laws ofcountries in terms of which features count as sensitive. Further, other features mightalso be considered sensitive even though they are not protected by law. Hence, thesefeatures need to be selectable by the user, and then highlighted as especially importantin the UI.Outcomes related to features values and their ratios and distributions formed a corepart of the assessments of the stakeholders. This information will need to be made avail-able in an easily understandable format, perhaps through appropriate visualizationsthat can support comparisons between applicant groups, or ratio percentages.Exploring relationships between features should also be extended to features thatare not considered sensitive and features that do not directly in\ufb02uence the outcome. Inaddition, information about combination of features needs to be provided to investigateissues of subgroup fairness. This also allows intersectional issues of fairness to beexplored, and would allow stakeholders to \ufb01nd biases that are perhaps less obvious.Exposing causal relationships can add information that would otherwise not be12available when looking at a dataset. For loan applications this is important, as thefeatures used in calculations are often interrelated.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "406d89d6-e511-4084-9bab-85bb5366422c",
                    "text": "While a lot of the exploration of the data focused on investigating group fairness,we showed earlier (3.3.2) that loan o\ufb03cers also considered individual fairness. In theworkshops we also investigated how to support this.Feedback indicated that both stakeholder groups liked the ability to see and compareindividual applications:However, the visualizations we presented proved to be problematic as participantsdid not understand how similarity was calculated and how the data was mapped tothe axes. What they wanted to compare was the similarity of applications, especiallyif they led to di\ufb00erent outcomes, and what attributes caused this.What this means for user interfaces: Comparing similar applications was ap-preciated and should be supported in fairness exploration, in order to investigate indi-vidual fairness. These should provide information about similarity of cases, includingtheir attributes, across decision boundaries.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "bb903ef7-d20f-4b25-a88a-22736a3b505a",
                    "text": "Both groups wanted more transparency about how metrics or other information wasderived or calculated. For example, they wanted information on how the causal graphwas created, and how the \u201csimilarity\u201d score was calculated.In particular, data scientists wanted a clear distinction between what belongs to theML model and what belongs to the dataset from which the ML model is derived:It was obvious that loan o\ufb03cers struggled with ML concepts that were very famil-iar to data scientists. For example, they had di\ufb03culty with concepts such as target,precision, etc. and also how ML algorithms worked.What this means for user interfaces: Much e\ufb00ort has been directed at provid-ing explanations of ML systems and that transparency is required (Kulesza, Burnett,Wong, & Stumpf, 2015; Lim & Dey, 2009). Our \ufb01ndings echo this, showing thatusers require detailed descriptions so they can understand how information and visu-alizations have been generated, in order to interpret them e\ufb00ectively. Contextual helpcould aid in this. In addition, explanations need to be carefully targeted at di\ufb00erentstakeholders to extend users\u2019 understanding, and mental models (Kulesza, Stumpf,Burnett, & Kwan, 2012).",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "fb6df810-357e-4287-b4b1-5e7909a8ee8c",
                    "text": "Table 3 shows a list of requirements that we gathered through the workshops. Partici-pants in our study supplied many informational and functional requirements that needto be supported to explore fairness. These requirements provide a much richer designspace than the functionality provided by existing tools described in section 2.2.6. Wefound that data scientists and loan o\ufb03cers have slightly di\ufb00erent priorities but overall13their needs are surprisingly similar. Based on these requirements, we started to de-velop a prototype UI that would support data scientists and loan o\ufb03cers needs. Wedescribe this UI next.[Table 3 near here]",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "dbe4741b-bd72-48c6-a8b6-826f9806a293",
                    "text": "Our design space exploration that started with the literature review and design work-shops continued with the implementation of a UI of 7 components. We now presenta detailed description of these UI components we developed; for each component wereference the corresponding requirements we captured from the design workshops (seeTable 3. Please note that we focused on informational and functional requirementsthat allow stakeholders to investigate fairness. Requirements that focus on mitigatingfairness by adjusting the model were considered out of scope for the purposes of ourresearch. For each of these UI components we also show how it could be generalizedfor any dataset and model.The UI is divided into a setup process and the main UI screen to explore theinput dataset and the model, respectively. The setup comprises \ufb01ve steps for datascientists and four for loan o\ufb03cers, inspired by a wizard approach which is frequentlyimplemented in UI setup processes (e.g. Fairlearn (Bird et al., 2020)) and involvesloading the dataset, setting the target feature, selecting the prediction model, markingup sensitive features, and choosing fairness metrics, including the ability to de\ufb01ne a\u2019custom metric\u2019 which leads to a function builder. (In order to simplify the interface,we did not include a choice of prediction model or metrics for loan o\ufb03cers.) We willfocus on the metrics choice UI component in section 4.1.[Figure 5 near here]The main UI screen (Figure 5) allows users to rapidly switch between informationabout the dataset and the model through tabbing, and thus it supports them to easilycompare the fairness of underlying loan decisions as well as the resulting predictions.The UIs to explore the dataset and the model are very similar, each with six main com-ponents that relate to each other. The system overview (Figure 5 A) provides overallinformation about the data and target, described in section 4.2. The main componentsupporting users to explore fairness is a causal graph (Figure 5 B) which shows featuresand causal relationships between them and the target. We discuss this UI componentin detail in section 4.3. Detailed information about features and relationships, such asfairness metrics, distributions, and acceptance ratios in the causal graph, is displayedin a component underneath the graph (Figure 5 C). This UI component is describedin section 4.4. Users are also able to see how intersections of feature values a\ufb00ectedthe acceptance ratios by exploring feature combinations (Figure 5 D), as we will de-scribe in section 4.5. We also provide access to the underlying dataset (Figure 5 E)which allows \ufb01ltering and more detailed explorations, see section 4.6. Finally, usersare able to select an application in the dataset component and compare it with otherapplications in the compare applications component (Figure 5 F), which we will detailin section 4.7.Throughout we focus on providing transparency and explanations in UI compo-nents, to satisfy user requirements uncovered in section 3.3.5. We add detailed expla-nations, particularly targeting loan o\ufb03cers who struggled in the workshops with MLterminology and concepts. We concentrate on explaining the target and the algorithm,sensitive features, and the fairness metrics, and their calculation. Furthermore, we ex-14plain how the causal graph was developed and the information it showed in detail.We also provide contextual information on individual UI elements, for example whenfairness metrics are shown in the UI components.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "3856d3ed-70df-4118-a00e-07af7b0deb77",
                    "text": "A main way of exploring fairness is through the use of fairness metrics (REQ1.3,REQ2.8). Providing users with a range of popular metrics is important as fairnesscriteria are both subjective and contextual to the problem at hand. To support this,data scientists are able to choose one or more of \ufb01ve metrics: statistical parity dif-ference (SPD), equality of opportunity di\ufb00erence, average odds di\ufb00erence, disparateimpact, and Theil index (see Figure 6). These metrics are commonly provided in othertools (Bellamy et al., 2018b). For loan o\ufb03cers, we provide only one metric, SPD,because it is relatively simple and most widely used in di\ufb00erent contexts. To extendand generalize this UI component, further metrics could be added to the selection, e.g.drawn from (Mitchell et al., 2018) or (Verma & Rubin, 2018).[Figure 6 near here]We also provide the ability to specify a custom attribute which could be used asan informal \u2019metric\u2019 (REQ2.7) , in order to support alternative user-generated ordomain speci\ufb01c notions of fairness, as seen in section 3.3.2. For example, during theworkshop, stakeholders repeatedly raised notions of a\ufb00ordability which could be addedin addition to the prede\ufb01ned fairness metrics. The custom metric builder (Figure 7)allows users to give the metric a name, to select from existing features along withtheir value distributions against the target, and a work area where they are ableto construct a metric using features and mathematical operators, inspired by otherformula builders. Further extensions could obviously be made to the formula builder,for instance to show example output, or to incorporate more complex functions thanthe basic mathematical operators.[Figure 7 near here]",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "6d64d5a3-70de-4303-b61a-874debca1b78",
                    "text": "This component provides important contextual information around fairness of the dataor the model to the user by showing the number of instances in the dataset (REQ1.1),an overall target ratio, shown as a percentage and a visual pie slice, which in our usecase was the acceptance rate (REQ1.4). For the model, we show the same informationcalculated on the test data.The system overview could of course be extended with other global information.For example, for the model, we could add a positive prediction rate, alongside otheraccuracy metrics. Other forms of representing and visualizing this data could also beexplored.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "031d5c5d-890f-4fcd-9fb0-515048f93023",
                    "text": "The main way for a user to explore features and relationships between features, akey need that was highlighted in the workshops, is through a causal graph (Figure 8)(REQ2, REQ3). The causal graph is made up of nodes representing features andedges representing causal relationships between features, presented in a circular lay-out. Causal graphs can be inferred by applying causal discovery algorithms (Zheng,15Aragam, Ravikumar, & Xing, 2018) and then followed up and validated by domainexperts. Sensitive features marked up during the setup process are highlighted in gold(REQ1.5); relationship strengths are indicated by line thickness (REQ3.2); out-degreeis represented by node size. The target is shown as a circle node to di\ufb00erentiate it fromthe other features.[Figure 8 near here]In the dataset view, each feature node contains a bar graph which shows the tar-get distribution, i.e. acceptance rate, for each feature value (REQ2.2). In the case ofcategorical data this is fairly straightforward, while binning is used for continuousfeatures. We use a simple and commonly used square root approach for the binningprocess but other ways of determining bin size, such as Sturges approach, could beexplored. Above this information, we show a small horizontal bar: its length indicatesthe relative value of the statistical parity di\ufb00erence metric for this feature, mappedto a 0 to 1 bar chart. This allows users to quickly identify features for further explo-ration, either through an imbalance in target distributions for feature values, or a highfairness metric value, as discussed in section 3.3.3. We choose percentage bar graphsas they normalize the data and allow easy comparison to enable the identi\ufb01cation oftrends or possible relationships.Using a contextual menu, users are able to add a feature to a combination (seesection 4.5) (REQ2.9), toggle the feature\u2019s status as sensitive (REQ1.5), or mark thefeature as potentially \u201cunfair\u201d for further discussion (REQ1.7, REQ2.16), which setthe feature color to red.To ease visual processing load, clicking or hovering over a feature makes all con-nected relationships more prominent. Outgoing edges are colored blue to encode thedirection of causality (REQ3.2). In addition, if a relationship is clicked or hovered over,the relationship\u2019s strength is shown in numerical form, generated from the causal dis-covery algorithm.Initially, all features are displayed in the causal graph to get an overall sense ofthe data, based on feedback from the workshops (REQ1.1, REQ1.3). Users are alsoallowed to temporarily \ufb01lter features and their relationships that interest them mostfor further exploration, reducing the visual \u2018clutter\u2019 of the graph. To do so, users areable to enter a drill-down mode and select the features of interest (REQ2.1, REQ2.3).Only the selected features and their relationships are then shown in this UI component.The user is able to easily exit this drill-down view and return to the full causal graph.When investigating the model, feature importance is encoded as the feature\u2019s colorsaturation (Figure 9) (REQ3.1). Feature importance in the model represents the ab-solute value of the feature weight and therefore can be mapped to a saturation scalebetween 0 to 1. The less saturated a node, the smaller of an impact it has on the model,giving users an indication of how much features contribute to the model\u2019s predictions.[Figure 9 near here]The causal graph o\ufb00ers opportunities for extension based on domain and informa-tion requirements. Obviously, di\ufb00erent choices could have been made for encoding in-formation in the causal graph, by choosing di\ufb00erent mappings of information to visualattributes such as colors and size, by selecting other graph types, or by highlightingother important information such as di\ufb00erent feature types. However, we encouragedesigners to favor simplicity and ease of understanding in order to communicate thisinformation to data scientists but particularly to domain experts. We also implementeda common \u2018overview and drill-down\u2019 data visualisation strategy which was suited toour stakeholders. 16",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "150d06df-22dd-4e9c-a8ad-99357826e027",
                    "text": "The causal graph works in conjunction with a supporting information component.This displays additional contextual information triggered when selecting features andrelationships (REQ1.1, REQ 1.3, REQ2.2), satisfying the main requirements outlinedin section 3.3.3.In the dataset view when clicking on a feature (Figure 10), the component showsin-degree and out-degree values derived from the causal graph, a bar graph of fairnessmetric scores, and a table which shows, for each categorical or binned feature value,the number of instances in each target category \u2013 here \u2018accepted\u2019 and \u2018rejected\u2019 \u2013 andthe acceptance rate in the same way as in the system overview. In the model view, weadd prediction con\ufb01dence, i.e. the certainty for making this prediction represented byits closeness to the decision boundary (Kulesza et al., 2015), as an additional columnto this table, and display this as pie slices. This table provides the same informationas the feature bar graph in the causal graph and also allows easy, visual comparisonbetween feature values and against the overall target ratio.[Figure 10 near here]When clicking on a relationship in the causal graph, the component presents astacked bar graph (Figure 11), showing the percentage of the intersection of the tworelated feature values (REQ2.1). The outgoing feature\u2019s value (the cause) is mapped tothe x-axis of the graph, while the other feature\u2019s values are shown on the y-axis. Thissimpli\ufb01ed the Sankey diagrams originally shown in the workshops (see section 3.3.3)while supporting easier comparisons.[Figure 11 near here]This UI component could be extended to suit other use cases. The bar graph offairness metrics is extensible to show more metrics, if desired. The table of detailedfeature information could also include other information that is of importance to theuser group. As always, information could be encoded di\ufb00erently. In particular, while wechose to show relationships between two features in a very simplistic way, in preferenceto Sankey diagrams, there might be other way of better visualizing relationships forspeci\ufb01c user group.The bar graphs within this component are also connected to the dataset component(see section 4.6); clicking on a particular value or bin \ufb01lters applications shown in thedataset component. This allows users to further explore applications that might beproblematic (REQ4).",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "8185f1d3-1f38-4913-af1c-94bd0ec9b845",
                    "text": "To support the investigation of intersectional bias and subgroup fairness, as evidencedin the workshops (see section 3.3.3), users are able to add features to a combination(REQ2.9). Subgroups (Figure 12) are presented as \u2018cards\u2019 which communicate thesubgroup\u2019s acceptance rate, number of application instances in this subgroup and thecombination\u2019s feature values. To focus attention on possible issues of fairness, cardswere ordered by low to high acceptance ratio. Users were able to highlight combinationsas \u2018unfair\u2019 for further exploration and discussion.[Figure 12 near here]In the implemented prototype, users have to select subgroups of interest. In a realworking system, this could be extended through automatic calculation of all possiblesubgroup combinations. This might be computationally expensive, however. On theother hand, more support for focused user-driven exploration could also be provided,17such as \ufb01ltering against a minimum number of instances that makes up a subgroup.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "0c375ec7-d949-42b1-b1ee-d9bc1952b0f1",
                    "text": "This component enables users to see the underlying raw data and investigate individualapplications (REQ4). The dataset component (Figure 13) presents all features in aspreadsheet-like table, familiar to many users (REQ4.1). It allows users to \ufb01lter andsort by feature and value, and is also connected to the feature information componentdescribed in section 4.4. The target feature (here, the result column) and its valuesare always visible, with additional color-coding to help with rapid visual processing.[Figure 13 near here]The dataset table in the model view (Figure 14) also includes a column for predictioncon\ufb01dence. i.e. the application\u2019s certainty or closeness to the decision boundary, whichis shown as a percentage and a pie slice visualization. In addition, if a user selectsan application we show the corresponding feature values and their relative featureweights. The weights are shown as bi-directional bars, either negative (red) or positive(blue). In addition, the criticality of the feature value, i.e. the feature value multipliedby the weight of the feature, is encoded by color depth; the deeper the color, themore critical the feature value is in determining the target value. This enables usersto understand how features contribute to the model.[Figure 14 near here]Additionally, this component is also connected to the compare applications com-ponent (section 4.7). By selecting an individual application, a similarity comparisongraph is generated.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "116cc435-0206-49e1-a721-eb91c5ccd328",
                    "text": "This component is designed to give users the ability to explore similar applications thatmight have very di\ufb00erent outcomes, hence uncovering potential biases or unfairness atthe individual level (see section 3.3.3) (REQ4.3, REQ4.4). This is initiated by selectingan application in the dataset component to show a similarity comparison graph.In this graph in the dataset view (Figure 15), we plot all applications as data points.The target categories are encoded with color (accepted: blue, rejected: red) to be con-sistent with the dataset table\u2019s encoding of the target. On the y-axis, the plot showssimilarity of all applications to the currently selected application, shown in a squareoutline (REQ4.3). Di\ufb00erent similarity metrics could be applied, we chose the Pearsoncorrelation coe\ufb03cient. When investigating similarity in the dataset, the target cate-gories are presented on the x-axis. In the model view, the x-axis represents predictioncon\ufb01dence (or certainty) of the target outcome for each application; either to be ac-cepted or rejected. This allows users to understand how close a selected application isto a decision boundary, and also its relationship to all other applications (REQ4.5).[Figure 15 near here]The user can select another application on the plot, which shows the comparedfeatures side by side, along with the feature similarity score represented as a bar chart(REQ4.4). This information is ordered from least similar values to most similar.Constructing the plot relies to a great extent on the binary target which aids map-ping this information on a 2D visualisation. However, if the target had been multi-categorical or continuous, di\ufb00erent visualization choices are needed while still encodingsimilarity and prediction con\ufb01dence as important information.18",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "1c59acf0-c4a4-43c4-aabd-8493e1ee507d",
                    "text": "The aim of this study was to evaluate FairHIL to understand if the UI componentswe instantiated were useful and usable by loan o\ufb03cers and data scientists, our twostakeholder groups, to investigate fairness. As fairness is such a subjective notion, wedid not measure any \u2019task success\u2019, instead we paid attention to qualitative feedbackwhich point the way to improvements of the UI or further requirements.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "0b0ddd38-fc16-4a1f-b417-66df0c1fbd75",
                    "text": "We conducted one-to-one 1-hour sessions with 17 participants, 8 loan o\ufb03cers (meanage=38, stdev=4.54) and 9 data scientists (mean age=31.8, stdev=3.56). Table 4 showsthe background details of the participants in more detail. Due to COVID-19 restric-tions all sessions were held online through remote meeting software. Recruitment wasfacilitated by <anonymised>, a partner bank involved in the project. This study wasapproved by <anonymised> Ethics Committee. No incentives were o\ufb00ered.[Table 4 near here]",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "afbd59b2-5acf-4240-ba9c-d41d419e63f8",
                    "text": "We focused in detail on the dataset UI components, as they were very similar to theAI model UI components. We probed participants\u2019 reactions to each UI componentas they used the prototype: \ufb01rst we gathered opinions about the setup process ofloading the dataset and initialising the model, focusing in detail on selection of sen-sitive features and metrics. We then explored how they would use the prototype toinvestigate fairness of a dataset, which consisted of using the causal graph, the featureand relationship information, feature combinations, and investigating the dataset andcomparing applications. We repeated this for the model exploration where we focusedalso on the use of prediction con\ufb01dence and feature weights.As they interacted with the prototype, we encouraged the participants to \u2018thinkaloud\u2019, and verbalize if they would use the UI component, and if so, how. The studyconcluded with two post-session questionnaires to evaluate users\u2019 experience in inves-tigating fairness using the UI. First, we asked participants to answer the followingquestions around the usefulness of the prototype, using a 7-point Likert scale:\u2022 The prototype supports my ability to assess fairness e\ufb00ectively\u2022 The interface provides the information I need to assess fairness e\ufb00ectively\u2022 The interface provides a su\ufb03cient amount of detail needed to assess fairnesse\ufb00ectively\u2022 The interface provides the functionality I need to assess fairness e\ufb00ectively\u2022 The interface supports the way I reason when making a decisionWe also allowed participants to add free-text comments on their experience of theprototype. Second, we employed the simpli\ufb01ed NASA-TLX questionnaire (Hart &Staveland, 1988), which is widely used in assessing UIs and XAI to understand work-load.Sessions were screen and audio recorded and analyzed qualitatively through the-matic analysis (Clarke et al., 2015) focusing on themes of understanding, use andusefulness for each UI component; in addition, we also analyzed the usefulness ques-tionnaire\u2019s free comments qualitatively. We report the quantitative results of the ques-19tionnaires through simple descriptive statistics for each stakeholder group.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "3c720993-3f35-40ad-8c8d-d904ea14f1ec",
                    "text": "Overall, the prototype was received equally well by both sets of stakeholders, as evi-denced in the usefulness questionnaire responses (Figure 16). Loan o\ufb03cers viewed theprototype slightly more positively but we did not \ufb01nd any signi\ufb01cant di\ufb00erences in theresponses.[Figure 16 near here]Workload as measured through the NASA-TLX questionnaire was acceptable (Fig-ure 17), and there were no signi\ufb01cant di\ufb00erences between data scientists and loano\ufb03cers. We note that ratings on mental demand and e\ufb00ort were relatively high. Thisis likely due to the complexity of the UI and the high amount of information that needsto be processed by the user. However, this is balanced by the perceived performancewhich indicates that users also felt that the fairness exploration through the prototypewas paying o\ufb00 and led to success. This shows that the UI components are suitable forboth sets of stakeholders.[Figure 17 near here]Our e\ufb00orts of explaining the UI concepts in AI fairness were mainly successful, withvery few areas that caused confusion. The ML and fairness concepts that were par-ticularly di\ufb03cult for loan o\ufb03cers to grasp were targets (5 out of 8 loan o\ufb03cers) andsensitive features (4 out of 8 loan o\ufb03cers). Frequently, targets and sensitive featureswere seen as any features that mattered in decision-making, e.g. that were stronglyrelated to the target or to sensitive features. Possibly, this would require further clar-i\ufb01cations about input and output in ML predictive models, and what features areprotected by law.We now turn to investigating responses to individual UI design components.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "4dfc9cf9-f0de-4f4c-9323-c1f01bace4cb",
                    "text": "An area that requires additional transparency to increase understanding is fairnessmetrics. Both sets of stakeholders, but in particular data scientists, requested moreinformation about how they were calculated, which ones should be selected, and howthey should be applied. This re\ufb02ects the fact that in general they were not entirelytrusted as a method for assessing fairness, for example:The custom metric builder in the UI was very popular: 7 data scientists and 4loan o\ufb03cers stated that they would use it to create metrics. While data scientistsstated that they would use this to create metrics that were not included in the UI bydefault, loan o\ufb03cers wanted to create a new feature to measure a\ufb00ordability for eachapplication; it seems that they were trying to de\ufb01ne a similarity metric in the senseof (Dwork et al., 2012). For this, they wanted to de\ufb01ne and compute a\ufb00ordabilityby selecting other features, and then, if the applicant was able to a\ufb00ord the loan, usethis as an acceptance criterion, independent of other (sensitive) features. Of course, achallenge with this approach is to de\ufb01ne a\ufb00ordability in a fair way.20",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "a6aebf72-af12-43ce-bd0d-d4ec39a8cb9e",
                    "text": "Responses indicated that there were usually no problems in understanding the causalgraph; all users considered it very intuitive, for example:Encoding important relationships as thicker lines helped users to focus on the prob-lematic relationships, such as ones connected to sensitive features, which possibly causefairness issues, or to the target, which directly a\ufb00ected the \ufb01nal loan decisions. High-lighting sensitive features was mentioned positively by all data scientists and 6 loano\ufb03cers, while highlighting important relationships were mentioned by 8 data scientistsand 7 loan o\ufb03cers. Users also said that the causal graph encouraged exploration ofrelationships between features that were not directly a\ufb00ecting the target, or that itencouraged exploring features that were not sensitive.Using the casual graph, loan o\ufb03cers were able to con\ufb01rm or reject hypotheses theyhad about the data. For example:This can be useful in investigating decision-making biases, and possible discrimina-tory practices, based on domain expertise.Users could simplify the causal graph to drill down into exploring features of interestin more detail, and all participants found this useful. Participants either concentratedon sensitive features and their relationships to the target, or to hide features thatwere considered irrelevant or unimportant, usually because they did not have strongrelationships with other features.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "bac77e56-2a9d-4307-90ab-2ecb300ad436",
                    "text": "Both sets of stakeholders paid attention to the fairness metrics in the informationcomponent or metric indicator in the causal graph. However, possibly because it wasnot clear what these fairness metrics captured or how they could be used, they weremuch more interested in exploring the relationships. This was useful to explore possiblebiases and discrimination in decision-making. For example:When exploring the model, both sets of stakeholders found the information pro-vided useful for exploring possible issues in the decision-making process and identifyfairness issues. Nine data scientists and 6 loan o\ufb03cers stated that feature importancewas useful; 4 data scientists and 3 loan o\ufb03cers commented on the usefulness of theprediction con\ufb01dence indicators; 7 data scientists and 5 loan o\ufb03cers mentioned theusefulness of seeing feature weightings for individual applications.21",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "a2a5ce53-b0f2-4533-b970-21f5a2120ed1",
                    "text": "Users were able to combine features and explore their relationship to the target throughthe Group Combination UI component. Nine data scientists and 7 loan o\ufb03cers statedthat they would use the combination feature to help them investigate bias. For exam-ple:However, there were some concerns voiced by participants around scalability of thisfunctionality to include more feature intersections (we limited this to 3) or how tofacilitate easy comparisons between subgroups (we showed a percentage).",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "81b89b58-3f04-40b8-b6ab-88b14cf4e322",
                    "text": "Having access to the raw data was appreciated by all participants because it facili-tated the investigation of individual fairness. This was especially surprising since theworkshops reported in section 3.3.2 seemed to show that data scientists were not asinterested in exploring the dataset from this perspective.Linking the dataset component to the feature and relationship information compo-nent was especially appreciated:Participants stated that they liked how the dataset information was interconnectedwith other UI components which allowed them to \u2018trace\u2019 the decision-making throughto the raw data.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "57a3b649-09a1-4f99-bb2e-2b07d71e7361",
                    "text": "The compare applications component supported the exploration of individual fair-ness further. All data scientists and 6 loan o\ufb03cers stated that they would use thiscomponent. Participants contrasted individual applications in an attempt to explainalternative outcomes and to identify the features and their values that could haveimpacted the decisions, for example: 22",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "0f72f20a-54c4-418c-bf4d-c8f02009f3d8",
                    "text": "Our work has investigated the design space to support domain experts and data sci-entists to investigate fairness, and how a UI could be implemented for the loan appli-cations domain and more generally to ful\ufb01ll their requirements. We will now discuss,\ufb01rst, the limitations of our study, second, how current tools and our prototype coverthis design space, and third, implications for further extensions to FairHIL. We con-clude by discussing three areas of supporting human-in-the-loop fairness that warrantfurther attention.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "ca161a42-925d-4d3b-b250-400f20881197",
                    "text": "The work presented here has some limitations in its scope. First, our workshops andevaluation drew on a relatively small sample of participants, and thus cannot generateany quantitative \ufb01ndings. However, the value of a qualitative approach is to providerich insights into users\u2019 needs that is able to generate novel UI designs. A related issueis that we have not implemented a fully working prototype that could be evaluated ina larger \ufb01eld trial, embedded in the practices of domain experts and data scientists.Both of these concerns would need to be addressed in further work that goes beyondthe scope of this paper.Second, our investigations were only situated in the loan applications domain. Wehave already highlighted that this might a\ufb00ect the datasets used, the ML models andhence also the UI design and visualizations. Furthermore, we involved loan o\ufb03cersand data scientists from one organization. While we do not anticipate that practicesdi\ufb00er signi\ufb01cantly between banking institutions in terms of loan applications, we do notknow how fairness investigations extend to other stakeholders in loan applications, e.g.loan applicants or regulatory bodies. Similarly, domain experts and data scientists inother domains might have di\ufb00erent criteria and ways to assess fairness which warrantsfurther investigation.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "6a4714ce-f89d-4daa-a209-43480c91243e",
                    "text": "Table 5 shows how requirements that we uncovered during our workshops\u2014whichconstitute the design space for tools to support investigating fairness\u2014map to func-tionality by existing tools, including FairHIL. Currently, there are no tools that ful\ufb01llall of the requirements that we uncovered during our work. However, FairHIL is so farthe \ufb01rst to integrate most of these requirements to support both stakeholder groupsto investigate fairness. Still, particular gaps exist in providing advanced data manipu-lation functionality, comparing individual cases, and mitigating fairness by adjustingfeature weights or \u2019deleting\u2019 features. These are areas that future design endeavourscould target.[Table 5 near here]",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "e596fae5-d1d0-4f6b-b44c-0dd7e3c5bb07",
                    "text": "Our \ufb01ndings have concrete rami\ufb01cations for the design of human-in-the-loop fairnessUIs. We here summarize these insights.While metrics are useful for assessing fairness, data scientists sometimes had di\ufb03-culties in understanding fairness metrics, as found in section 5.3.1. Hence, in addition23to describing the metrics on a high level, as we did in FairHIL, the details of how theyare calculated should be given. Furthermore, users will need to be given guidance asto the expected ranges of these metrics, or when a metric indicates that there are dis-criminatory outcomes. This information could be integrated into visualizations. Loano\ufb03cers would also bene\ufb01t from this improvement, since they have less backgroundknowledge about fairness and ML concepts.From 5.3.3, we found that allowing users to see the features\u2019 relationship to thetarget and to each other was pivotal. We suggest that this could be used to identifypotential biases in decision-making, and that this could lead to further discussionswithin the organization for changing policy or applying fairness mitigation.As we have seen in 5.3.4, exploring subgroup fairness was viewed positively by bothstakeholder groups and could be a powerful way to explore intersectional bias (Kearnset al., 2018). However, further work needs to consider how to extend combinations offeatures, possibly automatically as discussed in section 4.5, while easing comparisonsbetween subgroups through visual means. To make the assessment of feature combina-tion more e\ufb00ective, it is necessary to extend the number of features taken into accountwhen comparing the combinations, and to show the number of applications includedin a speci\ufb01c subgroup.Users want a way to explore the raw data and its relationship to the target, as wefound in 5.3.5. This allows users to trace decisions back to individual data instances,and gain a deeper understanding of the data as it relates to fairness. Current interfacesrarely support this (Ahn & Lin, 2020) yet its inclusion, especially to drive furtherdetailed comparisons with other applications is warranted. Our work has shown thatcomparing applications was intuitive and useful for assessing fairness in loan applica-tions, as we found in 5.3.6. Yet there are few human-in-the-loop fairness UIs, FairSight(Ahn & Lin, 2020) being a notable exception, that currently support this approach.This component also clearly supports the exploration of individual fairness in loandecisions, helpful for both stakeholder groups. Yet, the compare applications compo-nent could be further extended. While only data scientists asked to compare \u2019clusters\u2019i.e. more than one application, this ability maybe useful for both stakeholder groups.Identifying these subgroups could also be automatically supported through, for exam-ple, k-nearest neighbor approaches. Comparisons could be extended even further, byletting users choose similarity metrics, or by letting the user build a custom similaritymetric by re-weighting features.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "9f646bf6-38c0-4531-b00c-2dcd5592e02e",
                    "text": "Our work also raises three wider questions.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "185cad37-cc0a-474e-9488-821daeec59c5",
                    "text": "Our \ufb01ndings con\ufb01rm that transparency and explanations of ML and fairness conceptsare necessary (Hohman, Head, Caruana, DeLine, & Drucker, 2019; Lim & Dey,2009). Adopting common XAI approaches (Kulesza et al., 2015; Lundberg & Lee,2017; Ribeiro et al., 2016) to explain models in a way that is understood by non-technical domain experts and technically-savvy data scientists alike might be one wayforward. To ensure understandability, information needs to be carefully encoded tosupport intuitive exploration and easy visual processing. While general principles forexplaining ML concepts have been suggested e.g. \u201dbe iterative, sound and completebut do not overwhelm\u201d (Kulesza et al., 2015), there is still much work to be done to24translate this into concrete design guidelines or design patterns.A lack of understanding around fairness measures often meant that they are nottransparent as to what they capture, how they are calculated and how they should beused. For example, in our studies, both domain experts and data scientists wanted moreinformation and explanations of when measures indicated unfairness or discrimination.There also seemed confusion when decision-making bias tipped over into unfairness,for example, older applicants had an easier time getting a loan because their creditrating was higher due to a longer credit history yet does this mean it is unfair? Fairnessmeasures could be made more transparent through a UI but solving these issues mightalso involve education of stakeholders around ML concepts and fairness principles.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "aa89cb97-f996-4629-b753-36956829c480",
                    "text": "It is becoming clear from our as well as others\u2019 work (Kasinidou, Kleanthous, Barlas,& Otterbacher, 2021; Lee, Jain, Cha, Ojha, & Kusbit, 2019; Saxena et al., 2020;Woodru\ufb00, Fox, Rousso-Schindler, & Warshaw, 2018) that di\ufb00erent stakeholders mighthave di\ufb00erent conceptualizations of what fairness is, how it should be measured, andhow fairness considerations should be integrated into the wider decision-making pro-cess when using ML. We found that in our domain, loan o\ufb03cers preferred to payattention to aspects related to individual fairness while data scientists tend to employnotions of group fairness. It could be argued that existing fairness measures captureboth of these perspectives, and thus could be integrated in a UI to support both do-main experts and data scientists. However, as previously covered in section 2.1, oftenthese measures are not compatible or there needs to be a trade-o\ufb00 between fairnessmeasures and accuracy. So who gets to decide which fairness measures to apply? Wesuggest that this is a question that urgently needs attention within an organizationaland also a socio-technical context.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "96cc2d6f-37a3-489a-a25e-a6de5ca360e5",
                    "text": "Our study suggests that requirements to support investigating fairness are very richand varied. While we found that domain experts and data scientists might go aboutinvestigating fairness slightly di\ufb00erently, it was surprising to us that in the main thesetwo stakeholder groups agreed on the information and functionality required for themto ably investigate fairness. This begs the question whether di\ufb00erent toolkits are in-deed necessary for these user groups, or whether we can \ufb01nd a way to communicatee\ufb00ectively across di\ufb00erent user types. Current perspectives in XAI argue that di\ufb00erentUIs might be necessary for di\ufb00erent stakeholder groups (Gunning et al., 2019) butour work suggests that it might be possible to craft UIs that suit a variety of users.An interesting yet open question is how fairness investigations should be integratedinto the AI model development. Most existing tools, such as What-if Tool and AIFairness 360, adopt a mitigation strategy after AI models have been developed. Whiletools such as ours and Silva can be used on datasets as well as AI models, urgentdiscussion is needed on how to integrate fairness in all steps in the AI developmentprocess, from data collection to after-deployment.25",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                },
                {
                    "id": "f5734708-5c57-409c-b434-ceef277d60bf",
                    "text": "In this paper, we presented our \ufb01ndings from a series of investigations to understandthe design space to support investigating AI fairness for domain experts and datascientists. We ran design workshops to gather requirements for human-in-the-loopfairness UIs. We then presented 7 UI components that support data scientists as wellas loan o\ufb03cers in assessing fairness and \ufb01nding potential fairness issues, instantiatedin FairHIL, a prototype UI. We addressed how these UI components could be adaptedand extended for other domains. Finally, we evaluated FairHIL through a user study.Our \ufb01ndings show that:\u2022 Requirements underlying the design space are rich and varied. Surprisingly, do-main experts and data scientists share many requirements to investigate fairness.\u2022 AI and fairness concepts need to be carefully explained, especially how theyshould be applied and what might indicate fairness issues.\u2022 Causal graphs, alongside relevant and easy-to-process information about featuresand relationships, were an intuitive way to support fairness investigations by bothstakeholder groups.\u2022 Both stakeholder groups appreciated access to the underlying data, and theability to explore and compare individual applications.\u2022 Comparing subgroups is important to users.This work will help researchers, designers and developers to better understand howdomain experts and data scientists investigate fairness, and to build better human-centred AI tools to support them. We hope that this will lead to AI systems that arefairer and more transparent for everyone.AcknowledgementWork by City, University of London was supported by Fujitsu Limited under a researchcontract.Disclosure statementWe don\u2019t have a con\ufb02ict of interest.",
                    "reference": "[1] Yuko Nakao, Lorenzo Strappelli, Simone Stumpf, Ali Naseer, et al. 2023. Towards responsible AI: a design space exploration of human-centered artificial intelligence user interfaces to investigate fairness. J. Humans. Taylor & Francis. Retrieved from https://arxiv.org/pdf/2206.00474"
                }
            ]
        },
        {
            "paper_title": "Responsible Artificial Intelligence: A Structured Literature Review",
            "authors": "S Goellner, M Tropmann-Frick, B Brumen",
            "publication_info": "arXiv preprint arXiv \u2026 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2403.06910",
            "chunks": [
                {
                    "id": "fc94c738-403a-48e1-9747-b564f2a676b9",
                    "text": ". Our research endeavors to advance the concept of responsible artificialintelligence (AI), a topic of increasing importance within EU policy discussions.The EU has recently issued several publications emphasizing the necessity of trustin AI, underscoring the dual nature of AI as both a beneficial tool and a potentialweapon. This dichotomy highlights the urgent need for international regulation.Concurrently, there\u2019s a need for frameworks that guide companies in AI develop-ment, ensuring compliance with such regulations. Our research aims to assist law-makers and machine learning practitioners in navigating the evolving landscape ofAI regulation, identifying focal areas for future attention. This paper introduces acomprehensive and, to our knowledge, the first unified definition of responsible AI.Through a structured literature review, we elucidate the current understanding ofresponsible AI. Drawing from this analysis, we propose an approach for develop-ing a future framework centered around this concept. Our findings advocate for ahuman-centric approach to Responsible AI. This approach encompasses the imple-mentation of AI methods with a strong emphasis on ethics, model explainability,and the pillars of privacy, security, and trust.Keywords. Artificial Intelligence, Responsible AI, Privacy-preserving AI, ExplainableAI, Ethical AI, Trustworthy AI",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "55bcab3b-9ea4-40e3-990e-d82edd7a13c0",
                    "text": "In the past years, a lot of research is being conducted to improve Artificial Intelligence(AI) even further, as it is already being used in many aspects of life and industry.The European Commision published a series of papers [1,2,3] in which they addresstheir strategy for AI. In their white paper on AI from 2020 \u201dA European Approach toExcellence and Trust\u201c the political options for promoting the use of AI while mitigatingthe risks associated with certain applications of this technology are set out. This proposalaims to establish a legal framework for trustworthy AI in Europe so that the secondobjective of building an ecosystem for trust can be implemented. The Framework shouldfully respect the values and rights of EU citizens. It is repeatedly emphasized that AIshould be human-centered and that European values have a high priority. The papersalso address challenging issues such as ethical issues, privacy, explainability, safety, andsustainability. It is pointed out how important security is in the context of AI and they alsopresent a risk framework in five risk groups for AI systems in short form. The documentCorresponding Author: Sabrina G\u00a8ollner, sabrina.goellner@haw-hamburg.deauthors recognize that \u201d[EU] Member States are pointing at the current absence of acommon European framework.\u201d This indicates that a common EU framework is missingand it is an important political issue.The document \u201dCommunication on Fostering a European Approach to AI\u201c repre-sents a plan of the EU Commission, where numerous efforts are presented that are in-tended to advance AI in the EU or have already been undertaken. In the beginning, itis stated that the EU wants to promote the development of a \u201dhuman-centric, sustain-able, secure, inclusive and trustworthy artificial intelligence (AI) [which] depends on theability of the European Union\u201c.The Commission\u2019s goal is to ensure that excellence in the field of AI is promoted.Collaborations with stakeholders, building research capacity, environment for develop-ers, and funding opportunities are talked about as well as bringing AI into the play forclimate and environment. Part of the discussion on trust led to the question of how tocreate innovation. It was pointed out that the EU approach should be \u201dhuman-centered,risk-based, proportionate, and dynamic.\u201cThe plan also says they want to develop \u201dcutting-edge, ethical and secure AI, (and) pro-moting a human-centric approach in the global context\u201c.At the end of the document there is an important statement: \u201dThe revised plan, therefore,provides a valuable opportunity to strengthen competitiveness, the capacity for innova-tion, and the responsible use of AI in the EU\u201c. The EC has also published the \u201dProposalfor a Regulation laying down harmonized rules on artificial intelligence\u201c which contains,for example, a list of prohibited AI practices and specific regulations for AI systems thatpose a high risk to health and safety as well as some transparency requirements.It becomes noticeable that terms in the mentioned political documents that are used todescribe the goal of trustworthy AI, however, keep changing (are inconsistent), and re-main largely undefined. The documents all reflect, on the one hand, the benefits andon the other hand the risks of AI from a political perspective. It becomes clear that AIcan improve our lives, solves problems in many ways, and is bringing added value butalso can be a deadly weapon. But on the other hand, the papers do not exactly definewhat trustworthy AI even means in concrete terms. Topics and subtopics are somehowaddressed but there is no clear definition of (excellence and) trustworthiness, but moreindirectly mentions some aspects which are important, e.g., ethical values, transparency,risks for safety as well as sustainability goals.Furthermore, we believe that trust as a goal (as defined vaguely in the documents) is alsonot sufficient to deploy AI. Rather, we need approaches for a \u201dresponsible AI\u201d, whichreflects on the EU values. This should of course also be trustworthy, but that conceptcovers just a part of the responsibility. Therefore, in this paper, our goal is to find out thestate-of-the-art from the scientific perspective and whether there is a general definitionfor \u201dtrustworthy AI\u201d. Furthermore, we want to clarify whether or not there is a definitionfor \u201dresponsible AI\u201d. The latter should actually be in the core of the political focus if wewant to go towards \u201dexcellence\u201c in AI.As a step towards responsible AI, we conduct a structured literature review that aimsto provide a clear answer to what it means to develop a \u201dresponsible AI\u201d.During our initial analysis, we found that there is a lot of inconsistency in the ter-minology overall, not only in the political texts. There is also a lot of overlap in the def-initions and principles for responsible AI. In addition, similar/content-wise similar ex-pressions exist that further complicate the understanding of responsible AI as a whole.There are already many approaches in the analyzed fields, namely trustworthy, ethical,explainable, privacy-preserving, and secure AI, but there are still many open problemsthat need to be addressed in the future.Best to our knowledge this is one of the first detailed and structured reviews dealingwith responsible AI.The paper is structured as follows. In the following section, our research method-ology is explained. This includes defining our research aims and objectives as well asspecifying the databases and research queries we used for searching. The third section isthe analysis part in which we first find out which definitions for responsible AI exist inthe literature so far. Afterward, we explore content-wise similar expressions and look fortheir definitions in the literature. These are then compared with each other. As a result,we extract the essence of the analysis to formulate our definition of responsible AI. Thesubsequent section then summarizes the key findings in the previously defined scopeswhich are part of our definition of responsible AI. We further conduct a qualitative anal-ysis of every single paper regarding the terms \u201dTrustworthy, Ethics, Explainability, Pri-vacy, and Security\u201d in a structured table and quantitative analysis of the study features.Furthermore, in the discussion part, we do specify the key points and describe the pillarsfor developing responsible AI. Finally, after mentioning the limitations of our work, weend with our conclusion and future work.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "9f745675-3626-4a83-8984-d8344548febc",
                    "text": "To answer the research questions, a systematic literature review (SLR) was performedbased on the guidelines developed in [4]. The process of doing the structured literaturereview in our research is described in detail in the following subsections and summarizedin the Systematic Review Protocol.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "c94036a8-72a0-40bf-9ce5-4d76f9ad534b",
                    "text": "In the present research, we aim to understand the role of \u201dResponsible AI\u201d from differ-ent perspectives, such as privacy, explainability, trust, and ethics. Firstly, our aim is tounderstand what constitutes the umbrella term \u201dresponsible AI\u201d, and secondly, to get anoverview of the state of the art in the field. Finally, we seek to identify the open problems,challenges, and opportunities where further research is needed.In summary, we provide the following contributions:1. Specify a concise Definition of \u201dResponsible AI\u201d2. Analyze the state of the art in the field of \u201dResponsible AI\u201d",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "c2e1626f-959f-4f77-9f0c-34fa1067e6b6",
                    "text": "Based on the aims of the research, we state the following research questions:\u2022 RQ1: What is a general or agreed on definition of \u201dResponsible AI\u201d and what arethe associated terms defining it?\u2022 RQ2: What does \u201dResponsible AI\u201d encompass?",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "861b3ac5-bf04-4fdd-88e1-f7d6bf5759e1",
                    "text": "In order to get the best results when searching for the relevant studies, we used theindexing data sources. These sources enabled us a wide search of publications that wouldotherwise be overlooked. The following databases were searched:\u2022 ACM Digital Library (ACM)\u2022 IEEE Explore (IEEE)\u2022 SpringerLink (SL)\u2022 Elsevier ScienceDirect (SD)The reason for selecting these databases was to limit our search to peer-reviewed researchpapers only.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "0231ae80-3966-4f0e-8fe4-4a1df2d0fda7",
                    "text": "To search for documents, the following search query was used in the different databases:(\"Artificial Intelligence\" OR \"Machine Learning\" OR \"Deep Learning\"OR \"Neural Network\" OR \"AI\" OR \"ML\") AND (Ethic* OR Explain* OR Trust*)AND (Privacy*).Considering that inconsistent terminology is used for \u201dArtificial Intelligence\u201d, the terms\u201dMachine Learning\u201d, \u201dDeep Learning\u201d and \u201dNeural Network\u201d were added, which shouldbe considered synonyms. Because there are already many papers using the abbreviationsAI and ML, these were included to the set of synonyms.The phrases \u201dEthic\u201d, \u201dTrust\u201d and \u201dExplain\u201d as well as \u201dPrivacy\u201d was included withan asterisk (*), for all combinations of the terms following the asterisk, are included inthe results (e.g. explain*ability). The search strings were combined using the Booleanoperator OR for inclusiveness and the operator AND for the intersection of all sets ofsearch strings. These sets of search strings were put within parentheses.The selection of the period of publication was set to two years: 2020 and 2021 to getall of the state-of-the-art papers. The search was performed in December 2021.The results were sorted by relevance prior to the inspection, which was importantbecause the lack of advanced options in some search engines returned many non-relevantresults.To exclude irrelevant papers, the authors followed a set of guidelines during thescreening stage. Papers did not pass the screening if:1. They mention AI in the context of cyber-security, embedded systems, robotics,autonomous driving or internet of things, or alike.2. They are not related to the defined terms of responsible AI.3. They belong to general AI studies.4. They only consist of an abstract.5. They are published as posters.These defined guidelines were used to greatly decrease the number of full-text pa-pers to be evaluated in subsequent stages, allowing the examiners to focus only on po-tentially relevant papers.The initial search produced 10.313 papers of which 4.121 were retrieved from ACM,1064 from IEEE, 1.487 from Elsevier Science Direct, and 3.641 from Springer Link.The screening using the title, abstract, and keywords removed 6.507 papers. During thecheck of the remaining papers for eligibility, we excluded 77 irrelevant studies and 9inaccessible papers. We ended up with 254 papers that we included for the qualitativeand quantitative analysis (see Figure 1).Figure 1. Structured review flow chart: the Preferred Reporting Items for Systematic Reviews and Meta\u2013Analyses (PRISMA) flow chart detailing the records identified and screened, the number of full-text articlesretrieved and assessed for eligibility, and the number of studies included in the review.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "4d3920f8-ceaf-42e9-a773-77a204674800",
                    "text": "This section includes the analysis part in which we first find out which definitions for\u2019responsible AI\u2019 existed in the literature so far. Afterward, we explore content-wise sim-ilar expressions and look for their definitions in the literature. These definitions are thencompared with each other and searched for overlaps. As a result, we extract the essenceof the analysis to formulate our definition of responsible AI.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "73744902-33f9-4cfa-a09b-9ee09ab1b16f",
                    "text": "In this subsection, we answer the first research question: What is a general or agreed ondefinition of \u2019Responsible AI\u2019, and what are the associated terms defining it?",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "ae597b33-6aa3-4247-a73d-ab73895e3cf5",
                    "text": "Out of all 254 analyzed papers, we only found 5 papers that explicitly introduce aspectsfor defining \u201dresponsible\u201d AI. The papers use the following terms in connection with\u2019responsible AI\u2019:\u2022 Fairness, Privacy, Accountability, Transparency and Soundness [5]\u2022 Fairness, Privacy, Accountability, Transparency, Ethics, Security & Safety [6]\u2022 Fairness, Privacy, Accountability, Transparency, Explainability [7]\u2022 Fairness, Accountability, Transparency, and Explainability [8]\u2022 Fairness, Privacy, Sustainability, Inclusiveness, Safety, Social Good, Dignity, Per-formance, Accountability, Transparency, Human Autonomy, Solidarity [9]However, after reading all 254 analyzed papers we strongly believe, that the termsthat are included in those definitions can be mostly treated as subterms or ambiguousterms.\u2022 \u2019Fairness\u2019[5] and \u2019Accountability\u2019 [5,6,7], as well as the terms \u2019Inclusiveness,Sustainability, Social Good, Dignity, Human Autonomy, Solidarity\u2019 [9] accordingto our definition, are subterms of Ethics.\u2022 \u2019Soundness\u2019[5], interpreted as \u2019Reliability\u2019 or \u2019Stability\u2019, is included within Secu-rity and Safety.\u2022 Transparency [5,6,7] is often used as a synonym for explainability in the wholeliterature.Therefore we summarize these terms of the above definitions to: \u201dEthics, Trustwor-thiness, Security, Privacy, and Explainability\u201d. However, only the terms alone are notenough to get a picture of responsible AI. Therefore, we will analyze and discuss whatthe meaning of the five terms \u201dEthics, Trustworthiness, Security, Privacy, and Explain-ability\u201d in the context of AI is, and how they depend on each other. During the analysis,we found also content-wise similar expressions to the concept of \u201dresponsible AI\u201d whichwe want to include in the findings. This topic will be dealt with in the next section.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "45c1527a-f6e9-42e3-934c-3624b4b1fd19",
                    "text": "During the analysis, we found that the term \u201dResponsible AI\u201d is often used interchange-ably with the terms \u201dEthical AI\u201d or \u201dTrustworthy\u201d AI, and \u201dHuman-Centered AI\u201d is acontent-wise similar expression.Therefore, we treat the terms:\u2022 \u201dTrustworthy AI\u201d, found in [10,11,12,13,14,15,16], and [17] as cited in [18]\u2022 \u201dEthical AI\u201d, found in [19,20,21,22,23], and [24] as cited in [25]\u2022 \u201dHuman-Centered AI\u201d, found in [26] as cited in [23]as the content-wise similar expressions for \u201dResponsible AI\u201d hereinafter.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "995f132a-5034-45b8-ab34-b8988f56e931",
                    "text": "The resulting collection of definitions from \u2019responsible AI\u2019 and \u2019content-wise similarexpressions for responsible AI\u2019 from the papers results in the following Venn diagram:Analysis: We compared the definitions in the Venn diagram and determine the follow-ing findings:\u2022 From all four sets there is an overlap of 24% of the terms: Explainability, Safety,Fairness, Accountability, Ethics, Security Privacy, Transparency.SetABCDEFGHIJKLMNO TermsSolidarity, Performance,Sustainability, Soundness,Inclusiveness--Equality, Usability,Accuracy under Uncertainty,Assessment, Reliability,Data Control, Data MinimizationReproducibility, GeneralizationUser AcceptanceSocial GoodHuman-Centered, Human Control,Human Agency-Autonomy, Non-Maleficience, Trust-Human Values, Non-Discrimination-Compliant with Rules and Laws,Social RobustnessHuman Autonomy, Dignity-Explainability, Safety, Fairness,Accountability, Ethics, SecurityPrivacy, TransparencyFigure 2. Venn diagram\u2022 The terms occurring in the set of the definition for \u2019trust\u2019 only occurred in these,which is why this makes up the second largest set in the diagram. This is due tothe fact that most of the terms actually come from definitions for trustworthy AI.\u2022 There are also 6 null sets.To tie in with the summary from the previous section, it should be pointed out onceagain that the terms \u2019Explainability, Safety, Fairness, Accountability, Ethics, SecurityPrivacy, Transparency\u2019 can be grouped into generic terms as follows: Ethics, Security,Privacy, and Explainability.We also strongly claim that \u2019trust/trustworthiness\u2019 should be seen as an outcomeof a responsible AI system, and therefore we determine, that it belongs to the set ofrequirements. And each responsible AI should be built in a \u2019human-centered\u2019 manner,which makes it therefore another important subterm.On top of these findings we specify our definition of Responsible AI in order toanswer the first research question:Responsible AI is human-centered and ensures users\u2019 trust through ethical ways ofdecision making. The decision-making must be fair, accountable, not biased, with goodintentions, non-discriminating, and consistent with societal laws and norms. ResponsibleAI ensures, that automated decisions are explainable to users while always preservingusers privacy through a secure implementation.As mentioned in the sections before, the terms defining \u201dresponsible AI\u201d result fromthe analysis of the terms in sections 3.1.1 and 3.1.2. We presented a figure depicting theoverlapping of the terms of content-wise similar expressions of Responsible AI, namely\u201dEthical AI, Trustworthy AI, and Human-Centered AI\u201d, and extracted the main termsof it. Also by summarizing the terms Fairness and Accountability into Ethics, and clar-ifying the synonyms (e.g., explainability instead of transparency), we finally redefinedthe terms defining \u201dresponsible AI\u201d as \u201dHuman-centered, Trustworthy, Ethical, Ex-plainable, Privacy(-preserving) and Secure AI\u201d.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "bfcfa820-07ca-4490-bb67-0d2bc72169d3",
                    "text": "According to our analysis of the literature, we have identified several categories in sec-tion 3 in connection to responsible AI, namely \u201dHuman-centered, Trustworthy, Ethical,Explainable, Privacy-preserving and Secure AI\u201d which should ensure the developmentand use of it.To answer the second research question (RQ2), we analyze the state-of-the-art of topics\u201dTrustworthy, Ethical, Explainable, Privacy-preserving and Secure AI\u201d in the followingsubsections. We have decided to deal with the topic of \u2019Human-Centered AI\u2019 in a sepa-rate paper so as not to go beyond the scope of this work.To find out the state of the art of the mentioned topics in AI, all 118 papers wereassigned to one of the categories \u201dTrustworthy AI, Ethical AI, Explainable AI, Privacy-preserving AI, and Secure AI\u201d, based on the prevailing content of the paper compared toeach of the topic. These papers were then analyzed and we highlight their most importantfeatures of them in the following subsections.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "a3ff69ce-8f8a-4306-816c-94bc8498a9a9",
                    "text": "A concise statement for trust in AI is as follows:\u201dTrust is an attitude that an agent will behave as expected and can be relied upon toreach its goal. Trust breaks down after an error or a misunderstanding between theagent and the trusting individual. The psychological state of trust in AI is an emer-gent property of a complex system, usually involving many cycles of design, training,deployment, measurement of performance, regulation, redesign, and retraining.\u201d[27]Trustworthy AI is about delivering the promise of AI\u2019s benefits while addressing thescenarios that have vital consequences for people and society.In this subsection, we summarize which are the aspects covered by the papers in thecategory \u201dTrustworthy AI\u201d and what are the issues to engender users\u2019 trust in AI.Surveys and Reviews The following papers analyze trustworthy AI in their survey orreview: [11,13,14,17,28,29]. The most important insights were the following:\u2022 According to [13] \u201dFormal verification is a way to provide provable guaranteesand thus increase one\u2019s trust that the system will behave as desired.\u201d However, thisis more difficult with AI because of the inherently probabilistic nature of machine-learned models and the critical role of data in training, testing, and deploying amachine-learned model.\u2022 The study of [29] observes that implementation projects of Trustworthy AI fromwhich best practices can be derived can only be found in the research contextsand not in the industry, with only a few exceptions. It is further suggested to breakdown existing implementation guidelines to the requirements of software engi-neers, computer scientists, and managers while embedding also social scientistsor ethicists in the implementation process.\u2022 The \u2019best practices\u2019 for Trusted AI formulated by [14] are Data and model trans-parency, data governance, data minimization, assessment methods (for fairness),and access requirements.\u2022 The review of [30] has revolved around trustworthy AI and discusses its need andimportance and requirements as well as testing techniques for verification.Perception of trust The following publications deal with how humans perceive Trust inAI: [31,32,33]. The interesting findings herein were as follows:\u2022 The study [31] deals with analyzing users\u2019 trust in AI. Therefore, the authors ex-amine the extent to which personal characteristics can be associated with percep-tions of automated decision-making (ADM) through AI. The insight of the studywas that Privacy can be seen as a central aspect as well as a human agency be-cause people who felt they had more control over their own online informationwere more likely to view ADM as fair and useful.\u2022 In the study of [32] the authors found out that \u201dThe general public are not usersof AI; they are subject to AI.\u201d There is a need for regulatory structures for trust-worthiness.\u2022 The study of [33] deals with Trust and Perceived Fairness around Healthcare AIand Cultural Mistrust. The key findings highlight that research around human ex-periences of AI should consider critical differences in social groups.Frameworks Frameworks for \u201dhow developing Trustworthy AI can be achieved\u201d werepresented in [26,34,35]. The most important finding herein as the \u201dchain of trust\u201d:\u2022 The authors in the study [34] use various interrelated stages of a system life cyclewithin the development process for their concept. They then describe this processas forming the \u201dChain of Trust\u201d.\u2022 [36] introduce a \u201dnew instrument to measure teachers\u2019 trust in AI-based EdTech,to portrayprovides evidence of its internal structure validity, and uses itsecondary-level school teachers\u2019 attitudes toward AI.\u201c\u2022 [37] develop a conceptual model called MATCH, which describes how trustwor-thiness is communicated in AI systems. They highlight transparency and interac-tion as AI systems\u2019 affordances that present a wide range of trustworthiness cuesto users.\u2022 [38] consider the challenge of verified AI from the perspective of formal methodsfor making AI more trustworthy.\u2022 [15] we provide AI practitioners with a comprehensive guide for building trust-worthy AI systems.\u2022 [39] propose a framework and outlined case studies for applying modern datascience to health care using a participatory design loop in which data scientists,clinicians, and patients work together.\u2022 [40] describes an architecture to support scalable trustworthy ML and describesthe features that have to be incorporated into the ML techniques to ensure thatthey are trustworthy.\u2022 [41] conceptualize trust in AI in a multidimensional, multilevel way and examinethe relationship between trust and ethics.Miscellaneouslowing: In other papers [42,43] related to \u201dTrustworthy AI\u201d, we found the fol-\u2022 Trust can be improved if the user can successfully understand the true reasoningprocess of the system (called intrinsic trust) [42]\u2022 in the paper of [44] is about information fusion as an integrative cross-sectionaltopic to gain more trustworthiness, robustness, and explainability.\u2022 In the paper of [45] reviews the state of the art in Trustworthy ML (TML) researchand shed light on the multilateral tradeoffs, which are defined as the trade-offsamong the four desiderata for TML they define as \u2019accuracy, robustness, fairness,and privacy\u2019 in the presence of adversarial attacks.\u2022 [46] showed how to assess Trustworthy AI (based on the EU guidelines) in prac-tice in times of pandemic based on a deep-learning-based solution deployed at apublic hospital.\u2022 The article of [16] discusses the tradeoffs between data privacy and fairness, ro-bustness as well as explainability in the scope of trustworthy ML.\u2022 [47] introduced the federated trustworthy Artificial Intelligence (FTAI) architec-ture.Some papers we did not primarily categorize as \u201dTrustworthy AI\u201d (they rather be-long to explainable AI) also mention important points dealing with trustworthiness:\u2022 According to [12], understanding AI is another important factor to achieve trust.Understanding means how AI-led decisions are made and what determining fac-tors were included that are crucial to understanding.\u2022 Understanding is directly linked to the confidence if a model will act as intendedwhen facing a given problem [6].\u2022 According to [48], in addition to understanding, also knowing the predictionmodel\u2019s strengths and weaknesses is important for gaining trust.We conclude that trust must be an essential goal of an AI application in order to beaccepted in society and that every effort must be made to maintain and measure it at alltimes and in every stage of development. However, trustworthy AI still remains as a bigchallenge as it is not addressed (yet) holistically.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "71c157e6-8231-4fb6-82b8-73665027fccb",
                    "text": "In this subsection, we list the findings in the field of ethical AI. In our opinion, thedefinition found in [49] best describes ethics in conjunction with AI:\u201dAI ethics is the attempt to guide human conduct in the design and use of artificialautomata or artificial machines, aka computers, in particular, by rationally formu-lating and following principles or rules that reflect our basic individual and socialcommitments and our leading ideals and values [49].\u201dNow we come to summarize the most important key points that came up while ana-lyzing the literature.Reviews and Surveys\u2022 [50] reviews the ethical and human rights challenges and proposed mitigationstrategies to discuss how a regulatory body could be designed to address thesechallenges.\u2022 [51] gives a comprehensive overview of the field of AI ethics, including a sum-mary and analysis of AI ethical issues, ethical guidelines, and principles, ap-proaches to addressing AI ethical issues, and methods for evaluating AI ethics.\u2022 In the survey of [52] an overview of the technical and procedural challenges in-volved in creating medical machine learning systems responsibly and in confor-mity with existing regulations, as well as possible solutions to address these chal-lenges, are discussed.\u2022 [53] conducted a scientometric analysis of publications on the ethical, legal, social,and economic (ELSE) implications of artificial intelligence.\u2022 [54] provide a systematic scoping review to identify the ethical issues of AI appli-cation in healthcare.\u2022 [55] conduct a semi-systematic literature review and thematic analysis to deter-mine the extent to which the ethics of AI business practices are addressed in awide range of guidelines.\u2022 The review of [56] contributes to the debate on the identification and analysis ofthe ethical implications of algorithms which aims to analyze epistemic and nor-mative concerns and offer actionable guidance for the governance of the design,development, and deployment of algorithms.Frameworks Implementing Ethical AI is often discussed and structured in frameworksbecause the difficulty in moving from principles to practice presents a significant chal-lenge to the implementation of ethical guidelines. As also stated in [22], there is still asignificant gap. The following papers deal with solutions in the form of frameworks onthis topic.\u2022 In [57] the authors present a systematic framework for \u201dsocially responsible AIalgorithms.\u201d The topics of AI indifference and the need to investigate sociallyresponsible AI algorithms are addressed.\u2022 [21] provided theoretical grounding of a concept named \u2019Ethics as a Service\u2019.\u2022 [58] developed a choices framework for the responsible use of AI for organizationswhich should help them to make better decisions toward the ethical use of AI.They distinguish between AI-specific technical choices e.g. continuous learningand generic digital technical choices, e.g., privacy, security, and safety.\u2022 [59] proposed the \u201dEthics by design\u201d framework which can be used to guide thedevelopment of AI systems. The \u201dI\u201d is based on three main aspects, \u201dintelligibility,fairness, auditability\u201d with the prototyping phase being crucial to establishing asolid ethical foundation for these systems.\u2022 The article [7] also discusses about the possibility of developing ethical AI in acompany by means of a framework. Different steps that lead through the wholedevelopment phase are discussed. It is also emphasized that rigorous testing andcontinuous measurement are of high importance to ensure that the system remainsethical and effective throughout its life cycle.\u2022 In [60] two frameworks are presented including one for a responsible design pro-cess and another for better resolution of technology experience to help address thedifficulty of moving from principle to practice in ethical impact assessment.\u2022 [61] presents \u201dECCOLA\u201d, a method using some kind of gamification method forimplementing ethically aligned AI systems.\u2022 [62] advocates the use of licensing to enable legally enforceable behavioral useconditions on software and code and provides several case studies that demon-strate the feasibility of behavioral use licensing. It\u2019s envisioned how licensing maybe implemented in accordance with existing responsible AI guidelines.\u2022 [63] present TEDS as a new ethical concept, which focuses on the application ofphenomenological methods to detect ethical errors in digital systems.\u2022 [64] present a framework for assessing AI ethics and show applications in the fieldof cybersecurity.\u2022 [65] propose a framework for developing and designing AI components within theManufacturing sector under responsible AI scrutiny (i.e. framework for develop-ing ethics in/by design).\u2022 [66] presents a novel approach for the assessment of the impact of bias to raiseawareness of bias and its causes within an ethical framework of action.\u2022 [67] relates the literature about AI ethics to the ethics of systemic risks, proposesa theoretical framework based on the ethics of complexity as well as applies thisframework to discuss implications for AI ethics.\u2022 [68] propose an extension to FMEA, the \u201dFailure mode and effects analysis\u201d,which is a popular safety engineering method, called \u201cFMEA-AI\u201d to support theconducting of \u201cAI fairness impact assessments\u201d in organizations.\u2022 [69] are mapping AI ethical principles onto the lifecycle of an AI-based digital ser-vice and combining it with an explicit governance model to clarify responsibilitiesin operationalization.\u2022 [70] summarise normative ethical theories to a set of \u201dprinciples for writing algo-rithms for the manufacture and marketing of artificially intelligent machines\u201d.\u2022 [71] offer a solution-based framework for operationalizing ethics in AI for health-care.\u2022 [72] provide a holistic maturity framework in the form of an AI ethics maturitymodel that includes six critical dimensions for operationalizing AI ethics withinan organization.Tools\u2022 [73] present a tool called: REvealing VIsual biaSEs (REVISE), that assists in theinvestigation of a visual dataset, surfacing potential biases along three dimensions:(1) object-based, (2) person-based, and (3) geography-based.Ethical issues of AI The following papers discuss many of the existing ethical issues ofAI: \u2022 [74] identify the gaps in current AI ethics tools in auditing and risk assessmentthat should be considered.\u2022 In [21] the explainability problem deals with the fact that an AI black-box modelis difficult to make understandable, and the public reason deficit, the translation ofcode into a set of justifications in natural language.\u2022 The work of [75] looks more closely at the concept of ethical debt in AI and itsconsequences. The authors point out that the biggest challenge is seen here as thediscrepancy between those who incur debt and those who ultimately pay for it.There is concern that the AI industry does little to address the complex sociotech-nical challenges and that the industry is predominantly composed of individualsleast likely to be affected by ethical debt.\u2022 The contribution of [76] to the literature on ethical AI concentrates on the workrequired to configure AI systems while addressing the AI engineer\u2019s responsibilityand refers to situations in which an AI engineer has to evaluate, decide and act ina specific way during the development.\u2022 [77] presents the findings of the \u201dSHERPA project\u201d, which used case studies anda Delphi study to identify what people perceived to be ethical issues. The primaryand frequent concern is privacy and data protection, which points to more generalquestions about the reliability of AI systems. Lack of transparency makes it moredifficult to recognize and address questions of bias and discrimination. Safety isalso a key ethical issue; mostly this involves autonomous driving or systems forcritical health services. It then addresses ethical issues arising from general artifi-cial intelligence and sociotechnical systems that incorporate AI.\u2022 [78] points out different dilemmas like \u201dHuman Alienation\u201d, \u201dPrivacy Disclosure\u201dand \u201dResponsibility Issues\u201d. First, the author goes into various points such as hu-man alienation (replacing human work with machines) which leads to higher un-employment rates; relying on smart technologies can lead to a decrease in inde-pendence; and the weakening of interpersonal relationships because of a closerrelationship between man and machine. Second, the author addresses the issue ofprivacy leakage. He claims that service providers such as Google and Amazonare not complying with the General Data Protection Regulations in terms of com-pleteness of information, clarity of language, and fairness of processing. Third, itwill inevitably bring moral decision-making risks.\u2022 [79] shows up several ethical issues an AI-Ethicist should consider when makingdecisions and especially the dilemma when an AI ethicist must weigh the extent towhich his or her own success in communicating a recognized problem involves ahigh risk of reducing the chances of successfully solving the problem. This is thenresolved through different ethical theories (such as virtue ethics, deontologicalethics, and consequentialist ethics).\u2022 [80] reviewed Nissenbaum\u2019s \u201dfour barriers\u201d to accountability, addressing the cur-rent situation in which data-driven algorithmic systems have become ubiquitousin decision contexts.\u2022 The key finding of [81] is, that there is indeed a notable gap between the practicesof the analyzed companies and the key requirements for ethical/trustworthy AI.\u2022 [82] assesses and compares existing critiques of current fairness-enhancing tech-nical interventions in machine learning that draw from a range of non-computingdisciplines e.g. philosophy.\u2022 [83] explores the ethical issues of AI in environmental protection.\u2022 The work of [84] outlines the ethical implications of AI from a climate perspec-tive.\u2022 The authors of [85] make the case for the emergence of novel kinds of bias withthe use of algorithmic decision-making systems.\u2022 [86] discusses blind spots regarding to topics that hold significant ethical impor-tance but are hardly or not discussed at all in AI ethics.\u2022 The critical discussion of [87] argues for the application of cognitive architecturesfor ethical AI and\u2022 [88] provide an overview of some of the ethical issues that both researchers andend users may face during data collection and development of AI systems, aswell as an introduction to the current state of transparency, interpretability, andexplainability of systems in radiological applications.\u2022 [89] contributes critically to the ethical discussion of AI principles, arguing thatthey are useless because they cannot in any way mitigate the racial, social, and en-vironmental harms of AI technologies, and seeks to suggest alternatives, thinkingmore broadly about systems of oppression and more narrowly about accuracy andauditing.Miscellaneous The papers of [90,19,91,92,93,94,22,21,95,96,97,98] deal with ethicalAI in their studies whereas they handled miscellaneous topics.\u2022 [90,94] evaluates existing ethical frameworks.\u2022 [19] gives a review of the documents that were published about ethical principlesand guidelines and the lessons learned from them.\u2022 [92] surveyed the ethical principles and also their implementations. The papersuggested checklist-style questionnaires as benchmarks for the implementation ofethical principles of AI.\u2022 [93] collected insights from a survey of machine learning researchers.\u2022 The study [96] reports the use of an interdisciplinary AI ethics program for highschool students. Using short stories during the study was effective in raisingawareness, focusing discussion, and helping students develop a more nuanced un-derstanding of AI ethical issues, such as fairness, bias, and privacy.\u2022 [97] provides an empirical study to investigate the discrepancies between the in-tended design of fairness mitigation tools and their practice and use in context.The focus is on: disaggregated assessments of AI systems designed to reveal per-formance differences across demographic groups.\u2022 [98] compare AI and Human Expert Collaboration in Ethical Decision Makingand investigate how the expert type (human vs. AI) and level of expert autonomy(adviser vs. decider) influence trust, perceived responsibility, and reliance.\u2022 [99] created a field guide for ethical mitigation strategies in machine learningthrough a web application.\u2022 [8] adds \u2019data provenance\u2019 as an important prerequisite to the table for mitigatingbiases stemming from the data\u2019s origins and pre-processing to realize responsibleAI-based systems.\u2022 [100] aims to provide a multi-disciplinary assessment of how fairness for machinelearning fits into the context of clinical trials research and practice.\u2022 [101] perform an empirical study involving interviews with 21 scientists and en-gineers to understand the practitioners\u2019 views on AI ethics principles and theirimplementation.\u2022 The work of [102] explores the design of interpretable and interactive human-in-the-loop interfaces that enable ordinary end-users with no technical or domainknowledge background to identify and potentially address potential fairness is-sues.\u2022 The article of [103] is an attempt to outline ethical aspects linked to iHealth byfocussing on three crucial elements that have been defined in the literature: self-monitoring, ecological momentary assessment (EMA), and data mining.\u2022 [104] have surveyed hundreds of datasets used in the fair ML and algorithmicequity literature to help the research community reduce its documentation debt,improve the utilization of existing datasets, and the curation of novel ones.\u2022 [105] surveyed the major ethical guidelines using content analysis and analyzedthe accessible information regarding their methodology and stakeholder engage-ment.\u2022 [106] introduce a business ethics perspective based on the normative theory ofcontractualism and conceptualize ethical implications as conflicts between the val-ues of different interest groups.\u2022 [107] proposes a comparative analysis of the AI ethical guidelines endorsed byChina and by the EU.\u2022 The empirical study of [108] deals with the ethics of using ML in psychiatricsettings.\u2022 [109] compares the discourses of computer ethics with AI ethics and discussestheir similarities, differences, issues, and social impact.\u2022 The work of [110] is moving from the AI practice towards principles: Ethical in-sights are generated from the lived experiences of AI designers working on tangi-ble human problems, and then cycled upward to influence theoretical debates.\u2022 The main aim of [111] has been to outline a new approach for AI ethics in heavyindustry.\u2022 [112] deals with research on pro-social rule breaking (PSRB) for AI.\u2022 [113] aims to provide an ethical analysis of AI recruiting from a human rightsperspective.\u2022 [114] identify and discuss a set of advantages and ethical concerns related to in-corporating recommender systems into the digital mental health ecosystem.\u2022 The article of [115] focuses on the design and policy-oriented computer ethicswhile investigating new challenges and opportunities.\u2022 The main goal of [116] was to shed philosophical light on how the responsibilityfor guiding the development of AI in a desirable direction should be distributedbetween individuals and between individuals and other actors.We also generally found during our analysis that Ethical AI deals often with fair-ness, therefore this should be mentioned here. Fair AI can be understood as \u201dAI systems[which] should not lead to any kind of discrimination against individuals or collectivesin relation to race, religion, gender, sexual orientation, disability, ethnicity, origin or anyother personal condition. Thus, fundamental criteria to consider while optimizing theresults of an AI system is not only their outputs in terms of error optimization but alsohow the system deals with those groups.\u201d[6]In any case, the development of ethical artificial intelligence should be also subjectto proper oversight within the framework of robust laws and regulations.It is also stated, that transparency is widely considered also as one of the central AIethical principles [61].In the state-of-the-art overview of [117] the authors deal with the relations betweenexplanation and AI fairness and examine, that fair decision-making requires extensivecontextual understanding, and AI explanations help identify potential variables that aredriving the unfair outcomes.Mostly, transparency and explainability are achieved using so-called explainability(XAI) methods. Therefore, it is discussed separately in the following/next section.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "f47bb3d1-0836-4cee-a768-051b5249533d",
                    "text": "Decisions made by AI systems or by humans using AI can have a direct impact on thewell-being, rights, and opportunities of those affected by the decisions. This is whatmakes the problem of the explainability of AI such a significant ethical problem. Thissubsection deals with the analysis of the literature in the field explainable AI (XAI).We found an interesting definition in [6] which is quite suitable for defining explain-able AI:Given a certain audience, explainability refers to the details and reasons a modelgives to make its functioning clear or easy to understand.[6]In the following subsections, we highlight the most interesting aspects of XAI.Black-box models problem According to [6] there is a trade-off between model explain-ability and performance. The higher accuracy comes at the cost of opacity: it is generallynot possible to understand the reasons that explain why an AI system has decided theway it did, that it is the correct decision, or course of action was taken properly. Thisis what, according to the literature, is often called interchangeably, AI\u2019s \u201cblack box,\u201d\u201cexplainability,\u201d \u201ctransparency,\u201d \u201cinterpretability,\u201d or \u201cintelligibility\u201d problem [118] or\u201dblack box model syndrome\u201d [119].Another point to mention here is, that \u2019Explainable AI\u2019, which aims to open the blackbox of machine learning, might also be a Pandora\u2019s Box according to [120]. This meansthat opening the black box might undermine trust in an organization and its decision-making processes by revealing potential limitations of the data or model defects.[121] claim also, that there is also a need for an explanation of how ML tools havebeen built, which requires documenting and justifying the technical choices that practi-tioners have made in designing such tools.Synonyms for XAI These papers deal with the synonyms in context with XAI:\u2022 There is not yet consensus within the research community on the distinction be-tween the terms interpretability, intelligibility, and explainability, and they are of-ten, though not always, used interchangeably [122,6,123].\u2022 [48] says that usually, interpretability is used in the sense of understanding howthe predictive model works as a whole. Explainability, on the other hand, is of-ten used when explanations are given by predictive models that are themselvesincomprehensible.\u2022 [124] mentioned 36 more notions related to the concept of explainability in theirsystematic review (e.g., \u2019Actionability\u2019, \u2019Causality\u2019, \u2019Completeness\u2019, \u2019Compre-hensibility\u2019, \u2019Cognitive relief\u2019, etc.) They also provided a description of each ofthese notions.\u2022 According to [125] the lack of consistent terminology hinders the dialog aboutXAI.Motivation for XAI The following papers address the motivation for XAI:\u2022 The key motivation of XAI is to \u201d(1) increase the trustworthiness of the AI, (2)increase the trust of the user in a trustworthy AI, or (3) increase the distrust of theuser in a non-trustworthy AI\u201d [42].\u2022 Explainability should be also considered as a bridge to avoid the unfair or unethi-cal use of the algorithm\u2019s outputs.[6]\u2022 According to [48] other motivating aspects are causality, transferability, infor-mativeness, fair and ethical decision-making, accountability, making adjustments,and proxy functionality.\u2022 It should also help end-users to build a complete and correct mental model of theinferential process of either a learning algorithm or a knowledge-based system andto promote users\u2019 trust for its outputs, [124] and reliance on the AI system [126].Reviews and Surveys\u2022 [127] have reviewed explainable and interpretable ML techniques for varioushealthcare applications while also highlighting security, safety, and robustnesschallenges along with ethical issues.\u2022 [128] provides a survey, that attempts to provide a comprehensive review of globalinterpretation methods that completely explain the behavior of the AI modelsalong with their strengths and weaknesses.\u2022 [129] presents an extensive systematic literature review of the use of knowledgegraphs in the context of Explainable Machine Learning.\u2022 [130] present a mini-review on explainable AI in health care, introducing solutionsfor XAI leveraging multi-modal and multi-center data fusion followed by twoshowcases of real clinical scenarios.\u2022 [131] proposed survey explicitly details the requirements of XAI in Healthcare5.0, the operational and data collection process.\u2022 The review of [132] aims to provide a unified and comprehensive review of thelatest XAI progress by discovering the critical perspectives of the rapidly growingbody of research associated with XAI.XAI Techniques There are many different XAI techniques discussed in the literature.[6] as well as [48] give a detailed overview of the known techniques and their strengthsand weaknesses, therefore we will only cover this topic in short.First, the models can be distinguished into two different approaches to XAI, the intrinsi-cally transparent models and the Post-hoc explainability target models that are not read-ily interpretable by design. These so-called \u201dblack-box models\u201d are the more problem-atic ones, because they are way more difficult to understand. The post-hoc explainabilitymethods can then be distinguished further into model-specific and model-agnostic tech-niques.We can also distinguish generally between data-independent and data-independent mech-anisms for gaining interpretability as well as global and local interpretability methods.\u2022 [133] highlight issues in explanation faithfulness when CNN models explain theirpredictions on images that are biased with systematic error and address this bydeveloping Debiased-CAM to improve the truthfulness of explanations.\u2022 In the work of [134] a comprehensive analysis of the explainability of Neural Net-work models in the context of power Side-Channel Analysis (SCA) is presented,to gain insight into which features or Points of Interest (PoI) contribute to the mostto the classification decision\u2022 [135] investigates the explainability of Generative AI for Code.\u2022 In the work of [136] provides a formal framework for achieving and analyzingdifferential privacy in model explanations and highlights the possible tradeoffsbetween fidelity of explanations and data privacy.\u2022 [137] deal with a transformation technique between black box models and ex-plainable (as well as interoperable) classifiers on the basis of semantic rules viaautomatic recreation of the training datasets and retraining the decision trees (ex-plainable models) in between.\u2022 This research of [138] presented an architecture that supports the creation of se-mantically enhanced explanations for demand forecasting AI models.\u2022 [139] introduced LEGIT, a model-agnostic framework that incorporates the bene-fits of locally interpretable explanations into graph sampling methods.\u2022 [140] presents six different saliency maps that can be used to explain any faceverification algorithm with no manipulation inside of the face recognition model.\u2022 [141] focus on explaining by means of model surrogates the (mis)behavior ofblack-box models trained via federated learning.Frameworks [142] presents a single framework for analyzing the robustness, fairness,and explainability of a classifier based on counterfactual explanations through a geneticalgorithm.Application areas of explainability methods Application areas of explainability meth-ods are, for example, medicine and health care, where these methods have a great influ-ence. As discussed in [119], the authors paid specific attention to the methods of data andmodel visualization and concluded that involving the medical experts in the analyticalprocess helped improve the interpretability and explainability of ML models even more.Evaluation of explainability methods Not only the explainability methods but also theevaluation of those is of great relevance.\u2022 According to [124] two main ways are objective evaluations and the other ishuman-centered evaluations.\u2022 [143] presented an explainability Fact Sheet Framework for guiding the develop-ment of new explainability approaches by aiding in their critical evaluation alongthe five proposed dimensions, namely: functional, operational, usability, safety,and validation.\u2022 [144] presets a categorization of XAI design goals and evaluation methods. Theauthors used a mapping between design goals for different XAI user groups,namely: Novice Users, Data Experts, and AI Experts, and their evaluation meth-ods. Further, they present a framework through a model and a series of guide-lines to provide a high-level guideline for a multidisciplinary effort to build XAIsystems. [145] proposed the \u201dXAI test\u201d, an application-based evaluation methodtailored to isolate the effects of providing the end user with different levels of in-formation. It became clear that the evaluation of XAI methods is still in the earlystages and has to be very specific due to different end-user requirements.\u2022 [126] analyzes XAI tools in the public sector. The case study based on a goal-question-metric analysis of explainability aims to quantitatively measure threestate-of-the-art XAI tools. The results show that experts welcome new insightsand more complex explanations with multiple causes. They also point out thatthe different levels of complexity may be appropriate for different stakeholdersdepending on their backgrounds.\u2022 [146] show that the generated explanations are volatile when the model trainingchanges, which is not consistent with the classification task and model structure.This raises further questions about confidence in deep learning models for health-care.\u2022 [147] propose two new measures to evaluate explanations borrowed from thefield of algorithmic stability: mean generalizability MeGe and relative consistencyReCo.Stakeholders of XAI The target groups receiving the explanations need to be analyzedand their individual requirements are of great importance, as well as the usability of thesoftware presenting these explanations:\u2022 XAI researchers often develop explanations based on their own intuition ratherthan the situated needs of their intended audience [148].\u2022 According to [6] it is very important that the generated explanations take into ac-count the profile of the user who receives these explanations, the so-called audi-ence.\u2022 There are different user personalities to be considered, which probably requiredifferent explanation strategies, and these are not evenly covered by the currentXAI tools [125].\u2022 There are significant opportunities for UI/UX designers to contribute through newdesign patterns that make aspects of AI more accessible to different audiences[125].\u2022 [48] suggests building blocks into \u201dWhat to explain\u201d (content type), \u201dHow toexplain\u201d (communication), and \u201dto Whom is the explanation addressed\u201d (targetgroup).\u2022 According to the study of [149] explanation interfaces with User-Centric Expla-nation (\u201cwhy\u201d, \u201cwhen\u201d, and \u201chow\u201d different users need transparency information)as well as \u201dInteractive Explanation\u201d methods, which increases awareness of howAI agents make decisions, play an important role.\u2022 [148] introduced Social Transparency (ST), a sociotechnically informed per-spective that incorporates socio-organizational context in explaining AI-mediateddecision-making. ST makes visible the socially situated technological context: thetrajectory of AI decision outcomes to date, as well as people\u2019s interactions withthose technological outcomes. Such contextual information could help people cal-ibrate trust in AI, not only by tracking AI performance but also by incorporatinghuman elements into AI that could elicit socially based perception and heuristics.\u2022 In the study of [150] proposes a three-tiered typology of stakeholder needs. Itconsists of long-term goals (understanding, building trust), shorter-term goals thatwork toward those goals (e.g., checking a model or questioning a decision), andimmediate tasks that stakeholders can perform to achieve their goals (e.g., evalu-ating the reliability of predictions and detecting errors).Miscellaneous\u2022 [151] discuss the potential benefits of XAI and Human-In-The-Loop (HITL)methods in future work practices and illustrate how such methods can create newinteractions and dynamics between human users and AI.\u2022 The position paper of [152] brings together different roles and perspectives on XAIto explore the concept in-depth and offers a functional definition and frameworkfor considering XAI in a medical context.\u2022 [153] focus on the problem of explainable medical image retrieval using neuralnetworks and different explanation methods.\u2022 The article of [154] article seeks to provide technical explanations that can begiven by XAI and to show how suitable explanations for liability can be reachedin court.The general public needs more transparency about how ML/AI systems can fail andwhat is at stake if they fail. Ideally, they should clearly communicate the outcomes andfocus on the downsides to help people think about the trade-offs and risks of differentchoices (for example, the costs associated with different outcomes). But in addition tothe general public also Data Scientists and ML Practitioners represent another key stake-holder group. In the study by [122] the effectiveness and interpretability of two existingtools: the InterpretML implementation of GAMs and the SHAP Python package were in-vestigated. Their results indicate that data scientists over-trust and misuse interpretabilitytools.There is a \u201cright to explanation\u201d in the context of AI systems that directly affect indi-viduals through their decisions, especially in legal and financial terms, which is one ofthe themes of the General Data Protection Regulation (GDPR) [123,119]. Therefore weneed to protect data through secure and privacy-preserving AI-methods. We will analyzethis in the next section.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "484b551f-34df-47b3-9cb1-122250599f73",
                    "text": "As it was noted before, privacy and security are seen as central aspects of building trustin AI. However, the fuel for the good performance of ML models is data, especiallysensitive data. This has led to growing privacy concerns, such as unlawful use of privatedata and disclosure of sensitive data[57,155]. We, therefore, need comprehensive privacyprotection through holistic approaches to privacy protection that can also take into ac-count the specific use of data and the transactions and activities of users [156] .Privacy-preserving and Secure AI methods can help mitigate those risks. We define \u201dSe-cure AI\u201d as protecting data from malicious threats, which means protecting personal datafrom any unauthorized third-party access or malicious attacks and exploitation of data.It is set up to protect personal data using different methods and techniques to ensure dataprivacy. Data privacy is about using data responsibly. This means proper handling, pro-cessing, storage, and usage of personal information. It is all about the rights of individ-uals with respect to their personal information. Therefore data security is a prerequisitefor data privacy.Security and privacy threats There are a lot of security threats in the branch of machinelearning like stealing the model or sensitive information from the user, reconstructionattacks, poisoning attacks, and membership inference attacks, while the latter is a rapidlyevolving research branch [157]. Selected papers deal with the security threats:\u2022 [158] provides a brief review of these threats as well as the defense methods on se-curity and privacy issues in such models while maintaining their performance andaccuracy. Therefore they classify three defense methods: gradient-level, function-level, and label-level, which are based on the differential privacy theory.\u2022 In the paper of [157] several of these membership inference attacks across a largenumber of different datasets were evaluated as well as the Differential PrivateStochastic Gradient Descent (DP-SGD) method as a defense.\u2022 [159] contribute to this topic while providing an evaluation and critical reflectionupon why prominent robustness methods fail to deliver a secure system despiteliving up to their promises of adding robustness in the light of facial authentica-tion.\u2022 [160] provide an empirical Evaluation of Adversarial Examples Defences, Com-binations, and Robustness Scores.\u2022 [161] argue, that a language model\u2019s privacy can be hardly preserved by suchmethods as for example Differential Privacy and conclude that the language modelshould be trained on text data that was explicitly produced for public use.\u2022 [162] study adversarial attacks on graph-level embedded methods.The security threats need to be mitigated through techniques such as presented inthe works of [163], where a privacy-preserving detection of poisoning attacks in Feder-ated Learning is presented and in the paper of [164], which is about mitigating modelpoisoning in privacy-preserving Federated Learning.Surveys and reviews on privacy-preserving techniques The following papers give anoverview of privacy-preserving machine learning (PPML)-techniques through a survey:\u2022 [165] evaluate privacy-preserving techniques in a comprehensive survey andpropose a multi-level taxonomy, which categorizes the current state-of-the-artprivacy-preserving deep learning techniques: (1) model training or learning, (2)PP inference or analysis, and (3) release a PP model.\u2022 [166] summarize infrastructure support for privacy-preserving machine learning(PPML) techniques at both the software and hardware levels. The authors empha-size that the software/hardware co-design principle plays an important role in thedevelopment of a fully optimized PPML system.\u2022 [167] provides a systematic review of deep learning methods for privacy-preserving natural language processing.\u2022 [168] have discussed visual privacy attacks and defenses in the context of deeplearning in their survey.The different PPML- techniques will be presented in the next few sections:Differential Privacy Differential Privacy (DP) is a strict mathematical definition of pri-vacy in the context of statistical and ML analyses [166] and many procedures are basedmainly on this concept. \u201dDifferentially private\u201d means to design query responses in sucha way that it is impossible to detect the presence or absence of information about a par-ticular individual in the database [169]. In the context of PPML, DP typically works byadding noise to the training database. The challenge is the trade-off between privacy andprecision for a dataset. This means the amount of noise added to the data is what allowsthe quantification of privacy of the dataset [170].\u2022 DP approaches are described and used in [171,172,173].\u2022 The study of [171] deals with the effects of differential privacy in the healthcaresector finding that DP-SDG is not well-suited for that kind of usage.\u2022 The study of [173] deals with prescriptive analytics using DP using synthetic andreal datasets and a new evaluation measure.\u2022 The study of [172] focuses on a practical method for private deep learning incomputer vision based on a k-nearest neighbor.\u2022 A a novel perturbed iterative gradient descent optimization (PIGDO) algorithm isproposed by [174].\u2022 [175] propose a novel Local Differential Privacy (LDP)-based feature selectionsystem, called LDP-FS, that estimates the importance of features over securelyprotected data while protecting the confidentiality of individual data before itleaves the user\u2019s device.\u2022 [176] used a deep privacy-preserving CTG data classification model by adoptingthe Differential Privacy (DP) framework.\u2022 The major contribution of [177] is adding differential privacy (DP) into continuallearning (CL) procedures, aimed at protecting against adversarial examples.\u2022 The paper of [178] proposes a novel model based on differential privacy namedDA-PMLM that protects the sensitive data and classification model outsourced bymultiple owners in a real cloud environment.\u2022 In [179] a new differential privacy decision tree building algorithm is proposedand secondly, this is used for developing a two-phase differential privacy randomforest method which increases the complementarity among decision trees.\u2022 [180] propose a novel correlated differential privacy of the multiparty data release(MPCRDP).\u2022 [181] provides a comparative evaluation of differentially private DL models inboth input and gradient perturbation settings for predicting multivariate aggregatemobility time series data.DP is often used in combination with other techniques like Homomorphic Encryption,Federated Learning, and Secure Multiparty Computation.Homomorphic Encryption Homomorphic Encryption (HE) allows performing compu-tations directly with encrypted data (ciphertext) without the need to decrypt them. Themethod is typically used as follows: First, the owner of the data encrypts it using a homo-morphic function and passes the result to a third party tasked with performing a specificcalculation; the third party then performs the computation using the encrypted data andreturns the result, which is encrypted because the input data is encrypted. The owner ofthe data then decrypts the result and receives the result of the calculation with the orig-inal plaintext data. HE schemes support two types of computation: HE addition and HEmultiplication [166]. Some noise is typically added to the input data during the encryp-tion process. In order to get the expected result when decrypting, the noise must be keptbelow a certain threshold. This threshold affects the number of computations that can beperformed on encrypted data. The technique is used in the study of [182]. The followingpapers also deal with HE:\u2022 In the study of [183] a privacy-preserving training algorithm for a fair supportvector machine classifier based on Homomorphic Encryption (HE) is proposed,where the privacy of both sensitive information and model secrecy can be pre-served.\u2022 [184] propose a privacy-preserving logistic regression scheme based on CKKS, aleveled fully homomorphic encryption with the assistance of trusted hardware.\u2022 [185] proposed a privacy-preserving ridge regression algorithm with homomor-phic encryption of multiple private variables and suggested an adversarial pertur-bation method that can defend attribute inference attacks on the private variables.Secure Multiparty Computation Secure Multiparty Computation (MPC / SMPC) is acryptographic protocol that distributes computation among multiple parties, where nosingle party can see the other parties\u2019 data. The parties are independent and do not trusteach other. The main idea is to allow to perform computation on private data while keep-ing the data secret. MPC guarantees that all participants learn nothing more than whatthey can learn from the output and their own input.In [186] this approach is used for a framework named \u201dSecure Decentralized TrainingFramework\u201d which is able to operate in a decentralized network that does not require atrusted third-party server while ensuring the privacy of local data with low communica-tion bandwidth costs.In the study of [187] the authors use this technique for the development of a novelPrivacy-preserving Speech Recognition framework using the Bidirectional Long short-term memory neural network based on SMC.Federated Learning Federated Learning (FL) is a popular framework for decentralizedlearning and FL is the most common method for preserving privacy found in this analy-sis.The central idea is to have a base model first shared and then trained with each clientnode. The ML provider then creates a global model and sends it to the selected clients.The local models are then updated and improved via backpropagation using the localdataset. The global model is updated by aggregating the local updates (through feder-ated averaging) only using the minimum necessary information [188]. FL ensures theprivacy of the local participants since the client\u2019s data never leaves its local platform andno updates from individual users are stored in the cloud. Two different federated learningsettings exist: cross-device (very large number of mobile or IoT devices) and cross-silo(a small number of clients) [188].In the state-of-the-art analysis of [189] the authors classify FL into different segmenta-tions and algorithms used. They also show current scenarios where FL is used includingthe Google GBoard System, Smart Medical Care, Smart Finance, Smart Transportation,and Smart Educational Systems.Also, [190] provides a brief introduction to key concepts in federated learning and an-alytics with an emphasis on how privacy technologies may be combined in real-worldsystems. In the article of [191] the authors specify their systematic literature on FL inthe context of electronic health records for healthcare applications, whereas the surveyof [192] is specifically about FL for smart healthcare. In [193] the authors present an ex-tensive literature review to identify state-of-the-art Federated Learning applications forcancer research and clinical oncology analysis.The following papers use special FL-Approaches in their study:\u2022 In [194] a user privacy preservation model for cross-silo Federated Learning sys-tems (CrossPriv) is proposed.\u2022 The study of [195] a federated deep learning algorithm was developed for biomed-ical data collected from wearable IoT devices.\u2022 In the literature FL is also used in [196] to create a federated parallel data platform(FPDP) including end-to-end data analytics pipeline.\u2022 [197] use FL for the design of SAFELearn, a generic private federated learning de-sign that enables efficient thwarting of strong inference attacks that require accessto clients\u2019 individual model updates.\u2022 [198] presents an efficient, private and byzantine-robust FL (SecureFL) frame-work considering the communication and computation costs are reduced withoutsacrificing robustness and privacy protection.\u2022 [199] have been working on implementing SA for Python users in the context ofthe \u2019Flower FL Framework\u2019.\u2022 [200] introduce an approach for vertically partitioned FL setup achieving reducedtraining time and data-transfer time and enabling a changing sets of parties. (sup-ports linear and regression models and SVM)\u2022 [201] present FedAT, a novel Federated learning system with Asynchronous Tiersunder Non-IID training data.\u2022 [202] proposing a scalable privacy-preserving federated learning (SPPFL) againstpoisoning attacks. The main contribution is crossing the chasm between these twocontrary issues of poisoning defense and privacy protection.\u2022 [203] introduces a new problem set in a multi-device context called FederatedLearning in Multi-Device Local Networks (FL-MDLN) as well as highlightingthe challenges of the proposed setting.\u2022 [204] presents a distributed FL framework in Trusted Execution Environment(TEE) to protect gradients from the perspective of hardware. The authors presentthe usage of trusted Software Guard eXtensions (SGX) as an instance to imple-ment the FL as well as the proposal of an SGX-FL framework.\u2022 The authors of [205] did not train a single global model, instead, the clients spe-cialize in their local data in their approach and use other clients\u2019 model updatesdepending on the similarity of their respective data (based on a directed acyclicgraph). The advantage of this approach are achieving more accuracy and less vari-ance than through federated averaging.\u2022 In this article of [206], the authors address the challenges of the standard FLtechniques such as the vulnerability to data corruptions from outliers, systematicmislabeling, or even adversaries by proposing Auto-weighted Robust FederatedLearning (ARFL), a novel approach that jointly learns the global model and theweights of local updates to provide robustness against corrupted data sources.\u2022 The work of [207] deals with the challenge, that aggregating the data from dif-ferent wearable devices to a central server introduces privacy concerns. Thereforethey propose an architecture, CloudyFL, by deploying cloudlets close to wearabledevices.\u2022 [208] designed a federated decision tree-based random forest algorithm using FLand conducted our experiments using different datasets. The test set was consid-ering a small number of corporate companies for collaborative machine learning.\u2022 [209] propose FedNKD, which utilizes knowledge distillation and random noise,to enable federated learning to work dependably in the real world with complexdata environments.\u2022 [210] propose a robust model aggregation mechanism called FLARE for FL,which is designed for defending against state-of-the-art model poisoning attacks.\u2022 In the paper of [211] a novel scheme based on blockchain architecture for Feder-ated Learning data sharing is proposed.\u2022 [212] present a novel decentralized Federated Learning algorithm, DECFEDAVG,obtained as a direct decentralization of the original Federated Learning algorithm,FEDAVG.\u2022 [213] propose a privacy-preserving and verifiable decentralized federated learningframework, named PVD-FL.\u2022 [214] present a blockchain-based trustworthy federated learning architecture toenhance the accountability and fairness of federated learning systems.\u2022 In [215] proposed trust as a metric to enable secure federated learning through amathematical framework for trust evaluation and propagation within a networkedsystem.\u2022 [216] developed a Blockchain-FL architecture to ensure security and privacy,which utilizes secure global aggregation and blockchain techniques to resist at-tacks from malicious edge devices and servers.\u2022 In [217] the authors design a new framework, called HealthFed, that leveragesFederated Learning and blockchain technologies to enable privacy-preserving anddistributed learning among multiple clinician collaborators.\u2022 [218] propose \u201dVFL-R\u201d, a novel Vertical FL framework combined with a ringarchitecture for multi-party cooperative modeling.\u2022 [219] presents a privacy-preserving federated learning-based approach, PriCell,for complex models such as convolutional neural networks.Although Federated learning is a promising candidate for developing powerful mod-els while preserving individual privacy and complying with the GDPR the followingpapers show the challenges and vulnerabilities of FL:\u2022 The study of [220] highlights some vulnerabilities of this approach. Attacks in afederated setup can are classified as poisoning attacks (preventing the model tolearn at all) or inference attacks (attacking the private data of the target partici-pants).\u2022 The study of [198] points out the weakness of destroying the integrity of the con-structed model through byzantine attacks.\u2022 The authors of [220] also point out that extensive communication is a big chal-lenge too as well as system heterogeneity.\u2022 Additionally to detecting these attacks and also identifying the attackers, [189]highlights the challenges of reducing communication overhead in the encryptionprocess and solving the noise threshold of different scenarios.According to the survey of [189] an \u2019ideal state of FL\u2019 can be considered if the FLmodel can be fully decentralized and the current development makes it clear, that thereare still many barriers on the way to this ideal state.All in all, FL is still not enough to guarantee privacy, therefore it is often combined inhybrid mechanisms with other techniques as we will discuss in the section below.Hybrid PPML-approaches There are many new papers looking at hybridizing ap-proaches as these could be promising solutions for the future. A hybrid PPML approachcan take advantage of each component, providing an optimal tradeoff between ML taskperformance and privacy overhead[156,165,166]. The following papers deal with hybridapproaches:\u2022 The study of [221] proposes an FL approach based on Gaussian differential pri-vacy, called Noisy-FL, which can more accurately track the changes in privacyloss during model training.\u2022 [222] presents \u201dPRICURE\u201d, a system that combines the complementary strengthsof secure multiparty computation (SMPC) and differential privacy (DP) to en-able privacy-compliant collaborative predictions between multiple model own-ers. SMPC is relevant for protecting data before inference, while DP targets post-inference protection to avoid attacks such as membership inference.\u2022 The study of [223] proposes a deep learning framework building upon distributeddifferential privacy and a homomorphic argmax operator specifically designed tomaintain low communication loads and efficiency.\u2022 [224] present a privacy-preserving DNN model known as Multi-Scheme Differ-ential Privacy (MSDP) depending on the fusion of Secure Multi-party Computa-tion (SMC) and \u03b5-differential privacy. The method reduces communication andcomputational cost at a minimal level.\u2022 [225] presents the Sherpa.ai Federated Learning framework that is built upon aholistic view of federated learning and differential privacy. The study results bothfrom exploring how the machine learning paradigm can be adapted to federatedlearning and from defining methodological guidelines for the development of ar-tificial intelligence services based on federated learning and differential privacy.\u2022 [226] proposes a privacy-preserving federated learning algorithm for medical datausing homomorphic encryption in a secure multi-party computation setting forprotecting the DL model from adversaries.\u2022 [227] study the privacy protection strategy of enterprise information data based onconsortium blockchain and federated learning.Miscellaneous PPML-approaches There are a few interesting PPML approaches, asoutlined below.\u2022 In [228] the Split-learning method is used on 1D CNN models. Unfortunately, theresults of the analysis show that it is possible to reconstruct the raw data from theactivation of the split intermediate layer and this method needs further research.\u2022 In [229] a fully decentralized peer-to-peer (P2P) approach to multi-party ML,which uses blockchain and cryptographic primitives to coordinate a privacy-preserving ML process between peering clients is proposed which produces a finalmodel that is similar in utility to federated learning and has the ability to withstandpoisoning attacks.\u2022 In [230] the authors propose a decentralized secure ML-training platform calledSecular for based on using a private blockchain and InterPlanetary File System(IPFS) networks.\u2022 In [231] the authors use an automatic face de-identification algorithm that gen-erates a new face from a face image that retains the emotions and non-biometricfacial attributes of a target face based on the StyleGAN technique.\u2022 In [232] the authors focus on designing an effective human-in-the-loop-aided(HitL-aided) scheme to preserve privacy in smart healthcare.\u2022 [233] focuses on designing a human-in-the-loop-aided (HitL-aided) scheme topreserve privacy in smart healthcare.\u2022 [234] investigates and analyzes machine learning privacy risks to understand therelationship between training data properties and privacy leakage and propose aprivacy risk assessment scheme based on the clustering distance of training data.\u2022 [235] propose a comprehensive approach for face recognition techniques in a pri-vacy preserving manner, i.e., without compromising the privacy of individuals inexchanged data while considering together the concepts of privacy and accuracy.\u2022 [236] contribute towards the development of more rigorous privacy-preservingmethodologies capable of anonymizing case-based explanations without compro-mising their explanatory value.\u2022 [237] designs a secure and efficient classification scheme based on SVM to protectthe privacy of private data and support vectors in the calculation and transmissionprocess.\u2022 The authors of [238] introduce PrivPAS (A real time Privacy-Preserving AI Sys-tem) as a a novel framework to identify sensitive content.\u2022 The authors of [239] present Sphinx, an efficient and privacy-preserving onlinedeep learning system without any trusted third parties.\u2022 The survey [240] explores the domain of personalized FL (PFL) to address thefundamental challenges of FL on heterogeneous data, a universal characteristicinherent in all real-world datasets.PPML-Measurement techniques Developing good techniques to preserve privacy isone thing but good techniques to measure it is needed as well. In this regard, the paper by[165] suggests these measurement techniques: effectiveness, which is typically evaluatedin terms of accuracy; efficiency, which primarily includes communication or computa-tional overhead and execution time; and privacy, which is primarily evaluated in termsof direct and indirect guarantees against leakage.We conclude that there is a lot of research related to privacy and security in the field ofAI and there is no approach yet to achieve perfectly privacy-preserving and secure AIand many challenges are left open.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "30e76231-f62b-4e4b-9830-4701437c902a",
                    "text": "The final set of 254 high-quality studies was selected for an in-depth analysis to aid inanswering the presented research questions.Our choice of features is based on their content in each of the following categories,\u201dTrustworthy AI, Ethical AI, Explainable AI, Privacy-preserving AI, and Secure AI\u201d, asderived from section 3.2. We analyzed the papers quantitatively. Table 1 presents studyfeatures along with their absolute and percentile representations in the reviewed litera-ture as well as their sources.The distribution of the paper is as follows: most papers covered the topic \u201dPrivacy-Preserving and Secure AI\u201d, followed by \u201dEthical AI\u201d and then \u201dExplainable AI\u201d andTrustworthy AI.Within the topic \u201dPrivacy-Preserving and Secure AI\u201d, most papers belong to \u201dFederatedlearning\u201d, obviously being a very emerging research field in the time frame.There were also many different papers that were not assigned to any specific category(see \u201dMiscellaneous)\u201d since the topic is very multifaceted.In the topic area of \u201dEthical AI\u201d, the most common category was \u2019Miscellaneous\u2019, sincethe authors of the ethical AI field handle very different topics. In addition, second mostof them could be assigned to the category \u2019ethical issues\u2019 since ths is a hot topic in thefield of ethics. The rest of the papers dealt with ethical frameworks that try to to integrateethical AI in context of a development process.Most studies in the field oxf XAI deal with coming up with new XAI approaches tosolve different explainability problems with new AI models. There were also a few thatpresented stakeholder analyses specifically in the context of explainability of AI models.Few of them presented miscellaneous topics that could not be assigned to any specificcategory or frameworks to integrate explainable AI.In Trustworthy AI, we saw that most presented a review or survey on the current stateof Trustworthy AI in research. There were also papers presented frameworks speciallyfor trustwothiness or papers that reported on how Trust is perceived and described bydifferent users. Feature Repr. Perc. SourcesTrustworthy AI (28/254, 11% ) *Reviews and SurveysPerceptions of trustFrameworksMiscellaneous 9/284/289/286/28 32%14%32%28% [11,17,28,13,29,14,30,130,45][31,32,33,27][26,34,35,37,38,15,39,40,41][42,43,44,46,16,47]Ethical AI ( 85/254,34%) *Frameworks 19/85 22% [57,58,59,7,20,60,61,24,62,63][64,65,66,67,68,69,70,71,72]Ethical issues 22/85 26% [74,20,118,241,78,242,243,84]Miscellaneous 33/85 39% [76,244,77,49,79,155,75,82][80,81,85,86,87,89][90,19,91,92,245,93,94,22,21,95,96][97,98,99,100,9,101,103][114,116,102,104,105,106,107][108,109,110,111,112,113,115,8]Reviews and SurveysTools 10/85 12%1%1/85 [50,127,51,83,52,53,54,55,56,117][73]Explainable AI ( 46/254 , 18%) *Reviews and Surveys 10/46 22%Stakeholders 7/46 15%XAI Approaches 14/46 30%FrameworksMiscellaneous 4/46 9%11/46 24% [6,48,123,12,149,119][124,128,131,132][125,148,246,145][122,150,126][247,5,143,182,248,133][134,135,137,129,139,140,141,138][144,142,249,36][250,251,136,151,152][146,147,153,154,121,120]Privacy-preserving and Secure AI ( 95/254 , 38%) *Reviews and Surveys 10/95 10%Differential Privacy 12/95 13%Secure Multi-Party ComputationHomomorphic Encryption 2/954/95 2%4%Federated learning 35/95 37% [165,166,252,253,254,156][169,188,167,168][173,171,172,170,174,175][176,177,178,179,180,181][186,187][182,183,184,185][195,196,194,197,220,255,229,189][207,208,213,205,198][199,200,204,201,203,202,256,206,190][191,192,209,211,210,212,214][215,216,217,193,219,164,218]Hybrid ApproachesSecurity ThreatsMiscellaneous 8/957/95 xx%8% [221,223,222,224,225,226,227,240][157,158,159,160,161,163,162]16/95 17% [231,257,258,259,232,260,261,228,230,233]Table 1. Quantitative Analysis*percentage does not add up to 100 due to rounding.[234,235,236,237,238,239]",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "409873a5-406a-41e6-8971-8afbafab214c",
                    "text": "The main categories of \u201dResponsible AI\u201d, namely \u201dTrustworthy AI, Ethical AI, Explain-able AI, Privacy-preserving AI, and Secure AI\u201d, were defined in Section 3.2. The aspectsof responsible AI were presented and discussed in detail in Section 3.3. Here, table 2summarizes section 3.2. and 3.3. by presenting the qualitative analysis of the literatureregarding the categories for responsible AI. Each of the papers was content-wise ana-lyzed and checked for membership in the defined categories.The legend in the table reads as follows: \u2022 = meets criteria (i.e., the focus of thepaper covers the topic of the category; \u25e6 = partially meets criteria (i.e., the paper coversthe topic of the category but its focus is elsewhere); no circle = does not meet criteria(i.e., the paper does deal with the topic of the category). Abbreviations in the table headsare defined as follows: Trustworthy AI = Tr. AI, Ethical AI = Eth. AI, Explainable AI =XAI, Pivacy-preserving and Secure AI = PP & Sec. AIAuthorAbbasi et al. 2022Abou El Houda et al. 2022Abolfazlian 2020Abuadbba et al. 2020Agarwal et al. 2020Allahabadi et al. 2022Alishahi et al. 2022Anderson and Fort 2022Antunes et al. 2022Aminifar et al. 2021Araujo et al. 2020Arcolezzi et al. 2022Arrieta et al. 2020Attard-Frost et al. 2022 Ref.[235][217][155][228][231][46][175][111][191][257][31][181][6][55]1234567891011121314 Tr. AI Eth. AI XAI PP & Sec. AI\u2022\u2022\u25e6\u2022\u2022\u25e6\u2022\u2022\u2022\u25e6\u2022\u2022\u25e6\u25e6\u2022\u2022\u2022\u2022\u2022 \u2022\u25e6\u2022\u2022\u2022\u2022Table 2. Qualitative Analysis 1/8Tr. AI Eth. AI XAI PP & Sec. AIAuthorAyling and Chapman 2021Bacciu and Numeroso 2022Banerjee et al. 2022Bai et al. 2022Beckert 2021Belenguer 2022B\u00b4elisle-Pipon 2022Beilharz et al. 2021Benefo et al. 2022Benjamins 2021Bertino 2020Bickley and Torgler 2021Biswas 2021Boenisch et al. 2021Bonawitz et al. 2022Boulemtafes et al. 2020Bourgais and Ibnouhsein 2021Boyd 2022Brennen 2020Brown et al. 2022Brusseau 2022Bruschi and Diamede 2022Burkart and Huber 2021Byun et al. 2022Can und Ersoy 2021Chai et al. 2021Chang and Shokri 2021Chen et al. 2020Chen et al. 2021Chien et al. 2022Cheng et al. 2021Cho et al. 2021Chora\u00b4s et al. 2020Choung et al. 2022Chowdhury et al. 20221516171819202122232425262728293031323334353637383940414243444546474849 \u2022\u2022Ref.[74][139][39][234][29][66][105][205][53][58][156] \u2022[87][253][261][190][165][59][99][125][161][110][64][48][185][195][201][254][166][196][100][57][201][123][41][193] \u25e6\u25e6\u25e6\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u25e6\u2022\u2022\u2022\u2022\u2022\u2022 \u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u25e6\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022Table 3. Qualitative Analysis 2/8",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "781fe10d-3002-4dff-a81a-2867fd869d5e",
                    "text": "Several key points have emerged from the analysis. It has become clear that AI willhave an ever-increasing impact on our daily lives, from delivery robots to e-health, smartnutrition and digital assistants, and the list is growing every day. AI should be viewedas a tool, not a system that has infinite control over everything. It should therefore notreplace humans or make them useless, nor should it lead to humans no longer using theirown intelligence and only letting AI decide. We need a system that we can truly callAuthorChuanxin et al. 2020Colaner et al. 2021Combi et al. 2022Contractor et al. 2022Cooper et al. 2022Diddee and Kansra 2020Ding et al. 2022Ehsan et al. 2021Ehsan et al. 2021bEitel-Porter 2021Fabris et al. 2022Fel et al. 2022Fereidooni et al. 2021Fernandez-Quillez 2022Feng and Chen 2022Forbes 2021Forsyth et al. 2021Fung and Etienne 2022Gambelin 2021Ghamry et al. 2021Gholami et al. 2022Gill 2021Giordano at al. 2022Girka et al. 2021Gittens et al. 2022Giorgieva et al. 2022Giuseppi et al. 2022Golder et al. 2022Goldsteen et al. 2021Gong et al. 2022Grivet S\u00b4ebert et al. 2021Guevara et al. 2021Gupta and Singh et al. 2022Ha et al. 2020Haffar et al. 2022 Ref.[221][251][152][251][80][194][174][148][246][7][104][147][197][88][227][94][96][107][241][230][215][242][162][258][45][69][212][134][260][207][223][170][178][158][141]5051525354555657585960616263646566676869707172737475767778798081828384 Tr. AI Eth. AI XAI PP & Sec. AI\u2022\u2022\u2022\u25e6\u25e6\u25e6\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u25e6\u2022\u2022\u25e6\u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u25e6\u2022\u2022\u2022Table 4. Qualitative Analysis 3/8\u201dresponsible\u201d AI. The analysis has clearly shown that the elements of ethics, privacy,security and explainability are the true pillars of responsible AI, which should lead to abasis of trust.858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119 AuthorHagendorff 2020Hagendorff 2022Hailemariam et al. 2020Hanna and Kazim 2021H\u00a8au\u00dfermann and L\u00a8utge 2022Hao et al. 2021Harichandana et al. 2022Harikumar et al. 2021Hassanpour et al. 2022He et al. 2020Heuillet et al. 2021Hickok 2021Holzinger et al. 2022Hu et al. 2021Hu et al. 2022Huang et al. 2022 Ref.[90][86][250][49][106][198][238][173][177][259][247][19][44][43][153][51]Hunkenschroer & Kriebitz 2022 [113]Ib\u00b4a\u02dcnez und Olmeda 2021Jacovi et al. 2021Jacobs and Simon 2022Jakesch et al. 2022Jain et al. 2020Jancovic & Mayer 2022Jarin and Eshete 2021Jatain et al. 2021Jesus et al. 2021Joisten et al., 2022Joos et al., 2022Kalloori and Klingler 2022Karimian et al. 2022Kaur et al. 2020Kaur et al. 2022Kiemde and Kora 2021Knowles and Richards 2021 [22][42][115][9][11][160][222][220][145][63][159][208][54][122][30][91][32] Tr. AI Eth. AI XAI PP & Sec. AI\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u25e6\u25e6\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u25e6\u25e6\u2022\u2022\u2022\u2022\u25e6\u2022\u25e6\u25e6\u2022\u25e6\u2022\u2022\u25e6\u25e6\u2022\u2022 \u2022\u2022\u2022\u2022\u25e6\u2022\u25e6\u2022\u2022\u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u25e6Krijger 2022 \u2022Table 5. Qualitative Analysis 4/8[72]",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "07edd31c-8a84-4389-92f6-401979a7711a",
                    "text": "Here we highlight the most important criteria that a responsible AI should fulfill. Theseare also the points that a developer should consider if he wants to develop responsibleAI. Therefore, they also form the pillars for the future framework.Key-requirements for the Ethical AI are as follows:\u2022 fair: non-biased and non-discriminating in every way,Tr. AI Eth. AI XAI PP & Sec. AIAuthorKumar et al. 2020Kumar and Chowdhury 2022Lal and Kartikeyan 2022Lee and Rich 2021Li et al. 2021Li et al. 2021Li et al. 2022Li et al. 2022Li et al. 2022Li et al. 2022Liao and Sundar 2022Lin et al. 2022Liu et al. 2021Liu et al. 2022Liu et al. 2022Lo et al. 2022Loi et al. 2020Lu et al. 2022120121122123124125126127128129130131132133134135136137138 Maclure 2021139 Madaio et al. 2022140 Maltbie et al. 2021141 Mao et al. 2022142 Ma et al. 2022143 Maree et al. 2020144 Mercier et al. 2021145 Mery and Morris 2022146 Middleton et al. 2022147 Milossi et al. 2021148 Minh et al. 2021149 Mohseni et al. 2021150 Montenegro et al. 2022151 Morley et al. 2021152 Mothukuri et al. 2021153 Muhr et al. 2021 Ref.[17][70][176][33][199][202][206][15][68][218][37][83][188][184][179][214][20][101][118][97][126][237][164][5][252][140][27][24][132][144][236][21][255][163] \u2022\u2022\u2022\u2022\u25e6\u2022\u25e6\u25e6\u2022\u25e6\u2022 \u25e6\u25e6\u25e6\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u25e6 \u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u25e6\u2022\u2022\u2022\u25e6\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u2022154 Mulligan & Elaluf-Calderwood 2022 [84]Table 6. Qualitative Analysis 5/8\u2022 accountability: justifying the decisions and actions,\u2022 sustainable: built with long-term consequences in mind, satisfying the SustainableDevelopment Goals,\u2022 compliant: with robust laws and regulations.Key-requirements for the privacy and security techniques are identified as follows:\u2022 need to comply with regulations: HIPAA, COPPA, and more recently the GDPR(like, for example, the Federated Learning),Author155 Munn 2022156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188 Nakao et al., 2022Nazaretsky et al., 2022Nguyen et al., 2022Owusu-Agyemeng et al. 2021Padovan et al. 2022Park et al. 2022Patel et al. 2022Persson & Hedlund 2022Peters et al. 2020Petersen et al. 2022Petrozzino 2021Prunkl and Whittlestone 2020Raab 2020Rahimian et al. 2021Ramanayake et al. 2021Rasheed et al. 2022Ratti et al. 2022Rochel and Ev\u00b4equoz 2020Rodr\u00b4\u0131guez-Barroso et al. 2020Rozanec et al. 2022Rubeis 2022Saetra et al. 2021Saleem et al. 2022Saraswat et al. 2022Sav et al. 2022Sharma et al. 2020Shayan et al. 2021Seshia et al. 2022Sheth et al. 2021Shneiderman et al. 2020Singh et al. 2021Sokol and Flach 2020Sokol and Flach 2020b Ref.[89][102][36][192][224][154][183][136][116][60][52][75][245][244][157][112][127][121][76][225][138][103][79][128][131][219][142][229][38][12][26][28][143][249] Tr. AI Eth. AI XAI PP & Sec. AI\u2022\u2022\u25e6\u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u25e6\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u25e6\u25e6\u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u25e6\u25e6\u2022\u25e6\u2022\u2022\u2022\u25e6\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u25e6\u2022\u2022\u2022Table 7. Qualitative Analysis 6/8\u2022 need to be complemented by proper organizational processes,\u2022 must be used depending on tasks to be executed on the data and on specific trans-actions a user is executing,\u2022 use hybrid PPML-approaches because they can take advantage of each compo-nent, providing an optimal trade-off between ML task performance and privacyoverhead,\u2022 use techniques that reduce communication and computational cost (especially indistributed approaches). Tr. AI Eth. AI XAI PP & Sec. AITartaglione and Grassetto 2020 [95]AuthorSolanki 2022Sousa and KernStahl 2021Stahl et al. 2021Stahl et al. 2022Stahl et al. 2022Starke et al. 2022Storey et al. 2022Strobel and Shokri 2022Sun et al. 2021Sun et al. 2022Suresh et al. 2021Suriyakumar et al. 2021Svetlova et al. 2021Tan et al. 2022Terziyan & Vitko 2022Thuraisingham 2022Tolmejer et al. 2022Toreini et al. 2020Tian 2022Tiddi and Schlobach 2022Tran et al. 2021Tsamados et al. 2022Tsiakis and Murray 2022Utomo et al. 2022Valentine 2022Vakkuri 2021Vakkuri et al. 2022Vellido et al. 2020Vilone and Logo 2021189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220 Waller and Waller 2022221 Wang et al. 2020222 Wang et al. 2022223 Wang et al. 2022 [243] \u2022Ref.[71][167][77][50][109][108][120][16][149][135][150][171][67][240][137][40][98][34][239][129][186][56][151][47][114][61][81][119][124][85][187][210][211] \u2022\u2022\u2022\u25e6\u2022\u2022\u2022\u25e6\u25e6\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\u2022\u25e6\u25e6\u2022\u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u2022Table 8. Qualitative Analysis 7/8Key-requirements for Explainable AI are the following:\u2022 Human-Centered: the user interaction plays a important role and how he under-stands and interacts with the system,\u2022 Explanations must be tailored to the user needs and target group\u2022 Intuitive User interface/experience: the results need to be presented in a under-standable visual language,\u2022 Explainable is also feature to say how well the system does its work (non func-Author224 Wang et al. 2022225 Wang and Moulden 2021226 Watson 2022227 Weinberg 2022228 Werder et al. 2022229 Wibawa 2022230 Wing 2021231 Wyhmeister et al. 2022232233234235236237238239240241242243244245246247248249250251252253254 Xiaoling et al. 2021Xu et al. 2021Xu et al. 2021Yang et al. 2021Yang et al. 2022Yang et al. 2022Yuan and Shen 2020Yuan et al. 2020Zapechnikov et al.Zhang et al. 2021Zhang et al. 2021Zhang et al. 2020Zhang et al. 2022Zhang et al. 2022Zhao et al. 2022Zhao et al. 2022Zhou et al. 2020Zhou et al. 2020Zhou et al. 2022Zhou et al. 2022Zhu et al. 2020Zhu et al. 2022Zytek et al. 2021 Ref.[73][35][146][82][8][226][13][65][262][200][204][189][130][216][182][263][169][93][14][256][133][168][213][180][232][92][233][117][172][209][248] Tr. AI Eth. AI XAI PP & Sec. AI\u2022\u2022\u2022\u25e6\u2022\u2022\u2022\u2022\u25e6\u25e6\u2022\u2022\u2022\u25e6\u2022\u2022\u25e6\u2022\u2022\u2022\u2022 \u2022\u25e6\u2022\u2022\u25e6\u2022\u2022 \u25e6\u2022\u25e6\u2022\u2022\u2022\u2022\u2022\u2022\u25e6\u25e6\u2022\u2022\u2022\u2022\u25e6\u2022\u2022\u2022\u2022Table 9. Qualitative Analysis 8/8tional requirement),\u2022 Impact of explanations on decision making process,Key-Perceptions of trustworthy AI are as follows:\u2022 ensure user data is protected,\u2022 probabilistic accuracy under uncertainty,\u2022 provides an understandable, transparent, explainable reasoning process to the user,\u2022 usability,\u2022 act \u201das intended\u201d when facing a given problem,\u2022 perception as fair and useful,\u2022 reliability.We define Responsible AI as an interdisciplinary and dynamic process: it goes be-yond technology and includes laws (compliance and regulations) and society standardssuch as ethics guidelines and the Sustainable Development Goals.Figure 3. Pillars of the Responsible AI frameworkFigure 3 shows that on the one hand there are social/ethical requirements/pillars andon the other hand the technical requirements/pillars. All of them are dependent on eachother. If the technical and ethical side is satisfied the user trust is maintained. Trust canbe seen as the perception of the users of AI.There are also \u201dsub-modules\u201d present in each of the pillars, like accountability, fairness,sustainability and compliance in the field of ethics. They are crucial that we can say theAI meets ethical requirements.Furthermore, the explainability methods must value privacy, meaning they must not havethat much access to a model so that it results in a privacy breach. Privacy is dependenton security, because security is a prerequisite for it.With each \u201dresponsible system\u201d there are the humans that care for the system. The peoplewho take care of the system must also handle it responsibly and constantly carry outmaintenance work and check by metrics whether the responsibility is fulfilled. This canbe ensured by special metrics which are considered as a kind of continuous check asstandard. This means responsible AI encompasses the system-side and the developer-side.Human-Centered AI (mentioned in 3.3) needs to be considered as a very important partof responsible AI and it is closely connected to the approach \u201dHuman-in-the-loop\u201d. Thehuman in the loop here is very important because this is the person who checks andimproves the system during the life cycle. so the whole responsible AI system needs tobe Human-Centered, too. This topic will not be dealt with in detail in this study, but is apart of the future work.Therefore, responsible AI is interdisciplinary, and it is not a static but it is a dynamicprocess that needs to be taken care of in the whole system lifecycle.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "7a3e8d3c-3585-49a1-8daa-48adfb2abcf8",
                    "text": "To fulfill all aspects comes with tradeoffs as discussed for example in [16] and comes forexample at cost of data privacy. For example the methods that make model more robustagainst attacks or methots that try to explain a models behaviour and could leak someinformation. But we have fo find a way to manage that AI Systems that are accurate, fair,private, robust and explainable at the same time, which will be a very challenging task.We think that one approach to start with would be to create a benchmark for the differentrequirements that can determine to which proportion a certain requirement is fulfilled, ornot.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "0524a956-91bd-45c8-932d-81f4db6ed7c2",
                    "text": "In the current study, we have included the literature available through various journalsand provided a comprehensive and detailed survey on the literature in the field of respon-sible AI.In conducting the study, we unfortunately had the limitation that some journals were notfreely accessible despite a comprehensive access provided by our institutions. Althoughwe made a good effort to obtain the information needed for the study on responsible AIfrom various international journals, accessibility was still a problem. It is also possiblethat some of the relevant research publications are not listed in the databases we used forsearching. Additional limitation is the time frame of searched articles; this was carefullyaddressed to include only the state-of-the-art in the field. However, some older yet stillcurrent developement might have been missed out.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                },
                {
                    "id": "47cdfc80-9cbc-4023-b0e4-fc95938863a6",
                    "text": "The field of AI is such a fast changing area and a legal framework for responsible AI isstrongly necessary. From the series of EU-Papers on Artificial Intelligence of the last 2years we noticed that \u201dtrustworthy AI\u201d and \u201dresponsible AI\u201d are not clearly defined, andas such a legal framework could not be efficiently established. Hence, the trust as a goalto define a framework/regulation for AI is not sufficient. Regulations for \u2019responsible AI\u2019need to be defined instead. As the EU is a leading authority when it comes to setting stan-dards (like the GDPR) we find it is absolutely necessary to help the politicians to reallyknow what they are talking about. On the other hand, helping practitioners to prepare forwhat is coming next in both research and legal regulations is also of great importance.The present research made important contributions to the concept of responsible AI. It isthe first contribution to wholly address the \u201dresponsible AI\u201d by conducting a structuredliterature research, and an overarching definition is presented as a result. The structuredliterature review covered 118 most recent high quality works on the topic. We have in-cluded a qualitative and quantitative analysis of the papers covered.By defining \u201dresponsible AI\u201d and further analyzing the state of the art of its components(i.e., Human-centered, Trustworthy, Ethical, Explainable, Privacy(-preserving) and Se-cure AI), we have shown which are the most important parts to consider when develop-ing AI products and setting up legal frameworks to regulate their development and use.In the discussion section we have outlined an idea for developing a future framework inthe context of Responsible AI based on the knowledge and insights gained in the analysispart.In future research the topic of Human-Centered AI and \u201dHuman-in-the-loop\u201d should bedeveloped further in the context responsible AI. Other important topics to be workedupon are the benchmarking approaches for responsible AI and a holistic framework forResponsible AI as the overarching goal.",
                    "reference": "[1] Sabrina Goellner, Maximilian Tropmann-Frick, and Bernhard Brumen. 2024. Responsible Artificial Intelligence: A Structured Literature Review. arXiv:2403.06910. Retrieved from https://arxiv.org/pdf/2403.06910"
                }
            ]
        },
        {
            "paper_title": "Resolving ethics trade-offs in implementing responsible AI",
            "authors": "C Sanderson, E Schleiger, D Douglas\u2026",
            "publication_info": "arXiv preprint arXiv \u2026 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2401.08103",
            "chunks": [
                {
                    "id": "a046c416-0f01-4e09-aeec-1b0dca83e26c",
                    "text": "",
                    "reference": "[1] Elizabeth Sanderson, Elise Schleiger, and Daniel Douglas. 2024. Resolving ethics trade-offs in implementing responsible AI. arXiv:2401.08103. Retrieved from https://arxiv.org/pdf/2401.08103"
                },
                {
                    "id": "9f57e7a3-3f14-422f-bf56-f18576b7976f",
                    "text": "The increasing impact of artificial intelligence (AI) andmachine learning (ML) across many sectors of society hasled to a broad consensus on the need to design and imple-ment these technologies in a responsible manner. Globally,governments, organisations and industry groups have definedmany sets of high-level AI ethics principles to support thisvision [23]; an example is shown in Table I. Such high-levelprinciples contain a set of underlying themes and aspects, whichtypically includes accuracy/performance, robustness/safety,fairness, privacy, explainability/interpretability, transparency,and accountability [16], [23].As the high-level principles are implemented in practice, theunderlying aspects interact, which inevitably leads to varioustensions and trade-offs [10], [39], [50]. Among the manyobserved interactions (summarised in Table II), a prominentexample is the trade-off between accuracy and explainability[4], [19], [36]. While the need to formally address the trade-offsis often stated in the literature [21], [35], [43], [50], there is cur-rently no generally agreed upon framework to accomplish this.A further complication is that many designers and developersof AI/ML systems1 are currently unaware of the tensions andtrade-offs, which may stem from unfamiliarity of (or theirunwillingness to engage with) AI ethics principles and/or theirunderlying aspects [48], [49]. Without regulatory enforcement,taking AI ethics principles into account can be contrary toindustry priorities [31]. For example, it has been observedthat taking into account the fairness aspect can considerablyreduce the accuracy of AI/ML systems, affecting the potentialprofitability to be gained from using these systems [26]. A recent analysis of highly cited research papers withinAI/ML fields shows that they contain many potentially harmfulimplicit biases and assumptions, as well as an inherentselection and prioritisation of ethics aspects [7]. The accuracyaspect (indirectly represented as performance) is the mostemphasised, at the cost of considerably de-emphasising almostall other aspects. The next most commonly prioritised qualityis generalisation, which is haphazardly and inconsistently usedas a proxy for the robustness aspect in AI/ML literature [44].The selection, prioritisation and trade-off resolution of AIethics aspects can occur at various points in the AI/MLsystem development pipeline. Without organisational policiesand formal governance, these can occur on an ad-hoc basisat the design and implementation levels, and as such canbe significantly affected by individual team members, theirknowledge and interpretation of Responsible AI issues, personalpreferences and bias [22], [49], and lack of understanding of theeffect of trade-offs on others [34]. On the other hand, explicitorganisational policies may be under-developed and/or can leadto overly rigid adherence due to lack of flexibility [37], [49].We can consider a hierarchy of needs at three levels forinforming the implementation of Responsible AI systems:\u2022 Societal level. Governments, industry bodies and regulatorsdetermine high-level principles, standards and regulationsfor AI/ML system development and use [2], [47]. These areinfluenced by cultural norms, values and existing legislation.\u2022 Organisational level. Instead of approaching trade-offs ad-hoc,acknowledge that a set of trade-offs exists [39]; ensure designersand developers are aware of these trade-offs; create frameworksand procedures for dealing with trade-offs that are aligned withorganisational, societal and regulatory expectations [37].\u2022 Practitioner level. Prevent personal bias going into trade-offdecisions; be aware that there is more than one point ofview [22] and that there may be implications for variousgroups [34]. Developers of AI/ML systems action acceptedframeworks like risk assessments, justifications and pat-terns [29], in order to translate principles into practice.Given the above considerations, an important next step ishence the development of frameworks and/or guidelines toprovide approaches to manage tensions and resolve trade-offsbetween AI ethics aspects, in order to facilitate the design andimplementation of well-rounded Responsible AI systems.To that end, we summarise and analyse various approachesto addressing trade-offs in Sec. II, noting their advantagesand disadvantages. We discuss the overall properties of theexamined approaches in Sec. III, and propose a multi-stepframework that draws on the gained insights as well as the needsat societal, organisational and practitioner levels. Concludingremarks are given in Sec. IV.Summary of the high-level AI ethics principles proposed by the Australian Government [2].",
                    "reference": "[1] Elizabeth Sanderson, Elise Schleiger, and Daniel Douglas. 2024. Resolving ethics trade-offs in implementing responsible AI. arXiv:2401.08103. Retrieved from https://arxiv.org/pdf/2401.08103"
                },
                {
                    "id": "21ce90b7-07f2-49a9-9cd6-21746747b78f",
                    "text": "A. Dominant AspectsA blunt and straightforward approach to resolve tensionsbetween ethics aspects is to select the most dominant orpertinent aspect in a given context. The prioritisation of aspects(eg. accuracy over privacy) can be driven by how difficult orcostly it is to implement a given aspect within an AI/MLsystem, and/or internal organisational needs (eg. regulatorycompliance), and/or the preferences of end users of the AI/MLsystems [22].The advantage of this approach is its overall simplicity andthe low degree of required effort. A major disadvantage isthat this is a winner-takes-all approach, which leaves no roomto devise balanced trade-offs and take into account nuance,which in turn implies that a thorough evaluation of associatedrisks and benefits is not performed. This approach is henceconsistent with the pejorative notions of ethical lip service [31]and ethics washing [6], where only minimal effort is expendedto address ethical issues that emerge in AI/ML systems.",
                    "reference": "[1] Elizabeth Sanderson, Elise Schleiger, and Daniel Douglas. 2024. Resolving ethics trade-offs in implementing responsible AI. arXiv:2401.08103. Retrieved from https://arxiv.org/pdf/2401.08103"
                },
                {
                    "id": "2cd0fe07-3f34-42d9-b5f1-175a24f181f4",
                    "text": "An indirect approach to resolving trade-offs is via prioritisa-tion of ethics aspects, as proposed in [12]. This involves a multi-step strategy to reduce the operational risk of AI/ML systems,summarised as follows. First, an undesirable operational eventin a given AI/ML system is identified (eg. a specific failure),through a risk assessment matrix that takes into account thelikelihood of the event and associated degree of loss. Secondly,the risk assessment matrix is expanded to allow degrees ofinfringement (de-prioritisation) of given ethics aspects in orderto reduce the risk of undesirable events. Lastly, additionalsafeguards are put into place with the intention to amelioratethe infringement of the affected ethics aspects.As per the example given in [12], an AI-based userauthentication system (eg. to prevent unauthorised access tobank accounts) can be made more accurate and/or more robust(eg. less susceptible to impersonation attacks) by requiringthe use of more personally identifiable information (eg. faceimages [14]). However, the use of such information \u201cinfringes\u201dthe privacy aspect. The infringement is then amelioratedthrough further security measures to protect the informationand to comply with applicable laws such as EU\u2019s General DataProtection Regulation (GDPR) [15].An advantage of the above multi-step strategy is that theprioritisation of ethics aspects is driven by system-specificrequirements and takes into account the context of systemoperation. The disadvantage is that the risk assessment stepmay be error-prone (eg. missing undesirable events, unreliableestimation of likelihood and loss), and may require expertknowledge which is beyond the level available to the designand development teams. Furthermore, it may not be possibleto adequately ameliorate the infringement of applicable ethicsaspects. Lastly, the incorporation of ethics aspects is doneduring later stages of AI/ML system design, which can beinterpreted as treating the aspects as add-ons, rather than takingthem into account from the very outset.",
                    "reference": "[1] Elizabeth Sanderson, Elise Schleiger, and Daniel Douglas. 2024. Resolving ethics trade-offs in implementing responsible AI. arXiv:2401.08103. Retrieved from https://arxiv.org/pdf/2401.08103"
                },
                {
                    "id": "1a40b461-d780-4ff8-8391-b3bb189ba6bc",
                    "text": "Trade-off analysis techniques used in requirements engineer-ing [1], [20], [28] may be applicable for determining howeach ethics aspect affects an AI system under construction andfor addressing the interplay between ethics aspects [25], [30].Each applicable ethics aspect is listed, followed by listingpossible ML models and data types that may be suitablefor implementing an AI system. Linkages between the ethicsaspects and system components (model types and data types)are then graphically noted, in conjunction with their positive ornegative effect (either in a quantitative or qualitative manner).An illustrative example of the above trade-off analysisapproach is shown in Fig. 1. The AI system under constructionis a biometric user authentication system employing speechand/or face data, with the overall goal to increase securityof banking services. Accuracy of the AI system is driven bytwo main components: model type and data type [8]. Two MLmodels are considered: Hidden Markov Model (HMM) [14]and Deep Neural Network (DNN) [3]. Furthermore, two datatypes are considered: speech only, and speech in conjunctionwith face data. In this example, using DNN can increase theaccuracy by 5%, but at the cost of reduced explainabilityin comparison to HMM. Using speech and face data overusing speech data alone can increase accuracy by 10%, thoughthis negatively affects the privacy aspect as more personallyidentifiable information is used.This type of trade-off analysis techniques can be consideredas part of the design phase, where many possible implementa-tions of an overall AI/ML system are explored. The techniquescan also be considered as part of the documentation phase,where the trade-offs are explicitly documented (rather than leftas tacit knowledge), along with discussions on the practicalpros and cons of each possible implementation. This ties inwith the dimension of justification suggested in [10], wherethe justification provides a context-specific rationale for thedrivers behind giving more weight to one aspect over another. An advantage of this approach is that many possible trade-offs can be explicitly shown and considered at the same time,and the consideration is done as part of the design phase.A disadvantage is that the graphical representation can becomequite complex when more model types, data types and ethicsaspects are considered. Furthermore, multiple graphs may berequired, depending on the complexity of the ML pipeline [52].",
                    "reference": "[1] Elizabeth Sanderson, Elise Schleiger, and Daniel Douglas. 2024. Resolving ethics trade-offs in implementing responsible AI. arXiv:2401.08103. Retrieved from https://arxiv.org/pdf/2401.08103"
                },
                {
                    "id": "cb4d9863-7057-4618-abae-1181e5c7f182",
                    "text": "Inspired by [27], a ranking approach can be used to choosetrade-off solutions. Given several technical solutions to a giventrade-off, such as a set of possible ML models with variousdegrees of explainability for the accuracy/explainability trade-off [36], each solution is ranked according to an overall score.The overall score is a weighted convex linear combinationof a set of normalised sub-scores [46], with each sub-scorerepresenting how well a given solution addresses a desiredcharacteristic pertinent to the trade-off at hand. The set ofcharacteristics can range from purely pragmatic (eg. complexityof the ML model), to various philosophical positions (such asutilitarianism and egalitarianism [27]). The weighting of thecharacteristics can be non-informative (ie. all weights are equaland sum to one), or it can be based on the importance of eachcharacteristic to an organisation (ie. weights are unequal andskewed towards focusing on a subset of characteristics).The main advantage of this approach is that it aims to providea quantitative procedure for resolving trade-offs, and explicitlyallows for the consideration of characteristics that are importantfor the technical implementation of the AI/ML system, as wellas wider organisational policies. However, the flexibility canalso be a disadvantage, in that the selection of characteristicscan be subject to errors, and hence may require well-informedreasoning that may be beyond the capability of the practitionersand/or the organisation. Furthermore, determining sub-scoresand associated weights for the characteristics can be subjective,especially when dealing with characteristics that are not easilyquantifiable [13]. For example, it is non-trivial to represent thedegree of egalitarianism as a precise numeric value.",
                    "reference": "[1] Elizabeth Sanderson, Elise Schleiger, and Daniel Douglas. 2024. Resolving ethics trade-offs in implementing responsible AI. arXiv:2401.08103. Retrieved from https://arxiv.org/pdf/2401.08103"
                },
                {
                    "id": "78a5bcf8-30be-42e8-a882-5daa8fe51386",
                    "text": "Principlism is an influential approach in bioethics thatuses a set of moral principles to guide ethical decisions,such as respect for autonomy, nonmaleficence (avoid causingharm), beneficence (promoting the welfare of others), andjustice [5]. The principlism approach has also been appliedto cybersecurity ethics [18]. The similarity with high-level AIethics principles makes principlism a useful approach to drawon when considering how to address trade-offs between theunderlying AI ethics aspects, and for identifying the limitationsof using a principlist approach to AI ethics [32], [41].Principlism uses two approaches to bridge the gap betweenabstract principles and addressing individual cases [5]: (i) spec-ification, and (ii) balancing. Specification elaborates on theprinciples to describe how individual cases are relevant to aspecific principle or to the underlying ethical aspect behind it.For example, the principle of justice may be further specifiedby a rule that prohibits using ethnicity or gender as a basis fordistributing access to resources [42]. AI ethics principles mayalso come with brief descriptions of how each principle maybe applied [2]. However, such elaborations of basic principleswill not cover all the possible cases, and will not remove allthe potential conflicts between them [5].When ethics principles or their underlying aspects give con-flicting recommendations, the principlism approach providessix conditions that any balancing or trade-off must meet [5],as summarised below:1. A stronger justification can be given for prioritising oneaspect over another.2. The purpose of overriding a given aspect has a realisticchance of being achieved.3. There are no alternatives to this trade-off that are morallypreferable.4. The overridden aspect is infringed to the smallest extentpossible to achieve the purpose of overriding it.5. The negative effects of overriding the aspect are min-imised.6. Those affected by this trade-off are treated impartially.The justifications for prioritising a given aspect over another(condition 1) may be grounded in practical limitations or inethical theory. Such justifications must still comply with theother five conditions. Ethical concepts or theories that mayground such trade-offs include:Proportionality: the methods used should be appropriatefor the problem the AI/ML system is intended to solve, andshould have a minimal impact on the system\u2019s compliancewith the other aspects [24].Benefit to the least-advantaged (maximin): decisionsshould be made based on maximising the benefits tothe worst-off [38], [51].Utilitarianism/Consequentialism: decisions should bemade based on maximising the utility or total benefits toall those affected by them [51].There are several advantages to adopting a principlist approachto addressing AI ethics concerns. A set of ethics principlesoffers developers a common vocabulary for discussing ethicsissues [41]. Each principle serves as a starting point for furtherethical reflection and discussion about how it may apply toa specific AI application. The explicit statement of principlescan also encourage cultural shifts within professions towardsprioritising the values and goals that they express [41]. Theymay serve to establish ethical norms and change the valuesthat are prioritised within professional communities [41].However, the principlism approach also has several disad-vantages. While the six conditions described above provideguidelines for how to make trade-offs between ethics aspects,how developers respond to these conditions is left up to thedevelopers\u2019 judgement [17]. One set of developers may takea consequentialist approach to justifying the trade-offs theymake, while another may favour using maximin. As a result,various developers may make different trade-offs when facedwith the same conflict between ethics aspects. Furthermore, AI/ML system development occurs in a sig-nificantly different context to medicine, where principlismhas been influential as an approach to ethics [32]. Unlikemedicine, AI development does not have common goals, a longprofessional history that has clear descriptions of the ethicalduties expected of practitioners, a relative lack of methods forinterpreting principles into practice, and the relative lack ofprofessional accountability [32]. These differences may limitthe effectiveness of ethics principles for influencing AI designand governance [32].",
                    "reference": "[1] Elizabeth Sanderson, Elise Schleiger, and Daniel Douglas. 2024. Resolving ethics trade-offs in implementing responsible AI. arXiv:2401.08103. Retrieved from https://arxiv.org/pdf/2401.08103"
                },
                {
                    "id": "9ba590fc-3ec9-4228-aaa0-5d89f6b6ab55",
                    "text": "In the preceding section we examined five approaches thatcan be incorporated into the AI/ML system design processwith the aim of addressing tensions between common AI ethicsaspects. From across the covered approaches we can extractfive properties: (1) prioritisation of ethics aspects based on riskassessment/context specific assessment; (2) proactive analysisat start of design process; (3) de-prioritisation of an ethicsaspect in favour of another; (4) analysis requires deliberationand/or resources to enact; (5) additional considerations requiredto ameliorate side-effects. Table III indicates the presence andabsence of the above properties for each of the approaches.There is no single approach that is likely to be appropriatefor all organisations, AI/ML systems, or applications. In orderto address this shortcoming, we propose a layered frameworkto introduce into the AI/ML system design pipeline, whichis enacted at a practitioner level while being supported byorganisational and societal influence.Guidance of the design process and high-level decisionmaking at a societal level can be informed by scientificliterature, AI ethics principles, standards and regulations. Thesedimensions, however, do not take into account context, whichis a key consideration in design decisions when managingtensions. Context includes the purpose of the AI/ML system,the groups of people impacted by the system, and the levelof understanding about the outputs of the system that arerequired [4]. At an organisational level, policies and governancecan be more specific to the general context of an AI applica-tion [9], but will generally be agnostic to the nuanced detailsrequired to make design decisions on a case-by-case basis.At the practitioner level, where the resolution of tensions isspecific to the context of the AI model, risk assessments aswell as design tools and justifications can be used.We can use insights from the approaches described inSec. II to build a multi-step framework around managingtrade-offs and tensions between ethics aspects. The proposedframework is comprised of three main components: (i) proactiveidentification, (ii) prioritisation and weighting, (iii) justificationand documentation. The components are elucidated below.Proactive identification. A proactive and structured assess-ment at the beginning of the AI/ML design pipeline providesa framework to examine and define the context and purpose ofthe AI/ML system. It also encourages a proactive approach forthe identification of potential tensions between ethics aspects aswell as the consideration of the methodology required to resolvethem. The requirements engineering approach (Sec. II-C) en-compasses proactive design consideration where ethics aspects,models and data types are examined, and the associated tensionsand solutions are explored [30]. Similarly, a Value SensitiveDesign (VSD) approach exemplifies a proactive approach toapplying high-level ethics principles in practice [10]. VSDis an iterative and multidisciplinary process where ethicalconsiderations are incorporated into the design process fromthe start [11]. Other approaches consider tensions betweenethics aspects as they arise in the design process. For example,ranking of trade-off solutions (Sec. II-D) would routinely occurwhen tensions arise and can be used in addition to a prospectiverisk analysis. Specifying what the ethics principles mean in thecontext of a specific AI/ML project (such as in the principlismapproach in Sec. II-E) may also identify possible tensionsbetween ethics aspects early in the design process and henceallow them to be addressed.Prioritisation and weighting. The nature of resolvingtensions between ethics aspects that arise when developingAI/ML systems is that trade-offs must be made, resulting in(a) one or more ethics aspects being prioritised over others,and/or (b) a weighted (balanced) combination of selectedethics aspects. The least critical technique to apply is thedominant aspects approach (Sec. II-A), where prioritisation ismade based on one-dimensional decisions such as difficulty orcost. In contrast, the other approaches listed in Sec. II requireprioritisation based on a degree of context-based assessment.To this end, the first step in prioritising one ethics aspect overanother is a consideration of the broader context. This can bedone in a qualitative manner to selectively introduce trade-offsin order to reduce identified operational risks, as per the riskreduction approach (Sec. II-B). Alternatively, the quantitativeranking approach (Sec. II-D) can be used, where varioussolutions are ranked according to desired characteristics. Suchrankings can also inform the responses to the six conditionsspecified for resolving conflicts between ethics aspects used bythe principlism approach (Sec. II-E). A hybrid approach is usedby the requirements engineering approach (Sec. II-C), whereboth quantitative and qualitative measurements are employed.Justification and documentation. Explicability and trans-parency are critical aspects of responsible design [4]. As part of that, documentation and reasoning are essential concerning thedesign decisions where trade-offs have been made. Justificationprovides a context-specific rationale for the drivers behindgiving more weight to one ethics aspect over another [10].The requirements engineering approach (Sec. II-C) can providejustification focused on practical aspects of AI/ML system func-tionality, while the quantitative ranking approach (Sec. II-D)can take into account dimensions that go beyond immediatepractical aspects. The principlism approach (Sec. II-E) takesa further step and provides a strong framework to developjustifications, where prescriptive conditions are explored in theprocess and can then be used to document and justify trade-offdecisions. For example, documentation that addresses how thesix conditions described in Sec. II-E are being met throughconsidered design decisions, so that there is accountability forhow the design decisions were made. These explanations (andthe impacts of the resulting trade-offs) may also inform futuredecisions about resolving ethical tensions that arise in otherprojects.",
                    "reference": "[1] Elizabeth Sanderson, Elise Schleiger, and Daniel Douglas. 2024. Resolving ethics trade-offs in implementing responsible AI. arXiv:2401.08103. Retrieved from https://arxiv.org/pdf/2401.08103"
                },
                {
                    "id": "f5983f90-2fce-420f-a355-05dc04c0c13e",
                    "text": "While progress has been made in the application of high-levelAI ethics principles in the design and implementation of AI/MLsystems [33], [40], [45], there are still notable areas of concern,including a theory-practice gap in how to manage tensionsthat arise between commonly accepted AI ethics aspects thatunderpin the principles [10], [39].In this work, we have covered five approaches for addressingthe tensions via trade-offs, ranging from rudimentary toquite complex. The approaches mainly differ in the types ofconsidered context, the scope of each context, the associatedmethods for qualitatively and/or quantitatively measuring eachcontext, and how each trade-off decision is justified. None ofthese approaches is likely to be appropriate for all organisations,AI/ML systems, or applications.In response, we have proposed a framework for AI/MLsystem developers for use in the design pipeline that draws onthe various strengths of the covered approaches. The frameworkhas three main components: (i) proactive identification, (ii) pri-oritisation and weighting, (iii) justification and documentation.At the start of the AI/ML design pipeline, potential tensionsbetween ethics aspects are identified, and consideration is givento the methodology for resolving them. The tensions betweenthe identified aspects are then addressed through prioritisationand weighting, using one or more of the covered approaches;both practical and organisational requirements can be taken intoaccount. Each trade-off decision is justified and documented,aiding transparency and accountability, as well as adding tothe pool of organisational knowledge.AI/ML systems built with rudimentary and/or shallow ethicalassessments are unlikely to be robust against potential legaland regulatory challenges. Employing a proactive and dynamicassessment method across the full AI/ML system pipeline(including the context of the system\u2019s application), such as theframework proposed here, is more likely to yield well-roundedsystems that are appropriately designed and implemented fortheir regulatory environment.References",
                    "reference": "[1] Elizabeth Sanderson, Elise Schleiger, and Daniel Douglas. 2024. Resolving ethics trade-offs in implementing responsible AI. arXiv:2401.08103. Retrieved from https://arxiv.org/pdf/2401.08103"
                }
            ]
        },
        {
            "paper_title": "Establishing data provenance for responsible artificial intelligence systems",
            "authors": "K Werder, B Ramesh, R Zhang",
            "publication_info": "ACM Transactions on Management \u2026 - dl.acm.org",
            "paper_url": "https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf",
            "chunks": [
                {
                    "id": "ddb37cb1-c90b-4fc8-94c7-79e5a620e11f",
                    "text": "Data provenance, a record that describes the origins and processing of data, offers new promises in the increasingly important role of artificial  intelligence  (AI)-based  systems  in  guiding  human decision making. To avoid disastrous outcomes that can result from bias-laden  AI  systems,  responsible  AI  builds  on  four  important characteristics:  transparency,  and explainability.  To  stimulate  further  research  on  data  provenance that enables responsible AI, this study outlines existing biases and discusses possible implementations of data provenance to mitigate them. We first review biases stemming from the data\u2019s origins and pre-processing. We then discuss the current state of practice, the challenges  it  presents,  and  corresponding  recommendations  to address  them.  We  present  a  summary  highlighting  how  our recommendations can help establish data provenance and thereby mitigate  biases  stemming  from  the  data\u2019s  origins  and  pre-processing to realize responsible AI-based systems. We conclude with a research agenda suggesting further research avenues. CCS CONCEPTS \u2022Data Provenance \u2022Artificial Intelligence KEYWORDS Data Provenance, Artificial Intelligence, Fairness, Accountability, Transparency, Explainability 1  Introduction As evidence-based decision making aided by data-driven artificial intelligence (AI) algorithms becomes increasingly common across all sectors of the economy, there is a growing concern among users about  whether  such  algorithms  are  developed  and  implemented responsibly. Prior reports have already provided a glimpse into the disastrous effects of inaccurate and bias-laden AI recommendations  in high-stakes applications, with examples from the healthcare and legal  domains,  such  as  incorrect  patient  treatment,  exacerbated poverty [62], wrongful arrest [33], and unjust criminal sentencing [43].  The  heightened  awareness  of  concerns  raised  in  recent movements for social justice has resulted in calls from professional associations [1] and researchers [18,34] for developing approaches that help establish responsible AI.  Rapid innovations in data-generating technologies, such as sensors, social media, and mobile devices, have exacerbated the problems resulting from poor data quality that threaten the development of responsible  AI  systems.  These  technologies  generate  an unprecedented  quantity  and  variety  of  data.  While  most applications  have  benefitted  from  explosive  growth  in  data availability (in terms of volume, variety, velocity, veracity, etc.), limited  attention  has  been  given  to  data  quality  [66],  in  turn undermining the quality of recommendations generated using such data. Motivated by these concerns, this study examines how data provenance can help improve data quality and enhance the fairness, accountability,  transparency,  and  explainability  (FATE)  of  AI-based  systems.  We  argue  that  data  provenance\u2014a  record  that describes the origins and processing of data [9]\u2014can help assess and  improve  the  FATE  of  recommendations  provided  by  AI algorithms and thus instill trust in them. Trust is enhanced by the capability to describe and follow the life of data (i.e., their origins, processing, and use) in both forward and backward directions [75]. The importance of provenance has long been recognized [14] in the pharmaceutical,  food,  and  fashion  industries.  It  helps  establish  a product\u2019s  origins  and  influences  consumers\u2019  decisions  about purchase and use.  Responsible AI is essentially related to a broad discourse, AI ethics, which  has  received  significant  attention  among  researchers  in recent years. Scholars have identified different high-level ethical principles  that  should  govern  the  development  of  AI  systems [25,48,97].  While  no  universal  consensus  exists,  fairness, accountability,  and  transparency  [48]  have  received  significant attention in this research community [27]. Simultaneously, research related to explainable AI has emerged [39], with recent discussions on  its  capability  to  bridge  the  gap  between  technical  and  ethical considerations [64]. AI explainability gives users and experts the ability  to  investigate  and  understand  the  inner  workings  of  AI, ACM Transactions on Management Information Systems allowing  them  to  identify  potential  biases.  Bridging  these  two perspectives, we focus on four important and related characteristics of responsible AI\u2014FATE. While there is ongoing research on other AI-based system characteristics, such as privacy and agency, we focus on how FATE can help organizations identify and mitigate the negative influences of biases within their data. We discuss how potential conflicts among different FATE characteristics emerge, how organizations can manage them, and where more research is needed. Most  current  researchers  and  practitioners  in  the  field  of responsible  AI  have  emphasized  the  quality  of  algorithms. However, an algorithm\u2019s recommendations or outputs also depend heavily on representations, structures, and data quality, which serve as  the  inputs.  In  this  study,  we  focus  on  data  provenance,  an important aspect of data quality, in the development of responsible AI systems [13]. For example, data provenance can help uncover data quality concerns related to labor-intensive data labeling, which is  often  performed  by  unqualified  workers  [7]  and  otherwise remains  concealed.  This  the recommendations  or  outputs  of  AI  algorithms  are  often  used  as inputs  for  other  AI  algorithms  [53],  further  exacerbating  the problem. For example, the classification of a radiology scan by an algorithm  as  benign  or  malignant  may  be  used  as  an  input  for another  algorithm  that  is  used  to  create  a  risk  score  for  patient is  particularly  alarming,  as Table 1 \u2013 Overview of FATE characteristics and examples  readmission. In such situations, data provenance can help identify the  causes  of  the  AI  algorithm\u2019s  poor  performance,  improve interpretability,  or  uncover  its  seemingly  acceptable that performance  was  achieved  for  invalid  reasons  (e.g.,  when identifying a malignant tumor, the system was learning from the circle made by the radiologist on the scan rather than the data from the scan itself). By  illuminating the origin and processing of the data  [14],  data  provenance  can  mitigate  these  shortcomings  and facilitate FATE assessments (see Table 1).  The  lack  of  data  provenance  is  a  serious  concern  in  AI-based systems  that  are  used  to  inform  critical  decisions.  While  the establishment of data provenance may increase short-term costs for organizations, it can provide long-term benefits by instilling trust in the implemented system and its recommendations. Specifically, our  study  addresses  the  following  question:  How  does  data provenance  affect  the  four  interrelated  characteristics  of responsible  AI:  fairness,  accountability,  transparency,  and explainability? The paper analyzes biases related to origins and pre-processing of data,  discusses  the  current  state  of  practice  and  attendant  challenges,  and  presents  recommendations  for  addressing  them. Our  recommendations  are  intended  to  help  establish  data provenance and mitigate biases stemming from the data's origins and pre-processing to realize responsible AI-based systems. Characteristic  Description  Example Fairness  AI-based  systems  may  introduce  discrimination because  of  imbalanced  data  [4].  The  data  used  in training  AI-based  the discriminations existing in our society, which, in turn, lead to algorithmic bias [4]. systems  often  reflect  Training the system using only medical records from male patients can lead to discrimination against female patients. Accountability  Because  of  the  increasing  complexity  of  AI-based systems,  it  is  difficult  for  a  user  to  judge  who  is accountable for the results [49]. The individual services provided  by  AI  algorithms  are  integrated  into  larger systems  [19],  further  exacerbating  opaqueness  and ambiguity about ownership.  When  an  AI-based  system  trained  on  photos  depicting cancer on the epidermis (outer skin layer) is integrated into a larger system, it may also be inappropriately used on data from  subcutaneous  tissue  (inner  skin  layer).  It  becomes unclear  who  is  accountable  for  the  resulting  incorrect recommendation.  Transparency  An often-cited limitation of AI-based systems is their black  box  nature  [2].  However,  to  understand  the training  data quality  of adequacy, we need transparency. recommendations  and  The  pharma  industry  has  well-established  practices  for providing  easy  access  to  relevant  information  about  drugs (either in the product package itself or in the accompanying documents),  whereas  AI  systems  seldom  provide  relevant information  in  developing recommendations.   the  data  used about Explainability  A lack of explainability of AI prediction outcomes can be caused by the black box nature of algorithms, which can lead to negligence of the inaccuracies and biases in data.  Yet,  understanding  a  prediction  is  an  important aspect of their acceptance [84].  Evidence-based  medicine  rests  on  high  standards  of explainability  of  both  algorithms  and  data,  as  medical decision  making  requires  a  sound  understanding  of  the underlying  disease  mechanisms  and  treatments  [88].  The lack of this understanding undermines the implementation of AI in healthcare [81].  Data Provenance for Responsible AI  ACM Transactions on Management Information Systems In the following sections, we review key biases, such as systematic distortions [3], resulting from the failure to adopt appropriate data provenance  practices  in  the  development  and  implementation  of AI-based systems. We also provide three key recommendations for establishing  data  provenance  to  enhance  the  FATE  of  AI-based systems. We propose a data provenance framework for responsible AI  and  discuss  exemplary  cases  for  its  application.  Before concluding, we present future research directions. 2  Sources of Data Biases in AI-based Systems In contrast to the majority of existing research, which has focused on biases resulting from algorithms (e.g., [28,35]), we concentrate on the origins of the data and the data pre-processing rather than on the algorithm that uses the data as inputs. Data sources are often where the original data were collected to train and build AI-based systems.  After  data  collection,  data  pre-processing  [30],  which commonly  integration,  cleaning, normalization, and transformation, can also introduce biases [96]. We identify five categories of potential biases that may originate from  data  sources  and  five  categories  of  biases  that  may  be introduced  during  data  pre-processing.  For  example,  the  data themselves might be subject to bias in the ways in which they are sampled or measured. Each bias has different implications for the FATE characteristics of AI-based systems. includes  data  preparation, Table 2 \u2013 Summary of the effect of data biases on responsible AI  Population data Measurement error Data quality chasm Data repurposing Data augmentation Dataset shifts Opaque pre-processing Data labeling Adversarial manipulations  X X X X  X X X X X X  X X X X X  X  X X X X X X X X X Transfer learning  X  X  X 2.1  Biases from the Data\u2019s Origins Below, we identify five key instances in which biases arise in the data  sources:  population  data,  measurement  error,  data  quality chasm, data repurposing, and data augmentation. We describe their implications regarding the FATE characteristics (see Table 2 for a summary).   Population data. In every data science project, sampling the right data  to  ensure  representativeness  is  important  [33].  However,  to develop  and  implement  powerful  AI-based  systems,  developers often  rely  on  access  to  unique  data.  For  example,  data  provided through  projects,  such  as  BigMedilytics,  comprise  the  medical records of more than 11 million patients from eight countries. The retraining or recalibration of AI-based systems developed with such unique  data  to  other  contexts  for  the  same  purpose  requires additional data that are representative of the new context.  However,  AI-based  systems  are  often  applied  in  new  contexts without  retraining  or  recalibration  because  of  the  significant challenges involved in collecting the necessary additional data. For example,  when  an  algorithm  is  trained  with  data  from  one population but is used to develop predictions on another population, any  differences  in  the  frequency  and  nature  of  events  in  these datasets  will  result  in  poor  performance  [19].  When  the  data collection mechanisms impose selection bias or  fail to recognize the mismatch between the training data and the target population, the  transparency  of  the  data\u2019s  origins  is  affected.  In  addition, spurious correlations and shortcut learning (i.e., decision rules that work  well  based  on  the  training  data  because  of  spurious phenomena [32]) of the AI system will lead to unreliable and unfair recommendations [20] that will undermine possible explanations.  Measurement  error.  Every  study  and  every  measurement instrument, however well designed, still generates some errors [72]. Many AI applications in domains such as medicine or business rely heavily on Bayesian statistics, as the results are always subject to probabilities.  Data  pre-processing  and  the  use  of  another algorithm\u2019s  predictions  as  an  input  could  further  compound  this issue  because  of  the  propagation  of  uncertainties  or  prior probabilities [61].  However,  in  AI  systems,  the  uncertainty  of  the  input  variables resulting either from the measurement itself or from pre-processing is  often  neglected.  An  AI-based  system  trained  with  such  data without a particular focus on and caution about potential errors can result in a poorly performing model. Consequently, the precision of an  AI-based  system  might  be  overestimated,  as  the  AI  system learns to fit against the error. The resulting recommendations would be  at  least  distorted  if  not  incorrect,  leading  to  problematic outcomes.  If  the  system  provides  corresponding  explanations,  a user can identify these inadequacies and correct them [19].  Data  quality  chasm.  Another  challenge  is  the  lack  of  data  with adequate quality in settings where the AI system is used [61]. While the data may look homogeneous at the surface level, a more careful evaluation  can  suggest  otherwise.  For  example,  an  AI  algorithm may  achieve  superior  prediction  quality  because  of  its  access  to state-of-the-art computed tomography (CT) scans. If CT scans from older  equipment  that  generates  lower-quality  scans  are  used  to retrain the AI-based system, the recommendations are likely to be inaccurate.  ACM Transactions on Management Information Systems In  contrast  to  the  measurement  error,  in  which  the  system  has learned to predict based on errors, here, the AI-based system was trained using fine granular data that are no longer available later, thus  resulting  in  poorer  performance.  This  provides  multiple challenges along the FATE characteristics. The poor performance can  lead  to  suboptimal  recommendations,  and  depending  on  the level  of  transparency  provided  initially,  questions  related  to accountability between the system developer and system provider can  arise.  Creating  transparency  regarding  the  training  data\u2019s origins and the data used for the recommendations helps mitigate this issue. Data repurposing. In addition to biases resulting from sampling, data  collection  practices  also  introduce  misuse  and  biases. Traditional  data  collection  practices  differ  significantly  from contemporary  practices  in  AI  systems  development  [33].  The traditional  practice  is  to  collect  data  for  a  specific  purpose.  For example,  a  clinical  trial  of  a  drug  used  to  treat  COVID-19  will collect experimental data to assess the drug\u2019s side effects.  However, repurposing data is the norm in AI-based systems. For example,  a  blood  test  result  in  a  patient\u2019s  electronic  healthcare record that has been captured to diagnose a certain disease may also be used by an AI-based system to diagnose other diseases. This can be a potential issue compromising the accountability characteristic of  the  algorithm.  For  example,  while  the  quality  of  data  from medical images can be sufficient for the original purpose, such as stroke detection, it may not meet the needs of subsequent data uses, such as finding new disease markers [5]. Repurposing data creates ambiguity  about  the  data  and  their  origins, making it difficult to clearly identify the person or entity accountable for any incorrect recommendations. Data  augmentation.  When  the  available  dataset  is  not  large enough for the intended computations, data augmentation might be used  (i.e.,  increasing  the  size  of  the  dataset  with  synthetically generated data or slightly modified copies of the existing data, for example, through translation, rotation, flip, or scale). For instance, augmented data are generated through the rotation, translation, and scaling  of  a  prior  dataset  on  liver  lesions  [26]  when  training  a generative  adversarial  network  (GAN).  These  modifications  and the synthetically generated data can amplify existing biases within the dataset and mask the inadequacies of the collected data.  Some AI algorithms rely solely on simulated data. For example, AI systems have been developed to design bridges and control robot arms using only simulation data [23]. Simulations can create useful data to learn from, especially when little input and manually labeled data are available. However, because deep learning can approach problems more intuitively by focusing on patterns in the core data, researchers have suggested that AI systems perform better without synthetic additions to the data [23].  Therefore, data augmentation and the use of simulation data bring about  new  challenges  the  fairness  and  accountability characteristics  of  AI  algorithms.  Data  augmentation  amplifies existing  biases  and  creates  opaqueness  about  the  actual representativeness  of  the  data,  thus  limiting  transparency  and to  making  it  more  challenging  to  identify  the  cause  of  an  incorrect recommendation.  2.2  Biases from Data Pre-Processing Data processing is vulnerable to errors that introduce biases, such as dataset shifts, opaque pre-processing, data labeling, adversarial manipulation, transfer learning, and data augmentation. Dataset shifts. An easily ignorable fact is the non-stationary nature of the environment and the population from which all the input data of AI-based systems are generated [59]. For example, when a data shift occurs, an important predictor of a specific disease at one point in time can be more or less important at a later point in time because of  improvements  in  the  quality  of  care  available.  For  instance, many predictions using the Medical Information Mart for Intensive Care  dataset  are  confounded  by  changes  in  hospital  operation practices [71].  Considering  time  as  an  influential  variable  shows dataset shifts caused by changing practices, which, in turn, result in significant changes in the observed data. Unless this data shift is identified and the AI algorithm is retrained or recalibrated [53], the performance  of  the  system  deteriorates,  affecting  the  fairness, transparency,  and  explainability  characteristics  of  the  algorithm. Low  performance  can  lead  to  incorrect  recommendations  that negatively affect users. If the data\u2019s origins and subsequent changes in  the  environment  are  not  made  transparent,  the  derived explanations will be at least distorted. Opaque pre-processing. AI-based systems are often characterized as black boxes [2]. While some AI-based systems provide accurate predictions, the rationale behind their predictions remains opaque. In  algorithms  with  intrinsic  obscurity,  such  as  deep  neural networks,  understanding  the  specific  patterns  being  learned  is difficult [53]. For example, in a study detecting hip fractures, an algorithm  was  confounded  by  the  scanner  model  and  by  scans marked as \u201curgent\u201d [8]. Therefore, assessing the potential biases introduced  when  using  the  output  of  an  opaque  algorithm  as  an input  for  another  AI-based  system  is  difficult.  Opaque  pre-processing limits the transparency and explainability of AI-based system recommendations. If it is unclear what data were used to train  the  system,  confounding  indicators  are  more  difficult  to identify and assess, and they do not allow users to learn relevant insights. However, deriving explanations for the recommendations can  help  experts  validate  the  model  and  its  recommendations. Different types of explanations (e.g., feature extraction, pre-defined models, and sensitivity [87]) can help an expert evaluate, improve, and correct the model. Data labeling. While data quality chasm refers to data that may appear to be similar but have different qualities, another issue arises with data labeling, as the identification and development of labels are  often  not  transparent.  Data  labeling  is  related  to  supervised learning, such as medical image classification. The outcome labels are  used  by  supervised  algorithms  in  the  training  stage.  While automated  labeling  (e.g.,  with  weak supervision)  are  on  the  rise  [50,76],  labeling  is  often  a  labor-intensive task and is frequently performed by unqualified or poorly trained  ghost  workers  or  through  crowd-based  platforms  [7]. techniques  for  data Data Provenance for Responsible AI  ACM Transactions on Management Information Systems Incorrect  labels  in  the  training  data  create  erroneous  or  unfair recommendations  and  explanations  developed  by  AI-based systems because of the inherent bias embedded in the training data. This  bias  affects  the  fairness,  transparency,  and  explainability characteristics  of  the  AI  algorithm.  Fairness  is  affected,  as unqualified or poorly trained ghost workers will make mistakes and possibly  bring  their  social  biases  into  the  data.  As  these  are undesirable  business  practices,  organizations  seldom  disclose them,  thereby  negatively  affecting  transparency.  While  these business  practices  introduce  biases,  hiding  them  from  customers makes it difficult for both the user and the expert to benefit from explanations.  As  the  majority  of  existing  data  are  non-labeled  and  are  usually very expensive to label, some researchers perceive the reliance on labeled  data  as  even  counterproductive  to  the  development  of effective AI [23]. A recent trend in the automatic labeling of data using AI [77] has emerged. The idea is simple. As labeling is often a bottleneck task in AI system development, we could use machine learning (ML) to extrapolate the labels. A labeling ML algorithm can  be  trained  based  on  a  limited  number  of  available  or  easily attainable  labels  and  can  then  be  used  to  label  a  larger  dataset. While this reduces the effort of manual labor, it may also increase the severity of biases already existing in the smaller sample, leading to erroneous or unfair recommendations and explanations. Adversarial  manipulation.  As  AI-based  systems  derive  their models based on nuanced variations in the data, sometimes, small changes in the data input can lead to significant differences in the output  [38].  Therefore,  AI-based  systems  are  potentially susceptible  to  adversarial  manipulation.  For  instance,  images  of benign moles may be misdiagnosed as malignant because of added adversarial  noise  or  seemingly  minor  changes  in  the  data  [53]. These manipulations can be intentional, such as when an attacker changes the input of an algorithm to fool it, or unintentional, such as  when  a  user  accidentally  rotates  an  image  used  as  an  input. the  data  preprocessing, Without  sufficient  transparency  of  identifying this potential threat in an otherwise effective model is difficult. These seemingly minor changes can result in significantly different  outcomes  that  make  explaining  the  recommendations difficult and the recommendation itself possibly incorrect. Transfer learning. Once an AI-based system is built, we may use the algorithm to solve similar problems. In particular, a new AI-based system benefits from the information learned from another system. For example, a pre-trained model can be used to encode radiographic  features  in  images  before  final  re-training  [8]  to improve the sample efficiency for a reinforcement learning agent. Transfer learning can also improve AI system performance when predicting cancer for ethnic groups with limited data availability [29]. However, transfer learning only works when the source task is closely related to the new task. If not, transfer learning introduces biases and negatively affects performance [91]. As transfer learning also  system\u2019s recommendations,  it  impedes  clear  accountability.  Therefore, transfer  learning  should  be  made  transparent  to  the  user,  as  it otherwise adds to the system\u2019s opaqueness. increases  ambiguity  about  the  AI-based 3  Recommendations for Implementing Data Provenance from  data Considering  the  importance  of  mitigating  data-induced  biases sources  and  data  pre-processing, originating organizations  need  to  establish  data  provenance  when implementing responsible AI-based systems that address the FATE characteristics.  We  propose  a  data  provenance  framework  for responsible  AI  to  enhance  its  FATE  characteristics  (Figure  1). Organizations  can  focus  on  three  key  areas:  establishing organizational data governance, demanding data traceability, and leveraging technological advances, such as explainable AI. Below, we  summarize  current  and  future  challenges  and  elaborate  on actionable  recommendations  and  how  these  enhance  the  specific characteristics of responsible AI (see Table 3).  Table 3 \u2013 Overview of the current state, challenges, and recommendations Current state  Challenges  Recommendations Organizational  data accountability are lacking. lineage  and  Governmental  organizations  demand control and protection of data integrity, confidentiality, and availability.  Establishing Organizational Data Governance: - Managing meta-data - Conducting data audits Organizations rely on data from multiple data sources in their AI systems, creating heterogeneity and opaqueness. Many current AI-based systems rely heavily on manually labeled data.   Organizations typically do not have a clear understanding of the source and processing of data, such as various experiences, goals, and perspectives of the people annotating the data.  Demanding Data Traceability: - Guiding data acquisition - Benefitting from blockchain technology Technologies seek to increase the transparency of AI models.  Little attention has been given to data opaqueness.  Leveraging Technological Advances for Data Provenance: - Deriving rules for explanations - Identifying possible adversarial manipulations - Finding the inherent structure in the data ACM Transactions on Management Information Systems 3.1  Establishing Organizational Data Governance Several  governmental  organizations  have  launched  directives, laws,  and  regulations  to  provide  control  and  protection  of  data integrity, confidentiality, and availability. Examples include the US Health Insurance Portability and Accountability Act (HIPAA) and the EU\u2019s General Data Protection Regulation (GDPR). However, current data governance practices are often limited to master data management, that is, a set of processes related to the who, what, and where of business transactions, communications, and events. Seemingly, organizations too often mimic what their competitors do rather than being proactive and shaping the course of action. For example,  many  organizations  are  still  seeking  to  become  data driven.  Yet,  once  they  achieve  this,  they  find  that  inadequate attention is given to data governance during the development of AI systems, which, in turn, creates additional challenges [44]. Organizations  need  to  establish  organizational  data  governance practices that enforce data lineage and accountability. This would help them not only meet increasingly strict regulatory requirements but  also  benefit  from  an  overarching  perspective  of  their  data assets. Particularly, organizations need to manage their meta-data and conduct data audits in order to respond to the organizational challenges associated with inadequate data governance. For some organizations, these goals stand in a potential conflict. For example, data privacy seeks to protect individuals from being identified\u2014often  through  personal  identifiable  information\u2014or being associated with such information. Data lineage, on the other hand,  refers  to  the  visibility  of  the  data\u2019s  origins  and  further processing. If the data\u2019s origins and further processing are done by individuals, both concepts stand in conflict. An organization will have to manage this conflict by enhancing responsible AI under the condition of privacy policy compliance, such as  the GDPR [98]. For  example,  an  organization  may  allow  identifiable  data  to  be traced only for specific legal purposes. Organizations also need to leverage  some  privacy-preserving  approaches,  such  as  federated learning, to allow the safe sharing of identifiable data or models across entities [69]. Managing  meta-data.  Meta-data  describe  data  and  consist  of detailed information about the data captured in a data source. Meta-data help maintain the data within an organization in a manner that ensures the timely, efficient, and accurate retrieval of the required information [68]. It also helps ensure that processes and activities are documented in a transparent and verifiable way [78]. Generally, there are two practices that organizations use to manage meta-data: cataloging data and curating data. A data catalog stores information about the data, such as the rationale for choosing a data source, the stakeholders  involved,  and  the  content  stored  within  it.  Such information may also be documented in a datasheet [31].  Extending  these  efforts,  organizations  should  establish  clear processes  and  responsibilities  for  data  curation.  Data  curation identifies and leverages the data within the organization and helps assess  the  FATE  of  system  recommendations.  For  example,  organizations  can  identify  representation  and  corresponding limitations  by  visualizing  and  clustering  data  annotations.  These annotations  identification  of  discriminatory correlations between features, labels, and groups.  facilitate  the Overall,  managing  meta-data  through  data  catalogs  and  data curation  helps  increase  the  benefits  of  existing  data  through increased  transparency  [68]  and  helps  reduce  costs  by  avoiding unnecessary  data  collection.  Managing  meta-data  also  requires clear accountability for the different data sources. Meta-data help organizations  benefit  transformation,  weighting,  and sampling  techniques  [4]  by  minimizing  the  extent  to  which  data deviate from the objectives of responsible AI, thus helping ensure fairness of the recommendations.  from Conducting data audits. Enhancing data auditing capability in an organization is another approach to establishing data provenance through  data  governance  [44].  Data  auditing  is  the  process  of assessing whether the data are fit for a specific purpose. Given the recent  increase  in  regulatory  requirements,  organizations  should conduct  data  audits  to  assess  the  data  used  within  their  systems, similar  to  the  way  they  assess  and  audit  other  aspects  of  their business  operations.  Data  audits  help  uncover  potential  biases related to data processing and their associated consequences. With a reasonable and suitable guarantee of authenticity and reliability, data  audits  help  enhance  the  accountability  and  fairness  of  AI-based  systems.  This  not  only  applies  to  high-reliability organizations  that  need  to  make  high-stakes  decisions  but  also provides  benefits  for  other  organizations  that  seek  to  act responsibly. Data audits consist of data profiling (e.g., assessing the availability and quality of data and the risks associated with data integration [45]) and impact analysis (assessing the impact of poor data quality on performance and profits) [57]. Data  audits  become  increasingly  important  when  individual services  are  integrated  into  larger  systems  [73].  Conducting  data audits enhances the fairness of AI systems by ensuring a good fit between the data and their use. Conducting data audits also requires clear  accountabilities  for  the  appropriate  handling  of  data.  In addition  to  establishing  data  accuracy,  data  audits  uncover  data silos  and  areas  where  more  depth  and/or  breadth  of  data  is to  provide  valid necessary recommendations. A data provenance record could document the data  capturing  and  data  processing  entities  for  the  dataset  in question, simplifying the audit process. Data provenance records also  help  in  understanding  the  data\u2019s  origins  and  pre-processing, thereby enhancing transparency. the  AI-based  system for 3.2  Demanding Data Traceability Managers need to be aware of the implications of using different data sources and processing methods, especially when they seek to achieve fair and transparent systems. Data traceability is gaining increasing attention as managers become aware of its importance. For  example,  it  usually  takes  Walmart  6  days  and  14  hours  to identify the source of a farm product. When the supply chain data are maintained in a blockchain, however, it takes only 2.2 seconds to  establish  complete  data  traceability.  Therefore,  platform Data Provenance for Responsible AI  ACM Transactions on Management Information Systems providers  need  to  enhance  the  traceability  characteristic  of  data provenance  in  order  to  improve  the  efficiency  of  business  and decision making.   and their processing [17] before they are used by the AI black box. A user can ascertain whether the data used to train the system are suitable and relevant [36]. Enhanced  traceability  provides  more  information  about  the historicity of data and increases overall transparency. Transparency enables  the  creation  of  an  intermediate  representation  of  the original data [4] encoding the responsible AI objectives, such as fairness. As a result, organizations mitigate biases resulting from data sources and improve the fairness of their systems. Demanding data  traceability  may  include  guiding  data  acquisition  and leveraging blockchain technology. Guiding data acquisition. Many current AI-based systems rely on manually  labeled  data.  Despite  the  recent  trend  of  increasingly using  automated  labeling  practices,  manual  labeling  is  still indispensable. Manual labeling either applies to the entire dataset or only a subset of datapoints for later extrapolation. Either way, if organizations  do  not  have  a  clear  understanding  of  the  various experiences, goals, and perspectives of the people annotating the data, they cannot account for the significant impact on data quality [51].  Organizations  should  develop  procurement  guidelines  that take  the  traceability  of  data  into  consideration.  For  example, managers need to demand transparency regarding data origin and quality  when  acquiring  external  training  datasets.  A  data provenance  record  identifies  the  true  source  and  subsequent processing of data, uncovering the often-hidden history of the data. Recent  end-to-end  provenance  projects  have  developed  a  set  of tools, such as R packages, that allow organizations to establish data provenance through enhanced data traceability [24].  Furthermore, some data used to train the system may not have been labeled  by  experts,  whereas  other  data  may  have  been  procured from data brokers (organizations that collect data for the purpose of reselling  them).  Understanding  the  sources  and  methods  used  to acquire  the  data  is  critical  to  ensure  that  they  are  ethically  and legally  collected  (e.g.,  with  informed  consent).  Demanding traceability  (e.g.,  through  a  data  provenance  record)  increases transparency  and  helps  organizations  identify  the  accountable actors for mitigating risks related to the use of AI-based systems\u2019 recommendations.  For  instance,  an  organization  should  provide  the  descriptive statistics  of  a  dataset  as  part  of  its  data  provenance  records, allowing  users  to  identify  the  potential  risk  for  discrimination. Based on these statistics, users can evaluate the AI-based system\u2019s future recommendations discrimination,  either  by  altering  the  input  data,  modifying  the algorithm, or changing the way in which predictions are made [4]. As a result, the user is likelier to perceive the recommendations of the AI-based system as fair.  to  correct,  mitigate,  and  avoid As  data  provenance  relates  to  a  record  of  the  data\u2019s  origins  and subsequent  processing  [9],  it  also  increases  transparency.  For example, data provenance is needed to develop a data information sheet  [31]  that  provides  details  on  the  most  important  variables influencing an AI-based system\u2019s recommendations. As such, data provenance provides users with basic information about the data  Benefitting from blockchain technology. Blockchain-based data provenance is a promising approach to enhance the traceability of data in responsible AI. Blockchains can record the meta-data and history  of  data  objects.  The  important  characteristics  of blockchains,  such  as  transparency  and  auditability,  enable  the security and traceability of the meta-data, which are crucial for data accountability. Data immutability in a blockchain also enhances the perceived  fairness  the  recommendations.  Various  data provenance architectures based on blockchain technology, such as ProvChain  [60]  and  LineageChain  [60],  have  been  proposed. Blockchain technology has also been leveraged to handle dark data [99], which are the data that organizations collect but fail to utilize for their value. As a secured distributed ledger, blockchain has the potential to upgrade the value of the data and provide more efficient and transparent results [70].  in Increased  transparency  supports  a  consumer-centric  strategy  that organizations increasingly follow. For example, in healthcare, the notion  of  patient-centered  care  refers  to  being  respectful  and responsive to individual patient needs, values, and preferences; this requires  health  IT  systems  to  prioritize  data  provenance  and  the transparency  of  patients\u2019  personal  health-related  data.  With increased  transparency,  patients  are  better  informed  and  are therefore  more  empowered  to  seek  clarification  on  diagnoses  or recommendations  [41].  This  interaction  improves  the  quality  of healthcare.  It  also  enhances  patients\u2019  confidence  in  the  care provided  and  hence  its  effectiveness.  Healthcare  organizations\u2019 attention  to  data  provenance  in  electronic  healthcare  records improves the transparency of their decisions and recommendations. 3.3  Leveraging Technological Advances for Data Provenance Given  the  opaque  nature  of  many  AI-based  systems,  data provenance  is  essential  for  understanding  AI-based  systems\u2019 recommendations  [74].  Recent  technological  advances  include explainable artificial intelligence (XAI) methods, GANs, and deep learning with advances in small data techniques. the  accuracy  and Deriving explanations. XAI methods, such as LIME, LORE, and Anchor [29], push the traditional boundaries imposed by trade-offs between  interpretability  of  AI  systems\u2019 recommendations.  More  recently,  XAI  solutions  have  allowed users  to  understand  the  most  important  features  that  lead  to  the outcomes,  make  changes  to  model  features,  and  customize  the model explanation [58].  Explainable  AI  methods  seek  to  increase  the  transparency  of  AI models,  but  little  attention  has  been  given  to  addressing  data opaqueness.  Data  provenance  provides  a  complementary perspective  toward  transparency  for  the  user  [6]  by  presenting information about the source and further processing of the data used to  feed  an  AI-based  system.  Data  provenance  helps  provide complementary information to the explanations provided by XAI ACM Transactions on Management Information Systems through  global  and systems. For example, expanding the data provenance concept to AI algorithms facilitates the documentation of the data processing performed  by  an  AI  algorithm  local explanations [22]. While a global explanation creates transparency regarding  the  model  used  to  make  all  recommendations  (e.g., answering the question of how the AI makes its recommendations for  all  patients),  a  local  explanation  provides  transparency  for  a specific recommendation (e.g., answering the question of why the AI makes a specific recommendation for a particular patient). For example,  through  explainable  AI,  healthcare  providers  and  their patients can better understand the important factors that lead to an algorithm\u2019s  recommendations  on  a  particular  diagnosis  or treatment,  thereby  enhancing  the  accountability  of  the  parties responsible  for  and  receiving  care.  Therefore,  we  suggest  that organizations  should  strive  the  most  of  recent technological advances related to XAI.  to  make In  particular,  we  suggest  that  organizations  should  leverage existing  XAI  methods,  such  as  LIME  and  LORE,  and  XAI techniques,  such  as  layer-wise  relevance  propagation  [85]  and gradient-based  explanations,  with  supporting  architectural frameworks, such as CaSE [55], to provide easily understandable explanations  of  AI-based  recommendations.  XAI  methods,  for example,  derive  rules  that  explain  how  a  recommendation  was reached  by  presenting  cut-off  values  that  lead  to  the  predicted outcome or by identifying the factors that most strongly influence the  recommendation.  Such  explanations  help  users  better understand the AI system\u2019s behavior and identify new patterns in the data.  However,  prior  studies  also  suggest  a  potential  conflict  between explainability and other FATE dimensions. For example, a trade-off  exists  between  explainability  and  fairness  [56].  While explainability  seeks  to  simplify  the  complex  nature  of  AI-based systems  so  that  they  can  be  understood  by  humans,  there  is  an inherent  loss  associated  with  this  simplification  that  may  lead  to new biases. Organizations can manage these conflicts, for example, by using multi-criteria decision-making methods (see [89] for an overview)  to  guide  and  prioritize  different  characteristics.  In  a given  scenario,  one  characteristic  might  be  more  important  than another.  For  example,  if  the  adoption  and  use  of  the  system  are concerns,  explainability  could  be  one  way  of  increasing  the transparency of a system to increase trust [80]. In organizations that provide  a  process  for  users  to  participate  in  the  evolution  of  the system in order to address potential fairness concerns [42], users are less likely to reject the system. The lack of explainability of AI prediction outcomes can be caused not only by the black box nature of algorithms but also by the biases in  the  data.  While  most  research  focuses  on  algorithm explainability, we suggest paying additional attention to how data provenance  can  enhance  the  explainability  of  outcomes.  By allowing individuals to meaningfully interact with the system and by enhancing the explainability of AI-based systems, organizations facilitate  autonomous  decision  making,  detect  errors,  minimize biases, and thus safeguard justice [15].   in  managing  noisy  data Managing noisy data. The presence of meaningless and irrelevant data  is  often  referred  to  as  noise  within  the  data.  Scholars  have made  significant  progress  that organizations can benefit from. A distinction is made as to whether the  noise  relates  to  predictive  attributes  (referred  to  as  attributed noise) or to target attributes (referred to as class noise). Different techniques are available for identifying and handling noise within the data. A recent systematic review provides a good overview of the current state of the art on the problems caused by noisy data in AI-based systems [40]. The  management  of  noisy  data  is  important  for  deriving  fair recommendations.  In  fact,  striving  to  achieve  fairness  without addressing  the  noise  within  a  given  dataset  could  backfire.  For example,  a  prior  study  investigated  the  use  of  noise  models  for denoising data during subset selection [65]. Scholars applied noise models to select a subset of data from an existing larger data set. The  goal  was  to  generate  a  fair  dataset  so  that  the  sub-dataset accounts for race while having noisy race data. The study points out that failing to account for noise has unintended side effects, as it decreases the fairness of the resulting subset selection.  Different techniques are available to handle noise within data [40]. For example, organizations can use filtering techniques to identify and remove noise, or they can alter the data, sometimes referred to as data polishing. They key difference between responding to class noise  and  to  attribute  noise  is  that  for  class  noise,  organizations should  also  consider  relabeling,  whereas  for  attribute  noise, organizations can use data imputation.  thereby  mitigating A related technique is the use of GANs (sets of neural networks that seek to generate new data with similar characteristics as the training data).  Organizations  should  use  GANs  to  identify  possible adversarial  manipulations,  negative consequences.  For  example,  GANs  are  used  in  image-to-image translations, such as the translation of low-dose CT scans that have noise  in  the  data  into  regular-dose  CT  scans.  In  this  case,  a generator network translates the low-dose scan into a regular-dose scan, whereas a discriminator tries to distinguish the artificial from real  regular-dose  scans.  As  a  result,  the  noise  in  image-to-image translation is reduced [96].  Identifying  inherent  data  structures.  Deep  learning  for  text, audio, and video recognition often involves performing a pre-text task to find an inherent structure in the data of their AI systems. The  pre-text  task  is  self-supervised  learning  with  the  purpose  of generating a useful feature representation for the downstream task [12]. Pre-text tasks may force ML models to deconstruct data in order to enhance explainability [23]. For example, the Facebook AI Research group uses a combination of clustering and training based on rotated images to improve the quantity of unlabeled data used in their image classifier. After this pre-text task processing, the second stage  of  training  uses  conventional  labeled  data  to  create interpretable results [23]. Furthermore, advances in small data techniques help organizations improve  the  performance  of  AI-based  systems.  While  many  AI-based systems rely on large data, some of the most valuable datasets Data Provenance for Responsible AI  ACM Transactions on Management Information Systems are  only  available  in  small  quantities  [51].  For  example,  the application of AI in the medical domain often requires data labeling by  medical  professionals,  such  as  radiologists  or  physicians.  A review by a radiologist is needed to reliably label an image scan with the correct diagnosis of the presence or absence of lung cancer. As  medical  professionals\u2019  time  is  scarce  and  expensive,  and  the task of data labeling is quite repetitive, the creation of large datasets is  a  challenge.  However,  it  is  this  high-quality  human  input  that facilitates high-quality recommendations by AI-based systems.  Overall, a clearer understanding of the system\u2019s behavior and the data helps judge the fairness of recommendations. This is important because,  for  example,  evidence-based  medicine  rests  on  high standards of explainability, as medical decision making requires a sound  understanding  of  underlying  disease  mechanisms  and treatments  under  particular  conditions  [88].  The  lack  of  this understanding undermines the implementation of AI in healthcare [81].  This  issue  is  crucial  because  of  the  promising  benefits provided by AI in healthcare.  Figure 1 \u2013 Data provenance framework for responsible artificial intelligence 4   Exemplar Application of the Data Provenance Framework We  discuss  the  application  of  our  framework  with  two  recent examples  that  highlight  the  problems  associated  with  a  lack  of responsible AI. A  recent  example  of  data  provenance  concerns  relates  to  the application of AI recommendations in healthcare. A recent study evaluated  the  performance  of  the  AI-based  system  that  is embedded  within  EPIC  [100],  a  major  electronic  healthcare records  system,  to  predict  sepsis  (a  potentially  life-threatening condition in which the body\u2019s response to an infection damages its own  tissues).  As  sepsis  is  the  number  one  killer  in  US  hospitals [67], hospitals attach great importance to identifying and treating conditions that may lead to sepsis. There is widespread adoption of sepsis  prediction  models,  such  as  the  one  provided  by  EPIC. However, the study suggests that i) the AI-based system does not deliver the advertised performance, ii) important assumptions that  underly  the  AI  system  require  careful  examination,  and  iii)  the system\u2019s high number of false positives contribute to alert fatigue for the medical staff [100].  This  case  highlights  four  important  biases:  data  repurposing, population bias, transfer learning, and data shifts. One important observation  of  the  evaluation  was  that  the  data  used  in  the development of the model may have been repurposed. To derive the  predictions,  EPIC  measured  positive  sepsis  cases  based  on billing  codes  but  not  on  the  clinical  definition  of  sepsis.  The decision to use billing codes also results in population bias, as the presence  of  sepsis  relies  on  the  identification  of  sepsis  by  the medical  staff.  Yet,  the  medical  staff  used  the  system  with  the expectation  that  it  would  help  predict  sepsis  before  medical personnel  could  identify  it.  In  response  to  the  study,  EPIC  has argued  that  transfer  learning  could  explain  the  suboptimal performance. That is, transfer learning works only when the source task is closely related to the new task, so the sepsis prediction model developed using the data from one environment may not work well ACM Transactions on Management Information Systems in  other  environments.  Transfer  learning  may  have  introduced biases,  negatively  affecting  the  performance  of  the  sepsis prediction  model  using  data  from  the  University  of  Michigan Hospital [94] in contrast to data from the University of Colorado Hospital [10]. Lastly, the researchers also describe the potential for a dataset shift resulting from changed practices in treating sepsis and suggest the need to retire old models entirely. We suggest that organizations using prediction models such as this should establish organizational  governance,  conduct  data  audits,  and  leverage technological advances in the area of XAI to derive explanations for these prediction models. Organizational data auditing capability establishes data provenance through data governance [44], whereas data auditing refers to the process that assesses the fit of the data for a specific purpose. A data audit would allow healthcare organizations to evaluate the data used to train the AI system and identify possible concerns. In our example,  a  data  audit  would  allow  a  medical  expert  to  identify potential errors resulting from the use of billing codes as a proxy for  the  presence  of  a  disease.  Yet,  billing  codes  are  used  in  the administrative process and can deviate from the medical diagnosis (e.g., [93]). When used in research, billing codes are often a means to identify patients for another study in order to narrow down those who are likely to have a specific disease or condition (e.g., [86]).  An  organization\u2019s  capability  to  audit  AI  systems  has  become increasingly more pressing, as a recent study suggests a severe lack of transparency by AI system providers and a lack of oversight by the  FDA  [67].  Medical  experts  criticized  the  opaqueness  and limited  transparency  offered  by  EPIC.  As  the  AI  system  is protected  by  intellectual  property  rights,  the  developer  has disclosed  very  limited  information  about  the  development  of  the prediction  model.  Medical  professionals  implicitly  relied  on  the FDA\u2019s  oversight,  but  the  recent  study  points  out  that  the  FDA\u2019s oversight is limited [101]. Medical devices are rated by the FDA into three classes [16], with the highest class being reserved for life support systems. Those systems that make autonomous decisions (e.g., a pacemaker or an automated insulin pump) are required to meet the highest standards set by the FDA. AI-based systems that provide  recommendations  to  healthcare  providers  (e.g.,  a  sepsis prediction model) are often considered class II systems that have much  lower  FDA  oversight.  In  the  EPIC  example,  the  study suggests  that  not  even  the  reduced  oversight  was  applied,  as  the system  may  have  been  checked  upon  market  launch,  but  later additions are not subject to further FDA approval.  Recent  technological  advances  help  organizations  identify  the needed  adjustments.  For  example,  explainable  AI  helps  provide insights and feedback to AI developers so that they can then further refine  the  AI  system  by  adjusting  the  network  architecture  or retraining the model. This concept is often referred to as human-in-the-loop and has been advocated by scholars for the debiasing of AI  systems  [47].  Here,  the  technological  advances  in  XAI  can enhance data provenance by supporting feedback through human-in-the-loop and, in turn, improve the transparency of the predictive model. For example, a medical expert could question the validity of  the  model  for  the  early  prediction  of  sepsis,  while  the  most  important  prediction  factor  of  the  trained  model  is,  in  fact,  the diagnosis of sepsis by medical staff (i.e., labels of the training data). The concerns described are not limited to the healthcare domain. Another example is the Amazon AI recruitment tool, which has received  attention  for  its  lack  of  adherence  to  the  facets  of responsible  AI  (e.g.,  [46]).  Amazon  developed  an  experimental hiring  system  that  was  designed  to  automatically  screen  the resumes of job applicants and identify the top candidates. Amazon later  realized  that  the  AI  system  did  not  select  candidates  for technical  jobs  in  a  gender-neutral  way  but  was  rather  biased negatively toward female candidates. In hindsight, the explanation for  this  behavior  seems  obvious.  It  was  reported  that  among Amazon\u2019s  entry-  and  mid-level  corporate  employees,  women accounted for 31% of the workforce last year [52]. The system had been  trained  with  data  for  the  past  10  years,  during  which  male candidates  were  predominantly  chosen  for  jobs. Meanwhile,  many  high-technology  companies  have  realized  the gender discrepancy when hiring employees for tech jobs and have changed their hiring practices to recruit more women. In such cases, the data shift would require the developers of AI systems to discard older data and rely on more recent data to train their models.  technical Amazon used its own recruitment data from the past 10 years in training  the  system.  An  auditing  process  would  have  helped enhance data provenance and thus uncover the presence of a dataset shift and population bias. Specifically, it would have highlighted that the hiring practices followed during the past 10 years have been significantly unfair to female candidates [54]. Further adjustments are  necessary  to  ensure  responsible  AI  recommendations.  Thus, data  auditing  can  help  increase  the  fairness  of  a  system  by establishing data provenance. In a similar vein, the human-in-the-loop that has been advocated for  debiasing  HR  recruitments  systems  [47]  helps  organizations evaluate the AI system. Technological advances in XAI enhance data  provenance  by  supporting  feedback  through  human-in-the-loop  and,  in  turn,  help  mitigate  the  negative  impact  of  a  dataset shift.  XAI  enhances  the  explainability  of  responsible  AI  through data provenance. 5  Research Agenda Organizations  continue  adopting  and  using  AI-based  systems  to support evidence-based decision making. A particular focus is on enhancing  the  FATE  of  the  implemented  AI-based systems.  Our review of data-induced biases and discussion of how organizations can  mitigate  these  by  establishing  data  provenance  within  their organizations  for three  central organizations.  Yet,  more  research  is  needed  to  improve  data provenance methods, tools, and practices for responsible AI. Thus, we develop recommendations for future research, identifying four central topics (see Table 4).  recommendations lead  to Data Provenance for Responsible AI  ACM Transactions on Management Information Systems Table 1 \u2013 Exemplar research questions for responsible AI Research topic  Exemplar future research question Conceptual Clarity  How can we classify central terms related to data provenance and responsible AI?  the  relationship  between research.  For  example,  regarding transparency  and  explainability,  some  scholars  suggest  that explainability enhances the transparency of systems [82], whereas others  argue  is  a  sub-characteristic  of transparency  [83].  More  research  is  needed  to  develop  a nomological network around data provenance for responsible AI.  that  explainability Resolving Tradeoffs AI ethics  What  are  the  relationships  between  AI explainability and AI interpretability? What are the relationships among FATE and what  are  the  boundary  conditions  for  the impact of date provenance on  the FATE of responsible AI? What are the existing tradeoffs or conflicts among the goals of responsible AI, and how can we resolve them? How  do  different  organizational  profiles affect  the  design  of  responsible  AI  in organizations?  What are the regional differences in moral and  legal  concerns  that  impact  responsible AI?  How  do  we  ensure  responsible  AI  with increasing role of AI in the future of work?  How  do  we  develop  and scalable, responsible AI solutions? implement Designing responsible AI  What  are principles for responsible AI systems? the  design  guidelines  and How do we design explainability to enhance interpretability, and what are the influential conditions? Conceptual clarity. Establishing a clear nomological network to better  understand  the  distinction  of  terms  and  their  relations  is crucial for the development of data provenance for responsible AI. More research is needed to determine the unique nature of different concepts  and  possibly  the  interchangeability  of  some  concepts. Scholars  can  use  taxonomy  development  methods  to  identify classifications with mutually exclusive and collectively exhaustive dimensions.  For  example,  explainability  and  interpretability  are essentially  two  related  but  different  concepts  but  often  are  used interchangeably; terms such as data lineage and data pedigree are closely related to data provenance, but they are distinct terms. With enhanced  conceptual  clarity,  more  research  can  be  conducted  to understand  the  relationships  between  ontologically  different concepts. Understanding the conditions in which these relationships occur is also  important.  For  example,  having  a  fair  dataset  or  fair recommendations  high transparency. This can help explain conflicting evidence in existing necessarily  guarantee does  not  tradeoffs.  Implementing  data  provenance Resolving  for responsible  AI  can  lead  to  tradeoffs  or  conflicts.  For  example, regulations, such as the GDPR, require the system to ensure data privacy,  whereas  other  requirements  demand  more  traceability, such  as  auditing  requirements.  The  case  of  Twitter\u2019s  cropping algorithm shows a conflict in speed and consistency versus the risk of  making  incorrect  predictions  [95].  Furthermore,  the  trade-off between  accuracy  and  interpretability  is  an  often-mentioned conflict related to responsible AI [90]. More research is needed to identify  these  conflicts  and  develop  corresponding  resolutions. Researchers  can  benefit,  for  example,  from  specific  research methods,  such  as  conjoint  analysis  [37]  and  analytic  hierarchy process  (AHP)  approach  [79],  in  order  to  prioritize  different characteristics  of or characteristics in different context.   configurations important identify In  order  to  resolve  these  conflicts,  we  suggest  two  important avenues. First, scholars may benefit from research on multi-criteria decision  making.  Prior  research  can  guide  managers  in  making decision while accounting for multiple and potentially conflicting goals. These require extension and evaluation for responsible AI before  they  can  be  used  to  derive  normative  recommendations. Second,  organizational  or  AI  project  profiles  may  be  created  to provide templates for  developing responsible AI projects. While prioritization  may  be  the  result  of  external  forces,  such  as governmental  regulations,  they  may  also  be  the  result  of organizational  values  and  culture.  For  example,  an  open  and progressive organization may prioritize transparency and fairness over  accountability  concerns.  risk-adverse organization  may  focus  on  accountability  and  performance  over transparency.  Similarly,  different  projects  within  an  organization may need to emphasize different aspects of FATE. Future research could  explore  the  role  of  organizational  and  AI  project  specific profiles in the development and use of responsible AI systems. In  contrast,  a AI ethics. Questions related to the fairness of responsible AI are often at the cross-section of research focused on novel technology and its ethical behavior [64]. Research related to ethics is closely associated with moral and legal questions. Legal research is often conducted at the national level according to the local needs of the judiciary  system.  By  contrast,  new  technical  challenges  emerge during the development and deployment of responsible AI-based systems regardless of local needs. For example, responsible AI has the potential for solutions that are easily scalable from a technical perspective  yet  raise  concerns  when  it  comes  to  local  legal requirements, such as the GDPR.  Prior  research  also  coined  the  term  responsibility  gap  [49], describing a situation in which artificial agents are used to decide on  a  course  of  actions  or  in  which  they  act  themselves  without ACM Transactions on Management Information Systems human  involvement.  As  the  rules  by  which  they  act  are  not inscribed during development, there is no individual who assumes responsibility for the machine\u2019s actions. Current ethical and legal frameworks have not been designed for these situations, leading to a responsibility gap [63]. In addition to mitigating or eliminating the  responsibility  gap,  organizations  must  often  follow  multiple goals,  such  as  transparency  and  accountability  [21],  in  the development of responsible AI systems. However,  how  governmental  regulations  that  organizations  must follow  map  toward  different  goals  of  responsible  AI  remains unclear. For example, future research should investigate whether and how we need to extend and modify regulations, such as HIPAA in the US and the GDPR in the EU, to allow platform providers to offer scalable yet responsible AI solutions. Designing  responsible  AI.  Designing  responsible  AI  provides  a particular challenge for future research, as it requires us to instill human and social values into the AI system in a way that users see and appreciate it [21]. However, current research often focuses on the  technical  implementations  of  FATE.  For  example,  much research  related  to  explainable  AI  offers  technical  solutions  for developing explanations. When an explanation is presented to the user, an interpretative process is triggered. The user will develop an autonomous interpretation of the explanation, a process that is often  described  as  the  interpretability  of  an  explanation.  This interpretation  may  or  may  not  be  in  line  with  the  expected interpretation intended by the system\u2019s designer.  Therefore,  more  research  is  needed  to  better  understand  the  link between  different  design  patterns  and  technological  solutions related  to  explainability  research  and  the  interpretability  of individual users. For example, certain user or task characteristics influence the interpretability of a user in the sense that an expert, compared  with  a  novice,  requires  different  explanations.  We suggest  that  data  provenance  requires  also  more  attentions, particularly  in  the  XAI  community,  as  it  provides  important complementary information that are crucial for the interpretation by the user. Future research could develop clear guidelines, design features,  and  design  principles  for  designing  responsible  AI systems,  6  Conclusion Data  provenance  is  important  to  mitigate  biases  and  improve responsible  AI-based  systems  (see  Figure  1).  Existing  practices view  data  provenance  as  a  mandate  of  directives,  laws,  and regulations designed to ensure the control and protection of data integrity,  confidentiality,  and  availability.  Data  provenance  is viewed as the cost of staying compliant with these requirements. Such practices result from a lack of organizational commitment to developing responsible AI-based systems.  By contrast, our recommended practices view data provenance as an  important  component  of  developing  responsible  AI-based systems.  Organizations  that  are  strategically  committed  to  their FATE  goals  are  likely  to  achieve  long-term  improvements  in organizational performance. Our recommended practices view data  provenance as an investment necessary to meet their FATE goals and recognize that the loss of data provenance at any point in the provenance  chain  leads  to  a  loss  of  data  provenance  in  all subsequent  parts.  Therefore,  organizations  need  to  recognize  the importance of establishing a comprehensive provenance for critical data that serve as inputs to AI systems.  In contemporary systems development projects, such as in the case of data-driven development and AI engineering, data repurposing is becoming more and more the norm. Recommended practices will help organizations benefit significantly from data provenance, as the data provenance established for one project is likely to benefit several  other  projects  that  use  the  same  data.  Therefore,  when examining the costs and benefits of data provenance, organizations need to take a comprehensive view that spans across projects, as different projects often draw from the same data sources. Whereas existing  practices  view  data  provenance  records  as  static, recommended  practices  recognize  the  need  to  maintain  dynamic data provenance information that is updated throughout the data\u2019s lifecycle.  We have outlined the multiple benefits of data provenance along and beyond the FATE characteristics. However, organizations will need  to  prioritize  their  investments  in  data  provenance  efforts based,  for  instance,  on  the  magnitude  of  benefits  resulting  from achieving FATE and the severity of negative consequences or the cost of failure that result from not achieving FATE. Organizations that view data provenance as an overhead cost are likely to neglect it when operating under budget or schedule constraints and, even worse,  perhaps  engage  in  undesirable  practices,  such  as  virtue washing [92].  Investments  in  data  provenance  should  be  driven  by  an  intrinsic motivation to improve the responsibility of AI-based systems. For example,  adopting  data  provenance  practices  to  achieve transparency  is  valuable  because  it  enables  users  to  understand, engage  with,  and  audit  the  AI-based  system  and  its  outcomes. Similarly, data provenance that enables accountability is a means to  ensure  justice  by  clarifying  responsibility  and  avoiding  harm from  deterrence  these  examples  show,  FATE characteristics are instrumental in upholding the intrinsic values of core principles, such as human autonomy and justice. In addition, organizations  that  take  a  lifecycle  perspective  recognize  that  the costs incurred in the early phases of data acquisition and processing lead to benefits later in the AI-based system lifecycle. Yet, these benefits,  such  as  increasing  reputation,  avoiding  the  loss  of reputation, and establishing the desired FATE characteristics, are often  difficult  to  quantify  despite  quickly  outweighing  negative implications.  [15].  As In  high-reliability  organizations,  such  as  healthcare  providers, suboptimal  decisions  can  have  severe  consequences.  The increasing  reliance  on  AI-based  systems  and  the  lack  of understanding  of  the  data  used  to  generate  recommendations highlight  the  importance  of  data  provenance.  Establishing  data provenance guidelines and policies can facilitate the FATE of AI-based  recommendations.  For  example,  in  the  context  of  the Data Provenance for Responsible AI  ACM Transactions on Management Information Systems COVID-19  pandemic,  the  provenance  of  data  is  important  for discerning  the  FATE  of  recommendations  made  by  AI-based systems that rely on data from varied and disparate data sources. While  more  guidelines  are  needed  to  develop  data  provenance throughout  the  entire  data  lifecycle  [11],  implementing  the recommended practices is an urgent task for organizations that aim to harness the benefits of AI-based systems. Our recommendations will  help  organizations  enhance  essential  data  provenance capabilities toward fair, transparent, accountable, and explainable evidence-based decision making by responsible AI-based systems. Our proposed research agenda suggests potential research avenues related to data provenance. We suggest that achieving conceptual clarity,  resolving  tradeoffs,  observing  AI  ethics,  and  designing responsible  AI  require  more  research  by  scholars  from  different disciplines. REFERENCES [1]  ACM  U.S.  Technology  Policy  Committee.  2020. Statement  on  Principles  and  Prerequisites  for  the  Development, Evaluation and Use of Unbiased Facial Recognition Technologies. from Retrieved https://www.acm.org/binaries/content/assets/public-policy/ustpc-facial-recognition-tech-statement.pdf August  2021 24, [2]  Amina  Adadi  and  Mohammed  Berrada.  2018.  Peeking Inside  the  Black-Box:  A  Survey  on  Explainable  Artificial Intelligence  (XAI).  IEEE  Access  6,  (2018),  52138\u201352160. DOI:https://doi.org/10.1109/ACCESS.2018.2870052 [3]  Gediminas  Adomavicius,  Jesse  Bockstedt,  Shawn Curley,  and  Jingjng  Zhang.  2019.  Reducing  Recommender Systems Biases: An Investigation of Rating Display Designs. MIS Quarterly 43, 4 (February 2019), 18\u201319. [4]  Gediminas  Adomavicius  and  Mochen  Yang.  2019. Integrating  Behavioral,  Economic,  and  Technical  Insights  to Address  Algorithmic  Bias:  Challenges  and  Opportunities  for  IS Research.  (2019). DOI:https://doi.org/10.2139/ssrn.3446944 Journal SSRN Alan  Alexander,  Megan  McGill,  Anna  Tarasova,  Cara [5] Ferreira,  and  Delphine  Zurkiya.  2019.  Scanning  the  Future  of Medical Imaging. Journal of the American College of Radiology 16,  501\u2013507. 2019), DOI:https://doi.org/10.1016/j.jacr.2018.09.050 (April 4 [6]  Ilkay  Altintas,  Oscar  Barney,  and  Efrat  Jaeger-Frank. 2006.  Provenance  Collection  Support  in  the  Kepler  Scientific Workflow System. In Provenance and Annotation of Data (Lecture Notes  in  Computer  Science),  Springer,  Berlin,  Heidelberg,  118\u2013132. DOI:https://doi.org/10.1007/11890850_14 [7]  Anand  Murali.  2019.  How  India\u2019s  data  labellers  are powering  the  global  AI  race.  FactorDaily.  Retrieved  August  24, 2021  from  https://archive.factordaily.com/indian-data-labellers-powering-the-global-ai-race/  Marcus  A.  Badgeley,  John  R.  Zech,  Luke  Oakden-[8] Rayner,  Benjamin  S.  Glicksberg,  Manway  Liu,  William  Gale, Michael V. McConnell, Bethany Percha, Thomas M. Snyder, and Joel  T.  Dudley.  2019.  Deep  learning  predicts  hip  fracture  using confounding patient and healthcare variables. npj Digit. Med. 2, 1 (December  2019),  31.  DOI:https://doi.org/10.1038/s41746-019-0105-1 [9]  Khalid  Belhajjame,  Reza  B\u2019Far,  James  Cheney,  Sam Coppens,  Stephen  Cresswell,  Yolanda  Gil,  Paul  Groth,  Graham Klyne, Timothy Lebo, Jim McCusker, Simon Miles, James Myers, Satya Sahoo, and Curt Tilmes. 2013. PROV-DM: The PROV Data Model. (2013). Tellen Bennett, Seth Russell, James King, Lisa Schilling, [10] Chan Voong, Nancy Rogers, Bonnie Adrian, Nicholas Bruce, and Debashis  Ghosh.  2019.  Accuracy  of  the  Epic  Sepsis  Prediction Model  in  a  Regional  Health  System.  arXiv:1902.07276  [cs,  stat] from (February  2019).  Retrieved  September  5,  2021 http://arxiv.org/abs/1902.07276 [11]  Francine Berman, Rob Rutenbar, Brent Hailpern, Henrik Christensen,  Susan  Davidson,  Deborah  Estrin,  Michael  Franklin, Margaret  Martonosi,  Padma  Raghavan,  Victoria  Stodden,  and Alexander S. Szalay. 2018. Realizing the potential of data science. Commun.  67\u201372. 4 DOI:https://doi.org/10.1145/3188721 (March  2018), ACM  61, Donald J. Berndt, James A. McCart, Dezon K. Finch, and [12] Stephen  L.  Luther.  2015.  A  Case  Study  of  Data  Quality  in  Text Mining Clinical Progress Notes. ACM Trans. Manage. Inf. Syst. 6, 1 (April 2015), 1\u201321. DOI:https://doi.org/10.1145/2669368 [13] \u2013 the foundation of data quality. 8. Peter Buneman and Susan B Davidson. Data provenance [14]  Peter Buneman, Sanjeev Khanna, and Tan Wang-Chiew. 2001. Why and Where: A Characterization of Data Provenance. In Database Theory \u2014 ICDT 2001, Jan Van den Bussche and Victor Vianu (eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 316\u2013330. DOI:https://doi.org/10.1007/3-540-44503-X_20 [15]  Cansu  Canca.  2020.  Operationalizing  AI  ethics principles.  Commun.  ACM  63,  12  (November  2020),  18\u201321. DOI:https://doi.org/10.1145/3430368 [16]  Center  for  Devices  and  Radiological  Health.  2020. Classify Your Medical Device. FDA. Retrieved September 9, 2021 https://www.fda.gov/medical-devices/overview-device-from regulation/classify-your-medical-device [17]  James Cheney, Laura Chiticariu, and Wang-Chiew Tan. 2007. Provenance in Databases: Why, How, and Where. FNT in Databases  379\u2013474. 1, DOI:https://doi.org/10.1561/1900000006 (2007), 4 [18]  Coalition  for  Critical  Technology.  2020.  Abolish  the #TechToPrisonPipeline. Medium. Retrieved August 24, 2021 from https://medium.com/@CoalitionForCriticalTechnology/abolish-the-techtoprisonpipeline-9b5b14366b16 ACM Transactions on Management Information Systems Enrico  Coiera.  2019.  The  Last  Mile:  Where  Artificial [19] Intelligence Meets Reality. J Med Internet Res 21, 11 (November 2019), e16323. DOI:https://doi.org/10.2196/16323 [20]  Alexander D\u2019Amour, Katherine Heller, Dan Moldovan, Ben  Adlam,  Babak  Alipanahi,  Alex  Beutel,  Christina  Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari,  Neil  Houlsby,  Shaobo  Hou,  Ghassen  Jerfel,  Alan Karthikesalingam,  Mario  Lucic,  Yian  Ma,  Cory  McLean,  Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim  Ramasamy,  Rory  Sayres,  Jessica  Schrouff,  Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov,  Xuezhi  Wang,  Kellie  Webster,  Steve  Yadlowsky, Taedong  Yun,  Xiaohua  Zhai,  and  D.  Sculley.  2020. Underspecification Presents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395 [cs, stat] (November 2020). Retrieved August 24, 2021 from http://arxiv.org/abs/2011.03395 [21] Intelligence: Designing AI for Human Values. 1 (2017), 9. Virginia  Dignum.  2017.  Responsible  Artificial  Yan Gao and Yan Cui. 2020. Deep transfer learning for [29] reducing  health  care  disparities  arising  from  biomedical  data inequality.  Nat  Commun  11,  1  (December  2020),  5131. DOI:https://doi.org/10.1038/s41467-020-18918-3 Salvador Garc\u00eda, Juli\u00e1n Luengo, and Francisco Herrera. [30] 2016.  Tutorial  on  practical  tips  of  the  most  influential  data preprocessing  algorithms  in  data  mining.  Knowledge-Based Systems  1\u201329. (April DOI:https://doi.org/10.1016/j.knosys.2015.12.006 2016), 98, [31]  Timnit  Gebru,  Jamie  Morgenstern,  Briana  Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. 2020. Datasheets for Datasets. arXiv:1803.09010 from (March  2020).  Retrieved  August  24,  2021 [cs] http://arxiv.org/abs/1803.09010 J\u00f6rn-Henrik Robert  Geirhos, [32]  Jacobsen,  Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix  A.  Wichmann.  2020.  Shortcut  learning  in  deep  neural networks.  Nat  Mach  Intell  2,  11  (November  2020),  665\u2013673. DOI:https://doi.org/10.1038/s42256-020-00257-z [22]  Mengnan  Du,  Ninghao  Liu,  and  Xia  Hu.  2019. Techniques for interpretable machine learning. Commun. ACM 63, 1 (December 2019), 68\u201377. DOI:https://doi.org/10.1145/3359786  [33]  Gerard  George,  Martine  R.  Haas,  and  Alex  Pentland. 2014.  Big  Data  and  Management.  Academy  of  Management Journal (April 2014). DOI:https://doi.org/10.5465/amj.2014.4002 [23] Communications  of DOI:https://doi.org/10.1145/3392496 Chris  Edwards.  2020.  Leveraging  unlabeled  data. (2020),  13\u201314. the  ACM  63,  6 [24]  Aaron M. Ellison, Emery R. Boose, Barbara S. Lerner, Elizabeth  Fong,  and  Margo  Seltzer.  2020.  The  End-to-End Provenance  Project.  Patterns  1,  2  (May  2020),  100016. DOI:https://doi.org/10.1016/j.patter.2020.100016 [25]  Jessica  Fjeld,  Nele  Achten,  Hannah  Hilligoss,  Adam Nagy,  and  Madhulika  Srikumar.  2020.  Principled  Artificial Intelligence:  Mapping  Consensus  in  Ethical  and  Rights-Based to  Principles  for  AI.  SSRN  Journal  (2020). Approaches DOI:https://doi.org/10.2139/ssrn.3518482 [26]  Maayan  Frid-Adar,  Eyal  Klang,  Michal  Amitai,  Jacob Goldberger,  and  Hayit  Greenspan.  2018.  Synthetic  data augmentation using GAN for improved liver lesion classification. In  2018  IEEE  15th  International  Symposium  on  Biomedical Imaging  IEEE,  Washington,  DC,  289\u2013293. DOI:https://doi.org/10.1109/ISBI.2018.8363576 (ISBI  2018), Sorelle A. Friedler and Christo Wilson. 2018. Conference [27] on Fairness, Accountability and Transparency. In Proceedings of Machine  Learning  Research,  PMLR,  1\u20132.  Retrieved  August  24, 2021 from https://proceedings.mlr.press/v81/friedler18a.html [28]  Runshan  Fu,  Yan  Huang,  and  Param  Vir  Singh.  2020. Artificial  Intelligence  and  Algorithmic  Bias:  Source,  Detection, Mitigation,  and  Implications.  Tutorials  in  Operations  Research (November  39\u201363. DOI:https://doi.org/10.1287/educ.2020.0215 2020),  [34]  Lise  Getoor.  2019.  Responsible  Data  Science.  In Proceedings of the 2019 International Conference on Management 1\u20131. Amsterdam of DOI:https://doi.org/10.1145/3299869.3314117 Netherlands, ACM, Data, [35]  Milena  A.  Gianfrancesco,  Suzanne  Tamang,  Jinoos Yazdany,  and  Gabriela  Schmajuk.  2018.  Potential  Biases  in Machine  Learning  Algorithms  Using  Electronic  Health  Record Data.  JAMA  Intern  Med  178,  11  (November  2018),  1544. DOI:https://doi.org/10.1001/jamainternmed.2018.3763 Justin  Scott  Giboney,  Susan  A.  Brown,  Paul  Benjamin [36] Lowry,  and  Jay  F.  Nunamaker.  2015.  User  acceptance  of knowledge-based  recommendations:  Explanations, arguments, and fit. Decision Support Systems 72, (April 2015), 1\u201310. DOI:https://doi.org/10.1016/j.dss.2015.02.005 system [37]  Paul  E.  Green,  Abba  M.  Krieger,  and  Yoram  (Jerry) Wind.  2001.  Thirty  Years  of  Conjoint  Analysis:  Reflections  and Prospects. Interfaces 31, 3 (2001), S56\u2013S73. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, [38] Franco  Turini,  Fosca  Giannotti,  and  Dino  Pedreschi.  2018.  A Survey  of  Methods  for  Explaining  Black  Box  Models.  ACM Comput.  2018). 5 Surv. DOI:https://doi.org/10.1145/3236009  (August 51, [39]  David  Gunning,  Mark  Stefik,  Jaesik  Choi,  Timothy Miller,  Simone  Stumpf,  and  Guang-Zhong  Yang.  2019.  XAI\u2014Explainable  artificial  intelligence.  Sci.  Robot.  4,  37  (December 2019), eaay7120. DOI:https://doi.org/10.1126/scirobotics.aay7120 [40]  Shivani Gupta and Atul Gupta. 2019. Dealing with Noise Problem  in  Machine  Learning  Data-sets:  A  Systematic  Review. Data Provenance for Responsible AI  ACM Transactions on Management Information Systems Procedia  Computer  Science  161,  (January  2019),  466\u2013474. DOI:https://doi.org/10.1016/j.procs.2019.11.146 [41]  Jianxing He, Sally L. Baxter, Jie Xu, Jiming Xu, Xingtao Zhou,  and  Kang  Zhang.  2019.  The  practical  implementation  of artificial  intelligence  technologies  in  medicine.  Nat  Med  25,  1 (January  2019),  30\u201336.  DOI:https://doi.org/10.1038/s41591-018-0307-0 [42]  Jun  He  and  William  R.  King.  2008.  The  Role  of  User Participation  in  Information  Systems  Development:  Implications from  a  Meta-Analysis.  Journal  of  Management  Information Systems  301\u2013331. 2008), 1 DOI:https://doi.org/10.2753/MIS0742-1222250111 (July 25, [43]  Andreas  Holzinger,  Georg  Langs,  Helmut  Denk,  Kurt Zatloukal, and Heimo M\u00fcller. 2019. Causability and explainability of  artificial  intelligence  in  medicine.  WIREs  Data  Mining  and Knowledge  e1312. 9, DOI:https://doi.org/10.1002/widm.1312 Discovery  (2019), 4 [44]  Marijn  Janssen,  Paul  Brous,  Elsa  Estevez,  Luis  S. Barbosa,  and  Tomasz  Janowski.  2020.  Data  governance: Organizing  data  Intelligence. Government  Information  Quarterly  37,  3  (July  2020),  101493. DOI:https://doi.org/10.1016/j.giq.2020.101493 trustworthy  Artificial for [45] Quality: A Review of the DWQ Project. 14. Matthias  Jarke  and  Yannis  Vassiliou.  Data  Warehouse [46]  Jeffrey Dastin. 2018. Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. Retrieved August 25,  2021  from  https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G [47]  Jeremy Hsu. 2020. AI Recruiting Tools Aim to Reduce Bias in the Hiring Process. IEEE Spectrum. Retrieved August 25, 2021 from https://spectrum.ieee.org/ai-tools-bias-hiring Anna  Jobin,  Marcello  Ienca,  and  Effy  Vayena.  2019. [48] Artificial  Intelligence:  the  global  landscape  of  ethics  guidelines. Nat  Mach  (September  2019),  389\u2013399. DOI:https://doi.org/10.1038/s42256-019-0088-2 Intell  1,  9 Deborah G. Johnson. 2015. Technology with No Human [49] Responsibility?  J  Bus  Ethics  127,  4  (April  2015),  707\u2013715. DOI:https://doi.org/10.1007/s10551-014-2180-1 Jonathan Vanian. 2021. This hot startup is now valued at [50] $1 billion for its A.I. skills. Fortune. Retrieved September 9, 2021 from  https://fortune.com/2021/08/09/snorkel-ai-funding-data-labeling-startup/ Gerald  C.  Kane.  2011.  A  multimethod  study  of [51] information  quality  in  wiki  collaboration.  ACM  Trans.  Manage. Inf.  1\u201316. DOI:https://doi.org/10.1145/1929916.1929920 (March  2011), Syst.  2,  1 Katherine Anne Long. New Amazon data shows Black, [52] Latino  and  female  employees  are  underrepresented  in  best-paid jobs.  The  Seattle  Times.  Retrieved  August  25,  2021  from  https://www.seattletimes.com/business/amazon/new-amazon-data-shows-black-latino-and-female-employees-are-underrepresented-in-best-paid-jobs/ [53]  Christopher  J.  Kelly,  Alan  Karthikesalingam,  Mustafa Suleyman, Greg Corrado, and Dominic King. 2019. Key challenges for delivering clinical impact with artificial intelligence. BMC Med 17, 1 (December 2019), 195. DOI:https://doi.org/10.1186/s12916-019-1426-2 Derek Khanna. 2013. We Need More Women in Tech: [54] The Data Prove It. The Atlantic. Retrieved September 5, 2021 from https://www.theatlantic.com/technology/archive/2013/10/we-need-more-women-in-tech-the-data-prove-it/280964/ [55]  Sebastian  Kiefer.  2021.  CaSE:  Explaining  Text Classifications by Fusion of Local Surrogate Explanation Models with Contextual and Semantic Knowledge. Information Fusion 77, (2021), 184\u2013195. DOI:https://doi.org/10.1016/j.inffus.2021.07.014 [56]  Jon  Kleinberg  and  Sendhil  Mullainathan.  2019. Simplicity Creates Inequity: Implications for Fairness, Stereotypes, and  Interpretability.  arXiv:1809.04578  [cs,  stat]  (June  2019). Retrieved September 9, 2021 from http://arxiv.org/abs/1809.04578 [57]  Robert W. Kolb (Ed.). 2010. Lessons from the financial crisis: causes, consequences, and our economic future. John Wiley & Sons, Hoboken, N.J. Himabindu  Lakkaraju,  Ece  Kamar,  Rich  Caruana,  and [58] Jure Leskovec. 2019. Faithful and Customizable Explanations of Black  Box  Models.  In  Proceedings  of  the  2019  AAAI/ACM Conference on AI, Ethics, and Society, ACM, Honolulu HI USA, 131\u2013138. DOI:https://doi.org/10.1145/3306618.3314229 [59]  Alexandra  L\u2019Heureux,  Katarina  Grolinger,  Hany  F. Elyamany,  and  Miriam  A.  M.  Capretz.  2017.  Machine  Learning With  Big  Data:  Challenges  and  Approaches.  IEEE  Access  5, 7776\u20137797. (2017), DOI:https://doi.org/10.1109/ACCESS.2017.2696365 [60]  Xueping  Liang,  Sachin  Shetty,  Deepak  Tosh,  Charles Kamhoua, Kevin Kwiat, and Laurent Njilla. 2017. ProvChain: A in  Cloud Blockchain-Based  Data  Provenance  Architecture Environment  with  Enhanced  Privacy  and  Availability.  IEEE, Madrid,  468\u2013477. Spain, DOI:https://doi.org/10.1109/CCGRID.2017.8 Christian Lovis. 2019. Unlocking the Power of Artificial [61] Intelligence and Big Data in Medicine. J Med Internet Res 21, 11 (November 2019), e16607. DOI:https://doi.org/10.2196/16607 (2020). [62]  Thomas  Macaulay.  2020.  Flawed  Algorithm  Used  to Determine  U.K.  Welfare  Payments  Is  \u201cPushing  People  Into Poverty.\u201d  from https://thenextweb.com/neural/2020/09/29/flawed-algorithm-used-to-determine-uk-welfare-payments-is-pushing-people-into-poverty/ used-to-determine-u-k-welfare-payments-is-pushing-people-into-poverty  http://cacm.acm.org/news/247831-flawed-algorithm-Retrieved ACM Transactions on Management Information Systems Andreas  Matthias.  2004.  The  responsibility  gap: [63] Ascribing responsibility for the actions of learning automata. Ethics Inf  175\u2013183. DOI:https://doi.org/10.1007/s10676-004-3422-1 Technol  (2004), 6,  3 John  A.  McDermid,  Yan  Jia,  Zoe  Porter,  and  Ibrahim [64] Habli. 2021. Artificial intelligence explainability: the technical and ethical  dimensions.  Phil.  Trans.  R.  Soc.  A.  379,  2207  (October 2021), 20200363. DOI:https://doi.org/10.1098/rsta.2020.0363 Anay Mehrotra and L. Elisa Celis. 2021. Mitigating Bias [65] in Set Selection with Noisy Protected Attributes. In Proceedings of the  2021  ACM  Conference  on  Fairness,  Accountability,  and Transparency (FAccT \u201921), Association for Computing Machinery, 237\u2013248. USA, New DOI:https://doi.org/10.1145/3442188.3445887 York,  NY, [66]  Xiao-Li Meng. 2018. Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election. Ann. Appl. Stat. 12, 2 (June 2018). DOI:https://doi.org/10.1214/18-AOAS1161SF [67]  Michael C. Ksiazek. Sepsis Accounts for 1 in 5 Deaths, Leading Cause of Death in Hospitals. The National Law Review. from Retrieved https://www.natlawreview.com/article/sepsis-accounts-1-5-deaths-leading-cause-death-hospitals September  2021 5, [68]  Kiran-Kumar  Muniswamy-Reddy,  David  Holland,  Uri Braun,  and  Margo  Seltzer.  2006.  Provenance-Aware  Storage Systems.  [69]  Shivaramakrishnan  Narayan,  Martin  Gagn\u00e9,  and Reihaneh  Safavi-Naini.  2010.  Privacy  preserving  EHR  system using  attribute-based  infrastructure.  In  Proceedings  of  the  2010 ACM workshop on Cloud computing security workshop - CCSW \u201910,  ACM  47. Chicago, DOI:https://doi.org/10.1145/1866835.1866845 Illinois,  USA, Press, [70]  Neha and Payal Pahwa. 2020. Dark Data Analytics Using Blockchain  Technology.  In  Advances  in  Data  Sciences,  Security and  Applications  (Lecture  Notes  in  Electrical  Engineering), Springer,  Singapore,  467\u2013474.  DOI:https://doi.org/10.1007/978-981-15-0372-6_38 [71]  B.  Nestor,  Matthew  B.  A.  McDermott,  Geeticka Chauhan, Tristan Naumann, Michael C. Hughes, A. Goldenberg, and  M.  Ghassemi.  2018.  Rethinking  clinical  prediction:  Why machine  learning  must  consider  year  of  care  and  feature aggregation. ArXiv (2018). [72] Nature DOI:https://doi.org/10.1038/506150a Regina Nuzzo. 2014. Scientific method: Statistical errors. 150\u2013152. (February  2014), 7487 506, [73]  Dino  Pedreschi,  Fosca  Giannotti,  Riccardo  Guidotti, Anna  Monreale,  Salvatore  Ruggieri,  and  Franco  Turini.  2019. Meaningful  Explanations  of  Black  Box  AI  Decision  Systems. AAAI  9780\u20139784. DOI:https://doi.org/10.1609/aaai.v33i01.33019780 2019), (July 33,  Yi Qu, Haitao Wu, Ting Liu, and Yue Zhao. 2018. Space [74] Mission  Data  Provenance  Traceability.  In  2018  SpaceOps Conference, American Institute of Aeronautics and Astronautics, Marseille, France. DOI:https://doi.org/10.2514/6.2018-2482 B. Ramesh and M. Jarke. 2001. Toward reference models [75] for  requirements  traceability.  IIEEE  Trans.  Software  Eng.  27,  1 (January 2001), 58\u201393. DOI:https://doi.org/10.1109/32.895989 [76]  Alexander  J.  Ratner,  Stephen  H.  Bach,  Henry  R. Ehrenberg,  and  Chris  R\u00e9.  2017.  Snorkel:  Fast  Training  Set Generation for Information Extraction. In Proceedings of the 2017 ACM International Conference on Management of Data (SIGMOD \u201917), Association for Computing Machinery, New York, NY, USA, 1683\u20131686. DOI:https://doi.org/10.1145/3035918.3056442 [77]  Russell Jurney. Hand labeling is the past. The future is #NoLabel  AI.  KDnuggets.  Retrieved  September  5,  2021  from https://www.kdnuggets.com/hand-labeling-is-the-past-the-future-is-nolabel-ai.html/ [78]  Daniel Russo, Paolo Ciancarini, Tommaso Falasconi, and Massimo Tomasi. 2018. A Meta-Model for Information Systems Quality:  A  Mixed  Study  of  the  Financial  Sector.  ACM  Trans. 1\u201338. 3 Inf.  Syst. Manage. DOI:https://doi.org/10.1145/3230713 (November  2018), 9, [79]  Thomas L. Saaty. 1988. What is the Analytic Hierarchy Process? In Mathematical Models for Decision Support, Gautam Mitra, Harvey J. Greenberg, Freerk A. Lootsma, Marcel J. Rijkaert and  Hans  J.  Zimmermann  (eds.).  Springer  Berlin  Heidelberg, Berlin,  Heidelberg,  109\u2013121.  DOI:https://doi.org/10.1007/978-3-642-83555-1_5 [80]  Philipp Schmidt, Felix Biessmann, and Timm Teubner. 2020.  Transparency  and  trust  in  artificial  intelligence  systems. Journal  of  Decision  Systems  29,  4  (October  2020),  260\u2013278. DOI:https://doi.org/10.1080/12460125.2020.1819094 [81]  James Shaw, Frank Rudzicz, Trevor Jamieson, and Avi Goldfarb.  2019.  Artificial  Intelligence  and  the  Implementation Challenge.  J  Med  Internet  Res  21,  7  (July  2019),  e13659. DOI:https://doi.org/10.2196/13659 [82]  Donghee  Shin.  2021.  The  effects  of  explainability  and causability  on  perception,  trust,  and  acceptance:  Implications  for explainable AI. International Journal of Human-Computer Studies 146,  102551. DOI:https://doi.org/10.1016/j.ijhcs.2020.102551 (February  2021), Donghee Shin and Yong Jin Park. 2019. Role of fairness, [83] accountability,  and  in  algorithmic  affordance. Computers  in  Human  Behavior  98,  (September  2019),  277\u2013284. DOI:https://doi.org/10.1016/j.chb.2019.04.019 transparency [84]  Paul  Slovic  and  Amos  Tversky.  1974.  Who  accepts Savage\u2019s  axiom?  Syst.  Res.  19,  6  (November  1974),  368\u2013373. DOI:https://doi.org/10.1002/bs.3830190603 [85]  Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, and Alexander Binder. 2022. Explain and improve: LRP-inference fine-Data Provenance for Responsible AI  ACM Transactions on Management Information Systems Agency. arXiv:2105.08667 [cs] (May 2021). Retrieved August 25, 2021 from http://arxiv.org/abs/2105.08667 [96]  Zhiqiang  Zheng,  Balaji  Padmanabhan,  and  Steven  O. Kimbrough.  2003.  On  the  Existence  and  Significance  of  Data Preprocessing Biases in Web-Usage Mining. INFORMS Journal on Computing  148\u2013170. DOI:https://doi.org/10.1287/ijoc.15.2.148.14449 2003), (May 15,  2 [97]  Liming Zhu, Xiwei Xu, Qinghua Lu, Guido Governatori, and  Jon  Whittle.  2021.  AI  and  Ethics  --  Operationalising Responsible  AI.  In  arXiv:2105.08867  [cs].  Retrieved  August  10, 2021 from http://arxiv.org/abs/2105.08867 2018. What is GDPR, the EU\u2019s new data protection law? [98] GDPR.eu. Retrieved August 25, 2021 from https://gdpr.eu/what-is-gdpr/ [99] 25, en/insights/financial-services/technology-advisory-dark-data Shining a light on dark data. Accenture. Retrieved August https://www.accenture.com/us-2021  from [100] Retrieved https://www.epic.com/epic/post/reducing-sepsis-mortality-epic Reducing  Sepsis  Mortality  by  One-Fifth  with  Epic. from 11, September  2021 tuning  for  image  captioning  models.  Information  Fusion  77, (January  233\u2013246. 2022), DOI:https://doi.org/10.1016/j.inffus.2021.07.008 [86]  Joel S. Tieder, Matthew Hall, Katherine A. Auger, Paul D. Hain, Karen E. Jerardi, Angela L. Myers, Suraiya S. Rahman, Derek  J.  Williams,  and  Samir  S.  Shah.  2011.  Accuracy  of Administrative  Billing  Codes  to  Detect  Urinary  Tract  Infection Hospitalizations.  Pediatrics  128,  2  (August  2011),  323\u2013330. DOI:https://doi.org/10.1542/peds.2010-2064 [87]  Erico  Tjoa  and  Cuntai  Guan.  2020.  A  Survey  on Explainable  Artificial  Intelligence  (XAI):  Toward  Medical  XAI. IEEE  Transactions  on  Neural  Networks  and  Learning  Systems (2020), 1\u201321. DOI:https://doi.org/10.1109/TNNLS.2020.3027314 [88]  Eric  J.  Topol.  2019.  High-performance  medicine:  the convergence  of  human  and  artificial  intelligence.  Nat  Med  25,  1 (January  2019),  44\u201356.  DOI:https://doi.org/10.1038/s41591-018-0300-7 [89] Analysis of Multi-Criteria Decision Making Methods.  Mark  Velasquez  and  Patrick  T.  Hester.  2013.  An [90] the  black  box  of  deep DOI:https://doi.org/10.1126/science.aan7059 Paul Voosen. 2017. How AI detectives are cracking open (July  2017). learning.  Science [91]  Zirui  Wang,  Zihang  Dai,  Barnabas  Poczos,  and  Jaime Carbonell. 2019. Characterizing and Avoiding Negative Transfer. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Long Beach, CA, USA, 11285\u201311294. DOI:https://doi.org/10.1109/CVPR.2019.01155 Maranke  Wieringa.  2020.  What  to  account  for  when [92] accounting  for  algorithms:  a  systematic  literature  review  on algorithmic accountability. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, ACM, Barcelona Spain, 1\u201318. DOI:https://doi.org/10.1145/3351095.3372833 [93]  Derek  J.  Williams,  Samir  S.  Shah,  Angela  Myers, Matthew  Hall,  Katherine  Auger,  Mary  Ann  Queen,  Karen  E. Jerardi, Lauren McClain, Catherine Wiggleton, and Joel S. Tieder. 2013.  Identifying  Pediatric  Community-Acquired  Pneumonia Hospitalizations:  Accuracy  of  Administrative  Billing  Codes. JAMA  Pediatrics  167,  9  (September  2013),  851\u2013858. DOI:https://doi.org/10.1001/jamapediatrics.2013.186 Andrew Wong, Erkin Otles, John P. Donnelly, Andrew [94] Krumm,  Jeffrey  McCullough,  Olivia  DeTroyer-Cooley,  Justin Pestrue, Marie Phillips, Judy Konye, Carleen Penoza, Muhammad Ghous,  and  Karandeep  Singh.  2021.  External  Validation  of  a Widely  Implemented  Proprietary  Sepsis  Prediction  Model  in Hospitalized  Patients.  JAMA  Internal  Medicine  181,  8  (August 2021),  1065\u20131070. DOI:https://doi.org/10.1001/jamainternmed.2021.2626 Kyra  Yee,  Uthaipon  Tantipongpipat,  and  Shubhanshu [95] Mishra. 2021. Image Cropping on Twitter: Fairness Metrics, their Limitations,  and  the  Importance  of  Representation,  Design,  and",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "b24a36dc-d4e6-49b6-9624-62d4e3441330",
                    "text": "Data provenance, a record that describes the origins and processing of data, offers new promises in the increasingly important role of artificial  intelligence  (AI)-based  systems  in  guiding  human decision making. To avoid disastrous outcomes that can result from bias-laden  AI  systems,  responsible  AI  builds  on  four  important characteristics:  transparency,  and explainability.  To  stimulate  further  research  on  data  provenance that enables responsible AI, this study outlines existing biases and discusses possible implementations of data provenance to mitigate them. We first review biases stemming from the data\u2019s origins and pre-processing. We then discuss the current state of practice, the challenges  it  presents,  and  corresponding  recommendations  to address  them.  We  present  a  summary  highlighting  how  our recommendations can help establish data provenance and thereby mitigate  biases  stemming  from  the  data\u2019s  origins  and  pre-processing to realize responsible AI-based systems. We conclude with a research agenda suggesting further research avenues.",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "545e7b2b-632e-4e97-9ad3-c750a0e25b94",
                    "text": "\u2022Data Provenance \u2022Artificial Intelligence",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "e6a5b487-8be4-4db3-970b-59bc0bc73ed5",
                    "text": "Data Provenance, Artificial Intelligence, Fairness, Accountability, Transparency, Explainability",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "182ba272-752c-4733-9f6a-123b3aa83c95",
                    "text": "As evidence-based decision making aided by data-driven artificial intelligence (AI) algorithms becomes increasingly common across all sectors of the economy, there is a growing concern among users about  whether  such  algorithms  are  developed  and  implemented responsibly. Prior reports have already provided a glimpse into the disastrous effects of inaccurate and bias-laden AI recommendations  in high-stakes applications, with examples from the healthcare and legal  domains,  such  as  incorrect  patient  treatment,  exacerbated poverty [62], wrongful arrest [33], and unjust criminal sentencing [43].  The  heightened  awareness  of  concerns  raised  in  recent movements for social justice has resulted in calls from professional associations [1] and researchers [18,34] for developing approaches that help establish responsible AI.  Rapid innovations in data-generating technologies, such as sensors, social media, and mobile devices, have exacerbated the problems resulting from poor data quality that threaten the development of responsible  AI  systems.  These  technologies  generate  an unprecedented  quantity  and  variety  of  data.  While  most applications  have  benefitted  from  explosive  growth  in  data availability (in terms of volume, variety, velocity, veracity, etc.), limited  attention  has  been  given  to  data  quality  [66],  in  turn undermining the quality of recommendations generated using such data. Motivated by these concerns, this study examines how data provenance can help improve data quality and enhance the fairness, accountability,  transparency,  and  explainability  (FATE)  of  AI-based  systems.  We  argue  that  data  provenance\u2014a  record  that describes the origins and processing of data [9]\u2014can help assess and  improve  the  FATE  of  recommendations  provided  by  AI algorithms and thus instill trust in them. Trust is enhanced by the capability to describe and follow the life of data (i.e., their origins, processing, and use) in both forward and backward directions [75]. The importance of provenance has long been recognized [14] in the pharmaceutical,  food,  and  fashion  industries.  It  helps  establish  a product\u2019s  origins  and  influences  consumers\u2019  decisions  about purchase and use.  Responsible AI is essentially related to a broad discourse, AI ethics, which  has  received  significant  attention  among  researchers  in recent years. Scholars have identified different high-level ethical principles  that  should  govern  the  development  of  AI  systems [25,48,97].  While  no  universal  consensus  exists,  fairness, accountability,  and  transparency  [48]  have  received  significant attention in this research community [27]. Simultaneously, research related to explainable AI has emerged [39], with recent discussions on  its  capability  to  bridge  the  gap  between  technical  and  ethical considerations [64]. AI explainability gives users and experts the ability  to  investigate  and  understand  the  inner  workings  of  AI, ACM Transactions on Management Information Systems allowing  them  to  identify  potential  biases.  Bridging  these  two perspectives, we focus on four important and related characteristics of responsible AI\u2014FATE. While there is ongoing research on other AI-based system characteristics, such as privacy and agency, we focus on how FATE can help organizations identify and mitigate the negative influences of biases within their data. We discuss how potential conflicts among different FATE characteristics emerge, how organizations can manage them, and where more research is needed. Most  current  researchers  and  practitioners  in  the  field  of responsible  AI  have  emphasized  the  quality  of  algorithms. However, an algorithm\u2019s recommendations or outputs also depend heavily on representations, structures, and data quality, which serve as  the  inputs.  In  this  study,  we  focus  on  data  provenance,  an important aspect of data quality, in the development of responsible AI systems [13]. For example, data provenance can help uncover data quality concerns related to labor-intensive data labeling, which is  often  performed  by  unqualified  workers  [7]  and  otherwise remains  concealed.  This  the recommendations  or  outputs  of  AI  algorithms  are  often  used  as inputs  for  other  AI  algorithms  [53],  further  exacerbating  the problem. For example, the classification of a radiology scan by an algorithm  as  benign  or  malignant  may  be  used  as  an  input  for another  algorithm  that  is  used  to  create  a  risk  score  for  patient is  particularly  alarming,  as Table 1 \u2013 Overview of FATE characteristics and examples  readmission. In such situations, data provenance can help identify the  causes  of  the  AI  algorithm\u2019s  poor  performance,  improve interpretability,  or  uncover  its  seemingly  acceptable that performance  was  achieved  for  invalid  reasons  (e.g.,  when identifying a malignant tumor, the system was learning from the circle made by the radiologist on the scan rather than the data from the scan itself). By  illuminating the origin and processing of the data  [14],  data  provenance  can  mitigate  these  shortcomings  and facilitate FATE assessments (see Table 1).  The  lack  of  data  provenance  is  a  serious  concern  in  AI-based systems  that  are  used  to  inform  critical  decisions.  While  the establishment of data provenance may increase short-term costs for organizations, it can provide long-term benefits by instilling trust in the implemented system and its recommendations. Specifically, our  study  addresses  the  following  question:  How  does  data provenance  affect  the  four  interrelated  characteristics  of responsible  AI:  fairness,  accountability,  transparency,  and explainability? The paper analyzes biases related to origins and pre-processing of data,  discusses  the  current  state  of  practice  and  attendant  challenges,  and  presents  recommendations  for  addressing  them. Our  recommendations  are  intended  to  help  establish  data provenance and mitigate biases stemming from the data's origins and pre-processing to realize responsible AI-based systems. Characteristic  Description  Example Fairness  AI-based  systems  may  introduce  discrimination because  of  imbalanced  data  [4].  The  data  used  in training  AI-based  the discriminations existing in our society, which, in turn, lead to algorithmic bias [4]. systems  often  reflect  Training the system using only medical records from male patients can lead to discrimination against female patients. Accountability  Because  of  the  increasing  complexity  of  AI-based systems,  it  is  difficult  for  a  user  to  judge  who  is accountable for the results [49]. The individual services provided  by  AI  algorithms  are  integrated  into  larger systems  [19],  further  exacerbating  opaqueness  and ambiguity about ownership.  When  an  AI-based  system  trained  on  photos  depicting cancer on the epidermis (outer skin layer) is integrated into a larger system, it may also be inappropriately used on data from  subcutaneous  tissue  (inner  skin  layer).  It  becomes unclear  who  is  accountable  for  the  resulting  incorrect recommendation.  Transparency  An often-cited limitation of AI-based systems is their black  box  nature  [2].  However,  to  understand  the training  data quality  of adequacy, we need transparency. recommendations  and  The  pharma  industry  has  well-established  practices  for providing  easy  access  to  relevant  information  about  drugs (either in the product package itself or in the accompanying documents),  whereas  AI  systems  seldom  provide  relevant information  in  developing recommendations.   the  data  used about Explainability  A lack of explainability of AI prediction outcomes can be caused by the black box nature of algorithms, which can lead to negligence of the inaccuracies and biases in data.  Yet,  understanding  a  prediction  is  an  important aspect of their acceptance [84].  Evidence-based  medicine  rests  on  high  standards  of explainability  of  both  algorithms  and  data,  as  medical decision  making  requires  a  sound  understanding  of  the underlying  disease  mechanisms  and  treatments  [88].  The lack of this understanding undermines the implementation of AI in healthcare [81].  Data Provenance for Responsible AI  ACM Transactions on Management Information Systems In the following sections, we review key biases, such as systematic distortions [3], resulting from the failure to adopt appropriate data provenance  practices  in  the  development  and  implementation  of AI-based systems. We also provide three key recommendations for establishing  data  provenance  to  enhance  the  FATE  of  AI-based systems. We propose a data provenance framework for responsible AI  and  discuss  exemplary  cases  for  its  application.  Before concluding, we present future research directions.",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "6ff38e3f-9c4b-41a0-a92b-70a589fe94bb",
                    "text": "In contrast to the majority of existing research, which has focused on biases resulting from algorithms (e.g., [28,35]), we concentrate on the origins of the data and the data pre-processing rather than on the algorithm that uses the data as inputs. Data sources are often where the original data were collected to train and build AI-based systems.  After  data  collection,  data  pre-processing  [30],  which commonly  integration,  cleaning, normalization, and transformation, can also introduce biases [96]. We identify five categories of potential biases that may originate from  data  sources  and  five  categories  of  biases  that  may  be introduced  during  data  pre-processing.  For  example,  the  data themselves might be subject to bias in the ways in which they are sampled or measured. Each bias has different implications for the FATE characteristics of AI-based systems. includes  data  preparation, Table 2 \u2013 Summary of the effect of data biases on responsible AI  Population data Measurement error Data quality chasm Data repurposing Data augmentation Dataset shifts Opaque pre-processing Data labeling Adversarial manipulations  X X X X  X X X X X X  X X X X X  X  X X X X X X X X X Transfer learning  X  X  X",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "e8863259-1690-4f5c-96b9-d39e7be50035",
                    "text": "Below, we identify five key instances in which biases arise in the data  sources:  population  data,  measurement  error,  data  quality chasm, data repurposing, and data augmentation. We describe their implications regarding the FATE characteristics (see Table 2 for a summary).   Population data. In every data science project, sampling the right data  to  ensure  representativeness  is  important  [33].  However,  to develop  and  implement  powerful  AI-based  systems,  developers often  rely  on  access  to  unique  data.  For  example,  data  provided through  projects,  such  as  BigMedilytics,  comprise  the  medical records of more than 11 million patients from eight countries. The retraining or recalibration of AI-based systems developed with such unique  data  to  other  contexts  for  the  same  purpose  requires additional data that are representative of the new context.  However,  AI-based  systems  are  often  applied  in  new  contexts without  retraining  or  recalibration  because  of  the  significant challenges involved in collecting the necessary additional data. For example,  when  an  algorithm  is  trained  with  data  from  one population but is used to develop predictions on another population, any  differences  in  the  frequency  and  nature  of  events  in  these datasets  will  result  in  poor  performance  [19].  When  the  data collection mechanisms impose selection bias or  fail to recognize the mismatch between the training data and the target population, the  transparency  of  the  data\u2019s  origins  is  affected.  In  addition, spurious correlations and shortcut learning (i.e., decision rules that work  well  based  on  the  training  data  because  of  spurious phenomena [32]) of the AI system will lead to unreliable and unfair recommendations [20] that will undermine possible explanations.  Measurement  error.  Every  study  and  every  measurement instrument, however well designed, still generates some errors [72]. Many AI applications in domains such as medicine or business rely heavily on Bayesian statistics, as the results are always subject to probabilities.  Data  pre-processing  and  the  use  of  another algorithm\u2019s  predictions  as  an  input  could  further  compound  this issue  because  of  the  propagation  of  uncertainties  or  prior probabilities [61].  However,  in  AI  systems,  the  uncertainty  of  the  input  variables resulting either from the measurement itself or from pre-processing is  often  neglected.  An  AI-based  system  trained  with  such  data without a particular focus on and caution about potential errors can result in a poorly performing model. Consequently, the precision of an  AI-based  system  might  be  overestimated,  as  the  AI  system learns to fit against the error. The resulting recommendations would be  at  least  distorted  if  not  incorrect,  leading  to  problematic outcomes.  If  the  system  provides  corresponding  explanations,  a user can identify these inadequacies and correct them [19].  Data  quality  chasm.  Another  challenge  is  the  lack  of  data  with adequate quality in settings where the AI system is used [61]. While the data may look homogeneous at the surface level, a more careful evaluation  can  suggest  otherwise.  For  example,  an  AI  algorithm may  achieve  superior  prediction  quality  because  of  its  access  to state-of-the-art computed tomography (CT) scans. If CT scans from older  equipment  that  generates  lower-quality  scans  are  used  to retrain the AI-based system, the recommendations are likely to be inaccurate.  ACM Transactions on Management Information Systems In  contrast  to  the  measurement  error,  in  which  the  system  has learned to predict based on errors, here, the AI-based system was trained using fine granular data that are no longer available later, thus  resulting  in  poorer  performance.  This  provides  multiple challenges along the FATE characteristics. The poor performance can  lead  to  suboptimal  recommendations,  and  depending  on  the level  of  transparency  provided  initially,  questions  related  to accountability between the system developer and system provider can  arise.  Creating  transparency  regarding  the  training  data\u2019s origins and the data used for the recommendations helps mitigate this issue. Data repurposing. In addition to biases resulting from sampling, data  collection  practices  also  introduce  misuse  and  biases. Traditional  data  collection  practices  differ  significantly  from contemporary  practices  in  AI  systems  development  [33].  The traditional  practice  is  to  collect  data  for  a  specific  purpose.  For example,  a  clinical  trial  of  a  drug  used  to  treat  COVID-19  will collect experimental data to assess the drug\u2019s side effects.  However, repurposing data is the norm in AI-based systems. For example,  a  blood  test  result  in  a  patient\u2019s  electronic  healthcare record that has been captured to diagnose a certain disease may also be used by an AI-based system to diagnose other diseases. This can be a potential issue compromising the accountability characteristic of  the  algorithm.  For  example,  while  the  quality  of  data  from medical images can be sufficient for the original purpose, such as stroke detection, it may not meet the needs of subsequent data uses, such as finding new disease markers [5]. Repurposing data creates ambiguity  about  the  data  and  their  origins, making it difficult to clearly identify the person or entity accountable for any incorrect recommendations. Data  augmentation.  When  the  available  dataset  is  not  large enough for the intended computations, data augmentation might be used  (i.e.,  increasing  the  size  of  the  dataset  with  synthetically generated data or slightly modified copies of the existing data, for example, through translation, rotation, flip, or scale). For instance, augmented data are generated through the rotation, translation, and scaling  of  a  prior  dataset  on  liver  lesions  [26]  when  training  a generative  adversarial  network  (GAN).  These  modifications  and the synthetically generated data can amplify existing biases within the dataset and mask the inadequacies of the collected data.  Some AI algorithms rely solely on simulated data. For example, AI systems have been developed to design bridges and control robot arms using only simulation data [23]. Simulations can create useful data to learn from, especially when little input and manually labeled data are available. However, because deep learning can approach problems more intuitively by focusing on patterns in the core data, researchers have suggested that AI systems perform better without synthetic additions to the data [23].  Therefore, data augmentation and the use of simulation data bring about  new  challenges  the  fairness  and  accountability characteristics  of  AI  algorithms.  Data  augmentation  amplifies existing  biases  and  creates  opaqueness  about  the  actual representativeness  of  the  data,  thus  limiting  transparency  and to  making  it  more  challenging  to  identify  the  cause  of  an  incorrect recommendation.",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "ea4b0f3d-b17a-465c-9b44-4e87115aeacf",
                    "text": "Data processing is vulnerable to errors that introduce biases, such as dataset shifts, opaque pre-processing, data labeling, adversarial manipulation, transfer learning, and data augmentation. Dataset shifts. An easily ignorable fact is the non-stationary nature of the environment and the population from which all the input data of AI-based systems are generated [59]. For example, when a data shift occurs, an important predictor of a specific disease at one point in time can be more or less important at a later point in time because of  improvements  in  the  quality  of  care  available.  For  instance, many predictions using the Medical Information Mart for Intensive Care  dataset  are  confounded  by  changes  in  hospital  operation practices [71].  Considering  time  as  an  influential  variable  shows dataset shifts caused by changing practices, which, in turn, result in significant changes in the observed data. Unless this data shift is identified and the AI algorithm is retrained or recalibrated [53], the performance  of  the  system  deteriorates,  affecting  the  fairness, transparency,  and  explainability  characteristics  of  the  algorithm. Low  performance  can  lead  to  incorrect  recommendations  that negatively affect users. If the data\u2019s origins and subsequent changes in  the  environment  are  not  made  transparent,  the  derived explanations will be at least distorted. Opaque pre-processing. AI-based systems are often characterized as black boxes [2]. While some AI-based systems provide accurate predictions, the rationale behind their predictions remains opaque. In  algorithms  with  intrinsic  obscurity,  such  as  deep  neural networks,  understanding  the  specific  patterns  being  learned  is difficult [53]. For example, in a study detecting hip fractures, an algorithm  was  confounded  by  the  scanner  model  and  by  scans marked as \u201curgent\u201d [8]. Therefore, assessing the potential biases introduced  when  using  the  output  of  an  opaque  algorithm  as  an input  for  another  AI-based  system  is  difficult.  Opaque  pre-processing limits the transparency and explainability of AI-based system recommendations. If it is unclear what data were used to train  the  system,  confounding  indicators  are  more  difficult  to identify and assess, and they do not allow users to learn relevant insights. However, deriving explanations for the recommendations can  help  experts  validate  the  model  and  its  recommendations. Different types of explanations (e.g., feature extraction, pre-defined models, and sensitivity [87]) can help an expert evaluate, improve, and correct the model. Data labeling. While data quality chasm refers to data that may appear to be similar but have different qualities, another issue arises with data labeling, as the identification and development of labels are  often  not  transparent.  Data  labeling  is  related  to  supervised learning, such as medical image classification. The outcome labels are  used  by  supervised  algorithms  in  the  training  stage.  While automated  labeling  (e.g.,  with  weak supervision)  are  on  the  rise  [50,76],  labeling  is  often  a  labor-intensive task and is frequently performed by unqualified or poorly trained  ghost  workers  or  through  crowd-based  platforms  [7]. techniques  for  data Data Provenance for Responsible AI  ACM Transactions on Management Information Systems Incorrect  labels  in  the  training  data  create  erroneous  or  unfair recommendations  and  explanations  developed  by  AI-based systems because of the inherent bias embedded in the training data. This  bias  affects  the  fairness,  transparency,  and  explainability characteristics  of  the  AI  algorithm.  Fairness  is  affected,  as unqualified or poorly trained ghost workers will make mistakes and possibly  bring  their  social  biases  into  the  data.  As  these  are undesirable  business  practices,  organizations  seldom  disclose them,  thereby  negatively  affecting  transparency.  While  these business  practices  introduce  biases,  hiding  them  from  customers makes it difficult for both the user and the expert to benefit from explanations.  As  the  majority  of  existing  data  are  non-labeled  and  are  usually very expensive to label, some researchers perceive the reliance on labeled  data  as  even  counterproductive  to  the  development  of effective AI [23]. A recent trend in the automatic labeling of data using AI [77] has emerged. The idea is simple. As labeling is often a bottleneck task in AI system development, we could use machine learning (ML) to extrapolate the labels. A labeling ML algorithm can  be  trained  based  on  a  limited  number  of  available  or  easily attainable  labels  and  can  then  be  used  to  label  a  larger  dataset. While this reduces the effort of manual labor, it may also increase the severity of biases already existing in the smaller sample, leading to erroneous or unfair recommendations and explanations. Adversarial  manipulation.  As  AI-based  systems  derive  their models based on nuanced variations in the data, sometimes, small changes in the data input can lead to significant differences in the output  [38].  Therefore,  AI-based  systems  are  potentially susceptible  to  adversarial  manipulation.  For  instance,  images  of benign moles may be misdiagnosed as malignant because of added adversarial  noise  or  seemingly  minor  changes  in  the  data  [53]. These manipulations can be intentional, such as when an attacker changes the input of an algorithm to fool it, or unintentional, such as  when  a  user  accidentally  rotates  an  image  used  as  an  input. the  data  preprocessing, Without  sufficient  transparency  of  identifying this potential threat in an otherwise effective model is difficult. These seemingly minor changes can result in significantly different  outcomes  that  make  explaining  the  recommendations difficult and the recommendation itself possibly incorrect. Transfer learning. Once an AI-based system is built, we may use the algorithm to solve similar problems. In particular, a new AI-based system benefits from the information learned from another system. For example, a pre-trained model can be used to encode radiographic  features  in  images  before  final  re-training  [8]  to improve the sample efficiency for a reinforcement learning agent. Transfer learning can also improve AI system performance when predicting cancer for ethnic groups with limited data availability [29]. However, transfer learning only works when the source task is closely related to the new task. If not, transfer learning introduces biases and negatively affects performance [91]. As transfer learning also  system\u2019s recommendations,  it  impedes  clear  accountability.  Therefore, transfer  learning  should  be  made  transparent  to  the  user,  as  it otherwise adds to the system\u2019s opaqueness. increases  ambiguity  about  the  AI-based",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "b5e8435f-b268-4d4f-be77-cb73bc5d2a58",
                    "text": "",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "344b2af4-4d5a-4fd9-a1cd-0c9dc260c064",
                    "text": "for Responsible AI  ACM Transactions on Management Information Systems Establishing Data Provenance for Responsible Artificial Intelligence Systems  Karl Werder   Cologne Institute for Information Systems  University of Cologne  Germany  werder@wiso.uni-koeln.de  Balasubramaniam Ramesh  Computer Information Systems  Georgia State University  USA bramesh@gsu.edu  Rongen (Sophia) Zhang  Computer Information Systems  Georgia State University  USA  rzhang6@gsu.edu fairness,  accountability, ABSTRACT Data provenance, a record that describes the origins and processing of data, offers new promises in the increasingly important role of artificial  intelligence  (AI)-based  systems  in  guiding  human decision making. To avoid disastrous outcomes that can result from bias-laden  AI  systems,  responsible  AI  builds  on  four  important characteristics:  transparency,  and explainability.  To  stimulate  further  research  on  data  provenance that enables responsible AI, this study outlines existing biases and discusses possible implementations of data provenance to mitigate them. We first review biases stemming from the data\u2019s origins and pre-processing. We then discuss the current state of practice, the challenges  it  presents,  and  corresponding  recommendations  to address  them.  We  present  a  summary  highlighting  how  our recommendations can help establish data provenance and thereby mitigate  biases  stemming  from  the  data\u2019s  origins  and  pre-processing to realize responsible AI-based systems. We conclude with a research agenda suggesting further research avenues. CCS CONCEPTS \u2022Data Provenance \u2022Artificial Intelligence KEYWORDS Data Provenance, Artificial Intelligence, Fairness, Accountability, Transparency, Explainability 1  Introduction As evidence-based decision making aided by data-driven artificial intelligence (AI) algorithms becomes increasingly common across all sectors of the economy, there is a growing concern among users about  whether  such  algorithms  are  developed  and  implemented responsibly. Prior reports have already provided a glimpse into the disastrous effects of inaccurate and bias-laden AI recommendations  in high-stakes applications, with examples from the healthcare and legal  domains,  such  as  incorrect  patient  treatment,  exacerbated poverty [62], wrongful arrest [33], and unjust criminal sentencing [43].  The  heightened  awareness  of  concerns  raised  in  recent movements for social justice has resulted in calls from professional associations [1] and researchers [18,34] for developing approaches that help establish responsible AI.  Rapid innovations in data-generating technologies, such as sensors, social media, and mobile devices, have exacerbated the problems resulting from poor data quality that threaten the development of responsible  AI  systems.  These  technologies  generate  an unprecedented  quantity  and  variety  of  data.  While  most applications  have  benefitted  from  explosive  growth  in  data availability (in terms of volume, variety, velocity, veracity, etc.), limited  attention  has  been  given  to  data  quality  [66],  in  turn undermining the quality of recommendations generated using such data. Motivated by these concerns, this study examines how data provenance can help improve data quality and enhance the fairness, accountability,  transparency,  and  explainability  (FATE)  of  AI-based  systems.  We  argue  that  data  provenance\u2014a  record  that describes the origins and processing of data [9]\u2014can help assess and  improve  the  FATE  of  recommendations  provided  by  AI algorithms and thus instill trust in them. Trust is enhanced by the capability to describe and follow the life of data (i.e., their origins, processing, and use) in both forward and backward directions [75]. The importance of provenance has long been recognized [14] in the pharmaceutical,  food,  and  fashion  industries.  It  helps  establish  a product\u2019s  origins  and  influences  consumers\u2019  decisions  about purchase and use.  Responsible AI is essentially related to a broad discourse, AI ethics, which  has  received  significant  attention  among  researchers  in recent years. Scholars have identified different high-level ethical principles  that  should  govern  the  development  of  AI  systems [25,48,97].  While  no  universal  consensus  exists,  fairness, accountability,  and  transparency  [48]  have  received  significant attention in this research community [27]. Simultaneously, research related to explainable AI has emerged [39], with recent discussions on  its  capability  to  bridge  the  gap  between  technical  and  ethical considerations [64]. AI explainability gives users and experts the ability  to  investigate  and  understand  the  inner  workings  of  AI, ACM Transactions on Management Information Systems allowing  them  to  identify  potential  biases.  Bridging  these  two perspectives, we focus on four important and related characteristics of responsible AI\u2014FATE. While there is ongoing research on other AI-based system characteristics, such as privacy and agency, we focus on how FATE can help organizations identify and mitigate the negative influences of biases within their data. We discuss how potential conflicts among different FATE characteristics emerge, how organizations can manage them, and where more research is needed. Most  current  researchers  and  practitioners  in  the  field  of responsible  AI  have  emphasized  the  quality  of  algorithms. However, an algorithm\u2019s recommendations or outputs also depend heavily on representations, structures, and data quality, which serve as  the  inputs.  In  this  study,  we  focus  on  data  provenance,  an important aspect of data quality, in the development of responsible AI systems [13]. For example, data provenance can help uncover data quality concerns related to labor-intensive data labeling, which is  often  performed  by  unqualified  workers  [7]  and  otherwise remains  concealed.  This  the recommendations  or  outputs  of  AI  algorithms  are  often  used  as inputs  for  other  AI  algorithms  [53],  further  exacerbating  the problem. For example, the classification of a radiology scan by an algorithm  as  benign  or  malignant  may  be  used  as  an  input  for another  algorithm  that  is  used  to  create  a  risk  score  for  patient is  particularly  alarming,  as Table 1 \u2013 Overview of FATE characteristics and examples  readmission. In such situations, data provenance can help identify the  causes  of  the  AI  algorithm\u2019s  poor  performance,  improve interpretability,  or  uncover  its  seemingly  acceptable that performance  was  achieved  for  invalid  reasons  (e.g.,  when identifying a malignant tumor, the system was learning from the circle made by the radiologist on the scan rather than the data from the scan itself). By  illuminating the origin and processing of the data  [14],  data  provenance  can  mitigate  these  shortcomings  and facilitate FATE assessments (see Table 1).  The  lack  of  data  provenance  is  a  serious  concern  in  AI-based systems  that  are  used  to  inform  critical  decisions.  While  the establishment of data provenance may increase short-term costs for organizations, it can provide long-term benefits by instilling trust in the implemented system and its recommendations. Specifically, our  study  addresses  the  following  question:  How  does  data provenance  affect  the  four  interrelated  characteristics  of responsible  AI:  fairness,  accountability,  transparency,  and explainability? The paper analyzes biases related to origins and pre-processing of data,  discusses  the  current  state  of  practice  and  attendant  challenges,  and  presents  recommendations  for  addressing  them. Our  recommendations  are  intended  to  help  establish  data provenance and mitigate biases stemming from the data's origins and pre-processing to realize responsible AI-based systems. Characteristic  Description  Example Fairness  AI-based  systems  may  introduce  discrimination because  of  imbalanced  data  [4].  The  data  used  in training  AI-based  the discriminations existing in our society, which, in turn, lead to algorithmic bias [4]. systems  often  reflect  Training the system using only medical records from male patients can lead to discrimination against female patients. Accountability  Because  of  the  increasing  complexity  of  AI-based systems,  it  is  difficult  for  a  user  to  judge  who  is accountable for the results [49]. The individual services provided  by  AI  algorithms  are  integrated  into  larger systems  [19],  further  exacerbating  opaqueness  and ambiguity about ownership.  When  an  AI-based  system  trained  on  photos  depicting cancer on the epidermis (outer skin layer) is integrated into a larger system, it may also be inappropriately used on data from  subcutaneous  tissue  (inner  skin  layer).  It  becomes unclear  who  is  accountable  for  the  resulting  incorrect recommendation.  Transparency  An often-cited limitation of AI-based systems is their black  box  nature  [2].  However,  to  understand  the training  data quality  of adequacy, we need transparency. recommendations  and  The  pharma  industry  has  well-established  practices  for providing  easy  access  to  relevant  information  about  drugs (either in the product package itself or in the accompanying documents),  whereas  AI  systems  seldom  provide  relevant information  in  developing recommendations.   the  data  used about Explainability  A lack of explainability of AI prediction outcomes can be caused by the black box nature of algorithms, which can lead to negligence of the inaccuracies and biases in data.  Yet,  understanding  a  prediction  is  an  important aspect of their acceptance [84].  Evidence-based  medicine  rests  on  high  standards  of explainability  of  both  algorithms  and  data,  as  medical decision  making  requires  a  sound  understanding  of  the underlying  disease  mechanisms  and  treatments  [88].  The lack of this understanding undermines the implementation of AI in healthcare [81].  Data Provenance for Responsible AI  ACM Transactions on Management Information Systems In the following sections, we review key biases, such as systematic distortions [3], resulting from the failure to adopt appropriate data provenance  practices  in  the  development  and  implementation  of AI-based systems. We also provide three key recommendations for establishing  data  provenance  to  enhance  the  FATE  of  AI-based systems. We propose a data provenance framework for responsible AI  and  discuss  exemplary  cases  for  its  application.  Before concluding, we present future research directions. 2  Sources of Data Biases in AI-based Systems In contrast to the majority of existing research, which has focused on biases resulting from algorithms (e.g., [28,35]), we concentrate on the origins of the data and the data pre-processing rather than on the algorithm that uses the data as inputs. Data sources are often where the original data were collected to train and build AI-based systems.  After  data  collection,  data  pre-processing  [30],  which commonly  integration,  cleaning, normalization, and transformation, can also introduce biases [96]. We identify five categories of potential biases that may originate from  data  sources  and  five  categories  of  biases  that  may  be introduced  during  data  pre-processing.  For  example,  the  data themselves might be subject to bias in the ways in which they are sampled or measured. Each bias has different implications for the FATE characteristics of AI-based systems. includes  data  preparation, Table 2 \u2013 Summary of the effect of data biases on responsible AI  Population data Measurement error Data quality chasm Data repurposing Data augmentation Dataset shifts Opaque pre-processing Data labeling Adversarial manipulations  X X X X  X X X X X X  X X X X X  X  X X X X X X X X X Transfer learning  X  X  X 2.1  Biases from the Data\u2019s Origins Below, we identify five key instances in which biases arise in the data  sources:  population  data,  measurement  error,  data  quality chasm, data repurposing, and data augmentation. We describe their implications regarding the FATE characteristics (see Table 2 for a summary).   Population data. In every data science project, sampling the right data  to  ensure  representativeness  is  important  [33].  However,  to develop  and  implement  powerful  AI-based  systems,  developers often  rely  on  access  to  unique  data.  For  example,  data  provided through  projects,  such  as  BigMedilytics,  comprise  the  medical records of more than 11 million patients from eight countries. The retraining or recalibration of AI-based systems developed with such unique  data  to  other  contexts  for  the  same  purpose  requires additional data that are representative of the new context.  However,  AI-based  systems  are  often  applied  in  new  contexts without  retraining  or  recalibration  because  of  the  significant challenges involved in collecting the necessary additional data. For example,  when  an  algorithm  is  trained  with  data  from  one population but is used to develop predictions on another population, any  differences  in  the  frequency  and  nature  of  events  in  these datasets  will  result  in  poor  performance  [19].  When  the  data collection mechanisms impose selection bias or  fail to recognize the mismatch between the training data and the target population, the  transparency  of  the  data\u2019s  origins  is  affected.  In  addition, spurious correlations and shortcut learning (i.e., decision rules that work  well  based  on  the  training  data  because  of  spurious phenomena [32]) of the AI system will lead to unreliable and unfair recommendations [20] that will undermine possible explanations.  Measurement  error.  Every  study  and  every  measurement instrument, however well designed, still generates some errors [72]. Many AI applications in domains such as medicine or business rely heavily on Bayesian statistics, as the results are always subject to probabilities.  Data  pre-processing  and  the  use  of  another algorithm\u2019s  predictions  as  an  input  could  further  compound  this issue  because  of  the  propagation  of  uncertainties  or  prior probabilities [61].  However,  in  AI  systems,  the  uncertainty  of  the  input  variables resulting either from the measurement itself or from pre-processing is  often  neglected.  An  AI-based  system  trained  with  such  data without a particular focus on and caution about potential errors can result in a poorly performing model. Consequently, the precision of an  AI-based  system  might  be  overestimated,  as  the  AI  system learns to fit against the error. The resulting recommendations would be  at  least  distorted  if  not  incorrect,  leading  to  problematic outcomes.  If  the  system  provides  corresponding  explanations,  a user can identify these inadequacies and correct them [19].  Data  quality  chasm.  Another  challenge  is  the  lack  of  data  with adequate quality in settings where the AI system is used [61]. While the data may look homogeneous at the surface level, a more careful evaluation  can  suggest  otherwise.  For  example,  an  AI  algorithm may  achieve  superior  prediction  quality  because  of  its  access  to state-of-the-art computed tomography (CT) scans. If CT scans from older  equipment  that  generates  lower-quality  scans  are  used  to retrain the AI-based system, the recommendations are likely to be inaccurate.  ACM Transactions on Management Information Systems In  contrast  to  the  measurement  error,  in  which  the  system  has learned to predict based on errors, here, the AI-based system was trained using fine granular data that are no longer available later, thus  resulting  in  poorer  performance.  This  provides  multiple challenges along the FATE characteristics. The poor performance can  lead  to  suboptimal  recommendations,  and  depending  on  the level  of  transparency  provided  initially,  questions  related  to accountability between the system developer and system provider can  arise.  Creating  transparency  regarding  the  training  data\u2019s origins and the data used for the recommendations helps mitigate this issue. Data repurposing. In addition to biases resulting from sampling, data  collection  practices  also  introduce  misuse  and  biases. Traditional  data  collection  practices  differ  significantly  from contemporary  practices  in  AI  systems  development  [33].  The traditional  practice  is  to  collect  data  for  a  specific  purpose.  For example,  a  clinical  trial  of  a  drug  used  to  treat  COVID-19  will collect experimental data to assess the drug\u2019s side effects.  However, repurposing data is the norm in AI-based systems. For example,  a  blood  test  result  in  a  patient\u2019s  electronic  healthcare record that has been captured to diagnose a certain disease may also be used by an AI-based system to diagnose other diseases. This can be a potential issue compromising the accountability characteristic of  the  algorithm.  For  example,  while  the  quality  of  data  from medical images can be sufficient for the original purpose, such as stroke detection, it may not meet the needs of subsequent data uses, such as finding new disease markers [5]. Repurposing data creates ambiguity  about  the  data  and  their  origins, making it difficult to clearly identify the person or entity accountable for any incorrect recommendations. Data  augmentation.  When  the  available  dataset  is  not  large enough for the intended computations, data augmentation might be used  (i.e.,  increasing  the  size  of  the  dataset  with  synthetically generated data or slightly modified copies of the existing data, for example, through translation, rotation, flip, or scale). For instance, augmented data are generated through the rotation, translation, and scaling  of  a  prior  dataset  on  liver  lesions  [26]  when  training  a generative  adversarial  network  (GAN).  These  modifications  and the synthetically generated data can amplify existing biases within the dataset and mask the inadequacies of the collected data.  Some AI algorithms rely solely on simulated data. For example, AI systems have been developed to design bridges and control robot arms using only simulation data [23]. Simulations can create useful data to learn from, especially when little input and manually labeled data are available. However, because deep learning can approach problems more intuitively by focusing on patterns in the core data, researchers have suggested that AI systems perform better without synthetic additions to the data [23].  Therefore, data augmentation and the use of simulation data bring about  new  challenges  the  fairness  and  accountability characteristics  of  AI  algorithms.  Data  augmentation  amplifies existing  biases  and  creates  opaqueness  about  the  actual representativeness  of  the  data,  thus  limiting  transparency  and to  making  it  more  challenging  to  identify  the  cause  of  an  incorrect recommendation.  2.2  Biases from Data Pre-Processing Data processing is vulnerable to errors that introduce biases, such as dataset shifts, opaque pre-processing, data labeling, adversarial manipulation, transfer learning, and data augmentation. Dataset shifts. An easily ignorable fact is the non-stationary nature of the environment and the population from which all the input data of AI-based systems are generated [59]. For example, when a data shift occurs, an important predictor of a specific disease at one point in time can be more or less important at a later point in time because of  improvements  in  the  quality  of  care  available.  For  instance, many predictions using the Medical Information Mart for Intensive Care  dataset  are  confounded  by  changes  in  hospital  operation practices [71].  Considering  time  as  an  influential  variable  shows dataset shifts caused by changing practices, which, in turn, result in significant changes in the observed data. Unless this data shift is identified and the AI algorithm is retrained or recalibrated [53], the performance  of  the  system  deteriorates,  affecting  the  fairness, transparency,  and  explainability  characteristics  of  the  algorithm. Low  performance  can  lead  to  incorrect  recommendations  that negatively affect users. If the data\u2019s origins and subsequent changes in  the  environment  are  not  made  transparent,  the  derived explanations will be at least distorted. Opaque pre-processing. AI-based systems are often characterized as black boxes [2]. While some AI-based systems provide accurate predictions, the rationale behind their predictions remains opaque. In  algorithms  with  intrinsic  obscurity,  such  as  deep  neural networks,  understanding  the  specific  patterns  being  learned  is difficult [53]. For example, in a study detecting hip fractures, an algorithm  was  confounded  by  the  scanner  model  and  by  scans marked as \u201curgent\u201d [8]. Therefore, assessing the potential biases introduced  when  using  the  output  of  an  opaque  algorithm  as  an input  for  another  AI-based  system  is  difficult.  Opaque  pre-processing limits the transparency and explainability of AI-based system recommendations. If it is unclear what data were used to train  the  system,  confounding  indicators  are  more  difficult  to identify and assess, and they do not allow users to learn relevant insights. However, deriving explanations for the recommendations can  help  experts  validate  the  model  and  its  recommendations. Different types of explanations (e.g., feature extraction, pre-defined models, and sensitivity [87]) can help an expert evaluate, improve, and correct the model. Data labeling. While data quality chasm refers to data that may appear to be similar but have different qualities, another issue arises with data labeling, as the identification and development of labels are  often  not  transparent.  Data  labeling  is  related  to  supervised learning, such as medical image classification. The outcome labels are  used  by  supervised  algorithms  in  the  training  stage.  While automated  labeling  (e.g.,  with  weak supervision)  are  on  the  rise  [50,76],  labeling  is  often  a  labor-intensive task and is frequently performed by unqualified or poorly trained  ghost  workers  or  through  crowd-based  platforms  [7]. techniques  for  data Data Provenance for Responsible AI  ACM Transactions on Management Information Systems Incorrect  labels  in  the  training  data  create  erroneous  or  unfair recommendations  and  explanations  developed  by  AI-based systems because of the inherent bias embedded in the training data. This  bias  affects  the  fairness,  transparency,  and  explainability characteristics  of  the  AI  algorithm.  Fairness  is  affected,  as unqualified or poorly trained ghost workers will make mistakes and possibly  bring  their  social  biases  into  the  data.  As  these  are undesirable  business  practices,  organizations  seldom  disclose them,  thereby  negatively  affecting  transparency.  While  these business  practices  introduce  biases,  hiding  them  from  customers makes it difficult for both the user and the expert to benefit from explanations.  As  the  majority  of  existing  data  are  non-labeled  and  are  usually very expensive to label, some researchers perceive the reliance on labeled  data  as  even  counterproductive  to  the  development  of effective AI [23]. A recent trend in the automatic labeling of data using AI [77] has emerged. The idea is simple. As labeling is often a bottleneck task in AI system development, we could use machine learning (ML) to extrapolate the labels. A labeling ML algorithm can  be  trained  based  on  a  limited  number  of  available  or  easily attainable  labels  and  can  then  be  used  to  label  a  larger  dataset. While this reduces the effort of manual labor, it may also increase the severity of biases already existing in the smaller sample, leading to erroneous or unfair recommendations and explanations. Adversarial  manipulation.  As  AI-based  systems  derive  their models based on nuanced variations in the data, sometimes, small changes in the data input can lead to significant differences in the output  [38].  Therefore,  AI-based  systems  are  potentially susceptible  to  adversarial  manipulation.  For  instance,  images  of benign moles may be misdiagnosed as malignant because of added adversarial  noise  or  seemingly  minor  changes  in  the  data  [53]. These manipulations can be intentional, such as when an attacker changes the input of an algorithm to fool it, or unintentional, such as  when  a  user  accidentally  rotates  an  image  used  as  an  input. the  data  preprocessing, Without  sufficient  transparency  of  identifying this potential threat in an otherwise effective model is difficult. These seemingly minor changes can result in significantly different  outcomes  that  make  explaining  the  recommendations difficult and the recommendation itself possibly incorrect. Transfer learning. Once an AI-based system is built, we may use the algorithm to solve similar problems. In particular, a new AI-based system benefits from the information learned from another system. For example, a pre-trained model can be used to encode radiographic  features  in  images  before  final  re-training  [8]  to improve the sample efficiency for a reinforcement learning agent. Transfer learning can also improve AI system performance when predicting cancer for ethnic groups with limited data availability [29]. However, transfer learning only works when the source task is closely related to the new task. If not, transfer learning introduces biases and negatively affects performance [91]. As transfer learning also  system\u2019s recommendations,  it  impedes  clear  accountability.  Therefore, transfer  learning  should  be  made  transparent  to  the  user,  as  it otherwise adds to the system\u2019s opaqueness. increases  ambiguity  about  the  AI-based 3  Recommendations for Implementing Data Provenance from  data Considering  the  importance  of  mitigating  data-induced  biases sources  and  data  pre-processing, originating organizations  need  to  establish  data  provenance  when implementing responsible AI-based systems that address the FATE characteristics.  We  propose  a  data  provenance  framework  for responsible  AI  to  enhance  its  FATE  characteristics  (Figure  1). Organizations  can  focus  on  three  key  areas:  establishing organizational data governance, demanding data traceability, and leveraging technological advances, such as explainable AI. Below, we  summarize  current  and  future  challenges  and  elaborate  on actionable  recommendations  and  how  these  enhance  the  specific characteristics of responsible AI (see Table 3).  Table 3 \u2013 Overview of the current state, challenges, and recommendations Current state  Challenges  Recommendations Organizational  data accountability are lacking. lineage  and  Governmental  organizations  demand control and protection of data integrity, confidentiality, and availability.  Establishing Organizational Data Governance: - Managing meta-data - Conducting data audits Organizations rely on data from multiple data sources in their AI systems, creating heterogeneity and opaqueness. Many current AI-based systems rely heavily on manually labeled data.   Organizations typically do not have a clear understanding of the source and processing of data, such as various experiences, goals, and perspectives of the people annotating the data.  Demanding Data Traceability: - Guiding data acquisition - Benefitting from blockchain technology Technologies seek to increase the transparency of AI models.  Little attention has been given to data opaqueness.  Leveraging Technological Advances for Data Provenance: - Deriving rules for explanations - Identifying possible adversarial manipulations - Finding the inherent structure in the data ACM Transactions on Management Information Systems",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "6acfe2ef-10f0-4736-aa95-ff20021c5793",
                    "text": "",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "9f273ca3-77b7-46f1-b00a-b3ed19503c2c",
                    "text": ": - Managing meta-data - Conducting data audits Organizations rely on data from multiple data sources in their AI systems, creating heterogeneity and opaqueness. Many current AI-based systems rely heavily on manually labeled data.   Organizations typically do not have a clear understanding of the source and processing of data, such as various experiences, goals, and perspectives of the people annotating the data.  Demanding Data Traceability: - Guiding data acquisition - Benefitting from blockchain technology Technologies seek to increase the transparency of AI models.  Little attention has been given to data opaqueness.  Leveraging Technological Advances for Data Provenance: - Deriving rules for explanations - Identifying possible adversarial manipulations - Finding the inherent structure in the data ACM Transactions on Management Information Systems 3.1  Establishing Organizational Data Governance Several  governmental  organizations  have  launched  directives, laws,  and  regulations  to  provide  control  and  protection  of  data integrity, confidentiality, and availability. Examples include the US Health Insurance Portability and Accountability Act (HIPAA) and the EU\u2019s General Data Protection Regulation (GDPR). However, current data governance practices are often limited to master data management, that is, a set of processes related to the who, what, and where of business transactions, communications, and events. Seemingly, organizations too often mimic what their competitors do rather than being proactive and shaping the course of action. For example,  many  organizations  are  still  seeking  to  become  data driven.  Yet,  once  they  achieve  this,  they  find  that  inadequate attention is given to data governance during the development of AI systems, which, in turn, creates additional challenges [44]. Organizations  need  to  establish  organizational  data  governance practices that enforce data lineage and accountability. This would help them not only meet increasingly strict regulatory requirements but  also  benefit  from  an  overarching  perspective  of  their  data assets. Particularly, organizations need to manage their meta-data and conduct data audits in order to respond to the organizational challenges associated with inadequate data governance. For some organizations, these goals stand in a potential conflict. For example, data privacy seeks to protect individuals from being identified\u2014often  through  personal  identifiable  information\u2014or being associated with such information. Data lineage, on the other hand,  refers  to  the  visibility  of  the  data\u2019s  origins  and  further processing. If the data\u2019s origins and further processing are done by individuals, both concepts stand in conflict. An organization will have to manage this conflict by enhancing responsible AI under the condition of privacy policy compliance, such as  the GDPR [98]. For  example,  an  organization  may  allow  identifiable  data  to  be traced only for specific legal purposes. Organizations also need to leverage  some  privacy-preserving  approaches,  such  as  federated learning, to allow the safe sharing of identifiable data or models across entities [69]. Managing  meta-data.  Meta-data  describe  data  and  consist  of detailed information about the data captured in a data source. Meta-data help maintain the data within an organization in a manner that ensures the timely, efficient, and accurate retrieval of the required information [68]. It also helps ensure that processes and activities are documented in a transparent and verifiable way [78]. Generally, there are two practices that organizations use to manage meta-data: cataloging data and curating data. A data catalog stores information about the data, such as the rationale for choosing a data source, the stakeholders  involved,  and  the  content  stored  within  it.  Such information may also be documented in a datasheet [31].  Extending  these  efforts,  organizations  should  establish  clear processes  and  responsibilities  for  data  curation.  Data  curation identifies and leverages the data within the organization and helps assess  the  FATE  of  system  recommendations.  For  example,  organizations  can  identify  representation  and  corresponding limitations  by  visualizing  and  clustering  data  annotations.  These annotations  identification  of  discriminatory correlations between features, labels, and groups.  facilitate  the Overall,  managing  meta-data  through  data  catalogs  and  data curation  helps  increase  the  benefits  of  existing  data  through increased  transparency  [68]  and  helps  reduce  costs  by  avoiding unnecessary  data  collection.  Managing  meta-data  also  requires clear accountability for the different data sources. Meta-data help organizations  benefit  transformation,  weighting,  and sampling  techniques  [4]  by  minimizing  the  extent  to  which  data deviate from the objectives of responsible AI, thus helping ensure fairness of the recommendations.  from Conducting data audits. Enhancing data auditing capability in an organization is another approach to establishing data provenance through  data  governance  [44].  Data  auditing  is  the  process  of assessing whether the data are fit for a specific purpose. Given the recent  increase  in  regulatory  requirements,  organizations  should conduct  data  audits  to  assess  the  data  used  within  their  systems, similar  to  the  way  they  assess  and  audit  other  aspects  of  their business  operations.  Data  audits  help  uncover  potential  biases related to data processing and their associated consequences. With a reasonable and suitable guarantee of authenticity and reliability, data  audits  help  enhance  the  accountability  and  fairness  of  AI-based  systems.  This  not  only  applies  to  high-reliability organizations  that  need  to  make  high-stakes  decisions  but  also provides  benefits  for  other  organizations  that  seek  to  act responsibly. Data audits consist of data profiling (e.g., assessing the availability and quality of data and the risks associated with data integration [45]) and impact analysis (assessing the impact of poor data quality on performance and profits) [57]. Data  audits  become  increasingly  important  when  individual services  are  integrated  into  larger  systems  [73].  Conducting  data audits enhances the fairness of AI systems by ensuring a good fit between the data and their use. Conducting data audits also requires clear  accountabilities  for  the  appropriate  handling  of  data.  In addition  to  establishing  data  accuracy,  data  audits  uncover  data silos  and  areas  where  more  depth  and/or  breadth  of  data  is to  provide  valid necessary recommendations. A data provenance record could document the data  capturing  and  data  processing  entities  for  the  dataset  in question, simplifying the audit process. Data provenance records also  help  in  understanding  the  data\u2019s  origins  and  pre-processing, thereby enhancing transparency. the  AI-based  system for",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "18fc757b-bb2d-4fe4-adb3-45bae5c798ed",
                    "text": "Managers need to be aware of the implications of using different data sources and processing methods, especially when they seek to achieve fair and transparent systems. Data traceability is gaining increasing attention as managers become aware of its importance. For  example,  it  usually  takes  Walmart  6  days  and  14  hours  to identify the source of a farm product. When the supply chain data are maintained in a blockchain, however, it takes only 2.2 seconds to  establish  complete  data  traceability.  Therefore,  platform Data Provenance for Responsible AI  ACM Transactions on Management Information Systems providers  need  to  enhance  the  traceability  characteristic  of  data provenance  in  order  to  improve  the  efficiency  of  business  and decision making.   and their processing [17] before they are used by the AI black box. A user can ascertain whether the data used to train the system are suitable and relevant [36]. Enhanced  traceability  provides  more  information  about  the historicity of data and increases overall transparency. Transparency enables  the  creation  of  an  intermediate  representation  of  the original data [4] encoding the responsible AI objectives, such as fairness. As a result, organizations mitigate biases resulting from data sources and improve the fairness of their systems. Demanding data  traceability  may  include  guiding  data  acquisition  and leveraging blockchain technology. Guiding data acquisition. Many current AI-based systems rely on manually  labeled  data.  Despite  the  recent  trend  of  increasingly using  automated  labeling  practices,  manual  labeling  is  still indispensable. Manual labeling either applies to the entire dataset or only a subset of datapoints for later extrapolation. Either way, if organizations  do  not  have  a  clear  understanding  of  the  various experiences, goals, and perspectives of the people annotating the data, they cannot account for the significant impact on data quality [51].  Organizations  should  develop  procurement  guidelines  that take  the  traceability  of  data  into  consideration.  For  example, managers need to demand transparency regarding data origin and quality  when  acquiring  external  training  datasets.  A  data provenance  record  identifies  the  true  source  and  subsequent processing of data, uncovering the often-hidden history of the data. Recent  end-to-end  provenance  projects  have  developed  a  set  of tools, such as R packages, that allow organizations to establish data provenance through enhanced data traceability [24].  Furthermore, some data used to train the system may not have been labeled  by  experts,  whereas  other  data  may  have  been  procured from data brokers (organizations that collect data for the purpose of reselling  them).  Understanding  the  sources  and  methods  used  to acquire  the  data  is  critical  to  ensure  that  they  are  ethically  and legally  collected  (e.g.,  with  informed  consent).  Demanding traceability  (e.g.,  through  a  data  provenance  record)  increases transparency  and  helps  organizations  identify  the  accountable actors for mitigating risks related to the use of AI-based systems\u2019 recommendations.  For  instance,  an  organization  should  provide  the  descriptive statistics  of  a  dataset  as  part  of  its  data  provenance  records, allowing  users  to  identify  the  potential  risk  for  discrimination. Based on these statistics, users can evaluate the AI-based system\u2019s future recommendations discrimination,  either  by  altering  the  input  data,  modifying  the algorithm, or changing the way in which predictions are made [4]. As a result, the user is likelier to perceive the recommendations of the AI-based system as fair.  to  correct,  mitigate,  and  avoid As  data  provenance  relates  to  a  record  of  the  data\u2019s  origins  and subsequent  processing  [9],  it  also  increases  transparency.  For example, data provenance is needed to develop a data information sheet  [31]  that  provides  details  on  the  most  important  variables influencing an AI-based system\u2019s recommendations. As such, data provenance provides users with basic information about the data  Benefitting from blockchain technology. Blockchain-based data provenance is a promising approach to enhance the traceability of data in responsible AI. Blockchains can record the meta-data and history  of  data  objects.  The  important  characteristics  of blockchains,  such  as  transparency  and  auditability,  enable  the security and traceability of the meta-data, which are crucial for data accountability. Data immutability in a blockchain also enhances the perceived  fairness  the  recommendations.  Various  data provenance architectures based on blockchain technology, such as ProvChain  [60]  and  LineageChain  [60],  have  been  proposed. Blockchain technology has also been leveraged to handle dark data [99], which are the data that organizations collect but fail to utilize for their value. As a secured distributed ledger, blockchain has the potential to upgrade the value of the data and provide more efficient and transparent results [70].  in Increased  transparency  supports  a  consumer-centric  strategy  that organizations increasingly follow. For example, in healthcare, the notion  of  patient-centered  care  refers  to  being  respectful  and responsive to individual patient needs, values, and preferences; this requires  health  IT  systems  to  prioritize  data  provenance  and  the transparency  of  patients\u2019  personal  health-related  data.  With increased  transparency,  patients  are  better  informed  and  are therefore  more  empowered  to  seek  clarification  on  diagnoses  or recommendations  [41].  This  interaction  improves  the  quality  of healthcare.  It  also  enhances  patients\u2019  confidence  in  the  care provided  and  hence  its  effectiveness.  Healthcare  organizations\u2019 attention  to  data  provenance  in  electronic  healthcare  records improves the transparency of their decisions and recommendations.",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "343bdb67-26e9-41cc-b8f1-e3feeaedc9c5",
                    "text": "",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "14a447c6-2c62-46a6-9465-6f866e9935db",
                    "text": "for Responsible AI  ACM Transactions on Management Information Systems Establishing Data Provenance for Responsible Artificial Intelligence Systems  Karl Werder   Cologne Institute for Information Systems  University of Cologne  Germany  werder@wiso.uni-koeln.de  Balasubramaniam Ramesh  Computer Information Systems  Georgia State University  USA bramesh@gsu.edu  Rongen (Sophia) Zhang  Computer Information Systems  Georgia State University  USA  rzhang6@gsu.edu fairness,  accountability, ABSTRACT Data provenance, a record that describes the origins and processing of data, offers new promises in the increasingly important role of artificial  intelligence  (AI)-based  systems  in  guiding  human decision making. To avoid disastrous outcomes that can result from bias-laden  AI  systems,  responsible  AI  builds  on  four  important characteristics:  transparency,  and explainability.  To  stimulate  further  research  on  data  provenance that enables responsible AI, this study outlines existing biases and discusses possible implementations of data provenance to mitigate them. We first review biases stemming from the data\u2019s origins and pre-processing. We then discuss the current state of practice, the challenges  it  presents,  and  corresponding  recommendations  to address  them.  We  present  a  summary  highlighting  how  our recommendations can help establish data provenance and thereby mitigate  biases  stemming  from  the  data\u2019s  origins  and  pre-processing to realize responsible AI-based systems. We conclude with a research agenda suggesting further research avenues. CCS CONCEPTS \u2022Data Provenance \u2022Artificial Intelligence KEYWORDS Data Provenance, Artificial Intelligence, Fairness, Accountability, Transparency, Explainability 1  Introduction As evidence-based decision making aided by data-driven artificial intelligence (AI) algorithms becomes increasingly common across all sectors of the economy, there is a growing concern among users about  whether  such  algorithms  are  developed  and  implemented responsibly. Prior reports have already provided a glimpse into the disastrous effects of inaccurate and bias-laden AI recommendations  in high-stakes applications, with examples from the healthcare and legal  domains,  such  as  incorrect  patient  treatment,  exacerbated poverty [62], wrongful arrest [33], and unjust criminal sentencing [43].  The  heightened  awareness  of  concerns  raised  in  recent movements for social justice has resulted in calls from professional associations [1] and researchers [18,34] for developing approaches that help establish responsible AI.  Rapid innovations in data-generating technologies, such as sensors, social media, and mobile devices, have exacerbated the problems resulting from poor data quality that threaten the development of responsible  AI  systems.  These  technologies  generate  an unprecedented  quantity  and  variety  of  data.  While  most applications  have  benefitted  from  explosive  growth  in  data availability (in terms of volume, variety, velocity, veracity, etc.), limited  attention  has  been  given  to  data  quality  [66],  in  turn undermining the quality of recommendations generated using such data. Motivated by these concerns, this study examines how data provenance can help improve data quality and enhance the fairness, accountability,  transparency,  and  explainability  (FATE)  of  AI-based  systems.  We  argue  that  data  provenance\u2014a  record  that describes the origins and processing of data [9]\u2014can help assess and  improve  the  FATE  of  recommendations  provided  by  AI algorithms and thus instill trust in them. Trust is enhanced by the capability to describe and follow the life of data (i.e., their origins, processing, and use) in both forward and backward directions [75]. The importance of provenance has long been recognized [14] in the pharmaceutical,  food,  and  fashion  industries.  It  helps  establish  a product\u2019s  origins  and  influences  consumers\u2019  decisions  about purchase and use.  Responsible AI is essentially related to a broad discourse, AI ethics, which  has  received  significant  attention  among  researchers  in recent years. Scholars have identified different high-level ethical principles  that  should  govern  the  development  of  AI  systems [25,48,97].  While  no  universal  consensus  exists,  fairness, accountability,  and  transparency  [48]  have  received  significant attention in this research community [27]. Simultaneously, research related to explainable AI has emerged [39], with recent discussions on  its  capability  to  bridge  the  gap  between  technical  and  ethical considerations [64]. AI explainability gives users and experts the ability  to  investigate  and  understand  the  inner  workings  of  AI, ACM Transactions on Management Information Systems allowing  them  to  identify  potential  biases.  Bridging  these  two perspectives, we focus on four important and related characteristics of responsible AI\u2014FATE. While there is ongoing research on other AI-based system characteristics, such as privacy and agency, we focus on how FATE can help organizations identify and mitigate the negative influences of biases within their data. We discuss how potential conflicts among different FATE characteristics emerge, how organizations can manage them, and where more research is needed. Most  current  researchers  and  practitioners  in  the  field  of responsible  AI  have  emphasized  the  quality  of  algorithms. However, an algorithm\u2019s recommendations or outputs also depend heavily on representations, structures, and data quality, which serve as  the  inputs.  In  this  study,  we  focus  on  data  provenance,  an important aspect of data quality, in the development of responsible AI systems [13]. For example, data provenance can help uncover data quality concerns related to labor-intensive data labeling, which is  often  performed  by  unqualified  workers  [7]  and  otherwise remains  concealed.  This  the recommendations  or  outputs  of  AI  algorithms  are  often  used  as inputs  for  other  AI  algorithms  [53],  further  exacerbating  the problem. For example, the classification of a radiology scan by an algorithm  as  benign  or  malignant  may  be  used  as  an  input  for another  algorithm  that  is  used  to  create  a  risk  score  for  patient is  particularly  alarming,  as Table 1 \u2013 Overview of FATE characteristics and examples  readmission. In such situations, data provenance can help identify the  causes  of  the  AI  algorithm\u2019s  poor  performance,  improve interpretability,  or  uncover  its  seemingly  acceptable that performance  was  achieved  for  invalid  reasons  (e.g.,  when identifying a malignant tumor, the system was learning from the circle made by the radiologist on the scan rather than the data from the scan itself). By  illuminating the origin and processing of the data  [14],  data  provenance  can  mitigate  these  shortcomings  and facilitate FATE assessments (see Table 1).  The  lack  of  data  provenance  is  a  serious  concern  in  AI-based systems  that  are  used  to  inform  critical  decisions.  While  the establishment of data provenance may increase short-term costs for organizations, it can provide long-term benefits by instilling trust in the implemented system and its recommendations. Specifically, our  study  addresses  the  following  question:  How  does  data provenance  affect  the  four  interrelated  characteristics  of responsible  AI:  fairness,  accountability,  transparency,  and explainability? The paper analyzes biases related to origins and pre-processing of data,  discusses  the  current  state  of  practice  and  attendant  challenges,  and  presents  recommendations  for  addressing  them. Our  recommendations  are  intended  to  help  establish  data provenance and mitigate biases stemming from the data's origins and pre-processing to realize responsible AI-based systems. Characteristic  Description  Example Fairness  AI-based  systems  may  introduce  discrimination because  of  imbalanced  data  [4].  The  data  used  in training  AI-based  the discriminations existing in our society, which, in turn, lead to algorithmic bias [4]. systems  often  reflect  Training the system using only medical records from male patients can lead to discrimination against female patients. Accountability  Because  of  the  increasing  complexity  of  AI-based systems,  it  is  difficult  for  a  user  to  judge  who  is accountable for the results [49]. The individual services provided  by  AI  algorithms  are  integrated  into  larger systems  [19],  further  exacerbating  opaqueness  and ambiguity about ownership.  When  an  AI-based  system  trained  on  photos  depicting cancer on the epidermis (outer skin layer) is integrated into a larger system, it may also be inappropriately used on data from  subcutaneous  tissue  (inner  skin  layer).  It  becomes unclear  who  is  accountable  for  the  resulting  incorrect recommendation.  Transparency  An often-cited limitation of AI-based systems is their black  box  nature  [2].  However,  to  understand  the training  data quality  of adequacy, we need transparency. recommendations  and  The  pharma  industry  has  well-established  practices  for providing  easy  access  to  relevant  information  about  drugs (either in the product package itself or in the accompanying documents),  whereas  AI  systems  seldom  provide  relevant information  in  developing recommendations.   the  data  used about Explainability  A lack of explainability of AI prediction outcomes can be caused by the black box nature of algorithms, which can lead to negligence of the inaccuracies and biases in data.  Yet,  understanding  a  prediction  is  an  important aspect of their acceptance [84].  Evidence-based  medicine  rests  on  high  standards  of explainability  of  both  algorithms  and  data,  as  medical decision  making  requires  a  sound  understanding  of  the underlying  disease  mechanisms  and  treatments  [88].  The lack of this understanding undermines the implementation of AI in healthcare [81].  Data Provenance for Responsible AI  ACM Transactions on Management Information Systems In the following sections, we review key biases, such as systematic distortions [3], resulting from the failure to adopt appropriate data provenance  practices  in  the  development  and  implementation  of AI-based systems. We also provide three key recommendations for establishing  data  provenance  to  enhance  the  FATE  of  AI-based systems. We propose a data provenance framework for responsible AI  and  discuss  exemplary  cases  for  its  application.  Before concluding, we present future research directions. 2  Sources of Data Biases in AI-based Systems In contrast to the majority of existing research, which has focused on biases resulting from algorithms (e.g., [28,35]), we concentrate on the origins of the data and the data pre-processing rather than on the algorithm that uses the data as inputs. Data sources are often where the original data were collected to train and build AI-based systems.  After  data  collection,  data  pre-processing  [30],  which commonly  integration,  cleaning, normalization, and transformation, can also introduce biases [96]. We identify five categories of potential biases that may originate from  data  sources  and  five  categories  of  biases  that  may  be introduced  during  data  pre-processing.  For  example,  the  data themselves might be subject to bias in the ways in which they are sampled or measured. Each bias has different implications for the FATE characteristics of AI-based systems. includes  data  preparation, Table 2 \u2013 Summary of the effect of data biases on responsible AI  Population data Measurement error Data quality chasm Data repurposing Data augmentation Dataset shifts Opaque pre-processing Data labeling Adversarial manipulations  X X X X  X X X X X X  X X X X X  X  X X X X X X X X X Transfer learning  X  X  X 2.1  Biases from the Data\u2019s Origins Below, we identify five key instances in which biases arise in the data  sources:  population  data,  measurement  error,  data  quality chasm, data repurposing, and data augmentation. We describe their implications regarding the FATE characteristics (see Table 2 for a summary).   Population data. In every data science project, sampling the right data  to  ensure  representativeness  is  important  [33].  However,  to develop  and  implement  powerful  AI-based  systems,  developers often  rely  on  access  to  unique  data.  For  example,  data  provided through  projects,  such  as  BigMedilytics,  comprise  the  medical records of more than 11 million patients from eight countries. The retraining or recalibration of AI-based systems developed with such unique  data  to  other  contexts  for  the  same  purpose  requires additional data that are representative of the new context.  However,  AI-based  systems  are  often  applied  in  new  contexts without  retraining  or  recalibration  because  of  the  significant challenges involved in collecting the necessary additional data. For example,  when  an  algorithm  is  trained  with  data  from  one population but is used to develop predictions on another population, any  differences  in  the  frequency  and  nature  of  events  in  these datasets  will  result  in  poor  performance  [19].  When  the  data collection mechanisms impose selection bias or  fail to recognize the mismatch between the training data and the target population, the  transparency  of  the  data\u2019s  origins  is  affected.  In  addition, spurious correlations and shortcut learning (i.e., decision rules that work  well  based  on  the  training  data  because  of  spurious phenomena [32]) of the AI system will lead to unreliable and unfair recommendations [20] that will undermine possible explanations.  Measurement  error.  Every  study  and  every  measurement instrument, however well designed, still generates some errors [72]. Many AI applications in domains such as medicine or business rely heavily on Bayesian statistics, as the results are always subject to probabilities.  Data  pre-processing  and  the  use  of  another algorithm\u2019s  predictions  as  an  input  could  further  compound  this issue  because  of  the  propagation  of  uncertainties  or  prior probabilities [61].  However,  in  AI  systems,  the  uncertainty  of  the  input  variables resulting either from the measurement itself or from pre-processing is  often  neglected.  An  AI-based  system  trained  with  such  data without a particular focus on and caution about potential errors can result in a poorly performing model. Consequently, the precision of an  AI-based  system  might  be  overestimated,  as  the  AI  system learns to fit against the error. The resulting recommendations would be  at  least  distorted  if  not  incorrect,  leading  to  problematic outcomes.  If  the  system  provides  corresponding  explanations,  a user can identify these inadequacies and correct them [19].  Data  quality  chasm.  Another  challenge  is  the  lack  of  data  with adequate quality in settings where the AI system is used [61]. While the data may look homogeneous at the surface level, a more careful evaluation  can  suggest  otherwise.  For  example,  an  AI  algorithm may  achieve  superior  prediction  quality  because  of  its  access  to state-of-the-art computed tomography (CT) scans. If CT scans from older  equipment  that  generates  lower-quality  scans  are  used  to retrain the AI-based system, the recommendations are likely to be inaccurate.  ACM Transactions on Management Information Systems In  contrast  to  the  measurement  error,  in  which  the  system  has learned to predict based on errors, here, the AI-based system was trained using fine granular data that are no longer available later, thus  resulting  in  poorer  performance.  This  provides  multiple challenges along the FATE characteristics. The poor performance can  lead  to  suboptimal  recommendations,  and  depending  on  the level  of  transparency  provided  initially,  questions  related  to accountability between the system developer and system provider can  arise.  Creating  transparency  regarding  the  training  data\u2019s origins and the data used for the recommendations helps mitigate this issue. Data repurposing. In addition to biases resulting from sampling, data  collection  practices  also  introduce  misuse  and  biases. Traditional  data  collection  practices  differ  significantly  from contemporary  practices  in  AI  systems  development  [33].  The traditional  practice  is  to  collect  data  for  a  specific  purpose.  For example,  a  clinical  trial  of  a  drug  used  to  treat  COVID-19  will collect experimental data to assess the drug\u2019s side effects.  However, repurposing data is the norm in AI-based systems. For example,  a  blood  test  result  in  a  patient\u2019s  electronic  healthcare record that has been captured to diagnose a certain disease may also be used by an AI-based system to diagnose other diseases. This can be a potential issue compromising the accountability characteristic of  the  algorithm.  For  example,  while  the  quality  of  data  from medical images can be sufficient for the original purpose, such as stroke detection, it may not meet the needs of subsequent data uses, such as finding new disease markers [5]. Repurposing data creates ambiguity  about  the  data  and  their  origins, making it difficult to clearly identify the person or entity accountable for any incorrect recommendations. Data  augmentation.  When  the  available  dataset  is  not  large enough for the intended computations, data augmentation might be used  (i.e.,  increasing  the  size  of  the  dataset  with  synthetically generated data or slightly modified copies of the existing data, for example, through translation, rotation, flip, or scale). For instance, augmented data are generated through the rotation, translation, and scaling  of  a  prior  dataset  on  liver  lesions  [26]  when  training  a generative  adversarial  network  (GAN).  These  modifications  and the synthetically generated data can amplify existing biases within the dataset and mask the inadequacies of the collected data.  Some AI algorithms rely solely on simulated data. For example, AI systems have been developed to design bridges and control robot arms using only simulation data [23]. Simulations can create useful data to learn from, especially when little input and manually labeled data are available. However, because deep learning can approach problems more intuitively by focusing on patterns in the core data, researchers have suggested that AI systems perform better without synthetic additions to the data [23].  Therefore, data augmentation and the use of simulation data bring about  new  challenges  the  fairness  and  accountability characteristics  of  AI  algorithms.  Data  augmentation  amplifies existing  biases  and  creates  opaqueness  about  the  actual representativeness  of  the  data,  thus  limiting  transparency  and to  making  it  more  challenging  to  identify  the  cause  of  an  incorrect recommendation.  2.2  Biases from Data Pre-Processing Data processing is vulnerable to errors that introduce biases, such as dataset shifts, opaque pre-processing, data labeling, adversarial manipulation, transfer learning, and data augmentation. Dataset shifts. An easily ignorable fact is the non-stationary nature of the environment and the population from which all the input data of AI-based systems are generated [59]. For example, when a data shift occurs, an important predictor of a specific disease at one point in time can be more or less important at a later point in time because of  improvements  in  the  quality  of  care  available.  For  instance, many predictions using the Medical Information Mart for Intensive Care  dataset  are  confounded  by  changes  in  hospital  operation practices [71].  Considering  time  as  an  influential  variable  shows dataset shifts caused by changing practices, which, in turn, result in significant changes in the observed data. Unless this data shift is identified and the AI algorithm is retrained or recalibrated [53], the performance  of  the  system  deteriorates,  affecting  the  fairness, transparency,  and  explainability  characteristics  of  the  algorithm. Low  performance  can  lead  to  incorrect  recommendations  that negatively affect users. If the data\u2019s origins and subsequent changes in  the  environment  are  not  made  transparent,  the  derived explanations will be at least distorted. Opaque pre-processing. AI-based systems are often characterized as black boxes [2]. While some AI-based systems provide accurate predictions, the rationale behind their predictions remains opaque. In  algorithms  with  intrinsic  obscurity,  such  as  deep  neural networks,  understanding  the  specific  patterns  being  learned  is difficult [53]. For example, in a study detecting hip fractures, an algorithm  was  confounded  by  the  scanner  model  and  by  scans marked as \u201curgent\u201d [8]. Therefore, assessing the potential biases introduced  when  using  the  output  of  an  opaque  algorithm  as  an input  for  another  AI-based  system  is  difficult.  Opaque  pre-processing limits the transparency and explainability of AI-based system recommendations. If it is unclear what data were used to train  the  system,  confounding  indicators  are  more  difficult  to identify and assess, and they do not allow users to learn relevant insights. However, deriving explanations for the recommendations can  help  experts  validate  the  model  and  its  recommendations. Different types of explanations (e.g., feature extraction, pre-defined models, and sensitivity [87]) can help an expert evaluate, improve, and correct the model. Data labeling. While data quality chasm refers to data that may appear to be similar but have different qualities, another issue arises with data labeling, as the identification and development of labels are  often  not  transparent.  Data  labeling  is  related  to  supervised learning, such as medical image classification. The outcome labels are  used  by  supervised  algorithms  in  the  training  stage.  While automated  labeling  (e.g.,  with  weak supervision)  are  on  the  rise  [50,76],  labeling  is  often  a  labor-intensive task and is frequently performed by unqualified or poorly trained  ghost  workers  or  through  crowd-based  platforms  [7]. techniques  for  data Data Provenance for Responsible AI  ACM Transactions on Management Information Systems Incorrect  labels  in  the  training  data  create  erroneous  or  unfair recommendations  and  explanations  developed  by  AI-based systems because of the inherent bias embedded in the training data. This  bias  affects  the  fairness,  transparency,  and  explainability characteristics  of  the  AI  algorithm.  Fairness  is  affected,  as unqualified or poorly trained ghost workers will make mistakes and possibly  bring  their  social  biases  into  the  data.  As  these  are undesirable  business  practices,  organizations  seldom  disclose them,  thereby  negatively  affecting  transparency.  While  these business  practices  introduce  biases,  hiding  them  from  customers makes it difficult for both the user and the expert to benefit from explanations.  As  the  majority  of  existing  data  are  non-labeled  and  are  usually very expensive to label, some researchers perceive the reliance on labeled  data  as  even  counterproductive  to  the  development  of effective AI [23]. A recent trend in the automatic labeling of data using AI [77] has emerged. The idea is simple. As labeling is often a bottleneck task in AI system development, we could use machine learning (ML) to extrapolate the labels. A labeling ML algorithm can  be  trained  based  on  a  limited  number  of  available  or  easily attainable  labels  and  can  then  be  used  to  label  a  larger  dataset. While this reduces the effort of manual labor, it may also increase the severity of biases already existing in the smaller sample, leading to erroneous or unfair recommendations and explanations. Adversarial  manipulation.  As  AI-based  systems  derive  their models based on nuanced variations in the data, sometimes, small changes in the data input can lead to significant differences in the output  [38].  Therefore,  AI-based  systems  are  potentially susceptible  to  adversarial  manipulation.  For  instance,  images  of benign moles may be misdiagnosed as malignant because of added adversarial  noise  or  seemingly  minor  changes  in  the  data  [53]. These manipulations can be intentional, such as when an attacker changes the input of an algorithm to fool it, or unintentional, such as  when  a  user  accidentally  rotates  an  image  used  as  an  input. the  data  preprocessing, Without  sufficient  transparency  of  identifying this potential threat in an otherwise effective model is difficult. These seemingly minor changes can result in significantly different  outcomes  that  make  explaining  the  recommendations difficult and the recommendation itself possibly incorrect. Transfer learning. Once an AI-based system is built, we may use the algorithm to solve similar problems. In particular, a new AI-based system benefits from the information learned from another system. For example, a pre-trained model can be used to encode radiographic  features  in  images  before  final  re-training  [8]  to improve the sample efficiency for a reinforcement learning agent. Transfer learning can also improve AI system performance when predicting cancer for ethnic groups with limited data availability [29]. However, transfer learning only works when the source task is closely related to the new task. If not, transfer learning introduces biases and negatively affects performance [91]. As transfer learning also  system\u2019s recommendations,  it  impedes  clear  accountability.  Therefore, transfer  learning  should  be  made  transparent  to  the  user,  as  it otherwise adds to the system\u2019s opaqueness. increases  ambiguity  about  the  AI-based 3  Recommendations for Implementing Data Provenance from  data Considering  the  importance  of  mitigating  data-induced  biases sources  and  data  pre-processing, originating organizations  need  to  establish  data  provenance  when implementing responsible AI-based systems that address the FATE characteristics.  We  propose  a  data  provenance  framework  for responsible  AI  to  enhance  its  FATE  characteristics  (Figure  1). Organizations  can  focus  on  three  key  areas:  establishing organizational data governance, demanding data traceability, and leveraging technological advances, such as explainable AI. Below, we  summarize  current  and  future  challenges  and  elaborate  on actionable  recommendations  and  how  these  enhance  the  specific characteristics of responsible AI (see Table 3).  Table 3 \u2013 Overview of the current state, challenges, and recommendations Current state  Challenges  Recommendations Organizational  data accountability are lacking. lineage  and  Governmental  organizations  demand control and protection of data integrity, confidentiality, and availability.  Establishing Organizational Data Governance: - Managing meta-data - Conducting data audits Organizations rely on data from multiple data sources in their AI systems, creating heterogeneity and opaqueness. Many current AI-based systems rely heavily on manually labeled data.   Organizations typically do not have a clear understanding of the source and processing of data, such as various experiences, goals, and perspectives of the people annotating the data.  Demanding Data Traceability: - Guiding data acquisition - Benefitting from blockchain technology Technologies seek to increase the transparency of AI models.  Little attention has been given to data opaqueness.  Leveraging Technological Advances for Data Provenance: - Deriving rules for explanations - Identifying possible adversarial manipulations - Finding the inherent structure in the data ACM Transactions on Management Information Systems 3.1  Establishing Organizational Data Governance Several  governmental  organizations  have  launched  directives, laws,  and  regulations  to  provide  control  and  protection  of  data integrity, confidentiality, and availability. Examples include the US Health Insurance Portability and Accountability Act (HIPAA) and the EU\u2019s General Data Protection Regulation (GDPR). However, current data governance practices are often limited to master data management, that is, a set of processes related to the who, what, and where of business transactions, communications, and events. Seemingly, organizations too often mimic what their competitors do rather than being proactive and shaping the course of action. For example,  many  organizations  are  still  seeking  to  become  data driven.  Yet,  once  they  achieve  this,  they  find  that  inadequate attention is given to data governance during the development of AI systems, which, in turn, creates additional challenges [44]. Organizations  need  to  establish  organizational  data  governance practices that enforce data lineage and accountability. This would help them not only meet increasingly strict regulatory requirements but  also  benefit  from  an  overarching  perspective  of  their  data assets. Particularly, organizations need to manage their meta-data and conduct data audits in order to respond to the organizational challenges associated with inadequate data governance. For some organizations, these goals stand in a potential conflict. For example, data privacy seeks to protect individuals from being identified\u2014often  through  personal  identifiable  information\u2014or being associated with such information. Data lineage, on the other hand,  refers  to  the  visibility  of  the  data\u2019s  origins  and  further processing. If the data\u2019s origins and further processing are done by individuals, both concepts stand in conflict. An organization will have to manage this conflict by enhancing responsible AI under the condition of privacy policy compliance, such as  the GDPR [98]. For  example,  an  organization  may  allow  identifiable  data  to  be traced only for specific legal purposes. Organizations also need to leverage  some  privacy-preserving  approaches,  such  as  federated learning, to allow the safe sharing of identifiable data or models across entities [69]. Managing  meta-data.  Meta-data  describe  data  and  consist  of detailed information about the data captured in a data source. Meta-data help maintain the data within an organization in a manner that ensures the timely, efficient, and accurate retrieval of the required information [68]. It also helps ensure that processes and activities are documented in a transparent and verifiable way [78]. Generally, there are two practices that organizations use to manage meta-data: cataloging data and curating data. A data catalog stores information about the data, such as the rationale for choosing a data source, the stakeholders  involved,  and  the  content  stored  within  it.  Such information may also be documented in a datasheet [31].  Extending  these  efforts,  organizations  should  establish  clear processes  and  responsibilities  for  data  curation.  Data  curation identifies and leverages the data within the organization and helps assess  the  FATE  of  system  recommendations.  For  example,  organizations  can  identify  representation  and  corresponding limitations  by  visualizing  and  clustering  data  annotations.  These annotations  identification  of  discriminatory correlations between features, labels, and groups.  facilitate  the Overall,  managing  meta-data  through  data  catalogs  and  data curation  helps  increase  the  benefits  of  existing  data  through increased  transparency  [68]  and  helps  reduce  costs  by  avoiding unnecessary  data  collection.  Managing  meta-data  also  requires clear accountability for the different data sources. Meta-data help organizations  benefit  transformation,  weighting,  and sampling  techniques  [4]  by  minimizing  the  extent  to  which  data deviate from the objectives of responsible AI, thus helping ensure fairness of the recommendations.  from Conducting data audits. Enhancing data auditing capability in an organization is another approach to establishing data provenance through  data  governance  [44].  Data  auditing  is  the  process  of assessing whether the data are fit for a specific purpose. Given the recent  increase  in  regulatory  requirements,  organizations  should conduct  data  audits  to  assess  the  data  used  within  their  systems, similar  to  the  way  they  assess  and  audit  other  aspects  of  their business  operations.  Data  audits  help  uncover  potential  biases related to data processing and their associated consequences. With a reasonable and suitable guarantee of authenticity and reliability, data  audits  help  enhance  the  accountability  and  fairness  of  AI-based  systems.  This  not  only  applies  to  high-reliability organizations  that  need  to  make  high-stakes  decisions  but  also provides  benefits  for  other  organizations  that  seek  to  act responsibly. Data audits consist of data profiling (e.g., assessing the availability and quality of data and the risks associated with data integration [45]) and impact analysis (assessing the impact of poor data quality on performance and profits) [57]. Data  audits  become  increasingly  important  when  individual services  are  integrated  into  larger  systems  [73].  Conducting  data audits enhances the fairness of AI systems by ensuring a good fit between the data and their use. Conducting data audits also requires clear  accountabilities  for  the  appropriate  handling  of  data.  In addition  to  establishing  data  accuracy,  data  audits  uncover  data silos  and  areas  where  more  depth  and/or  breadth  of  data  is to  provide  valid necessary recommendations. A data provenance record could document the data  capturing  and  data  processing  entities  for  the  dataset  in question, simplifying the audit process. Data provenance records also  help  in  understanding  the  data\u2019s  origins  and  pre-processing, thereby enhancing transparency. the  AI-based  system for 3.2  Demanding Data Traceability Managers need to be aware of the implications of using different data sources and processing methods, especially when they seek to achieve fair and transparent systems. Data traceability is gaining increasing attention as managers become aware of its importance. For  example,  it  usually  takes  Walmart  6  days  and  14  hours  to identify the source of a farm product. When the supply chain data are maintained in a blockchain, however, it takes only 2.2 seconds to  establish  complete  data  traceability.  Therefore,  platform Data Provenance for Responsible AI  ACM Transactions on Management Information Systems providers  need  to  enhance  the  traceability  characteristic  of  data provenance  in  order  to  improve  the  efficiency  of  business  and decision making.   and their processing [17] before they are used by the AI black box. A user can ascertain whether the data used to train the system are suitable and relevant [36]. Enhanced  traceability  provides  more  information  about  the historicity of data and increases overall transparency. Transparency enables  the  creation  of  an  intermediate  representation  of  the original data [4] encoding the responsible AI objectives, such as fairness. As a result, organizations mitigate biases resulting from data sources and improve the fairness of their systems. Demanding data  traceability  may  include  guiding  data  acquisition  and leveraging blockchain technology. Guiding data acquisition. Many current AI-based systems rely on manually  labeled  data.  Despite  the  recent  trend  of  increasingly using  automated  labeling  practices,  manual  labeling  is  still indispensable. Manual labeling either applies to the entire dataset or only a subset of datapoints for later extrapolation. Either way, if organizations  do  not  have  a  clear  understanding  of  the  various experiences, goals, and perspectives of the people annotating the data, they cannot account for the significant impact on data quality [51].  Organizations  should  develop  procurement  guidelines  that take  the  traceability  of  data  into  consideration.  For  example, managers need to demand transparency regarding data origin and quality  when  acquiring  external  training  datasets.  A  data provenance  record  identifies  the  true  source  and  subsequent processing of data, uncovering the often-hidden history of the data. Recent  end-to-end  provenance  projects  have  developed  a  set  of tools, such as R packages, that allow organizations to establish data provenance through enhanced data traceability [24].  Furthermore, some data used to train the system may not have been labeled  by  experts,  whereas  other  data  may  have  been  procured from data brokers (organizations that collect data for the purpose of reselling  them).  Understanding  the  sources  and  methods  used  to acquire  the  data  is  critical  to  ensure  that  they  are  ethically  and legally  collected  (e.g.,  with  informed  consent).  Demanding traceability  (e.g.,  through  a  data  provenance  record)  increases transparency  and  helps  organizations  identify  the  accountable actors for mitigating risks related to the use of AI-based systems\u2019 recommendations.  For  instance,  an  organization  should  provide  the  descriptive statistics  of  a  dataset  as  part  of  its  data  provenance  records, allowing  users  to  identify  the  potential  risk  for  discrimination. Based on these statistics, users can evaluate the AI-based system\u2019s future recommendations discrimination,  either  by  altering  the  input  data,  modifying  the algorithm, or changing the way in which predictions are made [4]. As a result, the user is likelier to perceive the recommendations of the AI-based system as fair.  to  correct,  mitigate,  and  avoid As  data  provenance  relates  to  a  record  of  the  data\u2019s  origins  and subsequent  processing  [9],  it  also  increases  transparency.  For example, data provenance is needed to develop a data information sheet  [31]  that  provides  details  on  the  most  important  variables influencing an AI-based system\u2019s recommendations. As such, data provenance provides users with basic information about the data  Benefitting from blockchain technology. Blockchain-based data provenance is a promising approach to enhance the traceability of data in responsible AI. Blockchains can record the meta-data and history  of  data  objects.  The  important  characteristics  of blockchains,  such  as  transparency  and  auditability,  enable  the security and traceability of the meta-data, which are crucial for data accountability. Data immutability in a blockchain also enhances the perceived  fairness  the  recommendations.  Various  data provenance architectures based on blockchain technology, such as ProvChain  [60]  and  LineageChain  [60],  have  been  proposed. Blockchain technology has also been leveraged to handle dark data [99], which are the data that organizations collect but fail to utilize for their value. As a secured distributed ledger, blockchain has the potential to upgrade the value of the data and provide more efficient and transparent results [70].  in Increased  transparency  supports  a  consumer-centric  strategy  that organizations increasingly follow. For example, in healthcare, the notion  of  patient-centered  care  refers  to  being  respectful  and responsive to individual patient needs, values, and preferences; this requires  health  IT  systems  to  prioritize  data  provenance  and  the transparency  of  patients\u2019  personal  health-related  data.  With increased  transparency,  patients  are  better  informed  and  are therefore  more  empowered  to  seek  clarification  on  diagnoses  or recommendations  [41].  This  interaction  improves  the  quality  of healthcare.  It  also  enhances  patients\u2019  confidence  in  the  care provided  and  hence  its  effectiveness.  Healthcare  organizations\u2019 attention  to  data  provenance  in  electronic  healthcare  records improves the transparency of their decisions and recommendations. 3.3  Leveraging Technological Advances for Data Provenance Given  the  opaque  nature  of  many  AI-based  systems,  data provenance  is  essential  for  understanding  AI-based  systems\u2019 recommendations  [74].  Recent  technological  advances  include explainable artificial intelligence (XAI) methods, GANs, and deep learning with advances in small data techniques. the  accuracy  and Deriving explanations. XAI methods, such as LIME, LORE, and Anchor [29], push the traditional boundaries imposed by trade-offs between  interpretability  of  AI  systems\u2019 recommendations.  More  recently,  XAI  solutions  have  allowed users  to  understand  the  most  important  features  that  lead  to  the outcomes,  make  changes  to  model  features,  and  customize  the model explanation [58].  Explainable  AI  methods  seek  to  increase  the  transparency  of  AI models,  but  little  attention  has  been  given  to  addressing  data opaqueness.  Data  provenance  provides  a  complementary perspective  toward  transparency  for  the  user  [6]  by  presenting information about the source and further processing of the data used to  feed  an  AI-based  system.  Data  provenance  helps  provide complementary information to the explanations provided by XAI ACM Transactions on Management Information Systems through  global  and systems. For example, expanding the data provenance concept to AI algorithms facilitates the documentation of the data processing performed  by  an  AI  algorithm  local explanations [22]. While a global explanation creates transparency regarding  the  model  used  to  make  all  recommendations  (e.g., answering the question of how the AI makes its recommendations for  all  patients),  a  local  explanation  provides  transparency  for  a specific recommendation (e.g., answering the question of why the AI makes a specific recommendation for a particular patient). For example,  through  explainable  AI,  healthcare  providers  and  their patients can better understand the important factors that lead to an algorithm\u2019s  recommendations  on  a  particular  diagnosis  or treatment,  thereby  enhancing  the  accountability  of  the  parties responsible  for  and  receiving  care.  Therefore,  we  suggest  that organizations  should  strive  the  most  of  recent technological advances related to XAI.  to  make In  particular,  we  suggest  that  organizations  should  leverage existing  XAI  methods,  such  as  LIME  and  LORE,  and  XAI techniques,  such  as  layer-wise  relevance  propagation  [85]  and gradient-based  explanations,  with  supporting  architectural frameworks, such as CaSE [55], to provide easily understandable explanations  of  AI-based  recommendations.  XAI  methods,  for example,  derive  rules  that  explain  how  a  recommendation  was reached  by  presenting  cut-off  values  that  lead  to  the  predicted outcome or by identifying the factors that most strongly influence the  recommendation.  Such  explanations  help  users  better understand the AI system\u2019s behavior and identify new patterns in the data.  However,  prior  studies  also  suggest  a  potential  conflict  between explainability and other FATE dimensions. For example, a trade-off  exists  between  explainability  and  fairness  [56].  While explainability  seeks  to  simplify  the  complex  nature  of  AI-based systems  so  that  they  can  be  understood  by  humans,  there  is  an inherent  loss  associated  with  this  simplification  that  may  lead  to new biases. Organizations can manage these conflicts, for example, by using multi-criteria decision-making methods (see [89] for an overview)  to  guide  and  prioritize  different  characteristics.  In  a given  scenario,  one  characteristic  might  be  more  important  than another.  For  example,  if  the  adoption  and  use  of  the  system  are concerns,  explainability  could  be  one  way  of  increasing  the transparency of a system to increase trust [80]. In organizations that provide  a  process  for  users  to  participate  in  the  evolution  of  the system in order to address potential fairness concerns [42], users are less likely to reject the system. The lack of explainability of AI prediction outcomes can be caused not only by the black box nature of algorithms but also by the biases in  the  data.  While  most  research  focuses  on  algorithm explainability, we suggest paying additional attention to how data provenance  can  enhance  the  explainability  of  outcomes.  By allowing individuals to meaningfully interact with the system and by enhancing the explainability of AI-based systems, organizations facilitate  autonomous  decision  making,  detect  errors,  minimize biases, and thus safeguard justice [15].   in  managing  noisy  data Managing noisy data. The presence of meaningless and irrelevant data  is  often  referred  to  as  noise  within  the  data.  Scholars  have made  significant  progress  that organizations can benefit from. A distinction is made as to whether the  noise  relates  to  predictive  attributes  (referred  to  as  attributed noise) or to target attributes (referred to as class noise). Different techniques are available for identifying and handling noise within the data. A recent systematic review provides a good overview of the current state of the art on the problems caused by noisy data in AI-based systems [40]. The  management  of  noisy  data  is  important  for  deriving  fair recommendations.  In  fact,  striving  to  achieve  fairness  without addressing  the  noise  within  a  given  dataset  could  backfire.  For example,  a  prior  study  investigated  the  use  of  noise  models  for denoising data during subset selection [65]. Scholars applied noise models to select a subset of data from an existing larger data set. The  goal  was  to  generate  a  fair  dataset  so  that  the  sub-dataset accounts for race while having noisy race data. The study points out that failing to account for noise has unintended side effects, as it decreases the fairness of the resulting subset selection.  Different techniques are available to handle noise within data [40]. For example, organizations can use filtering techniques to identify and remove noise, or they can alter the data, sometimes referred to as data polishing. They key difference between responding to class noise  and  to  attribute  noise  is  that  for  class  noise,  organizations should  also  consider  relabeling,  whereas  for  attribute  noise, organizations can use data imputation.  thereby  mitigating A related technique is the use of GANs (sets of neural networks that seek to generate new data with similar characteristics as the training data).  Organizations  should  use  GANs  to  identify  possible adversarial  manipulations,  negative consequences.  For  example,  GANs  are  used  in  image-to-image translations, such as the translation of low-dose CT scans that have noise  in  the  data  into  regular-dose  CT  scans.  In  this  case,  a generator network translates the low-dose scan into a regular-dose scan, whereas a discriminator tries to distinguish the artificial from real  regular-dose  scans.  As  a  result,  the  noise  in  image-to-image translation is reduced [96].  Identifying  inherent  data  structures.  Deep  learning  for  text, audio, and video recognition often involves performing a pre-text task to find an inherent structure in the data of their AI systems. The  pre-text  task  is  self-supervised  learning  with  the  purpose  of generating a useful feature representation for the downstream task [12]. Pre-text tasks may force ML models to deconstruct data in order to enhance explainability [23]. For example, the Facebook AI Research group uses a combination of clustering and training based on rotated images to improve the quantity of unlabeled data used in their image classifier. After this pre-text task processing, the second stage  of  training  uses  conventional  labeled  data  to  create interpretable results [23]. Furthermore, advances in small data techniques help organizations improve  the  performance  of  AI-based  systems.  While  many  AI-based systems rely on large data, some of the most valuable datasets Data Provenance for Responsible AI  ACM Transactions on Management Information Systems are  only  available  in  small  quantities  [51].  For  example,  the application of AI in the medical domain often requires data labeling by  medical  professionals,  such  as  radiologists  or  physicians.  A review by a radiologist is needed to reliably label an image scan with the correct diagnosis of the presence or absence of lung cancer. As  medical  professionals\u2019  time  is  scarce  and  expensive,  and  the task of data labeling is quite repetitive, the creation of large datasets is  a  challenge.  However,  it  is  this  high-quality  human  input  that facilitates high-quality recommendations by AI-based systems.  Overall, a clearer understanding of the system\u2019s behavior and the data helps judge the fairness of recommendations. This is important because,  for  example,  evidence-based  medicine  rests  on  high standards of explainability, as medical decision making requires a sound  understanding  of  underlying  disease  mechanisms  and treatments  under  particular  conditions  [88].  The  lack  of  this understanding undermines the implementation of AI in healthcare [81].  This  issue  is  crucial  because  of  the  promising  benefits provided by AI in healthcare.  Figure 1 \u2013 Data provenance framework for responsible artificial intelligence",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "0aabdf64-9c84-4a6d-bfd8-4b4d0c4957b8",
                    "text": "",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "5faa3323-0b34-4bd6-993a-0c0368325ea3",
                    "text": "We  discuss  the  application  of  our  framework  with  two  recent examples  that  highlight  the  problems  associated  with  a  lack  of responsible AI. A  recent  example  of  data  provenance  concerns  relates  to  the application of AI recommendations in healthcare. A recent study evaluated  the  performance  of  the  AI-based  system  that  is embedded  within  EPIC  [100],  a  major  electronic  healthcare records  system,  to  predict  sepsis  (a  potentially  life-threatening condition in which the body\u2019s response to an infection damages its own  tissues).  As  sepsis  is  the  number  one  killer  in  US  hospitals [67], hospitals attach great importance to identifying and treating conditions that may lead to sepsis. There is widespread adoption of sepsis  prediction  models,  such  as  the  one  provided  by  EPIC. However, the study suggests that i) the AI-based system does not deliver the advertised performance, ii) important assumptions that  underly  the  AI  system  require  careful  examination,  and  iii)  the system\u2019s high number of false positives contribute to alert fatigue for the medical staff [100].  This  case  highlights  four  important  biases:  data  repurposing, population bias, transfer learning, and data shifts. One important observation  of  the  evaluation  was  that  the  data  used  in  the development of the model may have been repurposed. To derive the  predictions,  EPIC  measured  positive  sepsis  cases  based  on billing  codes  but  not  on  the  clinical  definition  of  sepsis.  The decision to use billing codes also results in population bias, as the presence  of  sepsis  relies  on  the  identification  of  sepsis  by  the medical  staff.  Yet,  the  medical  staff  used  the  system  with  the expectation  that  it  would  help  predict  sepsis  before  medical personnel  could  identify  it.  In  response  to  the  study,  EPIC  has argued  that  transfer  learning  could  explain  the  suboptimal performance. That is, transfer learning works only when the source task is closely related to the new task, so the sepsis prediction model developed using the data from one environment may not work well ACM Transactions on Management Information Systems in  other  environments.  Transfer  learning  may  have  introduced biases,  negatively  affecting  the  performance  of  the  sepsis prediction  model  using  data  from  the  University  of  Michigan Hospital [94] in contrast to data from the University of Colorado Hospital [10]. Lastly, the researchers also describe the potential for a dataset shift resulting from changed practices in treating sepsis and suggest the need to retire old models entirely. We suggest that organizations using prediction models such as this should establish organizational  governance,  conduct  data  audits,  and  leverage technological advances in the area of XAI to derive explanations for these prediction models. Organizational data auditing capability establishes data provenance through data governance [44], whereas data auditing refers to the process that assesses the fit of the data for a specific purpose. A data audit would allow healthcare organizations to evaluate the data used to train the AI system and identify possible concerns. In our example,  a  data  audit  would  allow  a  medical  expert  to  identify potential errors resulting from the use of billing codes as a proxy for  the  presence  of  a  disease.  Yet,  billing  codes  are  used  in  the administrative process and can deviate from the medical diagnosis (e.g., [93]). When used in research, billing codes are often a means to identify patients for another study in order to narrow down those who are likely to have a specific disease or condition (e.g., [86]).  An  organization\u2019s  capability  to  audit  AI  systems  has  become increasingly more pressing, as a recent study suggests a severe lack of transparency by AI system providers and a lack of oversight by the  FDA  [67].  Medical  experts  criticized  the  opaqueness  and limited  transparency  offered  by  EPIC.  As  the  AI  system  is protected  by  intellectual  property  rights,  the  developer  has disclosed  very  limited  information  about  the  development  of  the prediction  model.  Medical  professionals  implicitly  relied  on  the FDA\u2019s  oversight,  but  the  recent  study  points  out  that  the  FDA\u2019s oversight is limited [101]. Medical devices are rated by the FDA into three classes [16], with the highest class being reserved for life support systems. Those systems that make autonomous decisions (e.g., a pacemaker or an automated insulin pump) are required to meet the highest standards set by the FDA. AI-based systems that provide  recommendations  to  healthcare  providers  (e.g.,  a  sepsis prediction model) are often considered class II systems that have much  lower  FDA  oversight.  In  the  EPIC  example,  the  study suggests  that  not  even  the  reduced  oversight  was  applied,  as  the system  may  have  been  checked  upon  market  launch,  but  later additions are not subject to further FDA approval.  Recent  technological  advances  help  organizations  identify  the needed  adjustments.  For  example,  explainable  AI  helps  provide insights and feedback to AI developers so that they can then further refine  the  AI  system  by  adjusting  the  network  architecture  or retraining the model. This concept is often referred to as human-in-the-loop and has been advocated by scholars for the debiasing of AI  systems  [47].  Here,  the  technological  advances  in  XAI  can enhance data provenance by supporting feedback through human-in-the-loop and, in turn, improve the transparency of the predictive model. For example, a medical expert could question the validity of  the  model  for  the  early  prediction  of  sepsis,  while  the  most  important  prediction  factor  of  the  trained  model  is,  in  fact,  the diagnosis of sepsis by medical staff (i.e., labels of the training data). The concerns described are not limited to the healthcare domain. Another example is the Amazon AI recruitment tool, which has received  attention  for  its  lack  of  adherence  to  the  facets  of responsible  AI  (e.g.,  [46]).  Amazon  developed  an  experimental hiring  system  that  was  designed  to  automatically  screen  the resumes of job applicants and identify the top candidates. Amazon later  realized  that  the  AI  system  did  not  select  candidates  for technical  jobs  in  a  gender-neutral  way  but  was  rather  biased negatively toward female candidates. In hindsight, the explanation for  this  behavior  seems  obvious.  It  was  reported  that  among Amazon\u2019s  entry-  and  mid-level  corporate  employees,  women accounted for 31% of the workforce last year [52]. The system had been  trained  with  data  for  the  past  10  years,  during  which  male candidates  were  predominantly  chosen  for  jobs. Meanwhile,  many  high-technology  companies  have  realized  the gender discrepancy when hiring employees for tech jobs and have changed their hiring practices to recruit more women. In such cases, the data shift would require the developers of AI systems to discard older data and rely on more recent data to train their models.  technical Amazon used its own recruitment data from the past 10 years in training  the  system.  An  auditing  process  would  have  helped enhance data provenance and thus uncover the presence of a dataset shift and population bias. Specifically, it would have highlighted that the hiring practices followed during the past 10 years have been significantly unfair to female candidates [54]. Further adjustments are  necessary  to  ensure  responsible  AI  recommendations.  Thus, data  auditing  can  help  increase  the  fairness  of  a  system  by establishing data provenance. In a similar vein, the human-in-the-loop that has been advocated for  debiasing  HR  recruitments  systems  [47]  helps  organizations evaluate the AI system. Technological advances in XAI enhance data  provenance  by  supporting  feedback  through  human-in-the-loop  and,  in  turn,  help  mitigate  the  negative  impact  of  a  dataset shift.  XAI  enhances  the  explainability  of  responsible  AI  through data provenance.",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "34517de9-f5cd-482b-8dc4-c789d6c74155",
                    "text": "Organizations  continue  adopting  and  using  AI-based  systems  to support evidence-based decision making. A particular focus is on enhancing  the  FATE  of  the  implemented  AI-based systems.  Our review of data-induced biases and discussion of how organizations can  mitigate  these  by  establishing  data  provenance  within  their organizations  for three  central organizations.  Yet,  more  research  is  needed  to  improve  data provenance methods, tools, and practices for responsible AI. Thus, we develop recommendations for future research, identifying four central topics (see Table 4).  recommendations lead  to Data Provenance for Responsible AI  ACM Transactions on Management Information Systems Table 1 \u2013 Exemplar research questions for responsible AI Research topic  Exemplar future research question Conceptual Clarity  How can we classify central terms related to data provenance and responsible AI?  the  relationship  between research.  For  example,  regarding transparency  and  explainability,  some  scholars  suggest  that explainability enhances the transparency of systems [82], whereas others  argue  is  a  sub-characteristic  of transparency  [83].  More  research  is  needed  to  develop  a nomological network around data provenance for responsible AI.  that  explainability Resolving Tradeoffs AI ethics  What  are  the  relationships  between  AI explainability and AI interpretability? What are the relationships among FATE and what  are  the  boundary  conditions  for  the impact of date provenance on  the FATE of responsible AI? What are the existing tradeoffs or conflicts among the goals of responsible AI, and how can we resolve them? How  do  different  organizational  profiles affect  the  design  of  responsible  AI  in organizations?  What are the regional differences in moral and  legal  concerns  that  impact  responsible AI?  How  do  we  ensure  responsible  AI  with increasing role of AI in the future of work?  How  do  we  develop  and scalable, responsible AI solutions? implement Designing responsible AI  What  are principles for responsible AI systems? the  design  guidelines  and How do we design explainability to enhance interpretability, and what are the influential conditions? Conceptual clarity. Establishing a clear nomological network to better  understand  the  distinction  of  terms  and  their  relations  is crucial for the development of data provenance for responsible AI. More research is needed to determine the unique nature of different concepts  and  possibly  the  interchangeability  of  some  concepts. Scholars  can  use  taxonomy  development  methods  to  identify classifications with mutually exclusive and collectively exhaustive dimensions.  For  example,  explainability  and  interpretability  are essentially  two  related  but  different  concepts  but  often  are  used interchangeably; terms such as data lineage and data pedigree are closely related to data provenance, but they are distinct terms. With enhanced  conceptual  clarity,  more  research  can  be  conducted  to understand  the  relationships  between  ontologically  different concepts. Understanding the conditions in which these relationships occur is also  important.  For  example,  having  a  fair  dataset  or  fair recommendations  high transparency. This can help explain conflicting evidence in existing necessarily  guarantee does  not  tradeoffs.  Implementing  data  provenance Resolving  for responsible  AI  can  lead  to  tradeoffs  or  conflicts.  For  example, regulations, such as the GDPR, require the system to ensure data privacy,  whereas  other  requirements  demand  more  traceability, such  as  auditing  requirements.  The  case  of  Twitter\u2019s  cropping algorithm shows a conflict in speed and consistency versus the risk of  making  incorrect  predictions  [95].  Furthermore,  the  trade-off between  accuracy  and  interpretability  is  an  often-mentioned conflict related to responsible AI [90]. More research is needed to identify  these  conflicts  and  develop  corresponding  resolutions. Researchers  can  benefit,  for  example,  from  specific  research methods,  such  as  conjoint  analysis  [37]  and  analytic  hierarchy process  (AHP)  approach  [79],  in  order  to  prioritize  different characteristics  of or characteristics in different context.   configurations important identify In  order  to  resolve  these  conflicts,  we  suggest  two  important avenues. First, scholars may benefit from research on multi-criteria decision  making.  Prior  research  can  guide  managers  in  making decision while accounting for multiple and potentially conflicting goals. These require extension and evaluation for responsible AI before  they  can  be  used  to  derive  normative  recommendations. Second,  organizational  or  AI  project  profiles  may  be  created  to provide templates for  developing responsible AI projects. While prioritization  may  be  the  result  of  external  forces,  such  as governmental  regulations,  they  may  also  be  the  result  of organizational  values  and  culture.  For  example,  an  open  and progressive organization may prioritize transparency and fairness over  accountability  concerns.  risk-adverse organization  may  focus  on  accountability  and  performance  over transparency.  Similarly,  different  projects  within  an  organization may need to emphasize different aspects of FATE. Future research could  explore  the  role  of  organizational  and  AI  project  specific profiles in the development and use of responsible AI systems. In  contrast,  a AI ethics. Questions related to the fairness of responsible AI are often at the cross-section of research focused on novel technology and its ethical behavior [64]. Research related to ethics is closely associated with moral and legal questions. Legal research is often conducted at the national level according to the local needs of the judiciary  system.  By  contrast,  new  technical  challenges  emerge during the development and deployment of responsible AI-based systems regardless of local needs. For example, responsible AI has the potential for solutions that are easily scalable from a technical perspective  yet  raise  concerns  when  it  comes  to  local  legal requirements, such as the GDPR.  Prior  research  also  coined  the  term  responsibility  gap  [49], describing a situation in which artificial agents are used to decide on  a  course  of  actions  or  in  which  they  act  themselves  without ACM Transactions on Management Information Systems human  involvement.  As  the  rules  by  which  they  act  are  not inscribed during development, there is no individual who assumes responsibility for the machine\u2019s actions. Current ethical and legal frameworks have not been designed for these situations, leading to a responsibility gap [63]. In addition to mitigating or eliminating the  responsibility  gap,  organizations  must  often  follow  multiple goals,  such  as  transparency  and  accountability  [21],  in  the development of responsible AI systems. However,  how  governmental  regulations  that  organizations  must follow  map  toward  different  goals  of  responsible  AI  remains unclear. For example, future research should investigate whether and how we need to extend and modify regulations, such as HIPAA in the US and the GDPR in the EU, to allow platform providers to offer scalable yet responsible AI solutions. Designing  responsible  AI.  Designing  responsible  AI  provides  a particular challenge for future research, as it requires us to instill human and social values into the AI system in a way that users see and appreciate it [21]. However, current research often focuses on the  technical  implementations  of  FATE.  For  example,  much research  related  to  explainable  AI  offers  technical  solutions  for developing explanations. When an explanation is presented to the user, an interpretative process is triggered. The user will develop an autonomous interpretation of the explanation, a process that is often  described  as  the  interpretability  of  an  explanation.  This interpretation  may  or  may  not  be  in  line  with  the  expected interpretation intended by the system\u2019s designer.  Therefore,  more  research  is  needed  to  better  understand  the  link between  different  design  patterns  and  technological  solutions related  to  explainability  research  and  the  interpretability  of individual users. For example, certain user or task characteristics influence the interpretability of a user in the sense that an expert, compared  with  a  novice,  requires  different  explanations.  We suggest  that  data  provenance  requires  also  more  attentions, particularly  in  the  XAI  community,  as  it  provides  important complementary information that are crucial for the interpretation by the user. Future research could develop clear guidelines, design features,  and  design  principles  for  designing  responsible  AI systems,",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                },
                {
                    "id": "c780e97b-ec31-475c-acad-90609753e1c2",
                    "text": "Data  provenance  is  important  to  mitigate  biases  and  improve responsible  AI-based  systems  (see  Figure  1).  Existing  practices view  data  provenance  as  a  mandate  of  directives,  laws,  and regulations designed to ensure the control and protection of data integrity,  confidentiality,  and  availability.  Data  provenance  is viewed as the cost of staying compliant with these requirements. Such practices result from a lack of organizational commitment to developing responsible AI-based systems.  By contrast, our recommended practices view data provenance as an  important  component  of  developing  responsible  AI-based systems.  Organizations  that  are  strategically  committed  to  their FATE  goals  are  likely  to  achieve  long-term  improvements  in organizational performance. Our recommended practices view data  provenance as an investment necessary to meet their FATE goals and recognize that the loss of data provenance at any point in the provenance  chain  leads  to  a  loss  of  data  provenance  in  all subsequent  parts.  Therefore,  organizations  need  to  recognize  the importance of establishing a comprehensive provenance for critical data that serve as inputs to AI systems.  In contemporary systems development projects, such as in the case of data-driven development and AI engineering, data repurposing is becoming more and more the norm. Recommended practices will help organizations benefit significantly from data provenance, as the data provenance established for one project is likely to benefit several  other  projects  that  use  the  same  data.  Therefore,  when examining the costs and benefits of data provenance, organizations need to take a comprehensive view that spans across projects, as different projects often draw from the same data sources. Whereas existing  practices  view  data  provenance  records  as  static, recommended  practices  recognize  the  need  to  maintain  dynamic data provenance information that is updated throughout the data\u2019s lifecycle.  We have outlined the multiple benefits of data provenance along and beyond the FATE characteristics. However, organizations will need  to  prioritize  their  investments  in  data  provenance  efforts based,  for  instance,  on  the  magnitude  of  benefits  resulting  from achieving FATE and the severity of negative consequences or the cost of failure that result from not achieving FATE. Organizations that view data provenance as an overhead cost are likely to neglect it when operating under budget or schedule constraints and, even worse,  perhaps  engage  in  undesirable  practices,  such  as  virtue washing [92].  Investments  in  data  provenance  should  be  driven  by  an  intrinsic motivation to improve the responsibility of AI-based systems. For example,  adopting  data  provenance  practices  to  achieve transparency  is  valuable  because  it  enables  users  to  understand, engage  with,  and  audit  the  AI-based  system  and  its  outcomes. Similarly, data provenance that enables accountability is a means to  ensure  justice  by  clarifying  responsibility  and  avoiding  harm from  deterrence  these  examples  show,  FATE characteristics are instrumental in upholding the intrinsic values of core principles, such as human autonomy and justice. In addition, organizations  that  take  a  lifecycle  perspective  recognize  that  the costs incurred in the early phases of data acquisition and processing lead to benefits later in the AI-based system lifecycle. Yet, these benefits,  such  as  increasing  reputation,  avoiding  the  loss  of reputation, and establishing the desired FATE characteristics, are often  difficult  to  quantify  despite  quickly  outweighing  negative implications.  [15].  As In  high-reliability  organizations,  such  as  healthcare  providers, suboptimal  decisions  can  have  severe  consequences.  The increasing  reliance  on  AI-based  systems  and  the  lack  of understanding  of  the  data  used  to  generate  recommendations highlight  the  importance  of  data  provenance.  Establishing  data provenance guidelines and policies can facilitate the FATE of AI-based  recommendations.  For  example,  in  the  context  of  the Data Provenance for Responsible AI  ACM Transactions on Management Information Systems COVID-19  pandemic,  the  provenance  of  data  is  important  for discerning  the  FATE  of  recommendations  made  by  AI-based systems that rely on data from varied and disparate data sources. While  more  guidelines  are  needed  to  develop  data  provenance throughout  the  entire  data  lifecycle  [11],  implementing  the recommended practices is an urgent task for organizations that aim to harness the benefits of AI-based systems. Our recommendations will  help  organizations  enhance  essential  data  provenance capabilities toward fair, transparent, accountable, and explainable evidence-based decision making by responsible AI-based systems. Our proposed research agenda suggests potential research avenues related to data provenance. We suggest that achieving conceptual clarity,  resolving  tradeoffs,  observing  AI  ethics,  and  designing responsible  AI  require  more  research  by  scholars  from  different disciplines.",
                    "reference": "[1] Karen Werder, Balasubramaniam Ramesh, and Ruixuan Zhang. 2022. Establishing data provenance for responsible artificial intelligence systems. ACM Trans. Manage. Inf. Syst. 2022. Retrieved from https://kups.ub.uni-koeln.de/53868/1/TMIS_manuscript_DataProvenance.pdf"
                }
            ]
        },
        {
            "paper_title": "Fair and Responsible AI: A focus on the ability to contest",
            "authors": "H Lyons, E Velloso, T Miller",
            "publication_info": "arXiv preprint arXiv:2102.10787 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2102.10787",
            "chunks": [
                {
                    "id": "a275e39c-ff6c-4849-96d5-186e430cfa63",
                    "text": "As the use of arti\ufb01cial intelligence (AI) in high-stakes decision-making increases, the ability to contest such decisions isbeing recognised in AI ethics guidelines as an importantsafeguard for individuals. Yet, there is little guidance on howAI systems can be designed to support contestation. In thispaper we explain that the design of a contestation processis important due to its impact on perceptions of fairness andsatisfaction. We also consider design challenges, includ-ing a lack of transparency as well as the numerous designoptions that decision-making entities will be faced with. Weargue for a human-centred approach to designing for con-testability to ensure that the needs of decision subjects, andthe community, are met.Author KeywordsContestability; explainability; algorithmic fairness, ethics.IntroductionThere is great potential for Arti\ufb01cial Intelligence (AI) to en-hance decision-making, by making it more accurate, ef\ufb01-cient, and scalable than human decision-making [4, 12].To harness these bene\ufb01ts, AI systems should be designedresponsibly, to ensure that they are fair, accountable, andtransparent [6]. This is particularly important given the in-creasing use of AI in high-stakes decision-making, includingsentencing, hiring, and loan application determination [12].arXiv:2102.10787v1  [cs.HC]  22 Feb 2021Examples of Ethical AIGuidelines Calling for theAbility to ContestEthics Guidelines for Trust-worthy AI (High-Level ExpertGroup on Arti\ufb01cial Intelli-gence): \"[T]here are manydifferent interpretations offairness, we believe thatfairness has both a sub-stantive and a proceduraldimension... The procedu-ral dimension of fairnessentails the ability to contestand seek effective redressagainst decisions made by AIsystems and by the humansoperating them.\" [16]Principles on Arti\ufb01cial In-telligence (OECD): \"Thereshould be transparencyand responsible disclosurearound AI systems to ensurethat people understand AI-based outcomes and canchallenge them.\" [8]AI Ethics Framework (Aus-tralia): (Principle 7) \"Con-testability: When an AI sys-tem signi\ufb01cantly impacts aperson, community, group orenvironment, there shouldbe a timely process to allowpeople to challenge the useor output of the AI system.\"[15] In response to calls for AI systems to be designed, devel-oped, and deployed responsibly, numerous AI ethics guide-lines have been produced. One \u2018safeguard\u2019 that is gainingtraction within these guidelines is the ability to contest AIdecisions (see sidebar for examples) [11]. Article 22(3) ofthe European Union\u2019s General Data Protection Regula-tion provides a legal \u2018right to contest\u2019 decisions made usingsolely automated processes. However, none of these doc-uments provide guidance on how AI systems should be de-signed to enable contestation. In this paper, we outline whydesign is important, what the design challenges are, andour human-centred approach to designing for contestability.The importance of designThe importance of designing AI systems to enable \u2018con-testability\u2019 has been acknowledged in HCI and AlgorithmicAccountability literature (e.g. [10, 2]). Using a legal lens, theAlgorithmic Accountability work has taken a theoretical ap-proach to proposing requirements of a contestation scheme[3], and design requirements that enable contestation [2].Within HCI, the focus of contestability research has been onthe ability of expert users to work interactively with a systemto contest its output [10].HCI researchers [12, 4, 9] have also drawn on organisa-tional psychology literature to study how the design of AIsystems used in decision-making impact human percep-tions of procedural fairness, \u2018procedural justice\u2019 [18]. Theprocedural justice literature indicates that having a legiti-mate way to contest a decision increases a person\u2019s per-ception of procedural fairness, which can impact their per-ception of the fairness of the decision (\u2018distributive justice\u2019),their choice to accept or contest a decision, and their atti-tude towards the entity making the decision [18, 13]. In linewith this literature [13], Lee et al [12] found that having \u2018out-come control\u2019, the ability to correct or appeal a decision, in a cooperative group allocation task improved participants\u2019perceptions of the fairness of the outcome.The procedural justice literature also indicates that designof a contestation process (not just its availability) impactsperceptions of procedural fairness. For example, havingthe same decision-maker assess the decision on appealis seen as less fair than having a new decision-maker [13].In addition, in a study of content moderation across socialmedia platforms, Myers West [14] found that users weredissatis\ufb01ed with contestation processes, reporting a lack ofclear instruction about how to lodge an appeal, receivingno reply or resolution having lodged a challenge, and noaccess to human intervention. These \ufb01ndings indicate thatthe design of a contestation process matters.Design challengesTo meaningfully challenge a decision, a decision subject re-quires some form of information in order to understand thedecision, decide whether to contest, and to use as groundsfor contestation. Many AI systems used in decision-makingare effectively \u201cblack boxes\" [17]; their decision-making pro-cesses are hidden due to the use of complex algorithmsor techniques (e.g. deep learning) or intentionally by com-panies to protect trade secrets [5]. This opacity makes itdif\ufb01cult to understand why a decision was made, and con-sequently, to contest it in any meaningful way. In contrast,with human decision-making a person can generally seekan explanation from the decision maker as to why a deci-sion was made. Often, in high-stakes decisions, reasonsmust be documented during the decision-making processto mitigate the issue of an inaccurate post-hoc explanation.Promisingly, the \ufb01eld of explainable arti\ufb01cial intelligence(XAI) is progressing work into explainability [19]. To date,XAI has not focused on providing explanations for contesta-tion speci\ufb01cally, which offers a new avenue of research.Sample of preliminary\ufb01ndings from our currentresearchAI systems are not isolated,but exist in socio-technicalcontexts with existing legalframeworks, political sys-tems, and social norms, thatneed to be considered whendesigning for contestationDifferent processes for con-testation are likely to berequired depending on thecontext in which a decision isbeing madeContestation processesneed to be clear and easy toaccessContestation processesshould align with humanrights, to ensure equality, bedesigned for accessibility,and to provide compensationLack of transparency is anissue; explainability is impor-tant A second design challenge is that there are many ways tocontest a decision [13]. For example, existing contestationprocesses for human decisions (e.g. internal review, com-plaints mechanisms, external review via tribunal or court)could be adapted for decisions made using AI. However,with decisions made at scale, leaving appeal processes to acourt to determine would overwhelm an already pressuredsystem. Low perceptions of fairness are associated withprocedures that are time consuming, costly and resourceintensive [13]. An alternative contestation process might in-volve a decision subject directly contesting a decision withinAI system via an interface. However, the novelty of this ap-proach coupled with a lack of human touch could negativelyimpact perceptions of fairness. With an abundance of de-sign choices, it is dif\ufb01cult to know where to begin.We suggest that taking a human-centred approach to ex-plore how people conceptualise contestability in relation toAI systems is a key \ufb01rst step in designing for meaningfulcontestation. To understand the needs of decision subjects,and the expectations of the community more generally, weare currently conducting a thematic analysis of submissionsmade to Australia\u2019s \u2018Arti\ufb01cial Intelligence: Australia\u2019s EthicsFramework\u2019, a discussion paper that proposed \u2018contestabil-ity\u2019 as a core ethical principle [15]. The sidebar contains asample of our preliminary \ufb01ndings.ConclusionThe increasing use of AI in high-stakes decision-making,which has been deployed without appropriate safeguardslike procedural fairness, has had a signi\ufb01cant, negative im-pact on thousands of people, from teachers losing their jobs[1] to the erroneous loss of medical bene\ufb01ts [7]. To reducenegative consequences, AI systems must be responsiblydesigned, developed, and deployed [6]. Though the abilityto contest decisions is not the only mechanism required to ensure that AI systems are \u2018fair\u2019, it is a crucial safeguard,and in some circumstances, a legal requirement. How ac-cess to contestation, and the contestation process itself, isdesigned is important given the impact on perceptions offairness and satisfaction. Yet, there are many design chal-lenges including opacity, and an abundance of design op-tions. A key \ufb01rst step in designing for meaningful contes-tation is to explore and understand the needs of decisionsubjects as well as the community more generally.AcknowledgementsHenrietta Lyons is supported by the Melbourne School ofEngineering Ingenium scholarship program. This researchwas partly funded by Australian Research Council Discov-ery Grant DP190103414 Explanation in Arti\ufb01cial Intelli-gence: A Human-Centred Approach. Eduardo Velloso is therecipient of an Australian Research Council Discovery EarlyCareer Researcher Award (Project Number: DE180100315)funded by the Australian Government.REFERENCES[1] Houston Federation of Teachers, Local 2415, et al vHouston Independent School District, 251 F.Supp.3d116 (2017).[2] Marco Almada. 2019. Human intervention inautomated decision-making: Toward the constructionof contestable systems. In Proceedings of theSeventeenth International Conference on Arti\ufb01cialIntelligence and Law. 2\u201311.[3] Emre Bayamlioglu. 2018. Contesting AutomatedDecisions. European Data Protection Law Review 4(2018), 433\u2013446.[4] Reuben Binns, Max Van Kleek, Michael Veale, UlrikLyngs, Jun Zhao, and Nigel Shadbolt. 2018. It\u2019sReducing a Human Being to a Percentage. Proc of the2018 CHI Conference on Human Factors in ComputingSystems - CHI \u201918 (2018).[5] Jenna Burrell. 2016. How the machine \u2018thinks\u2019: outcome control for fair algorithmic mediation.Proceedings of the ACM on Human-ComputerInteraction 3, CSCW (2019), 1\u201326.Understanding opacity in machine learning algorithms.Big Data and Society (2016), 1\u201312. [13] Gerald S Leventhal. 1980. What should be done withequity theory? In Social exchange. Springer, 27\u201355.[6] Virginia Dignum. 2019. Responsible Arti\ufb01cialIntelligence: How to Develop and Use AI in aResponsible Way. Springer.[7] Virginia Eubanks. 2018. Automating Inequality: HowHigh-Tech Tools Pro\ufb01le, Police and Punish the Poor. StMartin\u2019s Publishing Group, Hillsdale, NJ.[8] Organisation for Economic Co-operation andDevelopment. 2019. OECD Principles on Arti\ufb01cialIntelligence. (2019). Retrieved 30 January 2020 fromhttps://www.oecd.org/going-digital/ai/principles/.[9] Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P.Gummadi, and Adrian Weller. 2018. BeyondDistributive Fairness in Algorithmic Decision Making:Feature Selection for Procedurally Fair Learning. InProc of Thirty-Second AAAI Conference on Arti\ufb01cialIntelligence (AAAI-18). 51\u201360.[10] Tad Hirsch, Kritzia Merced, Shrikanth Narayanan,Zac E Imel, and David C Atkins. 2017. Designingcontestability: Interaction design, machine learning,and mental health. In Proceedings of the 2017Conference on Designing Interactive Systems. 95\u201399.[11] Anna Jobin, Marcello Ienca, and Effy Vayena. 2019.The global landscape of AI ethics guidelines. NatureMachine Intelligence 1 (2019), 389\u2013399.[12] Min Kyung Lee, Anuraag Jain, Hea Jin Cha, ShashankOjha, and Daniel Kusbit. 2019. Procedural justice inalgorithmic fairness: Leveraging transparency and [14] Sarah Myers West. 2018. Censored, suspended,shadowbanned: User interpretations of contentmoderation on social media platforms. New Media &Society 2, 11 (2018), 4366\u20134383.[15] Australian Government Department ofIndustry Innovation and Science. 2019. AI EthicsFramework. (2019). Retrieved 30 January 2020 fromhttps://www.industry.gov.au/data-and-publications/building-australias-arti\ufb01cial-intelligence-capability/ai-ethics-framework.[16] Independent High-Level Expert Group onArti\ufb01cial Intelligence. 2019. Ethics Guidelines forTrustworthy AI. (2019). Retrieved on January 30, 2020from https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai.[17] Cynthia Rudin. 2018. Stop Explaining Black BoxMachine Learning Models for High Stakes Decisionsand Use Interpretable Models Instead. (2018).[18] John Thibaut and Laurens Walker. 1975. ProceduralJustice: A Psychological Analysis. Lawrence ErlbaumAssociates, Hillsdale, NJ.[19] Sandra Wachter, Brent Mittelstadt, and Chris Russell.2018. Counterfactual Explanations without Openingthe Black Box: Automated Decisions and the GDPR.Harvard Journal of Law and Technology 31, 2 (2018),841\u2013887.",
                    "reference": "[1] Helena Liang Lyons, Ehsan Velloso, and Tim Miller. 2021. Fair and Responsible AI: A focus on the ability to contest. arXiv:2102.10787. Retrieved from https://arxiv.org/pdf/2102.10787."
                },
                {
                    "id": "5a3c8b8a-b612-49e8-8758-1b64688babd9",
                    "text": "As the use of arti\ufb01cial intelligence (AI) in high-stakes decision-making increases, the ability to contest such decisions isbeing recognised in AI ethics guidelines as an importantsafeguard for individuals. Yet, there is little guidance on howAI systems can be designed to support contestation. In thispaper we explain that the design of a contestation processis important due to its impact on perceptions of fairness andsatisfaction. We also consider design challenges, includ-ing a lack of transparency as well as the numerous designoptions that decision-making entities will be faced with. Weargue for a human-centred approach to designing for con-testability to ensure that the needs of decision subjects, andthe community, are met.",
                    "reference": "[1] Helena Liang Lyons, Ehsan Velloso, and Tim Miller. 2021. Fair and Responsible AI: A focus on the ability to contest. arXiv:2102.10787. Retrieved from https://arxiv.org/pdf/2102.10787."
                },
                {
                    "id": "d2bfd6ad-00f5-4822-b64a-671bad23ed54",
                    "text": "Contestability; explainability; algorithmic fairness, ethics.",
                    "reference": "[1] Helena Liang Lyons, Ehsan Velloso, and Tim Miller. 2021. Fair and Responsible AI: A focus on the ability to contest. arXiv:2102.10787. Retrieved from https://arxiv.org/pdf/2102.10787."
                },
                {
                    "id": "04748296-68b7-4903-956f-068cf5761ff0",
                    "text": "There is great potential for Arti\ufb01cial Intelligence (AI) to en-hance decision-making, by making it more accurate, ef\ufb01-cient, and scalable than human decision-making [4, 12].To harness these bene\ufb01ts, AI systems should be designedresponsibly, to ensure that they are fair, accountable, andtransparent [6]. This is particularly important given the in-creasing use of AI in high-stakes decision-making, includingsentencing, hiring, and loan application determination [12].",
                    "reference": "[1] Helena Liang Lyons, Ehsan Velloso, and Tim Miller. 2021. Fair and Responsible AI: A focus on the ability to contest. arXiv:2102.10787. Retrieved from https://arxiv.org/pdf/2102.10787."
                },
                {
                    "id": "fdc827f0-bdb2-4028-aff3-97dbd9d38906",
                    "text": "Examples of Ethical AIGuidelines Calling for theAbility to ContestEthics Guidelines for Trust-worthy AI (High-Level ExpertGroup on Arti\ufb01cial Intelli-gence): \"[T]here are manydifferent interpretations offairness, we believe thatfairness has both a sub-stantive and a proceduraldimension... The procedu-ral dimension of fairnessentails the ability to contestand seek effective redressagainst decisions made by AIsystems and by the humansoperating them.\" [16]Principles on Arti\ufb01cial In-telligence (OECD): \"Thereshould be transparencyand responsible disclosurearound AI systems to ensurethat people understand AI-based outcomes and canchallenge them.\" [8]AI Ethics Framework (Aus-tralia): (Principle 7) \"Con-testability: When an AI sys-tem signi\ufb01cantly impacts aperson, community, group orenvironment, there shouldbe a timely process to allowpeople to challenge the useor output of the AI system.\"[15] In response to calls for AI systems to be designed, devel-oped, and deployed responsibly, numerous AI ethics guide-lines have been produced. One \u2018safeguard\u2019 that is gainingtraction within these guidelines is the ability to contest AIdecisions (see sidebar for examples) [11]. Article 22(3) ofthe European Union\u2019s General Data Protection Regula-tion provides a legal \u2018right to contest\u2019 decisions made usingsolely automated processes. However, none of these doc-uments provide guidance on how AI systems should be de-signed to enable contestation. In this paper, we outline whydesign is important, what the design challenges are, andour human-centred approach to designing for contestability.",
                    "reference": "[1] Helena Liang Lyons, Ehsan Velloso, and Tim Miller. 2021. Fair and Responsible AI: A focus on the ability to contest. arXiv:2102.10787. Retrieved from https://arxiv.org/pdf/2102.10787."
                },
                {
                    "id": "631bc8f2-3d2a-4f50-8224-42fff8898136",
                    "text": "The importance of designing AI systems to enable \u2018con-testability\u2019 has been acknowledged in HCI and AlgorithmicAccountability literature (e.g. [10, 2]). Using a legal lens, theAlgorithmic Accountability work has taken a theoretical ap-proach to proposing requirements of a contestation scheme[3], and design requirements that enable contestation [2].Within HCI, the focus of contestability research has been onthe ability of expert users to work interactively with a systemto contest its output [10].HCI researchers [12, 4, 9] have also drawn on organisa-tional psychology literature to study how the design of AIsystems used in decision-making impact human percep-tions of procedural fairness, \u2018procedural justice\u2019 [18]. Theprocedural justice literature indicates that having a legiti-mate way to contest a decision increases a person\u2019s per-ception of procedural fairness, which can impact their per-ception of the fairness of the decision (\u2018distributive justice\u2019),their choice to accept or contest a decision, and their atti-tude towards the entity making the decision [18, 13]. In linewith this literature [13], Lee et al [12] found that having \u2018out-come control\u2019, the ability to correct or appeal a decision, in a cooperative group allocation task improved participants\u2019perceptions of the fairness of the outcome.The procedural justice literature also indicates that designof a contestation process (not just its availability) impactsperceptions of procedural fairness. For example, havingthe same decision-maker assess the decision on appealis seen as less fair than having a new decision-maker [13].In addition, in a study of content moderation across socialmedia platforms, Myers West [14] found that users weredissatis\ufb01ed with contestation processes, reporting a lack ofclear instruction about how to lodge an appeal, receivingno reply or resolution having lodged a challenge, and noaccess to human intervention. These \ufb01ndings indicate thatthe design of a contestation process matters.",
                    "reference": "[1] Helena Liang Lyons, Ehsan Velloso, and Tim Miller. 2021. Fair and Responsible AI: A focus on the ability to contest. arXiv:2102.10787. Retrieved from https://arxiv.org/pdf/2102.10787."
                },
                {
                    "id": "52da9c4c-69c4-469e-8e3d-b4351f9f2031",
                    "text": "To meaningfully challenge a decision, a decision subject re-quires some form of information in order to understand thedecision, decide whether to contest, and to use as groundsfor contestation. Many AI systems used in decision-makingare effectively \u201cblack boxes\" [17]; their decision-making pro-cesses are hidden due to the use of complex algorithmsor techniques (e.g. deep learning) or intentionally by com-panies to protect trade secrets [5]. This opacity makes itdif\ufb01cult to understand why a decision was made, and con-sequently, to contest it in any meaningful way. In contrast,with human decision-making a person can generally seekan explanation from the decision maker as to why a deci-sion was made. Often, in high-stakes decisions, reasonsmust be documented during the decision-making processto mitigate the issue of an inaccurate post-hoc explanation.Promisingly, the \ufb01eld of explainable arti\ufb01cial intelligence(XAI) is progressing work into explainability [19]. To date,XAI has not focused on providing explanations for contesta-tion speci\ufb01cally, which offers a new avenue of research.Sample of preliminary\ufb01ndings from our currentresearchAI systems are not isolated,but exist in socio-technicalcontexts with existing legalframeworks, political sys-tems, and social norms, thatneed to be considered whendesigning for contestationDifferent processes for con-testation are likely to berequired depending on thecontext in which a decision isbeing madeContestation processesneed to be clear and easy toaccessContestation processesshould align with humanrights, to ensure equality, bedesigned for accessibility,and to provide compensationLack of transparency is anissue; explainability is impor-tant A second design challenge is that there are many ways tocontest a decision [13]. For example, existing contestationprocesses for human decisions (e.g. internal review, com-plaints mechanisms, external review via tribunal or court)could be adapted for decisions made using AI. However,with decisions made at scale, leaving appeal processes to acourt to determine would overwhelm an already pressuredsystem. Low perceptions of fairness are associated withprocedures that are time consuming, costly and resourceintensive [13]. An alternative contestation process might in-volve a decision subject directly contesting a decision withinAI system via an interface. However, the novelty of this ap-proach coupled with a lack of human touch could negativelyimpact perceptions of fairness. With an abundance of de-sign choices, it is dif\ufb01cult to know where to begin.We suggest that taking a human-centred approach to ex-plore how people conceptualise contestability in relation toAI systems is a key \ufb01rst step in designing for meaningfulcontestation. To understand the needs of decision subjects,and the expectations of the community more generally, weare currently conducting a thematic analysis of submissionsmade to Australia\u2019s \u2018Arti\ufb01cial Intelligence: Australia\u2019s EthicsFramework\u2019, a discussion paper that proposed \u2018contestabil-ity\u2019 as a core ethical principle [15]. The sidebar contains asample of our preliminary \ufb01ndings.",
                    "reference": "[1] Helena Liang Lyons, Ehsan Velloso, and Tim Miller. 2021. Fair and Responsible AI: A focus on the ability to contest. arXiv:2102.10787. Retrieved from https://arxiv.org/pdf/2102.10787."
                },
                {
                    "id": "f571595b-b98d-4853-9c4c-20014bc857a7",
                    "text": "The increasing use of AI in high-stakes decision-making,which has been deployed without appropriate safeguardslike procedural fairness, has had a signi\ufb01cant, negative im-pact on thousands of people, from teachers losing their jobs[1] to the erroneous loss of medical bene\ufb01ts [7]. To reducenegative consequences, AI systems must be responsiblydesigned, developed, and deployed [6]. Though the abilityto contest decisions is not the only mechanism required to ensure that AI systems are \u2018fair\u2019, it is a crucial safeguard,and in some circumstances, a legal requirement. How ac-cess to contestation, and the contestation process itself, isdesigned is important given the impact on perceptions offairness and satisfaction. Yet, there are many design chal-lenges including opacity, and an abundance of design op-tions. A key \ufb01rst step in designing for meaningful contes-tation is to explore and understand the needs of decisionsubjects as well as the community more generally.",
                    "reference": "[1] Helena Liang Lyons, Ehsan Velloso, and Tim Miller. 2021. Fair and Responsible AI: A focus on the ability to contest. arXiv:2102.10787. Retrieved from https://arxiv.org/pdf/2102.10787."
                },
                {
                    "id": "0ce59e5a-ec39-487e-9ee8-4fd0031a7d04",
                    "text": "Henrietta Lyons is supported by the Melbourne School ofEngineering Ingenium scholarship program. This researchwas partly funded by Australian Research Council Discov-ery Grant DP190103414 Explanation in Arti\ufb01cial Intelli-gence: A Human-Centred Approach. Eduardo Velloso is therecipient of an Australian Research Council Discovery EarlyCareer Researcher Award (Project Number: DE180100315)funded by the Australian Government.",
                    "reference": "[1] Helena Liang Lyons, Ehsan Velloso, and Tim Miller. 2021. Fair and Responsible AI: A focus on the ability to contest. arXiv:2102.10787. Retrieved from https://arxiv.org/pdf/2102.10787."
                }
            ]
        },
        {
            "paper_title": "The use of responsible artificial intelligence techniques in the context of loan approval processes",
            "authors": "E Purificato, F Lorenzo, F Fallucchi\u2026",
            "publication_info": "International Journal of \u2026 - Taylor & Francis",
            "paper_url": "https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf",
            "chunks": [
                {
                    "id": "25d0794b-38f6-4ea4-bb51-62b286bef477",
                    "text": "",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "f65d7d62-0637-4a0d-a15f-09943d005680",
                    "text": "The ability to predict the occurrence of certain events inadvance has always been a critical factor in the financialand banking fields (Board, 2017; Heaton et al., 2016). Anestimation of the risk associated with the granting of a loanby a banking institution requires deep expertise and longexperience on the part of loan and credit officers exploitinginformation related to the customers, concerning their per-sonal data, financial situation and credit history, as well asthe entity of the specific request made. Quoting the study ofDataRobot (2019), \u201cToday, you would be hard-pressed toidentify a line of business or function in a bank that does nothave multiple needs for predictive analytics\u201d and the amountof data required for the predictive analysis in money lend-ing, to which past information about granted loans must beadded, make this one of the most interesting applicationfields for artificial intelligence (AI) techniques of the wholebanking sector.In recent years, machine learning (ML) models havebeen used to predict stock prices (Hagenau et al., 2013;Schumaker & Chen, 2010), to identify the presence of can-cers (Hirasawa et al., 2018), or as in the case study presentedin this article, to decide whether to grant a loan to bank customers (Arun et al., 2016). Given the specific field ofapplication, the risk associated with the prediction beingcomputed may vary significantly. As reported in severalstudies (Goebel et al., 2018; Holzinger et al., 2017), althoughAI systems are equaling, or even exceeding human perform-ance, in many fields, their use is still viewed suspiciouslyand the human experience is considered irreplaceable(Jarrahi, 2018). There are contexts where understandingmotivations leading to a specific result is more importantthan the result itself, and it is crucial to be able to under-stand the reasons why a prediction was made to build trustin the decisions taken by a model. Trust is one of the keyfactors that influence the adoption of ML techniques insidehigh-risk applications and brought about the rise of thefields of study called explainable artificial intelligence (XAI)and responsible artificial intelligence (RAI).Due to this ubiquity, concerns are starting to arise aboutwhether the development of AI systems, and the decisionsmade by them, should be based on a set of ethical principlesto promote transparency, social equity, sustainability, andavoid social injustices. In particular, considering our casestudy on automatic predictions in loan approval processes,several critical elements in the European law have to be takeninto account when individuals are assessed by such analgorithm (Commission, n.d.; Goodman & Flaxman, 2017):their rights to not be subject to an automated decision in thefirst place, their right to get an explanation of the decisionand their right to non-discrimination. As well-reported in thearticle written by Angel Perez for 2021.AI, \u201cFairness inMachine Learning,\u201d ML practitioners should develop modelsthat, by design, take care of possible discriminations and thatare explainable to users, requiring high transparency andreproducibility throughout the whole ML workflow.A significant contribution in this direction has been pro-vided by the High-Level Expert Group on AI (AI-HLEG,2019), appointed by the European Commission, that pre-sented the document \u201cEthics Guidelines for TrustworthyArtificial Intelligence.\u201d As the guidelines\u2019 authors note, theconcept of trustworthy AI is made of three main compo-nents: compliance with existing laws and regulations (lawfulAI); alignment with society\u2019s ethical principles, even in thosesituations in which no regulation has been developed yet(ethical AI); robustness both from a technical and social per-spective to avoid incorrect behaviors that may cause unin-tentional harm (robust AI). The AI HLEG group identifiesfour ethical principles that must be satisfied for an AI sys-tem to be considered trustworthy: respect for human auton-omy, prevention of harm to other human beings, fairness ofthe AI system\u2019s decisions, and explicability of the outcomeof an AI system.According to the guidelines, AI systems should provideclear explanations for their outputs, and the way a systeminteracts with a user should never be interpreted as thoughthe decision were made by a human rather than a machine.Explainable AI has been defined by Gunning (2017): \u201cXAIwill create a suite of machine learning techniques that enableshuman users to understand, appropriately trust, and effect-ively manage the emerging generation of artificially intelligentpartners.\u201d Extending this concept to include principles likefairness, we can refer to responsible AI, the definition ofwhich has been provided by Dignum (2018, 2019):\u201cResponsible AI is about human responsibility for the develop-ment of intelligent systems along with fundamental humanprinciples and values, to ensure human flourishing and well-being in a sustainable world.\u201dWithin the AI community, the debate about the need forexplainability is very heated. For instance, G. Hinton consid-ers the constant search to explain how an AI system worksa \u201ccomplete disaster\u201d (Simonite, 2018). Our point of view isopposite to that, and according to Miller (2019b), we con-sider explainability significant for two main reasons: trust,because people cannot just read some statistics about themodel performance and believe a decision is correct, andethics because we have to prove that a developed system isnot producing discrimination of any kind. Thus, a successfulXAI, or more specifically, RAI system, must relate to socialsciences (Miller, 2019c). The evaluation of explainable userinterfaces (UIs) is another crucial topic. Even though the sat-isfaction of users interacting with systems providing explan-ations (in contrast with systems without them) can be seenas an easy and predictable outcome, we believe a concreteevaluation is always needed, especially in the domain where dealing with explainable UIs is not a daily routine. Ourclaim is also supported by some evidence in literature:Millecamp et al. (2019) showed that in certain contexts andfor specific users, explanations could even create a lack ofconfidence in the system; Wang et al. (2022) showed thatusers could prefer a biased model instead of an unbiasedone, in case of the lack of proper result explanations.Linked to explainability and ethics is the concept of fair-ness. The capability to understand how a prediction wasmade can help expose the ML model\u2019s discriminatory behav-ior, thus allowing detecting and mitigating biases derivingfrom the examples provided by humans as the foundationon which these models are built. As a result, predictionsmade by these systems may favor a majority group oversome minorities.In this regard, a canonical example comes from theCOMPAS algorithm, which several courts use in the U.S. asa risk assessment tool to estimate the probability of a personcommitting another crime. Based on the algorithm predic-tion, judges use COMPAS to decide whether to release anoffender. An analysis published by ProPublica (Angwinet al., 2016) has highlighted even more the problem withinthe scientific community (Washington, 2019), demonstratinghow the algorithm was unfairly judging black offenders,which were wrongly labeled as high-risk individuals atalmost twice the rate as white defendants.Strictly related to the desire of building a valuable, trust-worthy AI system, there is the need to develop an under-standable and easy-to-use user interface (UI), which iscurrently one of the weak points of research in the XAI field(Abdul et al., 2018).The scope of our work is to show how the use of explain-ability and fairness techniques can lead to the growth of adomain expert\u2019s trust and reliance on AI systems. With thisaim, four functionalities are proposed: a dataset & MLmodel handler, a standardized explainability tool, a fairnesstool, and a feedback loop. The first of these componentsallows users to load a dataset and preprocess it, training sev-eral ML models to find the most performing one on theprovided dataset and monitor its performance. The standar-dized explainability tool provides methods to get explana-tions for each prediction; since our goal is to build a tool tosupport the creation of a responsible AI system regardless ofthe specific ML model it is based on, only model-agnosticsolutions are employed. The fairness tool grants users theability to detect biases within the model\u2019s behavior througha proposed algorithm based on disparate impact metrics andmitigate them using the reweighing algorithm that followsthe independence criterion, one of the criteria that has legalsupport. An unbiased version of the original model can betrained at the end of the described procedure. The feedbackloop allows a domain expert to judge the model\u2019s resultswith the related explanations to create a new ground truthto train a new, more performing ML model.We applied the presented system to the context of theloan approval process developing a proprietary frameworkwith an intuitive UI and demonstrating its effectivenessthrough experimental results from field tests and subsequentuser studies. An experimental session for choosing the bestexplainability algorithm to use in the developed frameworkstate-of-the-art Explanationis performed following theGoodness Scale; a novel Trust & Reliance Scale is proposedto evaluate the system explainability, while an A/B test iscarried out for assessing the fairness feature; finally, we per-to evaluate the UI. Results areformed a usability testobtained by submitting the mentioned scales, tests, and theusability questionnaire, respectively, to data scientists andresearchers the explainability algorithms and bankdomain experts and loan officers to evaluate the otherfunctionalities.forThe article is structured as follows. In Section 2 some ofthe recent and most relevant research works and platformspublished or released in the last years about explainabilityand fairness are presented. The system approach and theproposed architecture for supporting the whole life cycle ofan ML model are discussed, respectively, in Sections 3 and4. In Section 5 the case study is illustrated. Experimentalevaluations are discussed in Section 6. Evaluation scales andquestionnaires are reported in Appendix. Conclusions andfuture work are discussed in Section 7.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "6fb5ee99-feca-408f-94d2-742cb8b0dd8e",
                    "text": "This section presents some relevant related research worksand commercial platforms regarding our context of interest.Because of our scenario\u2019s complexity and heterogeneity, weseparately discuss the literature on explainability and fair-ness. From the analysis of the following state of the art, wehave derived the foundations of our system\u2019s approach,which is based on the limitations of current XAI systems.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "3b71365f-d0b7-46ee-82b0-c421c285fdd1",
                    "text": "As the amount of data being collected grew and ML modelsbecame faster and more accurate, their applications haveextended to several different fields and stakeholders (Preeceet al., 2018). This is why it is becoming crucial to be able toexplain the reasons behind any predictions.aboutsurveys concepts,Comprehensive taxonomies,issues, methods, and challenges in the fields of explainabilityand XAI have been provided in the last three-year period(Adadi & Berrada, 2018; Barredo Arrieta et al., 2020; Biran& Cotton, 2017; Gilpin et al., 2018; Miller, 2019a; Muelleret al., 2019).In recent years many algorithms have been designed toprovide insights into which features are most likely to influ-ence the model predictions. They can be divided into twocategories: model-specific solutions and model-agnostic solu-tions. The first one is tailored to specific model classes. Theycan provide further insights into a model prediction byexploiting the specificities of the model class of interest,both for shallow ML models, such as ensemble classifiers(Palczewska et al., 2014; Rajani & Mooney, 2018) and SVMs(Landecker et al., 2013), and deep ML models, such asmulti-layer neural networks (Shrikumar et al., 2016) and(Selvaraju et al., 2017).convolutional neural networks Algorithms belonging to the second category aim to beapplied to any ML model. The strategy beyond these techni-ques consists in considering the model as a black box, andthey work by analyzing only the input features and themodel output. Relevant contributions to this category areLIME (Ribeiro et al., 2016b) and its variants (Ribeiro et al.,2016a), SHAP (Lundberg & Lee, 2017), based on the conceptof the Shapley values derived from games theory (Shapley,1953), and Anchors (Ribeiro et al., 2018).Most of the commercial platforms developed in the XAIfield exploit at least one of the aforementioned techniques,sometimes results.Usually, they can be helpful to data scientists and research-ers, but less for non-expert users. Some popular platformsand frameworks are briefly presented in the following.improving them to achieve betterIBM Watson OpenScale is a commercial solution belong-ing to the IBM Cloud suite, introduced to provide a plat-form that could be used by businesses to operationalize theirAI systems and to extend their deployments to the wholeenterprise. It offers several tools that help data scientists andmanagers monitor and understand their model\u2019s outcomes.OpenScale not only provides an online application to navi-gate through the results employing a graphical user inter-face, but it also offers an API that allows accessing theplatform\u2019s services programmatically. It currently providestwo different explainability techniques: LIME (Ribeiro et al.,2016b) and a variant of MACEM (Dhurandhar et al., 2019).While OpenScale does not directly provide the capability tomanage the user\u2019s datasets and train custom models, thesetasks are achieved by integrating OpenScale with the rest ofthe to whichOpenScale belongs includes other tools that assist the userduring all the steps related to developing a custom MLmodel. Several factors might limit the application of a com-mercial solution, such as OpenScale. The first one, and inmany cases the most crucial argument against OpenScale, isits expensiveness (although the cost comes with productsupport, documentation, and bug fixing, as well as brandreliability, it is not adequate for a bank institution that hasnot its core business in AI). Other issues are related to thelack of control over what happens inside OpenScale, and therequirement to have the training data stored on the IBMCloud platform if a user does not want to choose theirhybrid cloud solution, named IBM Cloud Pak for Data, thatprovides OpenScale on customer\u2019s machines.IBM cloud services. Indeed, suitetheGoogle What-If Tool (WIT) is an interactive tool thatallows users to investigate the model\u2019s behavior and perform-ance through a visual interface. This tool is an initiative ofGoogle\u2019s People \u00fe AI Research (PAIR) team. It has been pro-posed to enable people to evaluate machine learning modelswithout the need to write complex code. Through WIT, auser can investigate the behavior of multiple models, com-pare them, and extract insights from them. The visualapproach followed by WIT leverages the predictions obtainedfrom a test set to provide customizable graphs which offer abetter understanding of the relationship between differentattributes and the predicted label. The tool is also helpful forinstances and observing how eachcomparing differentprediction changes as the values of its attributes are modi-fied. While WIT is excellent for a data scientist trying tounderstand his model better, domain expert users wish for astraightforward explanation about why a specific result hasbeen obtained.Google Cloud Explainable AI is a set of tools and frame-works developed to aid data scientists to build interpretableML models. With it, developers can understand feature attri-butions in AutoML Tables and AI Platform (the GoogleCloud Platform catalog provides both tools) and visuallyinvestigate model behavior using the What-If Tool. It alsosimplifies model management using the AI Platform. GoogleExplainable AI leverages the integration of Google Cloud\u2019sAI Explanations service into AI Platform Prediction to pro-vide feature attribution. As for IBM Watson OpenScale, itneeds other services from Google Cloud Platform (GCP) tomake users able to manage the whole life cycle of an MLmodel. GCP\u2019s AI Explanations offer three methods to usesampled Shapley (Maleki et al.,for feature attributions:2013), integrated gradients (Sundararajan et al., 2017), andXRAI (Kapishnikov et al., 2019). Each of the mentionedmethods is based on Shapley values, and users can selectwhat they prefer for their explanations requests.AI Explainability 360 is a good open-source software tool-kit addressed to different stakeholders, from domain experts tosystem developers. It allows exploring eight state-of-the-artexplainability methods evaluation metrics.Noteworthy is the provided effective taxonomy to help to navi-gate the space of explanation methods, not restricted to thosein the toolkit, but also in the literature (Arya et al., 2019).and twoSeveral XAI research works presented in different fields,such as HCI (Abdul et al., 2018; Zhu et al., 2018), visualanalytics (Tamagnini et al., 2017) and medicine (Holzingeret al., 2017; Lamy et al., 2019) denote that most researchersare focusing on new algorithms, and not on usability or effi-cacy effective UIs understandable by non-expert users.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "00c98ad7-2470-44cb-b674-48e960670f63",
                    "text": "Defining a fairness criterion, to evaluate a system to considerit as fair, is a complex task due to the different ways biasescan arise. What can be considered fair in a specific contextmight be unfair in another one. Furthermore, different peoplehave different sensibilities about what is fair and what is not,and what is fair considering individuals or populations as agroup (Binns, 2020). Three criteria are commonly contem-plated: independence, separation, and sufficiency. Independencecriterion requires the sensitive characteristic to be statisticallyindependent of the score. Variants of this criterion includestatistical parity (Dwork et al., 2012), group fairness (Friedleret al., 2016; Yeom & Tschantz, 2018), and disparate impact(Feldman et al., 2015). Separation criterion seeks to acknow-ledge the existence and rightfulness of the correlation betweenthe sensitive feature and the target variable to the extent thatthe target variable justifies it. It appears under differentnames, such as equivalent odds (Hardt et al., 2016) and dis-parate mistreatment (Zafar et al., 2017). Sufficiency criterionis based on the idea that the sensitive feature is already subsumed in the score used for predicting the target label(Chouldechova, 2017; Kleinberg et al., 2016).Bias mitigation algorithms are mainly based on two fac-tors: the criterion selected for measuring fairness and the stepof the ML process in which it is applied. The steps involvedare three: pre-processing, in-processing, and post-processing. Inpre-processing, the goal is to produce a new representation ofthe training set in which the information correlated to the setof non-sensitive features is preserved, ignoring the informa-tion of the sensitive feature [e.g., reweighing (Kamiran &Calders, 2012), disparate impact remover (Feldman et al.,2015), optimized pre-processing (Calmon et al., 2017)]. Theaim of the in-processing step is to enforce fairness by chang-ing the learning strategy of the model at training time (e.g.,adversarial debiasing (Zhang et al., 2018), disparate impactremover). The algorithms of the post-processing step try tosatisfy fairness constraints by slightly modifying the output ofa model without the need to change the training data orretrain the model. These algorithms are usually used whenthe two previous approaches are not viable because the train-ing dataset or the ML algorithm is not accessible [e.g., rejectoption classification (Kamiran et al., 2012), equalized oddspost-processing (Hardt et al., 2016)].The application of fairness criteria to ML models is atopic that has received much attention lately, thanks togreater awareness about the risks that unfair AI systemsmight pose to specific social groups. Relevant research worksabout the nascent field of Fair ML have been published inthe last years (Chouldechova, 2017; Corbett-Davies & Goel,2018; Holstein et al., 2019).Not only has this popularity led to a growth in the num-ber of scientific publications related to fairness, but it alsoencouraged the introduction of several tools whose goal isto monitor the behavior of a model to alert the user in caseof unfair treatment. An example is IBM AI Fairness 360(AIF360), which is perhaps the most considerable open-source toolkit available for ML fairness. is to\u201cexamine, report, and mitigate discrimination and bias inML models throughout the AI application lifecycle.\u201d It is anextensible framework capable of unifying most of the met-rics and algorithms presented in this chapter. It alsoincludes a bias explanation feature that gives further insightsinto the computed metrics (Bellamy et al., 2018).Its goalIn addition to the capabilities already presented on explain-ability, even IBM Watson OpenScale gives the possibility to setup a monitor to track the fairness of the model at hand. Thepresence of biases in the model is estimated based on the dis-parate impact metric. One of the main OpenScale\u2019s limitationsis that the privileged and unprivileged groups must be selectedin advance when setting up the fairness monitor. This oper-ation may become cumbersome as the cardinality of the sensi-tivity attribute grows. Moreover, the user may not be aware ofwhich value belongs to which group.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "fc1a0052-7c82-437f-9b43-5780a778ab01",
                    "text": "Although loan approval processes might benefit from theintroduction of automated systems (i.e., ML models) tosupport the decision task, several factors have limited theirapplication in this field so far: loan approval processes arehigh-risk activities that require officers to understand themotivation behind every ML model prediction. It is notenough to demonstrate that a model performs well if con-sidered only a black box. With skeptical users the ability toexplain how it works, which data is important, and when iscrucial; model decisions have a considerable impact on thefuture of the customers who applied for the loan, and theymust be provided with explanations about why their applica-tion has been rejected; such a decision must be devoid ofbiases to ensure that individuals with different origins, cul-tures, and backgrounds are treated fairly.The main aim of the system presented in this article is toovercome the above challenges by proposing a single solu-tion to create a comprehensive trustworthy intelligent systemexploiting the principles of explainability and fairness. In thefollowing of this section, an in-depth presentation of thetwo concepts is provided, focusing on their relevance inthis article.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "d8e99c61-8550-4acd-81a8-36c457f545bb",
                    "text": "What do we mean by \u201cexplainability\u201d? Explainability is consid-ered the core of each AI system that aspires to be consideredtrustworthy, and although it is formally different from inter-pretability, the two terms are considered closely related in lit-erature (Biran & Cotton, 2017). Indeed, interpretability can beconsidered a static characteristic of a model, referring to thecapability to explain its meaning in a human-understandableway. At the same time, explainability is a dynamic characteris-tic of a model representing actions and procedures beneficialto provide explicit knowledge about why a specific predictionwas made using easy-to-understand terms. This means thatexplainability depends on the people who need to understandthe model, so the most appropriate definition could be: \u201cGivena certain audience, explainability refers to the details and rea-sons a model gives to make its functioning clear or easy tounderstand\u201d (Barredo Arrieta et al., 2020). This property iscrucial for building trust in the decisions taken by a model,and it is one of the critical factors that influence the adoptionof ML techniques inside high-risk applications. There are sit-uations in which having the capabilities to interpret a resultand understand the factors that contributed to it is far moreimportant than having a high-performance model. This is oneof the reasons why simpler algorithms, such as decision treesand K-nearest neighbors (KNN), are widely used despite beingless accurate than other options like neural networks and sup-port vector machines (SVMs).As our purpose is to develop a Trustworthy AI systemfor loan approval processes regardless of the specific MLis based on, only model-agnostic solutions (seemodelSection 2.1) are used in the implementation.it",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "e5922ec9-90a1-46d6-926e-5ea4ad293a2a",
                    "text": "To satisfy the principle of fairness, users must be aware ofexisting biases (i.e., prejudices) that may lead AI systems to discriminate against certain groups of people or individuals.Furthermore, every AI system should be accessible to peopleof any age, gender, and ability (AI-HLEG, 2019).So far, no standard definitions of fairness have beendrawn up. In the context of decision-making, fairness canbe formalized as \u201cthe absence of any prejudice or favoritismtoward an individual or a group based on their inherent oracquired characteristics\u201d (Mehrabi et al., 2019). A recentstudy about how people perceive fairness in the context ofloan allocations (Saxena et al., 2020) shows a preference fora specific definition, named calibrated fairness (Liu et al.,2017), that aims to select individuals in proportion to theirmerit. In particular, that study demonstrates that officerschoosing between two loan applicants tend to prefer to splitthe money in a proportion of their loan repayment rates,instead of an \u201cequal\u201d (50/50) splitting, or giving all themoney to the candidate with the higher payback rate. The\u201cratio\u201d decision is allowed under calibrated fair-ness definition. theA machine learning model can be biased because thesesystems are trained by examples: when using historical datafor modeling human behaviors, the provided sample reflectsthe prejudices of the people who made these decisions inthe first place.The choice of the right bias mitigation algorithm, amongthose described in Section 2.2, is constrained by several rea-sons: the definition of fairness may vary from case to case;different criteria cannot be pursued simultaneously and eachalgorithm mainly focuses on a single definition (the reweigh-ing algorithm is based on the independence criterion, whilethe disparate mistreatment remover technique uses the separ-ation definition); the step of the ML model pipeline in whichthe user is allowed to intervene: as a rule of thumb, the ear-lier the algorithms are applied, the most flexible and effectivethe intervention will be; the requirements of the algorithmitself: for instance, the equalized odds post-processing tech-nique, despite being a post-processing strategy, requiresaccess to the sensitive feature to compute the right label.Other algorithms have some limitations in terms of the typesof classifiers they can be applied to. Some algorithms, suchas reject option classification, are deterministic, while othershave a randomized component (e.g., disparate mistreat-ment remover).As detailed in Section 4.3, in the presented system wefollow the independence criterion and choose the reweighingalgorithm for bias mitigation. Independence is frequentlyused in literature because it is one of the few criteria havinglegal support. The so-called 80%-rule, or four-fifth rule,specified in the U.S. Equal Employment OpportunityCommission guidelines, prescribes that a selection rate forany group (classified by race, orientation, or ethnicity) thatis less than four-fifths of that for the group with the highestrate constitutes evidence of disparate is,discriminatory effects on a protected group (Biddle, 2006).impact, thatIn the next section, a top-down description of the systemcomponents is provided, from a logical overview up to theillustration of the UI of the developed framework, goingthrough an in-depth presentation of the explainability andfairness tools. name used as an identifier. The id attribute is also used toretrieve the content of the dataset from the local storage.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "fd1b414d-5115-4aac-95be-d2e003edaf72",
                    "text": "This section first provides an overview of the proprietaryframework developed for the implementation of the casestudy. It then illustrates the core of the presented system,that is, the application of explainability and fairness princi-ples to the context of a loan approval process.The developed framework provides all the following func-tionalities to ensure complete management of the ML modellife cycle, grouped by their high-level purpose, as shown inFigure 1.The dataset & ML model handler allows users to: load adataset and store it through a procedure involving a fixedpreprocessing step, a custom setup, and a fairness check forpreliminary bias detection; find the best ML model to useby training at the same time several models with differentalgorithms and evaluating them through standard metrics;monitor models performance using various metrics (thesame with which a model is evaluated after training).The standardized explainability tool provides users withthe ability to get explanations for each prediction to makethem (i.e., both loan officers and loan applicants) able tounderstand clearly the features (or attributes) that mostinfluence the results, both positively and negatively.The fairness tool provides a fairness check and a biasmitigation process. It can identify the presence of biaseswithin the trained model using one of the state-of-the-artfairness criteria (chosen for the reasons explained in Section4.3) to make users aware that they are supported or judgedby a fair system, with the possibility, otherwise, to retrain anew unbiased version of the same model.The feedback loop allows a domain expert (e.g., the loanofficer) to give feedback about a specific prediction to createa new ground truth and build a new ML model, hopefullywith better performance.From a conceptual and schematic point of view, the pro-posed framework is described in the class diagram in Figure 2.A brief description of each class is provided below.Dataset describes the dataset used to train the model. Itis characterized by the number of rows of the dataset and a Label represents the values that can be assigned to thelabels of a dataset. Each possible couple (dataset_id, label_-value) is described by an instance of this class. The informa-tion about its value and its number of occurrences in theassociated dataset is added to distinguish each label.Model denotes the model trained using a specific dataset.Each model is described by an identifier, a descriptive name,and the date it was added to the system. The Booleanthe model wasunbiased attribute is used to specify ifobtained after the application of a bias mitigation algorithmto another model.MLAlgorithm is used to describe the algorithm withwhich the model is trained. It is also employed to providesome additional information to the user during the biasmitigation process.PredictionData is used to represent the prediction com-puted by a model for a given input instance provided by theuser. The outcome of the prediction is stored as the probabil-ity returned by the model. The entity also includes, for eachattribute of the instance being predicted, its feature value andthe related weight (or score) obtained using a model-agnosticinterpretability algorithm (e.g., LIME or SHAP).FeedbackData describes feedback provided for a givenprediction. It is represented as a Boolean attribute, wheretrue means that the prediction aligns with the expectation ofthe user or with the actual outcome.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "89c46ec1-448a-4135-9a86-9d2e80d79bec",
                    "text": "A concise representation of the functionalities provided bythe developed framework is shown in Figure 3. Each appli-cation flow and its main components are described below.For a better understanding of the diagram, two premises areto be made: components having equal shape, size and nameare intended to be the same; they are duplicated only forbetter visualization; black dashed lines in the diagram repre-sent the connection between the data and the specific proc-esses used.Loan Approval System User Interface allows users toselect the functionality to run through the Tab menu. It isimplemented in an internet application with an intuitivelayout. The application is shortly illustrated below in thissection to show how the UI looks to its users.Load dataset is the functionality to load a dataset andstore it in the system. Intuitively, the first time the systemstarts, this is the only available functionality. Before effect-ively storing the loaded dataset, a preliminary predefinedData Preprocessing is required to prepare the data for thenext steps. The Dataset setup component allows users tocheck and modify dataset parameters, including the name tosave it under, columns names, and data types. Differentdatasets can be loaded and stored into the system simultan-eously to be selected by users when needed. Through the ML model training functionality, the train-ing phase can be started once a dataset is stored and avail-able for selection. Several models are built at the same timeusing different ML algorithms. Trained models are presentedto users along with metrics (i.e., Accuracy, Precision, Recall,F1-score) to evaluate the performance of each of them andcompare each other to select the best one to store inthe system.To request a prediction, users can select one of thestored ML models and query it to obtain the predictionresult and its explanation. In our case study, predictions arethe probabilities that customers can repay a loan given theirExplainer interface is the common interface that stand-ardizes the access to the different interpretability algorithmsto anotherto switch from one explainerand allowssmoothly or to use different explainers at the same time.Each explainer is initialized using the configuration class pre-viously described and their explanations are generatedthrough the compute_explanation method. Depending onthe specific implementation, the resulting output can bereturned in different formats (e.g., lists or dictionaries).Three different the interface illus-implementations oftrated above are possible in the presented framework. Eachof these classes, listed in the following, leverages some otherservices to work correctly; initialization and formatting ofthe requests to these external packages are handled by theexplainer and configuration components.LimeExplainer produces a score for each feature basedon the LIME algorithm (Ribeiro et al., 2016b), but normal-izes it so that, by summing up all the scores, the total valueamounts to 100. This operation is made to avoid a misun-derstanding of LIME results.ShapExplainer is a wrapper around the SHAP algorithmimplementation (Lundberg & Lee, 2017). ishandled to produce a result similar to the one provided bythe LimeExplainer. Its outputAnchorsExplainer is based on the Anchors algorithm(Ribeiro et al., 2018) and, as in the previous ones, leveragesthe implementation proposed by the article authors and pro-vides a standardized output score.The standard process of the explainability tool is shownin Figure 5. First, a configuration object is created by theservice layer using the original training dataset. Second, thegenerated instance is used to initialize an explainer object.In the third step, the service layer requires the new explainerobject to explain a given instance. Since the actual interpret-ability algorithm requires the instance to be provided in aspecific format, the explainer exploits its internal configur-ation to prepare the given instance accordingly. Once thecredit histories and some personal data as the input. Oncethe output is computed, users (domain experts at this point)can give feedback about the specific prediction to enable thepossibility to monitor the results of in use.Predictions, explanations, and feedback are saved in aninternal Model results storage. the modelModel performance monitoring functionality exploitsthe feedback collected from users to compute statistics aboutthe performance of the managed models (same metrics as inthe ML model training).Checking dataset/model fairness and bias mitigationprocess works as follows. Both the original stored datasetand the model in use can be inspected to check the presenceof any biases. While checking the dataset is meant for diag-nostic purposes to trace back the unfair attribute to thestarting labels distribution, the information derived from themodel predictions is used to monitor how a sensitive attri-bute influences the model behavior. If some decisions areconsidered to be unfair, then an unbiased model can betrained and stored.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "4ec19a72-1f0d-447f-b439-fee9955845b9",
                    "text": "By design, the developed framework provides several waysto obtain the given prediction.Components of this generalized and standardized explain-ability tool are described below and represented by the dia-gram depicted in Figure 4.interpretation of aConfiguration class performs the preprocessing stepsrequired to use the explainability algorithms and to trainexplainer models. Starting from the dataset, categorical andnumerical features are extracted to be evaluated, and theone-hot encoding procedure is applied to categorical values.The configuration class is also used to prepare the instanceto be explained for the application of the interpretabilityalgorithms by exploiting the data extracted during the men-tioned initialization phase.explainer method has finished its computation, explanationscan be retrieved by the service layer. relevant number of instances, while the 0.8 threshold hasbeen selected to comply with the 80%-rule (Biddle, 2006).",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "05803bdd-5bb6-4420-85a0-89db1a7222a8",
                    "text": "The second most important feature the presented systemprovides is the capability to inspect the original dataset labeldistributions and the trained models\u2019 behavior for biasdetection and potentially training an unbiased model. A dis-cussion about these processes follows.Our system can inspect both the original dataset andmodels\u2019 behavior to determine the lack of fairness. As forthe dataset, its rows are used to identify if a bias can betraced back to the original data (this feature is meant fordiagnostic purposes); instead, the system uses the predic-tions made by a model to analyze its behavior. If a bias isdetected from the model predictions, then an unbiased ver-sion of the model can be trained and stored for future pre-dictions. Apart the proposedfrom these differences,Algorithm 1 describes the bias detection procedure for boththe original dataset and trained models.The labeled dataset in input D (which also refers to thedata structure in which the results of the model predictionsin use are stored) is inspected for biases using the disparateimpact metric (Feldman et al., 2015). As part of the process,dataset rows d 2 D are grouped based on the sensitive attri-bute value s. These groups are later referred to as sensitivegroups g",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "e971c2c4-dedf-4274-b733-59f3c7ebd5a4",
                    "text": ": For each sensitive group g, positive outcomesratio is computed, and G is split into one or more privilegeclasses C 2 C, where C \u00bc fg j g 2 G, 0 < i (cid:2) jGjg, basedon two factors: each privilege class C must represent at leastthe 5% of the entire population of D, and the disparateimpact between C and any other sensitive group g (or viceversa) must be lower than 0.8. The first constraint is appliedto guarantee that each privilege class contains a statistically The output of the algorithm is a set of privilege classesC, where the class with the highest rate of positive outcomesis considered to be the privileged class, and the other classesare referred to as the unprivileged classes. If C turns out tohave cardinality >2, then the dataset or model being eval-uated is considered to be biased.Algorithm 1. Bias detection procedure.procedure ComputePrivilegeClasses(D)C \u00bc ;;C \u00bc ;;C \u00bc ;;G   Select (cid:3) From D GroupBy s;for all g 2 G doif \u00f0len\u00f0C\u00de (cid:4) t\u00de and \u00f0disparate \u00f0C, g\u00de < 0:8\u00de thenC   C [ fCg;C   ;;end ifC   C [ g;end forC   C [ fCg;return C;end procedureFollowing the procedure mentioned above, if the modelresults are unfair, the system provides the functionality totrain an unbiased version of the same model. Algorithm 1 isused to split the model into two classes: the privileged class,i.e., the set of the sensitive feature values with the highest ratioof positive outcomes, and the unprivileged class, which con-tains the remaining values. It must be noted that there maybe values of the sensitive feature for which no predictions areavailable yet. Based on the criterion described above, these val-ues will be assigned to the unprivileged class. The rationalebehind this choice is that, in a situation where the system hasno information about how a model perceives a specific value,assigning it to the unprivileged class, the system is guaranteednot to exacerbate pre-existing unknown prejudices.Once the division of the feature values between privilegedand unprivileged classes has been determined, the reweighingalgorithm (Kamiran & Calders, 2012) is used for bias mitiga-tion. This algorithm was chosen for multiple reasons: the sys-tem has access to the dataset used for training the inspectedmodel, so a preprocessing strategy like the reweighing algo-rithm, which is likely to provide better results, can beapplied; the reweighing algorithm bases its decision on theindependence criterion, that is the exact fairness definitionused to perform the distinction between privileged andunprivileged groups, and as previously mentioned, this defin-ition has legal support; the algorithm output is a set ofweights, which is easier to interpret than other techniques.Once the new set of weights has been determined, thethe inspected model can be trainedunbiased version ofusing the same ML algorithm used for the original modeland then stored and made available to be queried.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "f04333fc-51d9-432a-a67e-116ae8a3556d",
                    "text": "The focus of this section is on illustrating the application ofthe described system to the context of a loan approval process and providing an overview of the UI of the devel-oped framework to show how it looks to users without fur-ther details about its technical implementation.The diagram in Figure 6 shows how the functionalitiesexplained in the previous section are accessible to the differ-ent types of users who can access the presented systemthrough the developed framework.Figure 7 shows the screen through which users can loada dataset. In our case study, the data has been provided tous by an Italian banking institution (after a pseudonymiza-tion process) to build a prototype based on real data. Thescreen displays the characteristic of the loaded dataset andthe results of preliminary bias detection.After the dataset has been loaded users can select it andautomatically train a new ML model (Figure 8) consideringdifferent algorithms (in the presented case study, since it is abinary problem, we have chosen Logistic Regression, RandomForest, and Naive Bayes algorithms). As the dataset is unbal-anced, the system ranks the trained models based on the F1-score metric. It is also possible to compare the selected modelwith one of those already stored in the system.After requesting a prediction, users can consult the modeloutcome with the relative explanation in an intuitive layout.As Figure 9 shows, the prediction result, along with itsprobability, is presented in the top-left box, while the relatedexplanation is in the correct box (in the provided example,they are generated using the SHAP algorithm). Finally, theuser can provide feedback on the prediction by clicking onthe buttons contained in the bottom-left box.By navigating to the \u201cFairness\u201d tab, the user is presentedwith the privilege class division for the most recentlyuploaded dataset (Figure 10). In the presented case study,we have considered the \u201cnationality\u201d as the sensitive feature;the displayed partitioning is obtained by applying the pro-cedure described in Algorithm 1. Within the same screen,selecting the \u201cTraining\u201d option in the navigation menu onthe left, users can request the system to train a new version of the model by exploiting the reweighing bias mitigationalgorithm. Once the training is completed, the model is per-manently stored in the system, along with its previous ver-sion, and can be selected in the \u201cPrediction\u201d tab of the maininterface to request a prediction on it. In Figures 11 and 12comparisons between the explanations generated by anunfair and a fair model for the same instance are shown.The navigation menu on the left side in Figure 10 providesthe possibility to manually check the fairness of a dataset,although this functionality is automatically performed by thesystem when the dataset is loaded.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "7ab23c84-7f20-47e6-b1a7-e134429cee55",
                    "text": "scales andquestionnaires are reported in Appendix. Conclusions andfuture work are discussed in Section 7.2. Related workThis section presents some relevant related research worksand commercial platforms regarding our context of interest.Because of our scenario\u2019s complexity and heterogeneity, weseparately discuss the literature on explainability and fair-ness. From the analysis of the following state of the art, wehave derived the foundations of our system\u2019s approach,which is based on the limitations of current XAI systems.2.1. ExplainabilityAs the amount of data being collected grew and ML modelsbecame faster and more accurate, their applications haveextended to several different fields and stakeholders (Preeceet al., 2018). This is why it is becoming crucial to be able toexplain the reasons behind any predictions.aboutsurveys concepts,Comprehensive taxonomies,issues, methods, and challenges in the fields of explainabilityand XAI have been provided in the last three-year period(Adadi & Berrada, 2018; Barredo Arrieta et al., 2020; Biran& Cotton, 2017; Gilpin et al., 2018; Miller, 2019a; Muelleret al., 2019).In recent years many algorithms have been designed toprovide insights into which features are most likely to influ-ence the model predictions. They can be divided into twocategories: model-specific solutions and model-agnostic solu-tions. The first one is tailored to specific model classes. Theycan provide further insights into a model prediction byexploiting the specificities of the model class of interest,both for shallow ML models, such as ensemble classifiers(Palczewska et al., 2014; Rajani & Mooney, 2018) and SVMs(Landecker et al., 2013), and deep ML models, such asmulti-layer neural networks (Shrikumar et al., 2016) and(Selvaraju et al., 2017).convolutional neural networks Algorithms belonging to the second category aim to beapplied to any ML model. The strategy beyond these techni-ques consists in considering the model as a black box, andthey work by analyzing only the input features and themodel output. Relevant contributions to this category areLIME (Ribeiro et al., 2016b) and its variants (Ribeiro et al.,2016a), SHAP (Lundberg & Lee, 2017), based on the conceptof the Shapley values derived from games theory (Shapley,1953), and Anchors (Ribeiro et al., 2018).Most of the commercial platforms developed in the XAIfield exploit at least one of the aforementioned techniques,sometimes results.Usually, they can be helpful to data scientists and research-ers, but less for non-expert users. Some popular platformsand frameworks are briefly presented in the following.improving them to achieve betterIBM Watson OpenScale is a commercial solution belong-ing to the IBM Cloud suite, introduced to provide a plat-form that could be used by businesses to operationalize theirAI systems and to extend their deployments to the wholeenterprise. It offers several tools that help data scientists andmanagers monitor and understand their model\u2019s outcomes.OpenScale not only provides an online application to navi-gate through the results employing a graphical user inter-face, but it also offers an API that allows accessing theplatform\u2019s services programmatically. It currently providestwo different explainability techniques: LIME (Ribeiro et al.,2016b) and a variant of MACEM (Dhurandhar et al., 2019).While OpenScale does not directly provide the capability tomanage the user\u2019s datasets and train custom models, thesetasks are achieved by integrating OpenScale with the rest ofthe to whichOpenScale belongs includes other tools that assist the userduring all the steps related to developing a custom MLmodel. Several factors might limit the application of a com-mercial solution, such as OpenScale. The first one, and inmany cases the most crucial argument against OpenScale, isits expensiveness (although the cost comes with productsupport, documentation, and bug fixing, as well as brandreliability, it is not adequate for a bank institution that hasnot its core business in AI). Other issues are related to thelack of control over what happens inside OpenScale, and therequirement to have the training data stored on the IBMCloud platform if a user does not want to choose theirhybrid cloud solution, named IBM Cloud Pak for Data, thatprovides OpenScale on customer\u2019s machines.IBM cloud services. Indeed, suitetheGoogle What-If Tool (WIT) is an interactive tool thatallows users to investigate the model\u2019s behavior and perform-ance through a visual interface. This tool is an initiative ofGoogle\u2019s People \u00fe AI Research (PAIR) team. It has been pro-posed to enable people to evaluate machine learning modelswithout the need to write complex code. Through WIT, auser can investigate the behavior of multiple models, com-pare them, and extract insights from them. The visualapproach followed by WIT leverages the predictions obtainedfrom a test set to provide customizable graphs which offer abetter understanding of the relationship between differentattributes and the predicted label. The tool is also helpful forinstances and observing how eachcomparing differentprediction changes as the values of its attributes are modi-fied. While WIT is excellent for a data scientist trying tounderstand his model better, domain expert users wish for astraightforward explanation about why a specific result hasbeen obtained.Google Cloud Explainable AI is a set of tools and frame-works developed to aid data scientists to build interpretableML models. With it, developers can understand feature attri-butions in AutoML Tables and AI Platform (the GoogleCloud Platform catalog provides both tools) and visuallyinvestigate model behavior using the What-If Tool. It alsosimplifies model management using the AI Platform. GoogleExplainable AI leverages the integration of Google Cloud\u2019sAI Explanations service into AI Platform Prediction to pro-vide feature attribution. As for IBM Watson OpenScale, itneeds other services from Google Cloud Platform (GCP) tomake users able to manage the whole life cycle of an MLmodel. GCP\u2019s AI Explanations offer three methods to usesampled Shapley (Maleki et al.,for feature attributions:2013), integrated gradients (Sundararajan et al., 2017), andXRAI (Kapishnikov et al., 2019). Each of the mentionedmethods is based on Shapley values, and users can selectwhat they prefer for their explanations requests.AI Explainability 360 is a good open-source software tool-kit addressed to different stakeholders, from domain experts tosystem developers. It allows exploring eight state-of-the-artexplainability methods evaluation metrics.Noteworthy is the provided effective taxonomy to help to navi-gate the space of explanation methods, not restricted to thosein the toolkit, but also in the literature (Arya et al., 2019).and twoSeveral XAI research works presented in different fields,such as HCI (Abdul et al., 2018; Zhu et al., 2018), visualanalytics (Tamagnini et al., 2017) and medicine (Holzingeret al., 2017; Lamy et al., 2019) denote that most researchersare focusing on new algorithms, and not on usability or effi-cacy effective UIs understandable by non-expert users.2.2. FairnessDefining a fairness criterion, to evaluate a system to considerit as fair, is a complex task due to the different ways biasescan arise. What can be considered fair in a specific contextmight be unfair in another one. Furthermore, different peoplehave different sensibilities about what is fair and what is not,and what is fair considering individuals or populations as agroup (Binns, 2020). Three criteria are commonly contem-plated: independence, separation, and sufficiency. Independencecriterion requires the sensitive characteristic to be statisticallyindependent of the score. Variants of this criterion includestatistical parity (Dwork et al., 2012), group fairness (Friedleret al., 2016; Yeom & Tschantz, 2018), and disparate impact(Feldman et al., 2015). Separation criterion seeks to acknow-ledge the existence and rightfulness of the correlation betweenthe sensitive feature and the target variable to the extent thatthe target variable justifies it. It appears under differentnames, such as equivalent odds (Hardt et al., 2016) and dis-parate mistreatment (Zafar et al., 2017). Sufficiency criterionis based on the idea that the sensitive feature is already subsumed in the score used for predicting the target label(Chouldechova, 2017; Kleinberg et al., 2016).Bias mitigation algorithms are mainly based on two fac-tors: the criterion selected for measuring fairness and the stepof the ML process in which it is applied. The steps involvedare three: pre-processing, in-processing, and post-processing. Inpre-processing, the goal is to produce a new representation ofthe training set in which the information correlated to the setof non-sensitive features is preserved, ignoring the informa-tion of the sensitive feature [e.g., reweighing (Kamiran &Calders, 2012), disparate impact remover (Feldman et al.,2015), optimized pre-processing (Calmon et al., 2017)]. Theaim of the in-processing step is to enforce fairness by chang-ing the learning strategy of the model at training time (e.g.,adversarial debiasing (Zhang et al., 2018), disparate impactremover). The algorithms of the post-processing step try tosatisfy fairness constraints by slightly modifying the output ofa model without the need to change the training data orretrain the model. These algorithms are usually used whenthe two previous approaches are not viable because the train-ing dataset or the ML algorithm is not accessible [e.g., rejectoption classification (Kamiran et al., 2012), equalized oddspost-processing (Hardt et al., 2016)].The application of fairness criteria to ML models is atopic that has received much attention lately, thanks togreater awareness about the risks that unfair AI systemsmight pose to specific social groups. Relevant research worksabout the nascent field of Fair ML have been published inthe last years (Chouldechova, 2017; Corbett-Davies & Goel,2018; Holstein et al., 2019).Not only has this popularity led to a growth in the num-ber of scientific publications related to fairness, but it alsoencouraged the introduction of several tools whose goal isto monitor the behavior of a model to alert the user in caseof unfair treatment. An example is IBM AI Fairness 360(AIF360), which is perhaps the most considerable open-source toolkit available for ML fairness. is to\u201cexamine, report, and mitigate discrimination and bias inML models throughout the AI application lifecycle.\u201d It is anextensible framework capable of unifying most of the met-rics and algorithms presented in this chapter. It alsoincludes a bias explanation feature that gives further insightsinto the computed metrics (Bellamy et al., 2018).Its goalIn addition to the capabilities already presented on explain-ability, even IBM Watson OpenScale gives the possibility to setup a monitor to track the fairness of the model at hand. Thepresence of biases in the model is estimated based on the dis-parate impact metric. One of the main OpenScale\u2019s limitationsis that the privileged and unprivileged groups must be selectedin advance when setting up the fairness monitor. This oper-ation may become cumbersome as the cardinality of the sensi-tivity attribute grows. Moreover, the user may not be aware ofwhich value belongs to which group.3. System approachAlthough loan approval processes might benefit from theintroduction of automated systems (i.e., ML models) tosupport the decision task, several factors have limited theirapplication in this field so far: loan approval processes arehigh-risk activities that require officers to understand themotivation behind every ML model prediction. It is notenough to demonstrate that a model performs well if con-sidered only a black box. With skeptical users the ability toexplain how it works, which data is important, and when iscrucial; model decisions have a considerable impact on thefuture of the customers who applied for the loan, and theymust be provided with explanations about why their applica-tion has been rejected; such a decision must be devoid ofbiases to ensure that individuals with different origins, cul-tures, and backgrounds are treated fairly.The main aim of the system presented in this article is toovercome the above challenges by proposing a single solu-tion to create a comprehensive trustworthy intelligent systemexploiting the principles of explainability and fairness. In thefollowing of this section, an in-depth presentation of thetwo concepts is provided, focusing on their relevance inthis article.3.1. ExplainabilityWhat do we mean by \u201cexplainability\u201d? Explainability is consid-ered the core of each AI system that aspires to be consideredtrustworthy, and although it is formally different from inter-pretability, the two terms are considered closely related in lit-erature (Biran & Cotton, 2017). Indeed, interpretability can beconsidered a static characteristic of a model, referring to thecapability to explain its meaning in a human-understandableway. At the same time, explainability is a dynamic characteris-tic of a model representing actions and procedures beneficialto provide explicit knowledge about why a specific predictionwas made using easy-to-understand terms. This means thatexplainability depends on the people who need to understandthe model, so the most appropriate definition could be: \u201cGivena certain audience, explainability refers to the details and rea-sons a model gives to make its functioning clear or easy tounderstand\u201d (Barredo Arrieta et al., 2020). This property iscrucial for building trust in the decisions taken by a model,and it is one of the critical factors that influence the adoptionof ML techniques inside high-risk applications. There are sit-uations in which having the capabilities to interpret a resultand understand the factors that contributed to it is far moreimportant than having a high-performance model. This is oneof the reasons why simpler algorithms, such as decision treesand K-nearest neighbors (KNN), are widely used despite beingless accurate than other options like neural networks and sup-port vector machines (SVMs).As our purpose is to develop a Trustworthy AI systemfor loan approval processes regardless of the specific MLis based on, only model-agnostic solutions (seemodelSection 2.1) are used in the implementation.it3.2. FairnessTo satisfy the principle of fairness, users must be aware ofexisting biases (i.e., prejudices) that may lead AI systems to discriminate against certain groups of people or individuals.Furthermore, every AI system should be accessible to peopleof any age, gender, and ability (AI-HLEG, 2019).So far, no standard definitions of fairness have beendrawn up. In the context of decision-making, fairness canbe formalized as \u201cthe absence of any prejudice or favoritismtoward an individual or a group based on their inherent oracquired characteristics\u201d (Mehrabi et al., 2019). A recentstudy about how people perceive fairness in the context ofloan allocations (Saxena et al., 2020) shows a preference fora specific definition, named calibrated fairness (Liu et al.,2017), that aims to select individuals in proportion to theirmerit. In particular, that study demonstrates that officerschoosing between two loan applicants tend to prefer to splitthe money in a proportion of their loan repayment rates,instead of an \u201cequal\u201d (50/50) splitting, or giving all themoney to the candidate with the higher payback rate. The\u201cratio\u201d decision is allowed under calibrated fair-ness definition. theA machine learning model can be biased because thesesystems are trained by examples: when using historical datafor modeling human behaviors, the provided sample reflectsthe prejudices of the people who made these decisions inthe first place.The choice of the right bias mitigation algorithm, amongthose described in Section 2.2, is constrained by several rea-sons: the definition of fairness may vary from case to case;different criteria cannot be pursued simultaneously and eachalgorithm mainly focuses on a single definition (the reweigh-ing algorithm is based on the independence criterion, whilethe disparate mistreatment remover technique uses the separ-ation definition); the step of the ML model pipeline in whichthe user is allowed to intervene: as a rule of thumb, the ear-lier the algorithms are applied, the most flexible and effectivethe intervention will be; the requirements of the algorithmitself: for instance, the equalized odds post-processing tech-nique, despite being a post-processing strategy, requiresaccess to the sensitive feature to compute the right label.Other algorithms have some limitations in terms of the typesof classifiers they can be applied to. Some algorithms, suchas reject option classification, are deterministic, while othershave a randomized component (e.g., disparate mistreat-ment remover).As detailed in Section 4.3, in the presented system wefollow the independence criterion and choose the reweighingalgorithm for bias mitigation. Independence is frequentlyused in literature because it is one of the few criteria havinglegal support. The so-called 80%-rule, or four-fifth rule,specified in the U.S. Equal Employment OpportunityCommission guidelines, prescribes that a selection rate forany group (classified by race, orientation, or ethnicity) thatis less than four-fifths of that for the group with the highestrate constitutes evidence of disparate is,discriminatory effects on a protected group (Biddle, 2006).impact, thatIn the next section, a top-down description of the systemcomponents is provided, from a logical overview up to theillustration of the UI of the developed framework, goingthrough an in-depth presentation of the explainability andfairness tools. name used as an identifier. The id attribute is also used toretrieve the content of the dataset from the local storage.4. System implementationThis section first provides an overview of the proprietaryframework developed for the implementation of the casestudy. It then illustrates the core of the presented system,that is, the application of explainability and fairness princi-ples to the context of a loan approval process.The developed framework provides all the following func-tionalities to ensure complete management of the ML modellife cycle, grouped by their high-level purpose, as shown inFigure 1.The dataset & ML model handler allows users to: load adataset and store it through a procedure involving a fixedpreprocessing step, a custom setup, and a fairness check forpreliminary bias detection; find the best ML model to useby training at the same time several models with differentalgorithms and evaluating them through standard metrics;monitor models performance using various metrics (thesame with which a model is evaluated after training).The standardized explainability tool provides users withthe ability to get explanations for each prediction to makethem (i.e., both loan officers and loan applicants) able tounderstand clearly the features (or attributes) that mostinfluence the results, both positively and negatively.The fairness tool provides a fairness check and a biasmitigation process. It can identify the presence of biaseswithin the trained model using one of the state-of-the-artfairness criteria (chosen for the reasons explained in Section4.3) to make users aware that they are supported or judgedby a fair system, with the possibility, otherwise, to retrain anew unbiased version of the same model.The feedback loop allows a domain expert (e.g., the loanofficer) to give feedback about a specific prediction to createa new ground truth and build a new ML model, hopefullywith better performance.From a conceptual and schematic point of view, the pro-posed framework is described in the class diagram in Figure 2.A brief description of each class is provided below.Dataset describes the dataset used to train the model. Itis characterized by the number of rows of the dataset and a Label represents the values that can be assigned to thelabels of a dataset. Each possible couple (dataset_id, label_-value) is described by an instance of this class. The informa-tion about its value and its number of occurrences in theassociated dataset is added to distinguish each label.Model denotes the model trained using a specific dataset.Each model is described by an identifier, a descriptive name,and the date it was added to the system. The Booleanthe model wasunbiased attribute is used to specify ifobtained after the application of a bias mitigation algorithmto another model.MLAlgorithm is used to describe the algorithm withwhich the model is trained. It is also employed to providesome additional information to the user during the biasmitigation process.PredictionData is used to represent the prediction com-puted by a model for a given input instance provided by theuser. The outcome of the prediction is stored as the probabil-ity returned by the model. The entity also includes, for eachattribute of the instance being predicted, its feature value andthe related weight (or score) obtained using a model-agnosticinterpretability algorithm (e.g., LIME or SHAP).FeedbackData describes feedback provided for a givenprediction. It is represented as a Boolean attribute, wheretrue means that the prediction aligns with the expectation ofthe user or with the actual outcome.4.1. Application workflowA concise representation of the functionalities provided bythe developed framework is shown in Figure 3. Each appli-cation flow and its main components are described below.For a better understanding of the diagram, two premises areto be made: components having equal shape, size and nameare intended to be the same; they are duplicated only forbetter visualization; black dashed lines in the diagram repre-sent the connection between the data and the specific proc-esses used.Loan Approval System User Interface allows users toselect the functionality to run through the Tab menu. It isimplemented in an internet application with an intuitivelayout. The application is shortly illustrated below in thissection to show how the UI looks to its users.Load dataset is the functionality to load a dataset andstore it in the system. Intuitively, the first time the systemstarts, this is the only available functionality. Before effect-ively storing the loaded dataset, a preliminary predefinedData Preprocessing is required to prepare the data for thenext steps. The Dataset setup component allows users tocheck and modify dataset parameters, including the name tosave it under, columns names, and data types. Differentdatasets can be loaded and stored into the system simultan-eously to be selected by users when needed. Through the ML model training functionality, the train-ing phase can be started once a dataset is stored and avail-able for selection. Several models are built at the same timeusing different ML algorithms. Trained models are presentedto users along with metrics (i.e., Accuracy, Precision, Recall,F1-score) to evaluate the performance of each of them andcompare each other to select the best one to store inthe system.To request a prediction, users can select one of thestored ML models and query it to obtain the predictionresult and its explanation. In our case study, predictions arethe probabilities that customers can repay a loan given theirExplainer interface is the common interface that stand-ardizes the access to the different interpretability algorithmsto anotherto switch from one explainerand allowssmoothly or to use different explainers at the same time.Each explainer is initialized using the configuration class pre-viously described and their explanations are generatedthrough the compute_explanation method. Depending onthe specific implementation, the resulting output can bereturned in different formats (e.g., lists or dictionaries).Three different the interface illus-implementations oftrated above are possible in the presented framework. Eachof these classes, listed in the following, leverages some otherservices to work correctly; initialization and formatting ofthe requests to these external packages are handled by theexplainer and configuration components.LimeExplainer produces a score for each feature basedon the LIME algorithm (Ribeiro et al., 2016b), but normal-izes it so that, by summing up all the scores, the total valueamounts to 100. This operation is made to avoid a misun-derstanding of LIME results.ShapExplainer is a wrapper around the SHAP algorithmimplementation (Lundberg & Lee, 2017). ishandled to produce a result similar to the one provided bythe LimeExplainer. Its outputAnchorsExplainer is based on the Anchors algorithm(Ribeiro et al., 2018) and, as in the previous ones, leveragesthe implementation proposed by the article authors and pro-vides a standardized output score.The standard process of the explainability tool is shownin Figure 5. First, a configuration object is created by theservice layer using the original training dataset. Second, thegenerated instance is used to initialize an explainer object.In the third step, the service layer requires the new explainerobject to explain a given instance. Since the actual interpret-ability algorithm requires the instance to be provided in aspecific format, the explainer exploits its internal configur-ation to prepare the given instance accordingly. Once thecredit histories and some personal data as the input. Oncethe output is computed, users (domain experts at this point)can give feedback about the specific prediction to enable thepossibility to monitor the results of in use.Predictions, explanations, and feedback are saved in aninternal Model results storage. the modelModel performance monitoring functionality exploitsthe feedback collected from users to compute statistics aboutthe performance of the managed models (same metrics as inthe ML model training).Checking dataset/model fairness and bias mitigationprocess works as follows. Both the original stored datasetand the model in use can be inspected to check the presenceof any biases. While checking the dataset is meant for diag-nostic purposes to trace back the unfair attribute to thestarting labels distribution, the information derived from themodel predictions is used to monitor how a sensitive attri-bute influences the model behavior. If some decisions areconsidered to be unfair, then an unbiased model can betrained and stored.4.2. The standardized explainability toolBy design, the developed framework provides several waysto obtain the given prediction.Components of this generalized and standardized explain-ability tool are described below and represented by the dia-gram depicted in Figure 4.interpretation of aConfiguration class performs the preprocessing stepsrequired to use the explainability algorithms and to trainexplainer models. Starting from the dataset, categorical andnumerical features are extracted to be evaluated, and theone-hot encoding procedure is applied to categorical values.The configuration class is also used to prepare the instanceto be explained for the application of the interpretabilityalgorithms by exploiting the data extracted during the men-tioned initialization phase.explainer method has finished its computation, explanationscan be retrieved by the service layer. relevant number of instances, while the 0.8 threshold hasbeen selected to comply with the 80%-rule (Biddle, 2006).4.3. The fairness check and bias mitigation processesThe second most important feature the presented systemprovides is the capability to inspect the original dataset labeldistributions and the trained models\u2019 behavior for biasdetection and potentially training an unbiased model. A dis-cussion about these processes follows.Our system can inspect both the original dataset andmodels\u2019 behavior to determine the lack of fairness. As forthe dataset, its rows are used to identify if a bias can betraced back to the original data (this feature is meant fordiagnostic purposes); instead, the system uses the predic-tions made by a model to analyze its behavior. If a bias isdetected from the model predictions, then an unbiased ver-sion of the model can be trained and stored for future pre-dictions. Apart the proposedfrom these differences,Algorithm 1 describes the bias detection procedure for boththe original dataset and trained models.The labeled dataset in input D (which also refers to thedata structure in which the results of the model predictionsin use are stored) is inspected for biases using the disparateimpact metric (Feldman et al., 2015). As part of the process,dataset rows d 2 D are grouped based on the sensitive attri-bute value s. These groups are later referred to as sensitivegroups g 2 G: For each sensitive group g, positive outcomesratio is computed, and G is split into one or more privilegeclasses C 2 C, where C \u00bc fg j g 2 G, 0 < i (cid:2) jGjg, basedon two factors: each privilege class C must represent at leastthe 5% of the entire population of D, and the disparateimpact between C and any other sensitive group g (or viceversa) must be lower than 0.8. The first constraint is appliedto guarantee that each privilege class contains a statistically The output of the algorithm is a set of privilege classesC, where the class with the highest rate of positive outcomesis considered to be the privileged class, and the other classesare referred to as the unprivileged classes. If C turns out tohave cardinality >2, then the dataset or model being eval-uated is considered to be biased.Algorithm 1. Bias detection procedure.procedure ComputePrivilegeClasses(D)C \u00bc ;;C \u00bc ;;C \u00bc ;;G   Select (cid:3) From D GroupBy s;for all g 2 G doif \u00f0len\u00f0C\u00de (cid:4) t\u00de and \u00f0disparate \u00f0C, g\u00de < 0:8\u00de thenC   C [ fCg;C   ;;end ifC   C [ g;end forC   C [ fCg;return C;end procedureFollowing the procedure mentioned above, if the modelresults are unfair, the system provides the functionality totrain an unbiased version of the same model. Algorithm 1 isused to split the model into two classes: the privileged class,i.e., the set of the sensitive feature values with the highest ratioof positive outcomes, and the unprivileged class, which con-tains the remaining values. It must be noted that there maybe values of the sensitive feature for which no predictions areavailable yet. Based on the criterion described above, these val-ues will be assigned to the unprivileged class. The rationalebehind this choice is that, in a situation where the system hasno information about how a model perceives a specific value,assigning it to the unprivileged class, the system is guaranteednot to exacerbate pre-existing unknown prejudices.Once the division of the feature values between privilegedand unprivileged classes has been determined, the reweighingalgorithm (Kamiran & Calders, 2012) is used for bias mitiga-tion. This algorithm was chosen for multiple reasons: the sys-tem has access to the dataset used for training the inspectedmodel, so a preprocessing strategy like the reweighing algo-rithm, which is likely to provide better results, can beapplied; the reweighing algorithm bases its decision on theindependence criterion, that is the exact fairness definitionused to perform the distinction between privileged andunprivileged groups, and as previously mentioned, this defin-ition has legal support; the algorithm output is a set ofweights, which is easier to interpret than other techniques.Once the new set of weights has been determined, thethe inspected model can be trainedunbiased version ofusing the same ML algorithm used for the original modeland then stored and made available to be queried.5. Case studyThe focus of this section is on illustrating the application ofthe described system to the context of a loan approval process and providing an overview of the UI of the devel-oped framework to show how it looks to users without fur-ther details about its technical implementation.The diagram in Figure 6 shows how the functionalitiesexplained in the previous section are accessible to the differ-ent types of users who can access the presented systemthrough the developed framework.Figure 7 shows the screen through which users can loada dataset. In our case study, the data has been provided tous by an Italian banking institution (after a pseudonymiza-tion process) to build a prototype based on real data. Thescreen displays the characteristic of the loaded dataset andthe results of preliminary bias detection.After the dataset has been loaded users can select it andautomatically train a new ML model (Figure 8) consideringdifferent algorithms (in the presented case study, since it is abinary problem, we have chosen Logistic Regression, RandomForest, and Naive Bayes algorithms). As the dataset is unbal-anced, the system ranks the trained models based on the F1-score metric. It is also possible to compare the selected modelwith one of those already stored in the system.After requesting a prediction, users can consult the modeloutcome with the relative explanation in an intuitive layout.As Figure 9 shows, the prediction result, along with itsprobability, is presented in the top-left box, while the relatedexplanation is in the correct box (in the provided example,they are generated using the SHAP algorithm). Finally, theuser can provide feedback on the prediction by clicking onthe buttons contained in the bottom-left box.By navigating to the \u201cFairness\u201d tab, the user is presentedwith the privilege class division for the most recentlyuploaded dataset (Figure 10). In the presented case study,we have considered the \u201cnationality\u201d as the sensitive feature;the displayed partitioning is obtained by applying the pro-cedure described in Algorithm 1. Within the same screen,selecting the \u201cTraining\u201d option in the navigation menu onthe left, users can request the system to train a new version of the model by exploiting the reweighing bias mitigationalgorithm. Once the training is completed, the model is per-manently stored in the system, along with its previous ver-sion, and can be selected in the \u201cPrediction\u201d tab of the maininterface to request a prediction on it. In Figures 11 and 12comparisons between the explanations generated by anunfair and a fair model for the same instance are shown.The navigation menu on the left side in Figure 10 providesthe possibility to manually check the fairness of a dataset,although this functionality is automatically performed by thesystem when the dataset is loaded.6. EvaluationThe system presented in this article has been developed withthe dual goal of supporting a loan approval process and lay-ing out the foundation for a more general ML model man-agement suite. This section is divided into four parts:1. The first part shows the evaluation made for choosingthe best explainability algorithm (among those usablein the developed system and described in Section 4.2)in the context of the presented case study; this evalu-ation has been carried out by a group of data scientistsand researchers using the Explanation GoodnessChecklist proposed by Hoffman et al. (2018);2. The second part focuses on the experimental results ofthe Loan Approval System evaluation, from the explain-ability point of view, carried out through a novel Trust& Reliance Scale based on the Trust Scale Recommendedfor XAI proposed by Hoffman et al. (2018). Results areobtained by the submission of the mentioned novel scaleto a group of bank domain experts and loan officers;In the third part, the results of the A/B test and targetedinterviews performed to evaluate the effectiveness offairness usage in the presented system are displayed;4. A usability test has been performed to measure user sat-isfaction with the UI, and the results are shown in thelast part of this section.3.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "68e0d073-f7f5-4991-a683-27f915d694a0",
                    "text": "Training, and Human Factors) and set guidelines for theevaluation of XAI systems.To evaluate which explainability algorithm (LIME, SHAP,or Anchors) performs best in the context of the case studypresented in Section 5, we exploit the Explanation GoodnessChecklist proposed in the article mentioned above. Thischecklist represents a synopsis of the main features used inthe research literature to consider explanations good. Quotingthe authors, \u201cThe intended use context is for researchers [ \u2026 ]to provide an independent, a priori evaluation of the goodnessof explanations that are generated by [ \u2026 ] XAI systems.\u201d Tobe thorough, the checklist is reported in Appendix A.For the experimental session, a pool of 54 people withoutexperience with our framework have been asked to try thesystem through the developed UI for one month (betweenJune and July 2020) and then compile the checklist. The groupof participants was composed as follows: equally dividedbetween data scientists and researchers in the computer sci-ence field, the majority (90%) are daily involved in ML modeldevelopment; many (70%) are aware of XAI techniques. Thedivision between males and females is 75\u201325%, and the aver-age age is 27.2 years old. The pool has been split into threehomogeneous subgroups (with respect to the factors men-tioned earlier). We performed a between-subject evaluation: adifferent algorithm has been assigned to each of the three sub-groups to make the evaluation process independent of influen-ces due to having previously examined a different technique.The three post-hoc methods have been applied to thesame trained ML model with the following characteristics:Hoffman et al. (2018) put together key concepts that haveemerged from the literature in various fields of research(such as Philosophy of Science, Psychology, Education and (cid:5) Dataset size: 2440 samples;(cid:5) Training algorithm: Random Forest;(cid:5) Accuracy: 0.987;(cid:5) Precision/Recall (Rejected): 0.981/0.985;(cid:5) Precision/Recall (Granted): 0.997/0.996.Figure 13 shows the percentage of positive answers (Y-axis) for each question (X-axis) and each algorithm. Theevaluation outcomes display that Anchors are preferred onlyfor the level of details provided, while LIME\u2019s explanationsare considered as understandable and actionable as SHAP\u2019s,which overall result in the most satisfying, complete, accur-ate, reliable, and trustworthy. For this reason, we integratedthis algorithm for the implementation of the system.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "a4a7387f-e842-4f48-8851-940e95069e0b",
                    "text": "is on the Trustbased mainlyA novel Trust & Reliance Scale is proposed in Appendix B.This scale, used to evaluate the effectiveness of predictions\u2019explanations, ScaleRecommended for XAI proposed by Hoffman et al. (2018)(Q1, Q3, Q4, Q6, and Q7). We have adapted the mentionedscale to achieve a new one that is better suited for the evalu-ation of our system according to the proposed approach. Inparticular, we have removed questions about predictabilityand efficiency and added three new items: a questionderived from the work of Adams et al. (2003) (Q2) to askusers directly whether they trust the tool\u2019s output; a questionfrom the Hoffman\u2019s Explanation Satisfaction Scale (Q8) forhighlighting the judge about the importance of explanations;and another new question (Q5) to make users consider thepossibility of trusting the system\u2019s response if it is differentfrom theirs. This novel scale is realized as a 5-point Likertscale, by following the literature, which indicates that thefive-point format appears to be less confusing and tends toreduce the \u201cfrustration level\u201d of respondents and therebyincrease the response rate and the quality of the responsesthemselves (Babakus & Mangold, 1992; Devlin et al., 1993).For each of the corresponding statements, every user gives aresponse range between Strongly disagree andStrongly agree.in the Since this kind of scale is addressed to users with consid-erable experience, it has been submitted to the group of par-ticipants after two months of continuous use of the system(October\u2013November 2020). The group comprises 42 bankdomain experts with experience in loan approval processes,33 of whom are currently loan officers. All the practitionersbelong to the Italian banking institution that provided thedataset to create the system prototype. The average age ofthe participants is 39.3 years old, and the average years ofexperience in loan approval processes are 9.6.To build a baseline and be able to evaluate the actualimprovement given by the explanations, we divided the poolof selected testers into two homogeneous subgroups and setin the first one, theup two different test environments:group was not aware of the explanations, and the UI hasbeen modified to display the results of the predictions onlywith label and confidence, as shown in Figure 14. Theinteracted with the actual systemsecond group,prototype and the UI presented in Section 5 (see Figure 9).instead,The results of the two tests are shown, respectively, inFigures 15 and 16. We visualize the Likert scales with thediverging stacked bar charts as the graphical display tech-nique, based on Robbins and Heiberger\u2019s studies on thepresentation of results using rating scales (Heiberger &Robbins, 2014; Robbins & Heiberger, 2011).By analyzing the results charts, it is clear how the possibil-ity of checking the explanations of a given prediction has ledto a better overall assessment of the system. Although in bothtest environments users had the perception that the systemworks adequately well (Q1) and most of them appreciate theuse of such an automatic system to make these decisions(Q7), they have shown a concrete improvement in the systemjudgment in terms of trustworthiness and reliability (Q2, Q3,Q6). The explicit question about the usefulness of explanationsin the second test (Q8) confirmed that perception. Due to dis-playing predictions\u2019 explanations, other noteworthy results arethe overall decrease of \u201cnon-opinion\u201d answers and the increasein the number of users that would change their minds basedon the system\u2019s response (Q5). Finally, and perhaps surpris-ingly, in both environments, most users believe that such asystem can give better results than a novice human (Q4).Moreover, we investigated the characteristics of the users whodisagreed about reliability (Q3) and confidence (Q5) in thesystem with a displayed explanation. The resulting analysisshowed that the average expertise in loan approval processesis 11.6 years, 2.1 years more than the overall average of theparticipants. This result underlines that experienced loan officers can be unenthusiastic about the use of new technolo-gies in their daily work.",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "58861f4d-d413-46f6-a660-692136adbbdf",
                    "text": "To assess the effectiveness of the fairness, we decided to setup an A/B test as described in the following. We first selected24 loan officers (average age of 34.6 years old and averageexperience in the field of 5.5 years) not involved in the previ-ous evaluation to participate in a test session for checking theusefulness of the feedback loop. For each displayed predictionand related explanations, their task was to judge whether itwas correct by clicking on the specific button on the bottom-the UI shown in Figure 9. The participants,left part ofhowever, were unconsciously divided into two homogeneoussubgroups to assess the possible differences in the evaluationof predictions correctness based on the use of 2 opposed mod-els. The first group interacted with an unfair model, as inFigure 11, while the second one with a fair model, as inFigure 12, where the nationality attribute was missing at all.The assessment, lasting 2 hr, was carried out by showingeach user 50 predictions. Figure 17 displays the results interms of click rate on the feedback buttons.As can be seen from the chart, the percentage of negativeresponses is higher for the unfair-model testers. Based on this outcome, in the second part of the evaluation, we per-formed a series of targeted interviews with some of the loanofficers involved in both interactions. The most relevantconclusion is that, while the 92% of the \u201cfair-model testers\u201dstated that they \u201cfocused on assessing the actual correctness ofthe prediction based on their experience,\u201d the 88% of the\u201cunfair-model testers\u201d confirmed that often their attentionwas just on the nationality attribute weight because theywould \u201cnever agree to confirm the rejection or the approvalof a loan application in which the greater weight of the deci-sion is attributable to a potential discriminatory individualthe applicant.\u201dcharacteristicAlthough not part of that test, they all agreed to considerthe visualization of the predictions\u2019 explanations as an essen-tial feature for this kind of automated system.the nationality ofsuch as",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "39f02257-e928-4a06-ae26-d0ab1891780d",
                    "text": "Finally, we present a qualitative evaluation of the developedUI to measure user satisfaction with the system usability.This experimental session was attended by the bankdomain experts already involved in the previous systemevaluation. The questionnaire, reported in Appendix C, isbased on the usability test proposed by Purificato andRinaldi (2018) and structured following a methodology pre-sented by IBM (Lewis, 1995), but adapted to a five-pointformat for the abovementioned motivations. Each partici-pant tested the three functionalities for one month (March2021) and then evaluated them with the same procedure asthe one described in the previous section.We had the three main functionalities of the systemtested (dataset and ML model handler, explainability Tool,and fairness Tool) and the results displayed, respectively, inFigures 18\u201320, allow us to state that users consider thedeveloped UI effective. Some improvement is required forthe fairness toolneeded information to carry out the specific task.in terms of complexity in finding the",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                },
                {
                    "id": "f70ff9e1-a4c6-46dc-b288-af4a8982af54",
                    "text": "In this article, we presented a system that focuses on two ofthe fundamental ethical principles in the fields of Responsibleand Trustworthy AI, explainability and fairness. The system isapplied to the context of loan approval processes through theimplementation of a proprietary framework able to managethe whole life cycle of an ML model to show how the use ofexplainability and fairness techniques can lead to the growthof a bank domain expert\u2019s trust and reliance on AI systems.With this aim, four functionalities have been designedand developed: a dataset & ML model handler, a standar-dized explainability tool, a fairness tool, and a Feedback loop.In particular, the standardized Explainability tool providesmethods to get explanations for each prediction, allowingusers to choose among three different algorithms: LIME,SHAP, and Anchors. The Fairness tool allows users to detectbiases within the model\u2019s behavior through a proposed algo-rithm based on disparate impact metrics, and to mitigatethem using the reweighing algorithm, a method followingthe independence criterion, one of the few criteria that havelegal support. An unbiased version of the original model canbe trained at the end of the described procedure.A proprietary framework with an attractive and easy-to-use UI has been developed, and the whole system hasbeen evaluated in the context of loan approval processes.The effectiveness of our approach has been proven throughexperimental results from field tests and user studies. SHAPhas been chosen as the preferred explainability algorithmthrough the submission of the Explanation Goodness Scale toa group composed of data scientists and researchers. Theenhanced trust in the use of our system has been assessed bybank domain experts through a novel Trust & Reliance Scaleproposed in the article. Finally, a Usability Test has demon-strated the usefulness of the developed user interface.Notes AcknowledgmentsDisclosure statementORCID",
                    "reference": "[1] Erasmo Purificato, Francesco Lorenzo, and Francesco Fallucchi. 2023. The use of responsible artificial intelligence techniques in the context of loan approval processes. Int. J. Hum.-Comput. Interact. Taylor & Francis. Retrieved from https://erasmopurif.com/assets/pdf/publications/2022_IJHCI.pdf"
                }
            ]
        },
        {
            "paper_title": "Responsible AI: Gender bias assessment in emotion recognition",
            "authors": "A Domnich, G Anbarjafari",
            "publication_info": "arXiv preprint arXiv:2103.11436 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2103.11436",
            "chunks": [
                {
                    "id": "fd5be29f-ab05-409b-b2a3-6179e6c11d63",
                    "text": "",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "3484e29a-918a-4e96-98ca-641eddd83746",
                    "text": "",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "9eb6a15c-3a8a-4aca-b606-0fdf3cf65o8df",
                    "text": "Now we are reviewing the general information regardingand overview of deep learning method for FER. Further-more, this section describes de\ufb01nition of biases, principlesof fairness and has overview of related study about latter inmachine learning.Spreading of AI technologies over various areas ofmankind presence is allowing to achieve incredible per-formance and fascinating results. Computer vision, whichtakes honorable place in AI adopted \ufb01elds, became ubiq-uitous used by different industries. Nowadays, computervision applications became pivoting technologies in au-tonomous driving systems [19, 20], video surveillance [21],virtual and augmented reality [22, 23], robotics [24, 25, 26]and others [27, 28, 29, 30, 31, 32]. This increases the immedi-ate need of investigation on ethical and moral rules in orderto assure that such assistive systems will not interfere withhuman rights.Human emotion is an extremely wide \ufb01eld of research.During last decades, people were trying to justify, classifyand investigate emotions [33, 34, 35, 36, 37]. Emotion recog-nition, as a sub-domain of research, tries to \ufb01nd an answeron how to classify emotion which is displaying by humanbeing. Overall, we, as humans, rely on different modalities,which are taken into the account each time when we try torecognize an emotion. Usually, the indicators are facial ex-pression, articulation, gesticulation, body pose and contextof the situation. Worth noticing, that emotion recognition isdeeply entangled with psychology[38].However, still one of the easiest way to express and tounderstand an emotion is a facial expression. Even though,the emotion in facial expression (that is being displayed bya person) with high certainty can be distinguished evenwithout context and additional information. Hence, overthe time, FER became an independent task in computervision domain, usually interpreted as emotion recognition.As a result, this is a task on the edge of psychology andcomputer vision, where from the \ufb01rst perspective scientiststry to determine an emotion based only on face expressionand from the second one, to study algorithms to classifythem. There are various domains in which application ofFER have been utilized [39, 40, 41]. Usually emotions are described through Action Units(AU) [42] or determined as a point in Valence-Arousal (V-A) space [43]. The AU methodology does not determineemotion explicitly, but provides a set of state for eachdifferent parts of the face, such as \u201dlips apart\u201d, \u201douterbrow raiser\u201d and so on. Scientists can denote a displayedemotion as a group of AUs. Opposite from AUs, V-A spacerepresents human emotion from psychological perspective,i.e. group them on imaginary Cartesian space in such waythat the distance between similar emotions is lower than theopposite ones.Regardless of notation, scientists determine six basichuman emotions: anger, disgust, fear, happiness, sadnessand surprise [44]. More complex, compound emotions, areusually contains two basic emotion and arbitrary number ofadditional ones [45, 30].Data for FER usually presented as still images, sequenceof images or videos on which person is displaying emotion.Usually, depends on purpose of data, there are arti\ufb01cial(laboratory) [46] and real-world (wild) [47, 48] type of data.under special circumstances with particular requirements.The latter one is often collected from the internet, gatheringimages with an open access and label them.Before discovering that deep machine learning algo-rithms has huge ef\ufb01ciency in image domain, approachesfor FER typically were organized as a three stage pipeline:image preprocessing, feature extraction and classi\ufb01cation.Feature extraction is a core of pipeline and these approacheswere mainly distinguishable by method of feature extrac-tion.The most famous and common methods are GaborFeature Extraction and local binary pattern (LBP) [49, 50].The \ufb01rst method uses a set of various Gabor \ufb01lters [51],named Gabor Filter Bank, to extract particular features fromimage. This method is robust against scale, rotation andluminous intensity. [52, 53, 54] are good examples of usageof Gabor \ufb01lters for encoding facial expressions. The secondapproach is based on the histograms of binary maps, whichare local representations of relation between target pixel andits neighbours: if the center pixel of the map is greaterthan a neighbour, a corresponding map value equals to0, otherwise - 1. This simple yet effective approach hasfound extension in several works [55, 56, 57], consequentlyimproving a technique.In terms of classi\ufb01ers, the choices are well known andproven machine learning algorithms. Essential methodssuch as k-Nearest Neighbours (kNN) [58], Naive Bayes[59], Space Representation-based Classi\ufb01er (SRC) [60] andSupport Vector Machine (SVM) [61] where used intensively.In spite of the tremendous works that has been donein development of algorithms for FER, after resoundingsuccess of AlexNet [62] in 2012, it was only matter of time,when deep learning approaches would take a main stage inFER. In 2015, [57] and [63] presented convolutional neuralnetworks (CNN) for FER, trained on LBP and still imagescorrespondingly. Although feature-absence method showedmuch worse results, ability of deep learning algorithmsto extract features independently had been already wellknown, therefore, research is this direction was continued.The variety of works in the following years have shownextreme performance gain, with respect to to the classicalapproaches [64, 65, 66, 67].Most recent CNN approaches have gone far beyond andinvolve sophisticated ways to tackle a very speci\ufb01c prob-lems which rise only in FER domain. For example, PyramidWith Super Resolution [68] shows extreme performance ofFERPlus dataset [69]. Facial Motion Prior Networks [70] takelead position in AffectNet [71] benchmark. One of the fastestnetwork is MicroExpNet [72], which also shows incredibleperformance on Oulu-CASIA dataset [73].Success in still image domain naturally was desiredto be transferred onto domain of sequence of images orvideos. The two main ways how to address sequential datawith deep learning are recurrent based methods and 3Dconvolution based methods [74]. The target for both is to uti-lize temporal relation between frames to catch a displayedemotion over time. However, most of the recurrent neuralnetworks are using CNN as a feature extractor, meanwhile3D CNNs are trained from the scratch and usually do notutilize any additional networks.neural network (RNN) and CNN feature extractor havebeen utilized to achieve necessary goals. In [75] using bi-directional Long-Short Term Memory (LSTM) and CNN,a compound model has been trained for AUs recognition.As a competitors in EmotiW2016 [76] challenge, authors of[77] used both CNN-RNN and 3D CNN models to extractnecessary features to train a model. This kind of approachrequires a lot of computational power, yet not showing state-of-the-art performance, therefore was not widely used infuture. [78] is another good example of usage a combinationof CNN and LSTM. CNN is trained using frames of dif-ferent intensity expression-states, meanwhile LSTM utilizesfeatures which are extracted with aforementioned CNN tolearn temporal representations.In [79] authors designed a novel architecture whichcomprised 3D CNN and Nested LSTM. In detail, eachoutput feature map of 3D convolution layers is passed tothe Multi-dimensional Spatial Pyramid Pooling (extensionof Spatial Pyramid Pooling [80]), forming a feature vectorof \ufb01xed length. These vectors are served as an input to the\ufb01rst so called T-LSTM, modeling temporal relations. Next,hidden states of T-LSTM are going to C-LSTM, learningdependencies between convolutions. As a results, accordingto the experiments, this method had outperformed state-of-the-arts methods of that time.In the area of 3DCNNs, the types of proposed methodswere not that diverse, due to the extremely huge con-sumption of computational capacity. In 2014, [81] authorsproposed a simple architecture, showing that this methodoutperform existed classic approached and heavily dependon the size of the network. Three years later, [82] presenteda method, which comprises 3D convolutional neural net-work together with facial landmarks. Authors enhancedInception-ResNet architecture [83] within 3D convolutionand decreased model size in favor of compactness. As a re-sults, this model beats all that time state-of-the-art methods.Recent years, this kind of model has found appliance in themicro expression recognition [84, 85, 86].Although deep learning has opened new horizons formany \ufb01elds, thoughtless usage of this technology may lead to undesirable consequence. Recent years many problemshave arose due to implementation of AI algorithm in \ufb01eldswith critical importance. In 2019, high-level expert group onAI released Ethics Guidelines For Trustworthy AI [87]. Thisdocument de\ufb01nes what trustworthy AI is and determinea framework which has to help companies adjust their AItechnologies. According to the framework, three main com-ponents of trustworthy AI are lawful, robust and ethical.Nowadays, a lot of companies invest in research andutilize these principles. For example, Microsoft has ownMicrosoft AI Principles[88] which are correlated with afore-mentioned guidelines. Corporation tries to bring responsi-ble AI through the Of\ufb01ce of Responsible AI (ORA) and theAI, Ethics and Effects in Engineering and Research (Aether)Committee.According to the PwC 2019 AI predictions[89], only 3%of companies do not have plans to address problems ofRAI. This is a solid proof that rising concerns regarding AIare pushing industries to make AI more explainable andresponsible. However, following 2020 AI predictions[90],AI Ethics.According to Ethics guidelines for trustworthy AI [87],the one of the Ethical Principles in the Context of AI systemis the principle of fairness. Fairness is a one out of fourmain principles of AI Ethics, along with respect for humanautonomy, prevention of harm and explicability. However,this term is not unequivocally determined, opening a lot ofdoors to enter a discussion [91]. Bias in machine learningcould be represented from the very different perspectives:biased sampling, inappropriate feature selection, inferenceof \u201dblack box\u201d models, biased labeling and others. As hasbeen mentioned before, AI industry is currently tacklingrisks of AI, in particular, making efforts to \ufb01nd and mitigatedifferent kind of biases in machine learning.Despite broad discussion, wide range of options and dif-ferent de\ufb01nitions, in scope of this paper, fairness is treatedin three different ways:1) Equalized odds. Predictor \u02c6Y is fair, if for protectedattribute A and prediction Y , \u02c6Y and A are indepen-dent conditional on Y [92]. In math formulaP ( \u02c6Y |A = 0, Y = y) = P ( \u02c6Y |A = 1, Y = y) (1)2) Equal Opportunity [92] de\ufb01nes this type of fairnessfor binary predictor as follow:P ( \u02c6Y |A = 0, Y = 1) = P ( \u02c6Y |A = 1, Y = 1) (2)3) Demographic Parity. Predictor \u02c6Y is fair, if for pro-tected attribute A and prediction Y the likelihoodof prediction is the same[93]. In math formulaP ( \u02c6Y |A = 0) = P ( \u02c6Y |A = 1) (3)Direct discrimination [94] assumes biased results whichare depended on particular attributes. Hence, usage of theseattributes are considered as protected and defended by law[95, 96, 97].Explainable and Non-explainable discrimination are twosides of one coin. Explainable discrimination is legal insituation, when particular biases could be explained andjusti\ufb01ed. Meanwhile, non-explainable discrimination is ille-gal, and being considered such, when discrimination towardparticular groups could not be explained. However, specialtechniques are existed to identify and even mitigate illegaldiscrimination [98].Since deep learning does not have clear features as inputand decision are not explainable, often, due to the natureof data, different biases are arisen. There are more than20 different biases [94]. For example, Representation Bias[99] touches most of well known datasets (ImageNet [100],OpenImages [101]), since the population of data is biasedtowards one or another demographic group [102]. Anothercommon example is Population Bias, which \u201darises whenstatistics, demographics, representatives, and user charac-teristics are different in the user population represented inthe dataset or platform from the original target population\u201d[103]. In other words, Representation bias is hidden insidethe way of gathering data from the population, meanwhilePopulation Bias might be stashed in the population itself.Last couple of years, scientists around the world areare biased toward protected attributes such as age, genderand race. And indeed, many of them are. Usually, AI is rep-resented as trained neural networks, therefore, mimickinghuman behavior and being trained on datasets gathered byhumans, these networks overtake one of the most prevail-ing bias - race bias. However, racist behaviour by defaultcould not be expressed by AI, since it does not have ownmind. Hence, usually race bias is inherited implicitly fromthe data, on which neural network has been trained. Anexcellent examples are researches about a race bias in facerecognition [104, 105, 106].Although there are many existed biases, in this paper,we focus on a gender bias in FER. Unfortunately, only fewstudies have been conducted on examining different biasesin FER, particularly gender bias. In [107] authors target un-derrepresented class, showing discrimination in AI for FERsolutions. Race bias has been found in Microsoft\u2019s Face andFace++ API\u2019s by [108]. Several studies have proven[109, 110]a correlation between gender and smile on the face (whichimplicitly mean positive emotion) in CelebA dataset [111].[112], one of the most recent complete studies of bias andfairness in FER targets three different biases: age, race andgender. Authors of this research perform experiments onaforementioned CelebA [111] database and RAF-DB [113].However, although authors divide test data into entirelymale and female groups, they do not provide conclusionsregarding per class differences in accuracy. Instead of that,they show that model is biased towards female group, gain-ing overall 3% less accuracy score. Another outcome fromthe study is that according to the Equal opportunity metric(Formula 2), gender bias is the smallest, comparatively tothe age and race biases.3 MThis section contains detailed description of data, datapreprocessing and deep learning models that were used inexperiments. Data preprocessing comprises face extraction,decreasing sample size and data augmentation. There are \ufb01ve different architectures from two domains would belisted in total.3.1 Data descriptionThe database from University of Tartu has been taken asa target database for research. It contains video on whichpeople are displaying emotions. Despite other databases forFER, frame resolution is quite high: 1280 x 960. There are 50persons and 12 videos recorded per each, therefore in total600 videos. Each video has high frame rate, which is equalto 100 frame per second (FPS). As was mentioned before,there are six types of emotions: happiness, surprise, sadness,disgust, anger and contempt. Originally, this database havea bit different purpose, therefore, it has additional feature -videos are divided into the genuine and fake expressions.In other words, from 12 videos, on 6 a subject is expressingtrue emotion, on other 6 - fake.Displaying of arbitrary emotion is always started fromneutral emotion, following a command and expressionwhich was asked. Unfortunately, there are no timestampconsiderable issue and approach to solving it will be ex-plained further. Some samples from the dataset are shownon Figure 1. Fig. 1: SASE-FE database examples3.2 Data preprocessing3.2.1 Face extractionUsually, to perform face extraction, researchers use defaultmethod to extract faces by adopting Histogram of OrientedGradients (HOG) descriptors [114]. DLIB [115] implementa-tion has been used for this task. However, it does not workwell for particular dataset. Because of rich representationof skin color in dataset, the given face extractor could notextract faces for all subjects. More precisely, a race biashas been faced, since even with histogram equalization,it was not possible to extract faces for people with darkskin color. Hence, it has been decided to use another,more advance, approach - Multitask Cascade ConvolutionalNetworks (MTCNN) [116]. Using this model, four types ofdataset were generated, each of them has different margin(0, 25, 65, 40). The size of the cropped frame is 256 \u00d7 256.2) Random rotation3) Brightness augmentationEach augmentation is applied independently within p =0.5 probability on each sample. Figure 4 shows example ofaugmented train data.Fig. 4: Examples of augmented data3.3 Deep learning models for FER3.3.1 Common layers3.3.1.1 Convolution layer: Convolution layer is acentral building block for deep neural network for imagedata. Basically, this layer implement convolution operation,i.e. convolves the input image with given kernel. Dependswhether a layer works on 2D or 3D data, convolutionoperation is performed along 2 or 3 dimension respectively.In case of 2D, a kernel moves along spatial dimension,meanwhile in 3D case along temporal dimension too. Im-plementation details and parameters can be found in [119].3.3.1.2 Batch normalization: Essential presented in[120], this layer helps models to reduce impact of randomweight initialization, allowing to converge faster and toavoid unstable gradients. In fact, batch normalization layeronly standardizes the output of the layer, by shifting andscaling with mean and standard deviation respectively. Im-plementation details of PyTorch BatchNorm2d and Batch-Norm3d can be found in [119]3.3.1.3 Recti\ufb01ed linear unit layer: The simplest layerwithing single function: to shrink all negative values, re-placing them with zeros. In other words, applying functionx = max(0, x).3.3.1.4 Pooling layer: Pooling layer is somewhatsimilar to the convolution layer, however, does not con-tain any weights, instead just applies speci\ufb01c operation tothe perception map. As a results, Max Pooling Layer andAverage Pooling Layer are named after the function theyapply. In general, pooling layer produces a single value byapplying a function (maximum or average) to the perception\ufb01eld, sliding over input tensor dimensions. Detail informa-tion and exact implementation can be found in [119]term memory (LSTM): Intro-duced in 90s by [121], after huge success in 2013 [122],LSTM layer became a core module for any sequence relateddeep learning architecture. In this paper, LSTM layer is3.3.1.5 Long-shortFig. 2: Examples of cropped images3.2.2 Decreasing sizeSince database presented as video with high FPS rate, it wason High Performance Cluster (HPC). To decrease numberof frames without loss of the information, K-Means [117,118] algorithm was used. Simple yet effective, it allows todetermine K most distinguishable frames for each video.For the test purposes, it was generated K = 10, 20, 50. ForK = 10 the entire video was processed. For K = 20, \ufb01rst100 frames (1 sec) are excluded. For K = 50, 20 frames fromthe \ufb01rst half of the video and 30 from the second.Fig. 3: Example of emotion display through sequence offramesThe number of videos in database is relatively small. Sincedeep learning approaches operates better on large scale ofdata, data augmentation has been applied during training.In total, there are three augmentation techniques ap-plied:1) Horizontal \ufb02ipused together with CNN feature extractors, since latter areoperated only on single images, meanwhile the target inputis video. LSTM itself has sophisticated structure, with 4internal vectors named input gate(i), output gate(o), forgetgate(f ) and cell (C). Along with these vectors, on eachprocessing step LSTM contains hidden state (h) and cellinput activation ( \u02dcC) vectors. Visual explanation of howLSTM module operates is shown on Figure 5, where t istime step, U and W are parameters matrix, which have tobe learn. Implementation details could be found in [119]. 3.3.3 ResNet50-LSTMAnother great example of a model, which is a result ofconsecutive research in deep learning is ResNet50 [125].Following the same principle in construction, the model ispresented as a CNN feature extractor. ResNet50 backbonebegins with a sequence of convolutional layer, batch nor-malization, ReLU and MaxPooling. Next, four layers with 3,4, 6 and 3 Bottleneck blocks respectively. Model ends withAvgPooling layer to reduce output feature map size. At thispoint there are no more backbone layers, and data streamgoes down to LSTM module, following by fully connectedlayer. The entire architecture is visualised on Firuge 7. Sincethe model is too deep by itself, ImageNet pretrained weightsare used. In this paper, when we are referring to ResNet50,meaning entire architecture, which consists of a backboneand LSTM module.Fig. 5: LSTM module [123]3.3.1.6 Fully Connected layer (FC): Often the \ufb01nallayer, the only purpose of which is to apply af\ufb01ne transformy = W \u00d7 x + b, where x is input vector, y is output vector,W weights and b bias. Exact implementation which has beenused could be found in [119].3.3.2 VGG-LSTMOne of the most famous model for face recognition whichhas been widely used and has become a background forfurther research is VGG16 architecture [124], named afterVisual Geometry Group, who has conducted research. Inthis work, VGG16 model serve as a feature extractor whichpreceded LSTM block. Together with a fully connected layerat the end, they create a model for classi\ufb01cation sequentialdata. The backbone model consists of series of blocks, whichare two or three convlutional layers with ReLU and theMaxPooling layer on the end. The model ends with fullyconnected layer. Exactly this layer serves as feature vector,which further goes to LSTM module, next down to thefully connected layer. The entire structure is on Figure 6.As were mentioned before, VGG architecture is wide used,therefore there are many good pretrained weights availablefor the research. It has been chosen to use pretrained weightsfrom ImageNet dataset [100] and VGGFace dataset [124]. Tosimplify further presentation of results, VGG16 architecturewith pretrained ImageNet weights is denoted as VGG16,meanwhile VGG16 architecture with pretrained VGGFaceweights is denoted as VGGFACE. Fig. 7: Architecture of ResNet50-LSTM. Blue: ResNet50backbone with pretrained weights. Red: layers trained fromthe scratch.3.3.4 SENet-LSTMIn [126] authors discovered, that integrating mechanism forlearning relationship between channels can considerablyimprove results. The goal of such block is to explicitly traina network the cross-correlation between spatial channels.The main advantages of this approach are lightweight, sim-plicity to implement and application to all common layers.Basically, for any given transformation F : X \u2212\u2192 U , \ufb01rst,squeeze the feature map into the descriptor vector and thenmultiply it to the feature block channel wise. See Figure 8.Fig. 6: Architecture of VGGLSTM. Blue: VGG16 backbonewith pretrained weights. Red: layers trained from thescratch. Fig. 8: Squeeze-and-Excitation block [126]In general, architecture of SENet-LSTM consists of twoparts: a backbone and LSTM module. The backbone isSENet model, which copies the architecture of ResNet50(blue blocks on Figure 7), however, each bottleneck blockenhanced with SE feature. In other words, Bottleneck layerplays role of function F , according to the Figure 8. TheLSTM module is a one layer LSTM layer withing one fullyconnected follow up layer. SENet backbone has 8096 outputfeatures. LSTM block has 256 hidden neurons. Since thenumber of training data is quite small, it has been chosento use pretrained weights for a backbone. Ready to useweights, trained of VGGFace2 dataset[127] for SENet wastaken from [128]. For purpose of convenience, further thisarchitecture is referred as SENetLSTM.3.3.5 3D-CNNAmong others approaches which utilize 3D CNN [129, 130]for FER, in [131] authors propose an architecture withinoptimized hyperparameters for CK+ and OULU-CASIA112 \u00d7 112 resolution frames. However, in purposes of thisresearch work, to be consistent with other network, theinput resolution has been changed to 224 \u00d7 224, and thesequence length varies. Hence, the model architecture hasbeen preserved up to fully connected layers, since theirsize is directly depended on the input. The \ufb01nal modelis presented on Figure 9. This model does not have anypretrained weights and trained from the scratch. In thefurther sections this model is referred as 3DCNN.Fig. 9: Architecture of 3D-CNN model3.3.6 ResNet3DIn 2018, a group of researchers proposed a new way totreat 3D data such as videos, presenting another two con-volutional blocks: mixed convolutions and \u201d(2+1)D\u201d con-volutions [132]. Utilizing these block, authors constructedResNet3D, an analogue of already famous ResNet2D. De\ufb01n-ing the clone of shorted version of ResNet - ResNet18 -ResNet3D has got three different implementations: with 3Dconvolutions, with mixed convolutions and with \u201d2+1\u201dDconvolutions. Since authors claim that the best performancehas been demonstrated by \u201d2+1\u201dD convolutions, in thispaper exactly this type of model is used. The idea behind\u201d2+1\u201dD convolutions is to think about 3D convolutions as2D convolutions in spatial space, which are followed by1D convolutions in the temporal space. The bene\ufb01ts fromthese approach are following: \ufb01rst, due to the additionalReLU after 2D convolutions, this block has twice morenumber of nonlinearities, increasing capacity of the model,second, authors claim that during the training, a tensorwise error rate is smaller, compared to the 3D convolutioncounterparts. The overall architecture of ResNet3D is similarto ResNet50, but with different blocks. The \ufb01nal modelstructure is presented on Figure 10. Fig. 10: Structure of ResNet3D-LSTM model3.4 Task de\ufb01nitionThe goal is to train a classi\ufb01er to classify an emotion outof six classes: surprise, contempt, disgust, anger, sad andhappy. Let us de\ufb01ne a neural network as probabilistic clas-si\ufb01er P , with parameters \u0398. In this paper, a classi\ufb01cation isperformed in the \u201dsoft\u201d form, i.e.\u02c6y = Sof tmax (P (Y = y|X, \u0398)) (4)where X is an input, Y is an output, y \u2208 0, 1, 2, 3, 4, 5according to the six type of emotions. Softmax, in turn, isde\ufb01ned as followsSof tmax(x ) = e e(cid:80) (5)4 EThe following section has an overview of all details regard-ing experiments. Hence, below will be described data divi-sion and hyperparameters. In addition, this section containsdescription of necessary metrics, based on which conclusionwould be derived.4.1 Input sizeThe raw instance of database is a video \ufb01le. Due to highresolution of videos and limited computational capacity,as was mentioned in 3.2.2, each video has been squeezedfor particular number of frames. However, the observationsare that: within K = 20 models generalize data the best.Using K = 10, there were not any adequate results andwith K = 50 there were not observed any increase of inperformance. Moreover, due to the limited computationalresources, bigger length of the input leads to decreasing ofbatch size, consequently increasing training time or even notallowing to \ufb01t a model. The input resolution is (224 \u00d7 224)for all models, except ResNet3D. Due to the enormousnumber of weights, available computational capacity is notenough to \ufb01t the model. Hence, the input resolution forResNet3D is (112 \u00d7 112). Putting all together, the inputsample for ResNet3D is (10, 3, 112, 112) and for all othermodels is (10, 3, 224, 224).4.2 Data divisionSASE-FE dataset contains 18 female and 32 male subjects.The instances for the test group have been selected ran-domly and once. Overall we had selected 5 males and 5females. Hence, all videos corresponded to these persons arenot visible anyhow during the training. Since each personhas 12 videos, therefore test set contains 120 videos, 20videos per emotion.Often, during training process a model tends to under\ufb01tor over\ufb01t. To track these kinds of behavior, making trainingprocess more ef\ufb01cient and to pick the best model, it has beendecided to organize a validation test with \ufb01xed size as 15%each train run.To sum up, each training process contains train andvalidation sets with 32 and 8 subjects correspondingly. Mul-tiplying by 12, there are 384 train videos and 96 validationvideos. Test set with total 120 videos is not involved duringtraining process. The desired metrics which are related tothe goals of this paper are calculated exactly from inferenceon the test set.4.3 Training detailsAll training experiments were performed on University ofTartu HPC. The target GPU is NVIDIA Tesla-V100 with 32GB of VRAM. All code is written in Python with usage ofmachine learning python package - PyTorch. The hyperpa-rameter search during the entire research was consisted oftwo phases for each model. First phase of hyperparametersearch was performed to \ufb01nd out the optimal model, hencetarget hyperparameters were such things as number of fullyconnected layers, number of LSTM layers, number of nodesin these layers. After \ufb01nding an optimal structure, in thesecond phase the optimal set of training hyperparametersfor each model have been discovered. In the scope of ex-periments, such variables have been varied: learning rate,batch size and weight decay. The \ufb01nal hyperparameters arepresented in Table 1.TABLE 1: Hyperparameters values for each model Test set, Male set and Female set. Moreover, it was decidedto look how performance and metrics differ depends ontraining data. For this purpose, all train data also wasdivided into pure male train set and pure female trainset. Therefore, to follow results easier, models which aretrained on entire train set are called Regular, trained on puremale set and pure female set are called Male and Femalerespectively. Training of all models have been performedusing Stochastic Gradient Decent (SGD) optimization. Tosum up, it has been trained 18 different neural networks,6 different models by 3 different train sets.4.4 MetricsAccording to the previously mentioned de\ufb01nitions of fair-ness, the main metrics, based on which any conclusioncould be made are accuracy, true positive rate (TPR) andfalse positive rate (FPR). Although these metrics are trivial,formal de\ufb01nitions are listed below:ACC = T P + N TT P + T N + F P + F NT P R =F P R = T PT P + F NF PF P + T N (7)(8)where T P - true positive, F P - false positive, T N - truenegative, F N - false negative, ACC - accuracy, T P R andF P R - true positive and false positive rate respectively.The \ufb01rst de\ufb01nition of fairness (Equation 1) implies theequality of TPR and FPR simultaneously. The second def-inition (Equation 2), extending to the multi classi\ufb01cationcase, requires equal TPR for the groups with differentunprotected attribute value. And the third, and the last,de\ufb01nition of fairness (Equation 3) means the equal accuracyfor separated groups. In other words, the difference in thesemetrics shows how fair model is, having linear dependency- the higher discrepancy, the more biased model is.Although target database comprises six different emo-tions, usually it is extremely dif\ufb01cult to distinguish betweencontempt, disgust and anger. In order to relax constraintsand make classi\ufb01cation a bit easier, without losing generalidea, it has been decided to fuse these three emotions into asingle emotion and name it \u201dUpset\u201d.5 RThis section consists of 2 parts. First part contains confusionmatrices and shallow analysis of each. Second part is servedfor more detailed and precise analysis from the perspectiveof fairness, emotions and test sets.5.1 Confusion matricesIn order to perform gender bias analysis, test set has beendivided into two subsets: pure male set and pure female set,such that each has 5 subjects. In this scenario, pure means toinclude subjects of only one gender. Hence, from here andfurther, there are three sets for testing, named as follows: Regular models have shown decent performance on Test set.The major part of emotions are classi\ufb01ed correctly, however,Upset and Sad emotions are frequently misclassi\ufb01ed, whichis an expected outcome, since they are close in V-A space.Confusion matrices are shown on Figure 11.Fig. 11: Confusion matrix per method for Regular modelson Test set Fig. 13: Confusion matrix per method for Male models onTest setFig. 12: Confusion matrix per method for Female models onTest set Fig. 14: Confusion matrix per method for Regular modelson Female setA decent decrease in accuracy for Surprise, which indi-cates that female training data is lack of Surprise expression.Confusion matrices are on Figure 12.We can observe almost the same performance compara-tively to the Regular models, however, the misclasi\ufb01cationrate of Sad emotion as Upset is much higher. This impliesthat male training data includes more Sad samples whichare visually much closer to Upset emotion. Confusion ma-trices are on Figure 13.Regular models have shown almost excellent recognitionof Happy emotion for Female set. TPR for Surprise in nothigh, however, we observe less misclassi\ufb01cation betweenUpset and Sad. Confusion matrices are on Figure 14.Quite unexpected results for Female models on Femaleset. Only TPR for Happy emotion is high enough to consideras acceptable. Recognition of Surprise is very low, whichmeans that female training data has weak samples forSurprise. Confusion matrices are on Figure 15.As has been stated before, Female training data has weakSurprise samples, meanwhile Male models have shownaverage performance, hence, even with different genderdomain, male samples of Surprise are much stronger. Con-fusion matrices are on Figure 16.Comparatively to other test sets, inference of Regular Fig. 15: Confusion matrix per method for Female models onFemale setmodels on Male set has much lower TPR for Happy emotionand much higher TPR for Surprise emotion. Confusionmatrices are on Figure 17.Female models on the Male set have probably the worstmetrics overall, due to the lack of data in train data andopposite gender in test data. Confusion matrices are onperformance expectations have been higher. Confusion ma-trices are on Figure 19.Fig. 19: Confusion matrix per method for Male models onMale set5.2 AnalysisThe goal of this research work is to analyse a gender bias,according to the de\ufb01nition of fairness. Raw results on whichthe following analysis is based are listed in Appendix 7. Inthe matter of convenience, several chars are presented inAppendix 7.4.According to equal opportunity (EQOP) and equalizedodds (EQOD) (Figure 20 (a, b)), the least biased is ResNet3D,while VGGFACE is the most unfair. Meanwhile, 3DCNNis second in term of biasness. However, comparison inaccuracy difference (demographic parity (DP)) (Figure 20(c)) consider the most fair model VGG16, when ResNet3Dis the second most fair one. As for the most biased modelsaccording to DP, these are 3DCNN and VGGFACE. Hence,overall results are consider to be aligned along all threede\ufb01nitions of fairness.(a) (b) (c)Fig. 20: Metrics for models, which have been trained onentire train dataAnalysing aggregated results (Figure 21), several conclu-sions have been derived. For the Test and Male set, Regularmodels show the best accuracy for classi\ufb01cation Surprise,while on the Female set, the best accuracy is for Happyemotion. Fused Upset emotion has the lowest recognitionrate for all three sets. Worth to mention, inference of Femaleset has much higher variance rather than on Male set.However, average accuracy for Female set is higher.From the perspective of emotions (Figure 26), results asfollows: classi\ufb01cation of Surprise is better for males, Upsetand Sad are expressed better by females and Happy isalmost identical recognized for both genders.Fig. 16: Confusion matrix per method for Male models onFemale setFig. 17: Confusion matrix per method for Regular modelson Male setFigure 18.Fig. 18: Confusion matrix per method for Female models onMale setMale models on Male set have shown great accuracyfor Surprise and Happy emotions. There are high TPR forUpset, however, misclassi\ufb01cation rate of Sad as Upset ishigh too. Since train and test data share the same gender,Fig. 21: Aggregated accuracy for models, trained on theentire data. Comparison for three different test sets. (a) (b) (c)Trained only on female data, Female models show com-pletely different picture. For all three de\ufb01nition of fairness,SENetLSTM architecture is considered as the most genderbiased. 3DCNN is the least biased according to EQOP andEQOD (Figure 22 (a,b)), and VGG16 according to the DP(Figure 22 (c)). For each test set, classi\ufb01cation accuracy isthe worst for Upset emotion, Happy is the best recognisedfor Test set and Female set. Inference on Male set shows thebest recognition of Surprise. Fig. 24: Metrics for models, which have been trained on onlymale datamale set has high variance, being unstable, while inferenceon Male set has small variance, and therefore, more robust.Without a surprise here, average accuracy on Male set ishigher rather than on Female set.(a) (b) (c)Fig. 22: Metrics for models, which have been trained on onlyfemale dataAccording to aggregated results (Figure 23), variancesfor Male and Female set are relatively equal, thereforerecognition for both gender is considered as robust. Theoverall accuracy is better for Male set, which is unexpectedresults, since models are trained exclusively on the femaledata.In the per emotion competition (Figure 27), classi\ufb01cationof Happy and Surprise emotions are almost identical forboth genders, meanwhile Upset and Sad are better recog-nized on female subjects.Opposite to previously mentioned type of models, Malemodels are trained on male data. SENetLSTM architecture isshown as the most fair architecture, according to the EQOP,EQOD and DP. For the most biased model, ResNet50 is con-sidered of being so with respect to EQOP and EQOD, whileResNet3D is a choice according to DP (Figure 24). Noticeable,the according to DP, ResNet50 and VGGFACE share secondthe most biased place in ranking. Therefore, results for Malemodels are considered to be consistent. For different testsets, the best and the worst accurate classi\ufb01ed emotion arethe same. Upset emotion has the lowest accuracy among alltest sets and Happy - the highest one.Aggregate results (Figure 25) show that inference on Fe-Fig. 23: Aggregated accuracy for models, trained on onlyfemale data. Comparison for three different test sets. Fig. 25: Aggregated accuracy for models, trained on onlymale data. Comparison for three different test sets.In the per emotion accuracy (Figure 28), inference onMale set leads in recognition of Surprise and Happy emo-tions. Sad recognition rate is almost the same for both sets.Accuracy for Upset emotion is much higher for Female set.6 CThis paper provided a comprehensive overview on the faceemotion recognition biases in the context of reliable AI.Taking the concrete dataset, SASE-FE, two different groupsof methods for face emotion recognition have been analyzedon gender bias. Each group consists of three different neu-ral network architectures, where some of them have beenready available and some have been manually implementedbefore the analysis. The test sets have been organized inthree different ways: entire test data, only male data andonly female data. The train sets have been organized in thesimilar way. All architectures have been trained, resultinginto 18 different models, 6 architectures per each train set.Since model bias can be explained through fairness,there have been given three different de\ufb01nition of fairness,according to which proper analysis have been conducted.As a results, it has been found which architectures are mostbiased with respect to the de\ufb01nitions of fairness, and whichones are more likely to be fair. In addition, it has beendiscovered, which kinds of emotions are easier to recognizefor men and women. In addition, using three distinct trainsets the relationship between training data and inference hasbeen analyzed.The topic of gender bias is relatively young and not ad-dressed properly. Therefore, the amount of existed probabledirections to research is immense. Extending this researchwork, for sure, the next goal has to be to expand researchon other databases, which are widely used in FER. Also,the models which have been utilized in this paper, are notcompetitors to the state-of-the-art solutions. Hence, anotherdirection of future work is to implement these solutions andanalyze whether they comprise gender bias. Far-reachingextensions include other aspects of RAI and XAI. For ex-ample study of other biases (race, age, culture) or workingon explanation, understanding and discovering knowledgelimits. Altogether, these researches will create a backgroundto more standardized regulation and law creation in the \ufb01eldof AI. As a result, integration of AI in society will be reliableand safe. After all, modern AI still encompasses a decentamount of unknown and hazard, therefore future us haveto be ready.AThis work is supported by the Estonian Centre of Excellencein IT (EXCITE) funded by the European Regional Develop-ment Fund. The authors also gratefully acknowledge thesupport of NVIDIA Corporation with the donation of theTitan XP Pascal GPU.R [1] Aaron Smith and Janna Anderson. \u201cAI, Robotics, andthe Future of Jobs\u201d. In: Pew Research Center 6 (2014),p. 51.[3] intelligence: Who is[2] Emanuele Neri et al. Arti\ufb01cialresponsible for the diagnosis? 2020.Jean-Franc\u00b8ois Bonnefon, Azim Shariff, and Iyad Rah-wan. \u201cThe social dilemma of autonomous vehicles\u201d.In: Science 352.6293 (2016), pp. 1573\u20131576.James Zou and Londa Schiebinger. AI can be sexistand racist\u2014it\u2019s time to make it fair. 2018.[4][5] Eric Mack. Hawking, Musk, Wozniak Warn About Ar-ti\ufb01cial Intelligence\u2019s Trigger Finger. https : / / www .forbes.com/sites/ericmack/2015/07/27/hawking-musk-wozniak-freaked-about-arti\ufb01cial-intelligence-getting-a-trigger-\ufb01nger/?sh=7ad6f69f7416.[6] Frank Rosenblatt. \u201cThe perceptron: a probabilisticmodel for information storage and organization inthe brain.\u201d In: Psychological review 65.6 (1958), p. 386.[7] Seppo Linnainmaa. \u201cThe representation of the cu-mulative rounding error of an algorithm as a Taylorexpansion of the local rounding errors\u201d. In: Master\u2019sThesis (in Finnish), Univ. Helsinki (1970), pp. 6\u20137.[8] Ning Qian and Terrence J Sejnowski. \u201cPredicting thesecondary structure of globular proteins using neuralnetwork models\u201d. In: Journal of molecular biology 202.4(1988), pp. 865\u2013884.algorithmwatch.org. Finnish Credit Score Ruling raisesQuestions about Discrimination and how to avoid it.https : / / algorithmwatch . org / en / story / \ufb01nnish -credit - score - ruling - raises - questions - about -discrimination-and-how-to-avoid-it/.[9][10] Daisuke Wakabayashi. Self-Driving Uber Car KillsPedestrian in Arizona, Where Robots Roam. https : / /www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html.[11] Niraj Chokshi. Tesla Autopilot System Found Probablyat Fault in 2018 Crash. https://www.nytimes.com/2020/02/25/business/tesla-autopilot-ntsb.html.Julia Angwin. Facebook Enabled Advertisers to Reach\u2018Jew Haters\u2019. https://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters.[12] [13] Veronica Rocha. Crime-\ufb01ghting robot hits, rolls overchild at Silicon Valley mall. https : / / www. latimes .com/local/lanow/la- me- ln- crime\ufb01ghting- robot-hurts-child-bay-area-20160713-snap-story.html.John Kingston. \u201cArti\ufb01cial intelligence and legal lia-bility\u201d. In: arXiv preprint arXiv:1802.07782 (2018).[15] Davide Carneiro et al. \u201cOnline dispute resolution:an arti\ufb01cial intelligence perspective\u201d. In: Arti\ufb01cialIntelligence Review 41.2 (2014), pp. 211\u2013240.[14][16] Keri Stephens. \u201cRecent Studies Examine Lunit AIin Breast Cancer Detection\u201d. In: AXIS Imaging News(2020).[17] Appen Wilson Pang. Responsible AI becomes criticalin 2021. https : / / venturebeat . com / 2020 / 11 / 11 /responsible-ai-becomes-critical-in-2021/.[18] Tableau. 2019 Business Intelligence Trend. https : / /www. tableau . com / reports / business - intelligence -trends/machine-learning.[19] Mariusz Bojarski et al. \u201cEnd to end learning forself-driving cars\u201d. In: arXiv preprint arXiv:1604.07316[20] Martin Tammvee and Gholamreza Anbarjafari. \u201cHu-man activity recognition-based path planning forautonomous vehicles\u201d. In: Signal, Image and VideoProcessing (2020), pp. 1\u20138.[21] Kwang-Eun Ko and Kwee-Bo Sim. \u201cDeep convolu-tional framework for abnormal behavior detection ina smart surveillance system\u201d. In: Engineering Applica-tions of Arti\ufb01cial Intelligence 67 (2018), pp. 226\u2013234.[22] Eric Marchand, Hideaki Uchiyama, and FabienSpindler. \u201cPose estimation for augmented reality: ahands-on survey\u201d. In: IEEE transactions on visualiza-tion and computer graphics 22.12 (2015), pp. 2633\u20132651.[23] Dorota Kami \u00b4nska et al. \u201cStress Reduction Using Bi-lateral Stimulation in Virtual Reality\u201d. In: IEEE Access8 (2020), pp. 200351\u2013200366.[24] Patricio Loncomilla, Javier Ruiz-del-Solar, and LuzMartinez. \u201cObject recognition using local invariantfeatures for robotic applications: A survey\u201d. In: Pat-tern Recognition 60 (2016), pp. 499\u2013514.[25] Anastasia Bolotnikova, Hasan Demirel, and Gho-lamreza Anbarjafari. \u201cReal-time ensemble based facerecognition system for NAO humanoids using localbinary pattern\u201d. In: Analog Integrated Circuits andSignal Processing 92.3 (2017), pp. 467\u2013475.[26] Anastasia Bolotnikova et al. \u201cA circuit-breaker use-case operated by a humanoid in aircraft manufac-turing\u201d. In: 2017 13th IEEE Conference on AutomationScience and Engineering (CASE). IEEE. 2017, pp. 15\u201322.[27] Baris Kayalibay, Grady Jensen, and Patrick van derSmagt. \u201cCNN-based segmentation of medical imag-ing data\u201d. In: arXiv preprint arXiv:1701.03056 (2017).[28] Luis G \u00b4omez-Chova et al. \u201cMultimodal classi\ufb01cationof remote sensing images: A review and future di-rections\u201d. In: Proceedings of the IEEE 103.9 (2015),pp. 1560\u20131584.[29] Andreas Kamilaris and Francesc X Prenafeta-Bold \u00b4u.\u201cDeep learning in agriculture: A survey\u201d. In: Comput-ers and electronics in agriculture 147 (2018), pp. 70\u201390.[30] Jianzhu Guo et al. \u201cDominant and complementaryemotion recognition from still images of faces\u201d. In:IEEE Access 6 (2018), pp. 26391\u201326403.[31] Egils Avots et al. \u201cEnsemble approach for detectionof depression using EEG features\u201d. In: arXiv preprintarXiv:2103.08467 (2021).[32] Kadir Aktas et al. \u201cSpatio-Temporal Based TableTennis Stroke Type Assessment\u201d. In: Signal, Image andVideo Processing (2021), pp. 1\u20138.[33] Paula M Niedenthal and Markus Brauer. \u201cSocialfunctionality of human emotion\u201d. In: Annual reviewof psychology 63 (2012), pp. 259\u2013285.[34] Eric M Reiman et al. \u201cNeuroanatomical correlatesof externally and internally generated human emo-tion\u201d. In: American Journal of Psychiatry 154.7 (1997),pp. 918\u2013925.[35] Fatemeh Noroozi et al. \u201cSurvey on emotional bodygesture recognition\u201d. In: IEEE transactions on affectivecomputing (2018).[36] Gholamreza Anbarjafari et al. Machine Learning forFace, Emotion, and Pain Recognition[37] Raymond J Dolan. \u201cEmotion, cognition, and behav-ior\u201d. In: science 298.5596 (2002), pp. 1191\u20131194.John G Carlson and Elaine Hat\ufb01eld. Psychology ofemotion. Harcourt Brace Jovanovich, 1992.[38][39] Mengyi Liu et al. \u201cDeeply learning deformable facialaction parts model for dynamic expression analysis\u201d.In: Asian conference on computer vision. Springer. 2014,pp. 143\u2013157.[40] Chien-Hsu Chen, I-Jui Lee, and Ling-Yi Lin. \u201cAug-mented reality-based self-facial modeling to promotethe emotional expression and social skills of adoles-cents with autism spectrum disorders\u201d. In: Researchin developmental disabilities 36 (2015), pp. 396\u2013403.[41] Agata Ko\u0142akowska et al. \u201cEmotion recognition andits applications\u201d. In: Human-Computer Systems Inter-action: Backgrounds and Applications 3. Springer, 2014,pp. 51\u201362.[42] Y-I Tian, Takeo Kanade, and Jeffrey F Cohn. \u201cRec-ognizing action units for facial expression analysis\u201d.In: IEEE Transactions on pattern analysis and machineintelligence 23.2 (2001), pp. 97\u2013115.James A Russell. \u201cA circumplex model of affect.\u201d In:Journal of personality and social psychology 39.6 (1980),p. 1161.[43][44] Paul Ekman. \u201cBasic emotions\u201d. In: Handbook of cogni-tion and emotion 98.45-60 (1999), p. 16.[45] Christer Loob et al. \u201cDominant and complementarymulti-emotional facial expression recognition usingc-support vector classi\ufb01cation\u201d. In: 2017 12th IEEEInternational Conference on Automatic Face & GestureRecognition (FG 2017). IEEE. 2017, pp. 833\u2013838.[46] Tomasz Sapi \u00b4nski et al. \u201cMultimodal database ofemotional speech, video and gestures\u201d. In: Inter-national Conference on Pattern Recognition. Springer.2018, pp. 153\u2013163.[47] Gary B Huang et al. \u201cLabeled faces in the wild:A database forstudying face recognition in uncon-strained environments\u201d. In: Workshop on faces in\u2019Real-Life\u2019Images: detection, alignment, and recognition. 2008. [48] Abhinav Dhall et al. \u201cVideo and image based emo-tion recognition challenges in the wild: Emotiw2015\u201d. In: Proceedings of the 2015 ACM on internationalconference on multimodal interaction. 2015, pp. 423\u2013426.and MattiPietik\u00a8ainen. \u201cFace recognition with local binarypatterns\u201d. In: European conference on computer vision.Springer. 2004, pp. 469\u2013481.[49] Timo Ahonen, Abdenour Hadid,[50] Xiaoyi Feng, Matti Pietik\u00a8ainen, and Abdenour Ha-did. \u201cFacial expression recognition based on localbinary patterns\u201d. In: Pattern Recognition and ImageAnalysis 17.4 (2007), pp. 592\u2013598.[51] Rajiv Mehrotra, Kameswara Rao Namuduri, and Na-garajan Ranganathan. \u201cGabor \ufb01lter-based edge de-tection\u201d. In: Pattern recognition 25.12 (1992), pp. 1479\u20131494.[52] Michael Lyons et al. \u201cCoding facial expressions withgabor wavelets\u201d. In: Proceedings Third IEEE interna-tional conference on automatic face and gesture recogni-tion. IEEE. 1998, pp. 200\u2013205.expression recognition based on Gabor wavelets andsparse representation\u201d. In: 2012 IEEE 11th Interna-tional Conference on Signal Processing. Vol. 2. IEEE.2012, pp. 816\u2013819.[54] Govardhan Mattela and Sandeep K Gupta. \u201cFacialexpression recognition using Gabor-mean-DWT fea-ture extraction technique\u201d. In: 2018 5th InternationalConference on Signal Processing and Integrated Networks(SPIN). IEEE. 2018, pp. 575\u2013580.[55] Taskeed Jabid, Md Hasanul Kabir, and Oksam Chae.\u201cRobust facial expression recognition based on lo-cal directional pattern\u201d. In: ETRI journal 32.5 (2010),pp. 784\u2013794.[56] Zhen Wang and Zilu Ying. \u201cFacial expression recog-nition based on local phase quantization and sparserepresentation\u201d. In: 2012 8th International Conferenceon Natural Computation. IEEE. 2012, pp. 222\u2013225.[57] Wei-Lun Chao, Jian-Jiun Ding, and Jun-Zuo Liu.\u201cFacial expression recognition based on improved lo-cal binary pattern and class-regularized locality pre-serving projection\u201d. In: Signal Processing 117 (2015),pp. 1\u201310.[58] Abu Sayeed Md Sohail and Prabir Bhattacharya.\u201cClassi\ufb01cation of facial expressions using k-nearestneighbor classi\ufb01er\u201d. In: International Conference onComputer Vision/Computer Graphics Collaboration Tech-niques and Applications. Springer. 2007, pp. 555\u2013566.[59] Qirong Mao et al. \u201cHierarchical Bayesian thememodels for multipose facial expression recognition\u201d.IEEE Transactions on Multimedia 19.4 (2016),In:pp. 861\u2013873.[60] Ming-Wei Huang, Zhe-wei Wang, and Zi-Lu Ying.\u201cA new method for facial expression recognitionbased on sparse representation plus LBP\u201d. In: 20103rd International Congress on Image and Signal Process-ing. Vol. 4. IEEE. 2010, pp. 1750\u20131754.[61] Hung-Hsu Tsai and Yi-Cheng Chang. \u201cFacial expres-sion recognition using a combination of multiplefacial features and support vector machine\u201d. In: SoftComputing 22.13 (2018), pp. 4389\u20134405.[62] Alex Krizhevsky, Ilya Sutskever, and Geoffrey EHinton. \u201cImagenet classi\ufb01cation with deep convolu-tional neural networks\u201d. In: Advances in neural infor-mation processing systems 25 (2012), pp. 1097\u20131105.[63] Peter Burkert et al. \u201cDexpression: Deep convolu-tional neural network for expression recognition\u201d. In:arXiv preprint arXiv:1509.05371 (2015).[64] Xiangyun Zhao et al. \u201cPeak-piloted deep network forfacial expression recognition\u201d. In: European conferenceon computer vision. Springer. 2016, pp. 425\u2013442.[65] Xiaoguang Chen et al. \u201cConvolution neural networkfor automatic facial expression recognition\u201d. In: 2017International conference on applied system innovation(ICASI). IEEE. 2017, pp. 814\u2013817.[66] Biao Yang et al. \u201cFacial expression recognition us-ing weighted mixture deep neural network basedon double-channel facial images\u201d. In: IEEE Access 6(2017), pp. 4630\u20134640.[67] Abir Fathallah, Lot\ufb01 Abdi, and Ali Douik. \u201cFa-cial expression recognition via deep learning\u201d. In:2017 IEEE/ACS 14th International Conference on Com-puter Systems and Applications (AICCSA). IEEE. 2017,pp. 745\u2013750.[68] Thanh-Hung Vo et al. \u201cPyramid with Super Resolu-tion for In-the-Wild Facial Expression Recognition\u201d.In: IEEE Access 8 (2020), pp. 131988\u2013132001.[69] Emad Barsoum et al. \u201cTraining deep networks forfacial expression recognition with crowd-sourced la-bel distribution\u201d. In: Proceedings of the 18th ACM In-ternational Conference on Multimodal Interaction. 2016,pp. 279\u2013283.[70] Yuedong Chen et al. \u201cFacial motion prior networksfor facial expression recognition\u201d. In: 2019 IEEEVisual Communications and Image Processing (VCIP).IEEE. 2019, pp. 1\u20134.[72][71] Ali Mollahosseini, Behzad Hasani, and MohammadH Mahoor. \u201cAffectnet: A database for facial expres-sion, valence, and arousal computing in the wild\u201d. In:IEEE Transactions on Affective Computing 10.1 (2017),pp. 18\u201331.Ilke Cugu, Eren Sener, and Emre Akbas. \u201cMicroExp-Net: An Extremely Small and Fast Model For Expres-sion Recognition From Face Images\u201d. In: 2019 NinthInternational Conference on Image Processing Theory,Tools and Applications (IPTA). IEEE. 2019, pp. 1\u20136.[73] Guoying Zhao et al. \u201cFacial expression recognitionfrom near-infrared videos\u201d. In: Image and Vision Com-puting 29.9 (2011), pp. 607\u2013619.[74] Yunxin Huang et al. \u201cFacial expression recognition:A survey\u201d. In: Symmetry 11.10 (2019), p. 1189.[75] Shashank Jaiswal and Michel Valstar. \u201cDeep learningthe dynamic appearance and shape of facial actionunits\u201d. In: 2016 IEEE winter conference on applicationsof computer vision (WACV). IEEE. 2016, pp. 1\u20138.[76] Abhinav Dhall et al. \u201cEmotiw 2016: Video and group-level emotion recognition challenges\u201d. In: Proceedingsof the 18th ACM international conference on multimodalinteraction. 2016, pp. 427\u2013432.[77] Yin Fan et al. \u201cVideo-based emotion recognitionusing CNN-RNN and C3D hybrid networks\u201d. In: Proceedings of the 18th ACM International Conferenceon Multimodal Interaction. 2016, pp. 445\u2013450.[78] Dae Hoe Kim et al. \u201cMulti-objective based spatio-temporal feature representation learning robust toexpression intensity variations for facial expressionrecognition\u201d. In: IEEE Transactions on Affective Com-puting 10.2 (2017), pp. 223\u2013236.[79] Zhenbo Yu et al. \u201cSpatio-temporal convolutional fea-tures with nested LSTM for facial expression recog-nition\u201d. In: Neurocomputing 317 (2018), pp. 50\u201357.[80] Kaiming He et al. \u201cSpatial pyramid pooling in deepconvolutional networks for visual recognition\u201d. In:IEEE transactions on pattern analysis and machine intel-ligence 37.9 (2015), pp. 1904\u20131916.[81] Young-Hyen Byeon and Keun-Chang Kwak. \u201cFacialexpression recognition using 3d convolutional neuralnetwork\u201d. In: International journal of advanced com-puter science and applications 5.12 (2014).[82] Behzad Hasani and Mohammad H Mahoor. \u201cFacialexpression recognition using enhanced deep 3D con-Proceedings of theIEEE conference on computer vision and pattern recog-nition workshops. 2017, pp. 30\u201340.[83] Christian Szegedy et al. \u201cInception-v4, inception-resnet and the impact of residual connections onlearning\u201d. In: Proceedings of the AAAI Conference onArti\ufb01cial Intelligence. Vol. 31. 1. 2017.[84] Ruicong Zhi et al. \u201cCombining 3D convolutionalneural networks with transfer learning by super-vised pre-training for facial micro-expression recog-nition\u201d. In: IEICE Transactions on Information and Sys-tems 102.5 (2019), pp. 1054\u20131064.Jing Li et al. \u201cMicro-expression recognition based on3D \ufb02ow convolutional neural network\u201d. In: PatternAnalysis and Applications 22.4 (2019), pp. 1331\u20131339.[85][86] Chao Wu and Fan Guo. \u201cTSNN: Three-Stream Com-bining 2D and 3D Convolutional Neural Network forMicro-Expression Recognition\u201d. In: IEEJ Transactionson Electrical and Electronic Engineering 16.1 (2021),pp. 98\u2013107.[87] High-Level Expert Group on AI. Ethics guidelines fortrustworthy AI. https: // ec .europa . eu/ newsroom /dae/document.cfm?doc id=60419.[88] Microsoft. Microsoft AI principles. https : / / www .microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6.[89] PwC. 2019 AI Predictions. https://www.pwc.com/us / en / services / consulting / library / arti\ufb01cial -intelligence-predictions-2019.html.[90] PwC. 2020 AI Predictions. https://www.pwc.com/us / en / services / consulting / library / arti\ufb01cial -intelligence-predictions-2020.html.[91] Nripsuta Ani Saxena et al. \u201cHow do fairness def-initions fare? Examining public attitudes towardsalgorithmic de\ufb01nitions of fairness\u201d. In: Proceedingsof the 2019 AAAI/ACM Conference on AI, Ethics, andSociety. 2019, pp. 99\u2013106.[92] Moritz Hardt, Eric Price, and Nathan Srebro. \u201cEqual-ity of opportunity in supervised learning\u201d. In: arXivpreprint arXiv:1610.02413 (2016).[93] Matt J Kusner et al. \u201cCounterfactual fairness\u201d. In:arXiv preprint arXiv:1703.06856 (2017).[94] Ninareh Mehrabi et al. \u201cA survey on bias andIn: arXiv preprintfairness in machine learning\u201d.arXiv:1908.09635 (2019).Jiahao Chen et al. \u201cFairness under unawareness:Assessing disparity when protected class is unob-served\u201d. In: Proceedings of the conference on fairness,accountability, and transparency. 2019, pp. 339\u2013348.[95][96] Dorian Peters et al. \u201cResponsible AI\u2014two frame-works for ethical design practice\u201d. In: IEEE Transac-tions on Technology and Society 1.1 (2020), pp. 34\u201347.[97] Lu Cheng, Kush R Varshney, and Huan Liu. \u201cSo-cially Responsible AI Algorithms: Issues, Purposes,and Challenges\u201d. In: arXiv preprint arXiv:2101.02032(2021).[98] Faisal Kamiran and Indr \u02d9e \u02c7Zliobait \u02d9e. \u201cExplainableand non-explainable discrimination in classi\ufb01ca-tion\u201d. In: Discrimination and Privacy in the InformationSociety. Springer, 2013, pp. 155\u2013170.[100] understanding unintended consequences of machinelearning\u201d. In: arXiv preprint arXiv:1901.10002 (2019).Jia Deng et al. \u201cImagenet: A large-scale hierarchicalimage database\u201d. In: 2009 IEEE conference on computervision and pattern recognition. Ieee. 2009, pp. 248\u2013255.Ivan Krasin et al. \u201cOpenimages: A public dataset forlarge-scale multi-label and multi-class image clas-si\ufb01cation\u201d. In: Dataset available from https://github.com/openimages 2.3 (2017), p. 18.[101][102] Shreya Shankar et al. \u201cNo classi\ufb01cation without rep-resentation: Assessing geodiversity issues in opendata sets for the developing world\u201d. In: arXiv preprintarXiv:1711.08536 (2017).[103] Alexandra Olteanu et al. \u201cSocial data: Biases,methodological pitfalls, and ethical boundaries\u201d. In:Frontiers in Big Data 2 (2019), p. 13.[104] Brendan F Klare et al. \u201cFace recognition perfor-mance: Role of demographic information\u201d. In: IEEETransactions on Information Forensics and Security 7.6(2012), pp. 1789\u20131801.[105] Hachim El Khiyari and Harry Wechsler. \u201cFace veri\ufb01-cation subject to varying (age, ethnicity, and gender)demographics using deep learning\u201d. In: Journal ofBiometrics and Biostatistics 7.323 (2016), p. 11.[106] Cynthia M Cook et al. \u201cDemographic effects in facialrecognition and their dependence on image acquisi-tion: An evaluation of eleven commercial systems\u201d.In: IEEE Transactions on Biometrics, Behavior, and Iden-tity Science 1.1 (2019), pp. 32\u201341.[107] Ayanna Howard, Cha Zhang, and Eric Horvitz. \u201cAd-dressing bias in machine learning algorithms: A pilotstudy on emotion recognition for intelligent sys-tems\u201d. In: 2017 IEEE Workshop on Advanced Roboticsand its Social Impacts (ARSO). IEEE. 2017, pp. 1\u20137.[108] Lauren Rhue. \u201cRacial in\ufb02uence on automated per-ceptions of emotions\u201d. In: Available at SSRN 3281765(2018).[109] Emily Denton et al. \u201cDetecting bias with genera-tive counterfactual face attribute augmentation\u201d. In:arXiv preprint arXiv:1906.06439 (2019). [110] Zeyu Wang et al. \u201cTowards fairness in visual recog-nition: Effective strategies for bias mitigation\u201d. In:Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. 2020, pp. 8919\u20138928.[111] Ziwei Liu et al. \u201cDeep learning face attributes inthe wild\u201d. In: Proceedings of the IEEE internationalconference on computer vision. 2015, pp. 3730\u20133738.[112] Tian Xu et al. \u201cInvestigating bias and fairness in fa-cial expression recognition\u201d. In: European Conferenceon Computer Vision. Springer. 2020, pp. 506\u2013523.[113] Shan Li, Weihong Deng, and JunPing Du. \u201cReli-able Crowdsourcing and Deep Locality-PreservingLearning for Expression Recognition in the Wild\u201d. In:2017 IEEE Conference on Computer Vision and PatternRecognition (CVPR). IEEE. 2017, pp. 2584\u20132593.[114] Navneet Dalal and Bill Triggs. \u201cHistograms of ori-ented gradients for human detection\u201d. In: 2005 IEEEcomputer society conference on computer vision and pat-tern recognition (CVPR\u201905). Vol. 1. Ieee. 2005, pp. 886\u2013893.dlib C++ libraryJia Xiang and Gengming Zhu. \u201cJoint face detectionand facial expression recognition with MTCNN\u201d. In:2017 4th International Conference on Information Scienceand Control Engineering (ICISCE). IEEE. 2017, pp. 424\u2013427.[116][118][117] Stuart Lloyd. \u201cLeast squares quantization in PCM\u201d.In: IEEE transactions on information theory 28.2 (1982),pp. 129\u2013137.James MacQueen et al. \u201cSome methods for classi\ufb01-cation and analysis of multivariate observations\u201d. In:Proceedings of the \ufb01fth Berkeley symposium on mathemat-ical statistics and probability. Vol. 1. 14. Oakland, CA,USA. 1967, pp. 281\u2013297.[119] PyTorch Documentation. : https : / / pytorch . org /docs/stable/nn.html.[120] Sergey Ioffe and Christian Szegedy. \u201cBatch normal-ization: Accelerating deep network training by re-ducing internal covariate shift\u201d. In: International con-ference on machine learning. PMLR. 2015, pp. 448\u2013456.[121] Sepp Hochreiter and J \u00a8urgen Schmidhuber. \u201cLongIn: Neural computation 9.8short-term memory\u201d.(1997), pp. 1735\u20131780.[122] Alex Graves, Abdel-rahman Mohamed, and Geof-frey Hinton. \u201cSpeech recognition with deep recurrentneural networks\u201d. In: 2013 IEEE international confer-ence on acoustics, speech and signal processing. Ieee.2013, pp. 6645\u20136649.[123] Savvas Varsamopoulos, Koen Bertels, Carmen G Al-mudever, et al. \u201cDesigning neural network basedIn: arXiv preprintdecoders for surface codes\u201d.arXiv:1811.12456 (2018).[124] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zis-serman. \u201cDeep face recognition\u201d. In: (2015).[125] Kaiming He et al. \u201cDeep residual learning for imagerecognition\u201d. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. 2016, pp. 770\u2013778.Jie Hu, Li Shen, and Gang Sun. \u201cSqueeze-and-excitation networks\u201d. In: Proceedings of the IEEE con-[126]ference on computer vision and pattern recognition. 2018,pp. 7132\u20137141.[127] Qiong Cao et al. \u201cVggface2: A dataset for recognisingfaces across pose and age\u201d. In: 2018 13th IEEE interna-tional conference on automatic face & gesture recognition(FG 2018). IEEE. 2018, pp. 67\u201374.[128] VGGFace2-pytorch. : https : / / github . com /[129] cydonia999/VGGFace2-pytorch.Jianfeng Zhao, Xia Mao, and Jian Zhang. \u201cLearningdeep facial expression features from image and op-tical \ufb02ow sequences using 3D CNN\u201d. In: The VisualComputer 34.10 (2018), pp. 1461\u20131475.[130] Sai Prasanna Teja Reddy et al. \u201cSpontaneous facialmicro-expression recognition using 3D spatiotempo-ral convolutional neural networks\u201d. In: 2019 Inter-national Joint Conference on Neural Networks (IJCNN).IEEE. 2019, pp. 1\u20138.Jad Haddad, Olivier L\u00b4ezoray, and Philippe Hamel.\u201c3D-CNN for Facial Emotion Recognition in Videos\u201d.International Symposium on Visual Computing.In:[131][132] Du Tran et al. \u201cA closer look at spatiotemporalconvolutions for action recognition\u201d. In: Proceedingsof the IEEE conference on Computer Vision and PatternRecognition. 2018, pp. 6450\u20136459. 7 A7.1 Test setTABLE 2: Regular models, Test set, accuracyTABLE 3: Regular models, Test set, true positive rateTABLE 4: Regular models, Test set, false positive rateTABLE 5: Female models, Test set, accuracyTABLE 6: Female models, Test set, true positive rateTABLE 7: Female models, test set, false positive rateTABLE 8: Male models, Test set, accuracy TABLE 15: Regular models, Female set, true positive rateTABLE 9: Male models, Test set, true positive rate TABLE 16: Regular models, Female set, false positive rateTABLE 10: Male models, Test set, false positive rate TABLE 17: Regular models, Female set, accuracy7.2 Female set TABLE 18: Regular models, Female set, true positive rateTABLE 11: Regular models, Female set, accuracyTABLE 12: Regular models, Female set, true positive rate TABLE 19: Regular models, Female set, false positive rate7.3 Male setTABLE 13: Regular models, Female set, false positive rate TABLE 20: Regular models, Male set, accuracyTABLE 14: Regular models, Female set, accuracy TABLE 21: Regular models, Male set, true positive rateTABLE 22: Regular models, Male set, false positive rateTABLE 23: Regular models, Male set, accuracyTABLE 24: Regular models, Male set, true positive rateTABLE 25: Regular models, Male set, false positive rateTABLE 26: Regular models, Male set, accuracyTABLE 27: Regular models, Male set, true positive rateTABLE 28: Regular models, Male set, false positive rate7.4 Charts Fig. 26: Per model architecture, accuracy comparison be-tween all three test sets. These are Regular models, whichhave been trained on entire train data.Fig. 27: Per model architecture, accuracy comparison be-tween all three test sets. These are Female models, whichhave been trained on only female data.Fig. 28: Per model architecture, accuracy comparison be-tween all three test sets. These are Male models, which havebeen trained on only male data. Fig. 30: Per model architecture, Female set accuracy compar-ison between different model typesFig. 29: Per model architecture, Test set accuracy comparisonbetween different model types Fig. 31: Per model architecture, Male set accuracy compari-son between different model types",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "4391ccb4-81fa-4cd6-bac6-b8b5c671a9d3",
                    "text": "",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "8da33fd9-f423-49b0-a92e-c65f81642567",
                    "text": "The database from University of Tartu has been taken asa target database for research. It contains video on whichpeople are displaying emotions. Despite other databases forFER, frame resolution is quite high: 1280 x 960. There are 50persons and 12 videos recorded per each, therefore in total600 videos. Each video has high frame rate, which is equalto 100 frame per second (FPS). As was mentioned before,there are six types of emotions: happiness, surprise, sadness,disgust, anger and contempt. Originally, this database havea bit different purpose, therefore, it has additional feature -videos are divided into the genuine and fake expressions.In other words, from 12 videos, on 6 a subject is expressingtrue emotion, on other 6 - fake.Displaying of arbitrary emotion is always started fromneutral emotion, following a command and expressionwhich was asked. Unfortunately, there are no timestampconsiderable issue and approach to solving it will be ex-plained further. Some samples from the dataset are shownon Figure 1. Fig. 1: SASE-FE database examples3.2 Data preprocessing",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "d75f5b54-8508-4ade-9819-560dbbaf4ac9",
                    "text": "Usually, to perform face extraction, researchers use defaultmethod to extract faces by adopting Histogram of OrientedGradients (HOG) descriptors [114]. DLIB [115] implementa-tion has been used for this task. However, it does not workwell for particular dataset. Because of rich representationof skin color in dataset, the given face extractor could notextract faces for all subjects. More precisely, a race biashas been faced, since even with histogram equalization,it was not possible to extract faces for people with darkskin color. Hence, it has been decided to use another,more advance, approach - Multitask Cascade ConvolutionalNetworks (MTCNN) [116]. Using this model, four types ofdataset were generated, each of them has different margin(0, 25, 65, 40). The size of the cropped frame is 256 \u00d7 256.2) Random rotation3) Brightness augmentationEach augmentation is applied independently within p =0.5 probability on each sample. Figure 4 shows example ofaugmented train data.Fig. 4: Examples of augmented data3.3 Deep learning models for FER3.3.1 Common layers3.3.1.1 Convolution layer: Convolution layer is acentral building block for deep neural network for imagedata. Basically, this layer implement convolution operation,i.e. convolves the input image with given kernel. Dependswhether a layer works on 2D or 3D data, convolutionoperation is performed along 2 or 3 dimension respectively.In case of 2D, a kernel moves along spatial dimension,meanwhile in 3D case along temporal dimension too. Im-plementation details and parameters can be found in [119].3.3.1.2 Batch normalization: Essential presented in[120], this layer helps models to reduce impact of randomweight initialization, allowing to converge faster and toavoid unstable gradients. In fact, batch normalization layeronly standardizes the output of the layer, by shifting andscaling with mean and standard deviation respectively. Im-plementation details of PyTorch BatchNorm2d and Batch-Norm3d can be found in [119]3.3.1.3 Recti\ufb01ed linear unit layer: The simplest layerwithing single function: to shrink all negative values, re-placing them with zeros. In other words, applying functionx = max(0, x).3.3.1.4 Pooling layer: Pooling layer is somewhatsimilar to the convolution layer, however, does not con-tain any weights, instead just applies speci\ufb01c operation tothe perception map. As a results, Max Pooling Layer andAverage Pooling Layer are named after the function theyapply. In general, pooling layer produces a single value byapplying a function (maximum or average) to the perception\ufb01eld, sliding over input tensor dimensions. Detail informa-tion and exact implementation can be found in [119]term memory (LSTM): Intro-duced in 90s by [121], after huge success in 2013 [122],LSTM layer became a core module for any sequence relateddeep learning architecture. In this paper, LSTM layer is3.3.1.5 Long-shortFig. 2: Examples of cropped images3.2.2 Decreasing sizeSince database presented as video with high FPS rate, it wason High Performance Cluster (HPC). To decrease numberof frames without loss of the information, K-Means [117,118] algorithm was used. Simple yet effective, it allows todetermine K most distinguishable frames for each video.For the test purposes, it was generated K = 10, 20, 50. ForK = 10 the entire video was processed. For K = 20, \ufb01rst100 frames (1 sec) are excluded. For K = 50, 20 frames fromthe \ufb01rst half of the video and 30 from the second.Fig. 3: Example of emotion display through sequence offramesThe number of videos in database is relatively small. Sincedeep learning approaches operates better on large scale ofdata, data augmentation has been applied during training.In total, there are three augmentation techniques ap-plied:1) Horizontal \ufb02ipused together with CNN feature extractors, since latter areoperated only on single images, meanwhile the target inputis video. LSTM itself has sophisticated structure, with 4internal vectors named input gate(i), output gate(o), forgetgate(f ) and cell (C). Along with these vectors, on eachprocessing step LSTM contains hidden state (h) and cellinput activation ( \u02dcC) vectors. Visual explanation of howLSTM module operates is shown on Figure 5, where t istime step, U and W are parameters matrix, which have tobe learn. Implementation details could be found in [119]. 3.3.3 ResNet50-LSTMAnother great example of a model, which is a result ofconsecutive research in deep learning is ResNet50 [125].Following the same principle in construction, the model ispresented as a CNN feature extractor. ResNet50 backbonebegins with a sequence of convolutional layer, batch nor-malization, ReLU and MaxPooling. Next, four layers with 3,4, 6 and 3 Bottleneck blocks respectively. Model ends withAvgPooling layer to reduce output feature map size. At thispoint there are no more backbone layers, and data streamgoes down to LSTM module, following by fully connectedlayer. The entire architecture is visualised on Firuge 7. Sincethe model is too deep by itself, ImageNet pretrained weightsare used. In this paper, when we are referring to ResNet50,meaning entire architecture, which consists of a backboneand LSTM module.Fig. 5: LSTM module [123]3.3.1.6 Fully Connected layer (FC): Often the \ufb01nallayer, the only purpose of which is to apply af\ufb01ne transformy = W \u00d7 x + b, where x is input vector, y is output vector,W weights and b bias. Exact implementation which has beenused could be found in [119].3.3.2 VGG-LSTMOne of the most famous model for face recognition whichhas been widely used and has become a background forfurther research is VGG16 architecture [124], named afterVisual Geometry Group, who has conducted research. Inthis work, VGG16 model serve as a feature extractor whichpreceded LSTM block. Together with a fully connected layerat the end, they create a model for classi\ufb01cation sequentialdata. The backbone model consists of series of blocks, whichare two or three convlutional layers with ReLU and theMaxPooling layer on the end. The model ends with fullyconnected layer. Exactly this layer serves as feature vector,which further goes to LSTM module, next down to thefully connected layer. The entire structure is on Figure 6.As were mentioned before, VGG architecture is wide used,therefore there are many good pretrained weights availablefor the research. It has been chosen to use pretrained weightsfrom ImageNet dataset [100] and VGGFace dataset [124]. Tosimplify further presentation of results, VGG16 architecturewith pretrained ImageNet weights is denoted as VGG16,meanwhile VGG16 architecture with pretrained VGGFaceweights is denoted as VGGFACE. Fig. 7: Architecture of ResNet50-LSTM. Blue: ResNet50backbone with pretrained weights. Red: layers trained fromthe scratch.3.3.4 SENet-LSTMIn [126] authors discovered, that integrating mechanism forlearning relationship between channels can considerablyimprove results. The goal of such block is to explicitly traina network the cross-correlation between spatial channels.The main advantages of this approach are lightweight, sim-plicity to implement and application to all common layers.Basically, for any given transformation F : X \u2212\u2192 U , \ufb01rst,squeeze the feature map into the descriptor vector and thenmultiply it to the feature block channel wise. See Figure 8.Fig. 6: Architecture of VGGLSTM. Blue: VGG16 backbonewith pretrained weights. Red: layers trained from thescratch. Fig. 8: Squeeze-and-Excitation block [126]In general, architecture of SENet-LSTM consists of twoparts: a backbone and LSTM module. The backbone isSENet model, which copies the architecture of ResNet50(blue blocks on Figure 7), however, each bottleneck blockenhanced with SE feature. In other words, Bottleneck layerplays role of function F , according to the Figure 8. TheLSTM module is a one layer LSTM layer withing one fullyconnected follow up layer. SENet backbone has 8096 outputfeatures. LSTM block has 256 hidden neurons. Since thenumber of training data is quite small, it has been chosento use pretrained weights for a backbone. Ready to useweights, trained of VGGFace2 dataset[127] for SENet wastaken from [128]. For purpose of convenience, further thisarchitecture is referred as SENetLSTM.3.3.5 3D-CNNAmong others approaches which utilize 3D CNN [129, 130]for FER, in [131] authors propose an architecture withinoptimized hyperparameters for CK+ and OULU-CASIA112 \u00d7 112 resolution frames. However, in purposes of thisresearch work, to be consistent with other network, theinput resolution has been changed to 224 \u00d7 224, and thesequence length varies. Hence, the model architecture hasbeen preserved up to fully connected layers, since theirsize is directly depended on the input. The \ufb01nal modelis presented on Figure 9. This model does not have anypretrained weights and trained from the scratch. In thefurther sections this model is referred as 3DCNN.Fig. 9: Architecture of 3D-CNN model3.3.6 ResNet3DIn 2018, a group of researchers proposed a new way totreat 3D data such as videos, presenting another two con-volutional blocks: mixed convolutions and \u201d(2+1)D\u201d con-volutions [132]. Utilizing these block, authors constructedResNet3D, an analogue of already famous ResNet2D. De\ufb01n-ing the clone of shorted version of ResNet - ResNet18 -ResNet3D has got three different implementations: with 3Dconvolutions, with mixed convolutions and with \u201d2+1\u201dDconvolutions. Since authors claim that the best performancehas been demonstrated by \u201d2+1\u201dD convolutions, in thispaper exactly this type of model is used. The idea behind\u201d2+1\u201dD convolutions is to think about 3D convolutions as2D convolutions in spatial space, which are followed by1D convolutions in the temporal space. The bene\ufb01ts fromthese approach are following: \ufb01rst, due to the additionalReLU after 2D convolutions, this block has twice morenumber of nonlinearities, increasing capacity of the model,second, authors claim that during the training, a tensorwise error rate is smaller, compared to the 3D convolutioncounterparts. The overall architecture of ResNet3D is similarto ResNet50, but with different blocks. The \ufb01nal modelstructure is presented on Figure 10. Fig. 10: Structure of ResNet3D-LSTM model3.4 Task de\ufb01nitionThe goal is to train a classi\ufb01er to classify an emotion outof six classes: surprise, contempt, disgust, anger, sad andhappy. Let us de\ufb01ne a neural network as probabilistic clas-si\ufb01er P , with parameters \u0398. In this paper, a classi\ufb01cation isperformed in the \u201dsoft\u201d form, i.e.\u02c6y = Sof tmax (P (Y = y|X, \u0398)) (4)where X is an input, Y is an output, y \u2208 0, 1, 2, 3, 4, 5according to the six type of emotions. Softmax, in turn, isde\ufb01ned as followsSof tmax(x ) = e e(cid:80) (5)4 EThe following section has an overview of all details regard-ing experiments. Hence, below will be described data divi-sion and hyperparameters. In addition, this section containsdescription of necessary metrics, based on which conclusionwould be derived.4.1 Input sizeThe raw instance of database is a video \ufb01le. Due to highresolution of videos and limited computational capacity,as was mentioned in 3.2.2, each video has been squeezedfor particular number of frames. However, the observationsare that: within K = 20 models generalize data the best.Using K = 10, there were not any adequate results andwith K = 50 there were not observed any increase of inperformance. Moreover, due to the limited computationalresources, bigger length of the input leads to decreasing ofbatch size, consequently increasing training time or even notallowing to \ufb01t a model. The input resolution is (224 \u00d7 224)for all models, except ResNet3D. Due to the enormousnumber of weights, available computational capacity is notenough to \ufb01t the model. Hence, the input resolution forResNet3D is (112 \u00d7 112). Putting all together, the inputsample for ResNet3D is (10, 3, 112, 112) and for all othermodels is (10, 3, 224, 224).4.2 Data divisionSASE-FE dataset contains 18 female and 32 male subjects.The instances for the test group have been selected ran-domly and once. Overall we had selected 5 males and 5females. Hence, all videos corresponded to these persons arenot visible anyhow during the training. Since each personhas 12 videos, therefore test set contains 120 videos, 20videos per emotion.Often, during training process a model tends to under\ufb01tor over\ufb01t. To track these kinds of behavior, making trainingprocess more ef\ufb01cient and to pick the best model, it has beendecided to organize a validation test with \ufb01xed size as 15%each train run.To sum up, each training process contains train andvalidation sets with 32 and 8 subjects correspondingly. Mul-tiplying by 12, there are 384 train videos and 96 validationvideos. Test set with total 120 videos is not involved duringtraining process. The desired metrics which are related tothe goals of this paper are calculated exactly from inferenceon the test set.4.3 Training detailsAll training experiments were performed on University ofTartu HPC. The target GPU is NVIDIA Tesla-V100 with 32GB of VRAM. All code is written in Python with usage ofmachine learning python package - PyTorch. The hyperpa-rameter search during the entire research was consisted oftwo phases for each model. First phase of hyperparametersearch was performed to \ufb01nd out the optimal model, hencetarget hyperparameters were such things as number of fullyconnected layers, number of LSTM layers, number of nodesin these layers. After \ufb01nding an optimal structure, in thesecond phase the optimal set of training hyperparametersfor each model have been discovered. In the scope of ex-periments, such variables have been varied: learning rate,batch size and weight decay. The \ufb01nal hyperparameters arepresented in Table 1.TABLE 1: Hyperparameters values for each model Test set, Male set and Female set. Moreover, it was decidedto look how performance and metrics differ depends ontraining data. For this purpose, all train data also wasdivided into pure male train set and pure female trainset. Therefore, to follow results easier, models which aretrained on entire train set are called Regular, trained on puremale set and pure female set are called Male and Femalerespectively. Training of all models have been performedusing Stochastic Gradient Decent (SGD) optimization. Tosum up, it has been trained 18 different neural networks,6 different models by 3 different train sets.4.4 MetricsAccording to the previously mentioned de\ufb01nitions of fair-ness, the main metrics, based on which any conclusioncould be made are accuracy, true positive rate (TPR) andfalse positive rate (FPR). Although these metrics are trivial,formal de\ufb01nitions are listed below:ACC = T P + N TT P + T N + F P + F NT P R =F P R = T PT P + F NF PF P + T N (7)(8)where T P - true positive, F P - false positive, T N - truenegative, F N - false negative, ACC - accuracy, T P R andF P R - true positive and false positive rate respectively.The \ufb01rst de\ufb01nition of fairness (Equation 1) implies theequality of TPR and FPR simultaneously. The second def-inition (Equation 2), extending to the multi classi\ufb01cationcase, requires equal TPR for the groups with differentunprotected attribute value. And the third, and the last,de\ufb01nition of fairness (Equation 3) means the equal accuracyfor separated groups. In other words, the difference in thesemetrics shows how fair model is, having linear dependency- the higher discrepancy, the more biased model is.Although target database comprises six different emo-tions, usually it is extremely dif\ufb01cult to distinguish betweencontempt, disgust and anger. In order to relax constraintsand make classi\ufb01cation a bit easier, without losing generalidea, it has been decided to fuse these three emotions into asingle emotion and name it \u201dUpset\u201d.5 RThis section consists of 2 parts. First part contains confusionmatrices and shallow analysis of each. Second part is servedfor more detailed and precise analysis from the perspectiveof fairness, emotions and test sets.5.1 Confusion matricesIn order to perform gender bias analysis, test set has beendivided into two subsets: pure male set and pure female set,such that each has 5 subjects. In this scenario, pure means toinclude subjects of only one gender. Hence, from here andfurther, there are three sets for testing, named as follows: Regular models have shown decent performance on Test set.The major part of emotions are classi\ufb01ed correctly, however,Upset and Sad emotions are frequently misclassi\ufb01ed, whichis an expected outcome, since they are close in V-A space.Confusion matrices are shown on Figure 11.Fig. 11: Confusion matrix per method for Regular modelson Test set Fig. 13: Confusion matrix per method for Male models onTest setFig. 12: Confusion matrix per method for Female models onTest set Fig. 14: Confusion matrix per method for Regular modelson Female setA decent decrease in accuracy for Surprise, which indi-cates that female training data is lack of Surprise expression.Confusion matrices are on Figure 12.We can observe almost the same performance compara-tively to the Regular models, however, the misclasi\ufb01cationrate of Sad emotion as Upset is much higher. This impliesthat male training data includes more Sad samples whichare visually much closer to Upset emotion. Confusion ma-trices are on Figure 13.Regular models have shown almost excellent recognitionof Happy emotion for Female set. TPR for Surprise in nothigh, however, we observe less misclassi\ufb01cation betweenUpset and Sad. Confusion matrices are on Figure 14.Quite unexpected results for Female models on Femaleset. Only TPR for Happy emotion is high enough to consideras acceptable. Recognition of Surprise is very low, whichmeans that female training data has weak samples forSurprise. Confusion matrices are on Figure 15.As has been stated before, Female training data has weakSurprise samples, meanwhile Male models have shownaverage performance, hence, even with different genderdomain, male samples of Surprise are much stronger. Con-fusion matrices are on Figure 16.Comparatively to other test sets, inference of Regular Fig. 15: Confusion matrix per method for Female models onFemale setmodels on Male set has much lower TPR for Happy emotionand much higher TPR for Surprise emotion. Confusionmatrices are on Figure 17.Female models on the Male set have probably the worstmetrics overall, due to the lack of data in train data andopposite gender in test data. Confusion matrices are onperformance expectations have been higher. Confusion ma-trices are on Figure 19.Fig. 19: Confusion matrix per method for Male models onMale set5.2 AnalysisThe goal of this research work is to analyse a gender bias,according to the de\ufb01nition of fairness. Raw results on whichthe following analysis is based are listed in Appendix 7. Inthe matter of convenience, several chars are presented inAppendix 7.4.According to equal opportunity (EQOP) and equalizedodds (EQOD) (Figure 20 (a, b)), the least biased is ResNet3D,while VGGFACE is the most unfair. Meanwhile, 3DCNNis second in term of biasness. However, comparison inaccuracy difference (demographic parity (DP)) (Figure 20(c)) consider the most fair model VGG16, when ResNet3Dis the second most fair one. As for the most biased modelsaccording to DP, these are 3DCNN and VGGFACE. Hence,overall results are consider to be aligned along all threede\ufb01nitions of fairness.(a) (b) (c)Fig. 20: Metrics for models, which have been trained onentire train dataAnalysing aggregated results (Figure 21), several conclu-sions have been derived. For the Test and Male set, Regularmodels show the best accuracy for classi\ufb01cation Surprise,while on the Female set, the best accuracy is for Happyemotion. Fused Upset emotion has the lowest recognitionrate for all three sets. Worth to mention, inference of Femaleset has much higher variance rather than on Male set.However, average accuracy for Female set is higher.From the perspective of emotions (Figure 26), results asfollows: classi\ufb01cation of Surprise is better for males, Upsetand Sad are expressed better by females and Happy isalmost identical recognized for both genders.Fig. 16: Confusion matrix per method for Male models onFemale setFig. 17: Confusion matrix per method for Regular modelson Male setFigure 18.Fig. 18: Confusion matrix per method for Female models onMale setMale models on Male set have shown great accuracyfor Surprise and Happy emotions. There are high TPR forUpset, however, misclassi\ufb01cation rate of Sad as Upset ishigh too. Since train and test data share the same gender,Fig. 21: Aggregated accuracy for models, trained on theentire data. Comparison for three different test sets. (a) (b) (c)Trained only on female data, Female models show com-pletely different picture. For all three de\ufb01nition of fairness,SENetLSTM architecture is considered as the most genderbiased. 3DCNN is the least biased according to EQOP andEQOD (Figure 22 (a,b)), and VGG16 according to the DP(Figure 22 (c)). For each test set, classi\ufb01cation accuracy isthe worst for Upset emotion, Happy is the best recognisedfor Test set and Female set. Inference on Male set shows thebest recognition of Surprise. Fig. 24: Metrics for models, which have been trained on onlymale datamale set has high variance, being unstable, while inferenceon Male set has small variance, and therefore, more robust.Without a surprise here, average accuracy on Male set ishigher rather than on Female set.(a) (b) (c)Fig. 22: Metrics for models, which have been trained on onlyfemale dataAccording to aggregated results (Figure 23), variancesfor Male and Female set are relatively equal, thereforerecognition for both gender is considered as robust. Theoverall accuracy is better for Male set, which is unexpectedresults, since models are trained exclusively on the femaledata.In the per emotion competition (Figure 27), classi\ufb01cationof Happy and Surprise emotions are almost identical forboth genders, meanwhile Upset and Sad are better recog-nized on female subjects.Opposite to previously mentioned type of models, Malemodels are trained on male data. SENetLSTM architecture isshown as the most fair architecture, according to the EQOP,EQOD and DP. For the most biased model, ResNet50 is con-sidered of being so with respect to EQOP and EQOD, whileResNet3D is a choice according to DP (Figure 24). Noticeable,the according to DP, ResNet50 and VGGFACE share secondthe most biased place in ranking. Therefore, results for Malemodels are considered to be consistent. For different testsets, the best and the worst accurate classi\ufb01ed emotion arethe same. Upset emotion has the lowest accuracy among alltest sets and Happy - the highest one.Aggregate results (Figure 25) show that inference on Fe-Fig. 23: Aggregated accuracy for models, trained on onlyfemale data. Comparison for three different test sets. Fig. 25: Aggregated accuracy for models, trained on onlymale data. Comparison for three different test sets.In the per emotion accuracy (Figure 28), inference onMale set leads in recognition of Surprise and Happy emo-tions. Sad recognition rate is almost the same for both sets.Accuracy for Upset emotion is much higher for Female set.6 CThis paper provided a comprehensive overview on the faceemotion recognition biases in the context of reliable AI.Taking the concrete dataset, SASE-FE, two different groupsof methods for face emotion recognition have been analyzedon gender bias. Each group consists of three different neu-ral network architectures, where some of them have beenready available and some have been manually implementedbefore the analysis. The test sets have been organized inthree different ways: entire test data, only male data andonly female data. The train sets have been organized in thesimilar way. All architectures have been trained, resultinginto 18 different models, 6 architectures per each train set.Since model bias can be explained through fairness,there have been given three different de\ufb01nition of fairness,according to which proper analysis have been conducted.As a results, it has been found which architectures are mostbiased with respect to the de\ufb01nitions of fairness, and whichones are more likely to be fair. In addition, it has beendiscovered, which kinds of emotions are easier to recognizefor men and women. In addition, using three distinct trainsets the relationship between training data and inference hasbeen analyzed.The topic of gender bias is relatively young and not ad-dressed properly. Therefore, the amount of existed probabledirections to research is immense. Extending this researchwork, for sure, the next goal has to be to expand researchon other databases, which are widely used in FER. Also,the models which have been utilized in this paper, are notcompetitors to the state-of-the-art solutions. Hence, anotherdirection of future work is to implement these solutions andanalyze whether they comprise gender bias. Far-reachingextensions include other aspects of RAI and XAI. For ex-ample study of other biases (race, age, culture) or workingon explanation, understanding and discovering knowledgelimits. Altogether, these researches will create a backgroundto more standardized regulation and law creation in the \ufb01eldof AI. As a result, integration of AI in society will be reliableand safe. After all, modern AI still encompasses a decentamount of unknown and hazard, therefore future us haveto be ready.AThis work is supported by the Estonian Centre of Excellencein IT (EXCITE) funded by the European Regional Develop-ment Fund. The authors also gratefully acknowledge thesupport of NVIDIA Corporation with the donation of theTitan XP Pascal GPU.R [1] Aaron Smith and Janna Anderson. \u201cAI, Robotics, andthe Future of Jobs\u201d. In: Pew Research Center 6 (2014),p. 51.[3] intelligence: Who is[2] Emanuele Neri et al. Arti\ufb01cialresponsible for the diagnosis? 2020.Jean-Franc\u00b8ois Bonnefon, Azim Shariff, and Iyad Rah-wan. \u201cThe social dilemma of autonomous vehicles\u201d.In: Science 352.6293 (2016), pp. 1573\u20131576.James Zou and Londa Schiebinger. AI can be sexistand racist\u2014it\u2019s time to make it fair. 2018.[4][5] Eric Mack. Hawking, Musk, Wozniak Warn About Ar-ti\ufb01cial Intelligence\u2019s Trigger Finger. https : / / www .forbes.com/sites/ericmack/2015/07/27/hawking-musk-wozniak-freaked-about-arti\ufb01cial-intelligence-getting-a-trigger-\ufb01nger/?sh=7ad6f69f7416.[6] Frank Rosenblatt. \u201cThe perceptron: a probabilisticmodel for information storage and organization inthe brain.\u201d In: Psychological review 65.6 (1958), p. 386.[7] Seppo Linnainmaa. \u201cThe representation of the cu-mulative rounding error of an algorithm as a Taylorexpansion of the local rounding errors\u201d. In: Master\u2019sThesis (in Finnish), Univ. Helsinki (1970), pp. 6\u20137.[8] Ning Qian and Terrence J Sejnowski. \u201cPredicting thesecondary structure of globular proteins using neuralnetwork models\u201d. In: Journal of molecular biology 202.4(1988), pp. 865\u2013884.algorithmwatch.org. Finnish Credit Score Ruling raisesQuestions about Discrimination and how to avoid it.https : / / algorithmwatch . org / en / story / \ufb01nnish -credit - score - ruling - raises - questions - about -discrimination-and-how-to-avoid-it/.[9][10] Daisuke Wakabayashi. Self-Driving Uber Car KillsPedestrian in Arizona, Where Robots Roam. https : / /www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html.[11] Niraj Chokshi. Tesla Autopilot System Found Probablyat Fault in 2018 Crash. https://www.nytimes.com/2020/02/25/business/tesla-autopilot-ntsb.html.Julia Angwin. Facebook Enabled Advertisers to Reach\u2018Jew Haters\u2019. https://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters.[12] [13] Veronica Rocha. Crime-\ufb01ghting robot hits, rolls overchild at Silicon Valley mall. https : / / www. latimes .com/local/lanow/la- me- ln- crime\ufb01ghting- robot-hurts-child-bay-area-20160713-snap-story.html.John Kingston. \u201cArti\ufb01cial intelligence and legal lia-bility\u201d. In: arXiv preprint arXiv:1802.07782 (2018).[15] Davide Carneiro et al. \u201cOnline dispute resolution:an arti\ufb01cial intelligence perspective\u201d. In: Arti\ufb01cialIntelligence Review 41.2 (2014), pp. 211\u2013240.[14][16] Keri Stephens. \u201cRecent Studies Examine Lunit AIin Breast Cancer Detection\u201d. In: AXIS Imaging News(2020).[17] Appen Wilson Pang. Responsible AI becomes criticalin 2021. https : / / venturebeat . com / 2020 / 11 / 11 /responsible-ai-becomes-critical-in-2021/.[18] Tableau. 2019 Business Intelligence Trend. https : / /www. tableau . com / reports / business - intelligence -trends/machine-learning.[19] Mariusz Bojarski et al. \u201cEnd to end learning forself-driving cars\u201d. In: arXiv preprint arXiv:1604.07316[20] Martin Tammvee and Gholamreza Anbarjafari. \u201cHu-man activity recognition-based path planning forautonomous vehicles\u201d. In: Signal, Image and VideoProcessing (2020), pp. 1\u20138.[21] Kwang-Eun Ko and Kwee-Bo Sim. \u201cDeep convolu-tional framework for abnormal behavior detection ina smart surveillance system\u201d. In: Engineering Applica-tions of Arti\ufb01cial Intelligence 67 (2018), pp. 226\u2013234.[22] Eric Marchand, Hideaki Uchiyama, and FabienSpindler. \u201cPose estimation for augmented reality: ahands-on survey\u201d. In: IEEE transactions on visualiza-tion and computer graphics 22.12 (2015), pp. 2633\u20132651.[23] Dorota Kami \u00b4nska et al. \u201cStress Reduction Using Bi-lateral Stimulation in Virtual Reality\u201d. In: IEEE Access8 (2020), pp. 200351\u2013200366.[24] Patricio Loncomilla, Javier Ruiz-del-Solar, and LuzMartinez. \u201cObject recognition using local invariantfeatures for robotic applications: A survey\u201d. In: Pat-tern Recognition 60 (2016), pp. 499\u2013514.[25] Anastasia Bolotnikova, Hasan Demirel, and Gho-lamreza Anbarjafari. \u201cReal-time ensemble based facerecognition system for NAO humanoids using localbinary pattern\u201d. In: Analog Integrated Circuits andSignal Processing 92.3 (2017), pp. 467\u2013475.[26] Anastasia Bolotnikova et al. \u201cA circuit-breaker use-case operated by a humanoid in aircraft manufac-turing\u201d. In: 2017 13th IEEE Conference on AutomationScience and Engineering (CASE). IEEE. 2017, pp. 15\u201322.[27] Baris Kayalibay, Grady Jensen, and Patrick van derSmagt. \u201cCNN-based segmentation of medical imag-ing data\u201d. In: arXiv preprint arXiv:1701.03056 (2017).[28] Luis G \u00b4omez-Chova et al. \u201cMultimodal classi\ufb01cationof remote sensing images: A review and future di-rections\u201d. In: Proceedings of the IEEE 103.9 (2015),pp. 1560\u20131584.[29] Andreas Kamilaris and Francesc X Prenafeta-Bold \u00b4u.\u201cDeep learning in agriculture: A survey\u201d. In: Comput-ers and electronics in agriculture 147 (2018), pp. 70\u201390.[30] Jianzhu Guo et al. \u201cDominant and complementaryemotion recognition from still images of faces\u201d. In:IEEE Access 6 (2018), pp. 26391\u201326403.[31] Egils Avots et al. \u201cEnsemble approach for detectionof depression using EEG features\u201d. In: arXiv preprintarXiv:2103.08467 (2021).[32] Kadir Aktas et al. \u201cSpatio-Temporal Based TableTennis Stroke Type Assessment\u201d. In: Signal, Image andVideo Processing (2021), pp. 1\u20138.[33] Paula M Niedenthal and Markus Brauer. \u201cSocialfunctionality of human emotion\u201d. In: Annual reviewof psychology 63 (2012), pp. 259\u2013285.[34] Eric M Reiman et al. \u201cNeuroanatomical correlatesof externally and internally generated human emo-tion\u201d. In: American Journal of Psychiatry 154.7 (1997),pp. 918\u2013925.[35] Fatemeh Noroozi et al. \u201cSurvey on emotional bodygesture recognition\u201d. In: IEEE transactions on affectivecomputing (2018).[36] Gholamreza Anbarjafari et al. Machine Learning forFace, Emotion, and Pain Recognition[37] Raymond J Dolan. \u201cEmotion, cognition, and behav-ior\u201d. In: science 298.5596 (2002), pp. 1191\u20131194.John G Carlson and Elaine Hat\ufb01eld. Psychology ofemotion. Harcourt Brace Jovanovich, 1992.[38][39] Mengyi Liu et al. \u201cDeeply learning deformable facialaction parts model for dynamic expression analysis\u201d.In: Asian conference on computer vision. Springer. 2014,pp. 143\u2013157.[40] Chien-Hsu Chen, I-Jui Lee, and Ling-Yi Lin. \u201cAug-mented reality-based self-facial modeling to promotethe emotional expression and social skills of adoles-cents with autism spectrum disorders\u201d. In: Researchin developmental disabilities 36 (2015), pp. 396\u2013403.[41] Agata Ko\u0142akowska et al. \u201cEmotion recognition andits applications\u201d. In: Human-Computer Systems Inter-action: Backgrounds and Applications 3. Springer, 2014,pp. 51\u201362.[42] Y-I Tian, Takeo Kanade, and Jeffrey F Cohn. \u201cRec-ognizing action units for facial expression analysis\u201d.In: IEEE Transactions on pattern analysis and machineintelligence 23.2 (2001), pp. 97\u2013115.James A Russell. \u201cA circumplex model of affect.\u201d In:Journal of personality and social psychology 39.6 (1980),p. 1161.[43][44] Paul Ekman. \u201cBasic emotions\u201d. In: Handbook of cogni-tion and emotion 98.45-60 (1999), p. 16.[45] Christer Loob et al. \u201cDominant and complementarymulti-emotional facial expression recognition usingc-support vector classi\ufb01cation\u201d. In: 2017 12th IEEEInternational Conference on Automatic Face & GestureRecognition (FG 2017). IEEE. 2017, pp. 833\u2013838.[46] Tomasz Sapi \u00b4nski et al. \u201cMultimodal database ofemotional speech, video and gestures\u201d. In: Inter-national Conference on Pattern Recognition. Springer.2018, pp. 153\u2013163.[47] Gary B Huang et al. \u201cLabeled faces in the wild:A database forstudying face recognition in uncon-strained environments\u201d. In: Workshop on faces in\u2019Real-Life\u2019Images: detection, alignment, and recognition. 2008. [48] Abhinav Dhall et al. \u201cVideo and image based emo-tion recognition challenges in the wild: Emotiw2015\u201d. In: Proceedings of the 2015 ACM on internationalconference on multimodal interaction. 2015, pp. 423\u2013426.and MattiPietik\u00a8ainen. \u201cFace recognition with local binarypatterns\u201d. In: European conference on computer vision.Springer. 2004, pp. 469\u2013481.[49] Timo Ahonen, Abdenour Hadid,[50] Xiaoyi Feng, Matti Pietik\u00a8ainen, and Abdenour Ha-did. \u201cFacial expression recognition based on localbinary patterns\u201d. In: Pattern Recognition and ImageAnalysis 17.4 (2007), pp. 592\u2013598.[51] Rajiv Mehrotra, Kameswara Rao Namuduri, and Na-garajan Ranganathan. \u201cGabor \ufb01lter-based edge de-tection\u201d. In: Pattern recognition 25.12 (1992), pp. 1479\u20131494.[52] Michael Lyons et al. \u201cCoding facial expressions withgabor wavelets\u201d. In: Proceedings Third IEEE interna-tional conference on automatic face and gesture recogni-tion. IEEE. 1998, pp. 200\u2013205.expression recognition based on Gabor wavelets andsparse representation\u201d. In: 2012 IEEE 11th Interna-tional Conference on Signal Processing. Vol. 2. IEEE.2012, pp. 816\u2013819.[54] Govardhan Mattela and Sandeep K Gupta. \u201cFacialexpression recognition using Gabor-mean-DWT fea-ture extraction technique\u201d. In: 2018 5th InternationalConference on Signal Processing and Integrated Networks(SPIN). IEEE. 2018, pp. 575\u2013580.[55] Taskeed Jabid, Md Hasanul Kabir, and Oksam Chae.\u201cRobust facial expression recognition based on lo-cal directional pattern\u201d. In: ETRI journal 32.5 (2010),pp. 784\u2013794.[56] Zhen Wang and Zilu Ying. \u201cFacial expression recog-nition based on local phase quantization and sparserepresentation\u201d. In: 2012 8th International Conferenceon Natural Computation. IEEE. 2012, pp. 222\u2013225.[57] Wei-Lun Chao, Jian-Jiun Ding, and Jun-Zuo Liu.\u201cFacial expression recognition based on improved lo-cal binary pattern and class-regularized locality pre-serving projection\u201d. In: Signal Processing 117 (2015),pp. 1\u201310.[58] Abu Sayeed Md Sohail and Prabir Bhattacharya.\u201cClassi\ufb01cation of facial expressions using k-nearestneighbor classi\ufb01er\u201d. In: International Conference onComputer Vision/Computer Graphics Collaboration Tech-niques and Applications. Springer. 2007, pp. 555\u2013566.[59] Qirong Mao et al. \u201cHierarchical Bayesian thememodels for multipose facial expression recognition\u201d.IEEE Transactions on Multimedia 19.4 (2016),In:pp. 861\u2013873.[60] Ming-Wei Huang, Zhe-wei Wang, and Zi-Lu Ying.\u201cA new method for facial expression recognitionbased on sparse representation plus LBP\u201d. In: 20103rd International Congress on Image and Signal Process-ing. Vol. 4. IEEE. 2010, pp. 1750\u20131754.[61] Hung-Hsu Tsai and Yi-Cheng Chang. \u201cFacial expres-sion recognition using a combination of multiplefacial features and support vector machine\u201d. In: SoftComputing 22.13 (2018), pp. 4389\u20134405.[62] Alex Krizhevsky, Ilya Sutskever, and Geoffrey EHinton. \u201cImagenet classi\ufb01cation with deep convolu-tional neural networks\u201d. In: Advances in neural infor-mation processing systems 25 (2012), pp. 1097\u20131105.[63] Peter Burkert et al. \u201cDexpression: Deep convolu-tional neural network for expression recognition\u201d. In:arXiv preprint arXiv:1509.05371 (2015).[64] Xiangyun Zhao et al. \u201cPeak-piloted deep network forfacial expression recognition\u201d. In: European conferenceon computer vision. Springer. 2016, pp. 425\u2013442.[65] Xiaoguang Chen et al. \u201cConvolution neural networkfor automatic facial expression recognition\u201d. In: 2017International conference on applied system innovation(ICASI). IEEE. 2017, pp. 814\u2013817.[66] Biao Yang et al. \u201cFacial expression recognition us-ing weighted mixture deep neural network basedon double-channel facial images\u201d. In: IEEE Access 6(2017), pp. 4630\u20134640.[67] Abir Fathallah, Lot\ufb01 Abdi, and Ali Douik. \u201cFa-cial expression recognition via deep learning\u201d. In:2017 IEEE/ACS 14th International Conference on Com-puter Systems and Applications (AICCSA). IEEE. 2017,pp. 745\u2013750.[68] Thanh-Hung Vo et al. \u201cPyramid with Super Resolu-tion for In-the-Wild Facial Expression Recognition\u201d.In: IEEE Access 8 (2020), pp. 131988\u2013132001.[69] Emad Barsoum et al. \u201cTraining deep networks forfacial expression recognition with crowd-sourced la-bel distribution\u201d. In: Proceedings of the 18th ACM In-ternational Conference on Multimodal Interaction. 2016,pp. 279\u2013283.[70] Yuedong Chen et al. \u201cFacial motion prior networksfor facial expression recognition\u201d. In: 2019 IEEEVisual Communications and Image Processing (VCIP).IEEE. 2019, pp. 1\u20134.[72][71] Ali Mollahosseini, Behzad Hasani, and MohammadH Mahoor. \u201cAffectnet: A database for facial expres-sion, valence, and arousal computing in the wild\u201d. In:IEEE Transactions on Affective Computing 10.1 (2017),pp. 18\u201331.Ilke Cugu, Eren Sener, and Emre Akbas. \u201cMicroExp-Net: An Extremely Small and Fast Model For Expres-sion Recognition From Face Images\u201d. In: 2019 NinthInternational Conference on Image Processing Theory,Tools and Applications (IPTA). IEEE. 2019, pp. 1\u20136.[73] Guoying Zhao et al. \u201cFacial expression recognitionfrom near-infrared videos\u201d. In: Image and Vision Com-puting 29.9 (2011), pp. 607\u2013619.[74] Yunxin Huang et al. \u201cFacial expression recognition:A survey\u201d. In: Symmetry 11.10 (2019), p. 1189.[75] Shashank Jaiswal and Michel Valstar. \u201cDeep learningthe dynamic appearance and shape of facial actionunits\u201d. In: 2016 IEEE winter conference on applicationsof computer vision (WACV). IEEE. 2016, pp. 1\u20138.[76] Abhinav Dhall et al. \u201cEmotiw 2016: Video and group-level emotion recognition challenges\u201d. In: Proceedingsof the 18th ACM international conference on multimodalinteraction. 2016, pp. 427\u2013432.[77] Yin Fan et al. \u201cVideo-based emotion recognitionusing CNN-RNN and C3D hybrid networks\u201d. In: Proceedings of the 18th ACM International Conferenceon Multimodal Interaction. 2016, pp. 445\u2013450.[78] Dae Hoe Kim et al. \u201cMulti-objective based spatio-temporal feature representation learning robust toexpression intensity variations for facial expressionrecognition\u201d. In: IEEE Transactions on Affective Com-puting 10.2 (2017), pp. 223\u2013236.[79] Zhenbo Yu et al. \u201cSpatio-temporal convolutional fea-tures with nested LSTM for facial expression recog-nition\u201d. In: Neurocomputing 317 (2018), pp. 50\u201357.[80] Kaiming He et al. \u201cSpatial pyramid pooling in deepconvolutional networks for visual recognition\u201d. In:IEEE transactions on pattern analysis and machine intel-ligence 37.9 (2015), pp. 1904\u20131916.[81] Young-Hyen Byeon and Keun-Chang Kwak. \u201cFacialexpression recognition using 3d convolutional neuralnetwork\u201d. In: International journal of advanced com-puter science and applications 5.12 (2014).[82] Behzad Hasani and Mohammad H Mahoor. \u201cFacialexpression recognition using enhanced deep 3D con-Proceedings of theIEEE conference on computer vision and pattern recog-nition workshops. 2017, pp. 30\u201340.[83] Christian Szegedy et al. \u201cInception-v4, inception-resnet and the impact of residual connections onlearning\u201d. In: Proceedings of the AAAI Conference onArti\ufb01cial Intelligence. Vol. 31. 1. 2017.[84] Ruicong Zhi et al. \u201cCombining 3D convolutionalneural networks with transfer learning by super-vised pre-training for facial micro-expression recog-nition\u201d. In: IEICE Transactions on Information and Sys-tems 102.5 (2019), pp. 1054\u20131064.Jing Li et al. \u201cMicro-expression recognition based on3D \ufb02ow convolutional neural network\u201d. In: PatternAnalysis and Applications 22.4 (2019), pp. 1331\u20131339.[85][86] Chao Wu and Fan Guo. \u201cTSNN: Three-Stream Com-bining 2D and 3D Convolutional Neural Network forMicro-Expression Recognition\u201d. In: IEEJ Transactionson Electrical and Electronic Engineering 16.1 (2021),pp. 98\u2013107.[87] High-Level Expert Group on AI. Ethics guidelines fortrustworthy AI. https: // ec .europa . eu/ newsroom /dae/document.cfm?doc id=60419.[88] Microsoft. Microsoft AI principles. https : / / www .microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6.[89] PwC. 2019 AI Predictions. https://www.pwc.com/us / en / services / consulting / library / arti\ufb01cial -intelligence-predictions-2019.html.[90] PwC. 2020 AI Predictions. https://www.pwc.com/us / en / services / consulting / library / arti\ufb01cial -intelligence-predictions-2020.html.[91] Nripsuta Ani Saxena et al. \u201cHow do fairness def-initions fare? Examining public attitudes towardsalgorithmic de\ufb01nitions of fairness\u201d. In: Proceedingsof the 2019 AAAI/ACM Conference on AI, Ethics, andSociety. 2019, pp. 99\u2013106.[92] Moritz Hardt, Eric Price, and Nathan Srebro. \u201cEqual-ity of opportunity in supervised learning\u201d. In: arXivpreprint arXiv:1610.02413 (2016).[93] Matt J Kusner et al. \u201cCounterfactual fairness\u201d. In:arXiv preprint arXiv:1703.06856 (2017).[94] Ninareh Mehrabi et al. \u201cA survey on bias andIn: arXiv preprintfairness in machine learning\u201d.arXiv:1908.09635 (2019).Jiahao Chen et al. \u201cFairness under unawareness:Assessing disparity when protected class is unob-served\u201d. In: Proceedings of the conference on fairness,accountability, and transparency. 2019, pp. 339\u2013348.[95][96] Dorian Peters et al. \u201cResponsible AI\u2014two frame-works for ethical design practice\u201d. In: IEEE Transac-tions on Technology and Society 1.1 (2020), pp. 34\u201347.[97] Lu Cheng, Kush R Varshney, and Huan Liu. \u201cSo-cially Responsible AI Algorithms: Issues, Purposes,and Challenges\u201d. In: arXiv preprint arXiv:2101.02032(2021).[98] Faisal Kamiran and Indr \u02d9e \u02c7Zliobait \u02d9e. \u201cExplainableand non-explainable discrimination in classi\ufb01ca-tion\u201d. In: Discrimination and Privacy in the InformationSociety. Springer, 2013, pp. 155\u2013170.[100] understanding unintended consequences of machinelearning\u201d. In: arXiv preprint arXiv:1901.10002 (2019).Jia Deng et al. \u201cImagenet: A large-scale hierarchicalimage database\u201d. In: 2009 IEEE conference on computervision and pattern recognition. Ieee. 2009, pp. 248\u2013255.Ivan Krasin et al. \u201cOpenimages: A public dataset forlarge-scale multi-label and multi-class image clas-si\ufb01cation\u201d. In: Dataset available from https://github.com/openimages 2.3 (2017), p. 18.[101][102] Shreya Shankar et al. \u201cNo classi\ufb01cation without rep-resentation: Assessing geodiversity issues in opendata sets for the developing world\u201d. In: arXiv preprintarXiv:1711.08536 (2017).[103] Alexandra Olteanu et al. \u201cSocial data: Biases,methodological pitfalls, and ethical boundaries\u201d. In:Frontiers in Big Data 2 (2019), p. 13.[104] Brendan F Klare et al. \u201cFace recognition perfor-mance: Role of demographic information\u201d. In: IEEETransactions on Information Forensics and Security 7.6(2012), pp. 1789\u20131801.[105] Hachim El Khiyari and Harry Wechsler. \u201cFace veri\ufb01-cation subject to varying (age, ethnicity, and gender)demographics using deep learning\u201d. In: Journal ofBiometrics and Biostatistics 7.323 (2016), p. 11.[106] Cynthia M Cook et al. \u201cDemographic effects in facialrecognition and their dependence on image acquisi-tion: An evaluation of eleven commercial systems\u201d.In: IEEE Transactions on Biometrics, Behavior, and Iden-tity Science 1.1 (2019), pp. 32\u201341.[107] Ayanna Howard, Cha Zhang, and Eric Horvitz. \u201cAd-dressing bias in machine learning algorithms: A pilotstudy on emotion recognition for intelligent sys-tems\u201d. In: 2017 IEEE Workshop on Advanced Roboticsand its Social Impacts (ARSO). IEEE. 2017, pp. 1\u20137.[108] Lauren Rhue. \u201cRacial in\ufb02uence on automated per-ceptions of emotions\u201d. In: Available at SSRN 3281765(2018).[109] Emily Denton et al. \u201cDetecting bias with genera-tive counterfactual face attribute augmentation\u201d. In:arXiv preprint arXiv:1906.06439 (2019). [110] Zeyu Wang et al. \u201cTowards fairness in visual recog-nition: Effective strategies for bias mitigation\u201d. In:Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. 2020, pp. 8919\u20138928.[111] Ziwei Liu et al. \u201cDeep learning face attributes inthe wild\u201d. In: Proceedings of the IEEE internationalconference on computer vision. 2015, pp. 3730\u20133738.[112] Tian Xu et al. \u201cInvestigating bias and fairness in fa-cial expression recognition\u201d. In: European Conferenceon Computer Vision. Springer. 2020, pp. 506\u2013523.[113] Shan Li, Weihong Deng, and JunPing Du. \u201cReli-able Crowdsourcing and Deep Locality-PreservingLearning for Expression Recognition in the Wild\u201d. In:2017 IEEE Conference on Computer Vision and PatternRecognition (CVPR). IEEE. 2017, pp. 2584\u20132593.[114] Navneet Dalal and Bill Triggs. \u201cHistograms of ori-ented gradients for human detection\u201d. In: 2005 IEEEcomputer society conference on computer vision and pat-tern recognition (CVPR\u201905). Vol. 1. Ieee. 2005, pp. 886\u2013893.dlib C++ libraryJia Xiang and Gengming Zhu. \u201cJoint face detectionand facial expression recognition with MTCNN\u201d. In:2017 4th International Conference on Information Scienceand Control Engineering (ICISCE). IEEE. 2017, pp. 424\u2013427.[116][118][117] Stuart Lloyd. \u201cLeast squares quantization in PCM\u201d.In: IEEE transactions on information theory 28.2 (1982),pp. 129\u2013137.James MacQueen et al. \u201cSome methods for classi\ufb01-cation and analysis of multivariate observations\u201d. In:Proceedings of the \ufb01fth Berkeley symposium on mathemat-ical statistics and probability. Vol. 1. 14. Oakland, CA,USA. 1967, pp. 281\u2013297.[119] PyTorch Documentation. : https : / / pytorch . org /docs/stable/nn.html.[120] Sergey Ioffe and Christian Szegedy. \u201cBatch normal-ization: Accelerating deep network training by re-ducing internal covariate shift\u201d. In: International con-ference on machine learning. PMLR. 2015, pp. 448\u2013456.[121] Sepp Hochreiter and J \u00a8urgen Schmidhuber. \u201cLongIn: Neural computation 9.8short-term memory\u201d.(1997), pp. 1735\u20131780.[122] Alex Graves, Abdel-rahman Mohamed, and Geof-frey Hinton. \u201cSpeech recognition with deep recurrentneural networks\u201d. In: 2013 IEEE international confer-ence on acoustics, speech and signal processing. Ieee.2013, pp. 6645\u20136649.[123] Savvas Varsamopoulos, Koen Bertels, Carmen G Al-mudever, et al. \u201cDesigning neural network basedIn: arXiv preprintdecoders for surface codes\u201d.arXiv:1811.12456 (2018).[124] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zis-serman. \u201cDeep face recognition\u201d. In: (2015).[125] Kaiming He et al. \u201cDeep residual learning for imagerecognition\u201d. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. 2016, pp. 770\u2013778.Jie Hu, Li Shen, and Gang Sun. \u201cSqueeze-and-excitation networks\u201d. In: Proceedings of the IEEE con-[126]ference on computer vision and pattern recognition. 2018,pp. 7132\u20137141.[127] Qiong Cao et al. \u201cVggface2: A dataset for recognisingfaces across pose and age\u201d. In: 2018 13th IEEE interna-tional conference on automatic face & gesture recognition(FG 2018). IEEE. 2018, pp. 67\u201374.[128] VGGFace2-pytorch. : https : / / github . com /[129] cydonia999/VGGFace2-pytorch.Jianfeng Zhao, Xia Mao, and Jian Zhang. \u201cLearningdeep facial expression features from image and op-tical \ufb02ow sequences using 3D CNN\u201d. In: The VisualComputer 34.10 (2018), pp. 1461\u20131475.[130] Sai Prasanna Teja Reddy et al. \u201cSpontaneous facialmicro-expression recognition using 3D spatiotempo-ral convolutional neural networks\u201d. In: 2019 Inter-national Joint Conference on Neural Networks (IJCNN).IEEE. 2019, pp. 1\u20138.Jad Haddad, Olivier L\u00b4ezoray, and Philippe Hamel.\u201c3D-CNN for Facial Emotion Recognition in Videos\u201d.International Symposium on Visual Computing.In:[131][132] Du Tran et al. \u201cA closer look at spatiotemporalconvolutions for action recognition\u201d. In: Proceedingsof the IEEE conference on Computer Vision and PatternRecognition. 2018, pp. 6450\u20136459. 7 A7.1 Test setTABLE 2: Regular models, Test set, accuracyTABLE 3: Regular models, Test set, true positive rateTABLE 4: Regular models, Test set, false positive rateTABLE 5: Female models, Test set, accuracyTABLE 6: Female models, Test set, true positive rateTABLE 7: Female models, test set, false positive rateTABLE 8: Male models, Test set, accuracy TABLE 15: Regular models, Female set, true positive rateTABLE 9: Male models, Test set, true positive rate TABLE 16: Regular models, Female set, false positive rateTABLE 10: Male models, Test set, false positive rate TABLE 17: Regular models, Female set, accuracy7.2 Female set TABLE 18: Regular models, Female set, true positive rateTABLE 11: Regular models, Female set, accuracyTABLE 12: Regular models, Female set, true positive rate TABLE 19: Regular models, Female set, false positive rate7.3 Male setTABLE 13: Regular models, Female set, false positive rate TABLE 20: Regular models, Male set, accuracyTABLE 14: Regular models, Female set, accuracy TABLE 21: Regular models, Male set, true positive rateTABLE 22: Regular models, Male set, false positive rateTABLE 23: Regular models, Male set, accuracyTABLE 24: Regular models, Male set, true positive rateTABLE 25: Regular models, Male set, false positive rateTABLE 26: Regular models, Male set, accuracyTABLE 27: Regular models, Male set, true positive rateTABLE 28: Regular models, Male set, false positive rate7.4 Charts Fig. 26: Per model architecture, accuracy comparison be-tween all three test sets. These are Regular models, whichhave been trained on entire train data.Fig. 27: Per model architecture, accuracy comparison be-tween all three test sets. These are Female models, whichhave been trained on only female data.Fig. 28: Per model architecture, accuracy comparison be-tween all three test sets. These are Male models, which havebeen trained on only male data. Fig. 30: Per model architecture, Female set accuracy compar-ison between different model typesFig. 29: Per model architecture, Test set accuracy comparisonbetween different model types Fig. 31: Per model architecture, Male set accuracy compari-son between different model types",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "e26ee360-b69c-41f5-8946-e115ff9f3571",
                    "text": "",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "aa2f9294-27e9-482a-a5b4-48a19290d628",
                    "text": "Since database presented as video with high FPS rate, it wason High Performance Cluster (HPC). To decrease numberof frames without loss of the information, K-Means [117,118] algorithm was used. Simple yet effective, it allows todetermine K most distinguishable frames for each video.For the test purposes, it was generated K = 10, 20, 50. ForK = 10 the entire video was processed. For K = 20, \ufb01rst100 frames (1 sec) are excluded. For K = 50, 20 frames fromthe \ufb01rst half of the video and 30 from the second.Fig. 3: Example of emotion display through sequence offramesThe number of videos in database is relatively small. Sincedeep learning approaches operates better on large scale ofdata, data augmentation has been applied during training.In total, there are three augmentation techniques ap-plied:1) Horizontal \ufb02ipused together with CNN feature extractors, since latter areoperated only on single images, meanwhile the target inputis video. LSTM itself has sophisticated structure, with 4internal vectors named input gate(i), output gate(o), forgetgate(f ) and cell (C). Along with these vectors, on eachprocessing step LSTM contains hidden state (h) and cellinput activation ( \u02dcC) vectors. Visual explanation of howLSTM module operates is shown on Figure 5, where t istime step, U and W are parameters matrix, which have tobe learn. Implementation details could be found in [119]. 3.3.3 ResNet50-LSTMAnother great example of a model, which is a result ofconsecutive research in deep learning is ResNet50 [125].Following the same principle in construction, the model ispresented as a CNN feature extractor. ResNet50 backbonebegins with a sequence of convolutional layer, batch nor-malization, ReLU and MaxPooling. Next, four layers with 3,4, 6 and 3 Bottleneck blocks respectively. Model ends withAvgPooling layer to reduce output feature map size. At thispoint there are no more backbone layers, and data streamgoes down to LSTM module, following by fully connectedlayer. The entire architecture is visualised on Firuge 7. Sincethe model is too deep by itself, ImageNet pretrained weightsare used. In this paper, when we are referring to ResNet50,meaning entire architecture, which consists of a backboneand LSTM module.Fig. 5: LSTM module [123]3.3.1.6 Fully Connected layer (FC): Often the \ufb01nallayer, the only purpose of which is to apply af\ufb01ne transformy = W \u00d7 x + b, where x is input vector, y is output vector,W weights and b bias. Exact implementation which has beenused could be found in [119].",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "3e85f0cb-0f61-49a0-bd02-c825c7c1e24a",
                    "text": "One of the most famous model for face recognition whichhas been widely used and has become a background forfurther research is VGG16 architecture [124], named afterVisual Geometry Group, who has conducted research. Inthis work, VGG16 model serve as a feature extractor whichpreceded LSTM block. Together with a fully connected layerat the end, they create a model for classi\ufb01cation sequentialdata. The backbone model consists of series of blocks, whichare two or three convlutional layers with ReLU and theMaxPooling layer on the end. The model ends with fullyconnected layer. Exactly this layer serves as feature vector,which further goes to LSTM module, next down to thefully connected layer. The entire structure is on Figure 6.As were mentioned before, VGG architecture is wide used,therefore there are many good pretrained weights availablefor the research. It has been chosen to use pretrained weightsfrom ImageNet dataset [100] and VGGFace dataset [124]. Tosimplify further presentation of results, VGG16 architecturewith pretrained ImageNet weights is denoted as VGG16,meanwhile VGG16 architecture with pretrained VGGFaceweights is denoted as VGGFACE. Fig. 7: Architecture of ResNet50-LSTM. Blue: ResNet50backbone with pretrained weights. Red: layers trained fromthe scratch.",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "8290899e-5698-414b-86cf-8b96e3931781",
                    "text": "In [126] authors discovered, that integrating mechanism forlearning relationship between channels can considerablyimprove results. The goal of such block is to explicitly traina network the cross-correlation between spatial channels.The main advantages of this approach are lightweight, sim-plicity to implement and application to all common layers.Basically, for any given transformation F : X \u2212\u2192 U , \ufb01rst,squeeze the feature map into the descriptor vector and thenmultiply it to the feature block channel wise. See Figure 8.Fig. 6: Architecture of VGGLSTM. Blue: VGG16 backbonewith pretrained weights. Red: layers trained from thescratch. Fig. 8: Squeeze-and-Excitation block [126]In general, architecture of SENet-LSTM consists of twoparts: a backbone and LSTM module. The backbone isSENet model, which copies the architecture of ResNet50(blue blocks on Figure 7), however, each bottleneck blockenhanced with SE feature. In other words, Bottleneck layerplays role of function F , according to the Figure 8. TheLSTM module is a one layer LSTM layer withing one fullyconnected follow up layer. SENet backbone has 8096 outputfeatures. LSTM block has 256 hidden neurons. Since thenumber of training data is quite small, it has been chosento use pretrained weights for a backbone. Ready to useweights, trained of VGGFace2 dataset[127] for SENet wastaken from [128]. For purpose of convenience, further thisarchitecture is referred as SENetLSTM.3.3.5 3D-CNNAmong others approaches which utilize 3D CNN [129, 130]for FER, in [131] authors propose an architecture withinoptimized hyperparameters for CK+ and OULU-CASIA112 \u00d7 112 resolution frames. However, in purposes of thisresearch work, to be consistent with other network, theinput resolution has been changed to 224 \u00d7 224, and thesequence length varies. Hence, the model architecture hasbeen preserved up to fully connected layers, since theirsize is directly depended on the input. The \ufb01nal modelis presented on Figure 9. This model does not have anypretrained weights and trained from the scratch. In thefurther sections this model is referred as 3DCNN.Fig. 9: Architecture of 3D-CNN model3.3.6 ResNet3DIn 2018, a group of researchers proposed a new way totreat 3D data such as videos, presenting another two con-volutional blocks: mixed convolutions and \u201d(2+1)D\u201d con-volutions [132]. Utilizing these block, authors constructedResNet3D, an analogue of already famous ResNet2D. De\ufb01n-ing the clone of shorted version of ResNet - ResNet18 -ResNet3D has got three different implementations: with 3Dconvolutions, with mixed convolutions and with \u201d2+1\u201dDconvolutions. Since authors claim that the best performancehas been demonstrated by \u201d2+1\u201dD convolutions, in thispaper exactly this type of model is used. The idea behind\u201d2+1\u201dD convolutions is to think about 3D convolutions as2D convolutions in spatial space, which are followed by1D convolutions in the temporal space. The bene\ufb01ts fromthese approach are following: \ufb01rst, due to the additionalReLU after 2D convolutions, this block has twice morenumber of nonlinearities, increasing capacity of the model,second, authors claim that during the training, a tensorwise error rate is smaller, compared to the 3D convolutioncounterparts. The overall architecture of ResNet3D is similarto ResNet50, but with different blocks. The \ufb01nal modelstructure is presented on Figure 10. Fig. 10: Structure of ResNet3D-LSTM model",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "f549ea57-7eeb-4e7c-b822-01107a4578ff",
                    "text": "The goal is to train a classi\ufb01er to classify an emotion outof six classes: surprise, contempt, disgust, anger, sad andhappy. Let us de\ufb01ne a neural network as probabilistic clas-si\ufb01er P , with parameters \u0398. In this paper, a classi\ufb01cation isperformed in the \u201dsoft\u201d form, i.e.\u02c6y = Sof tmax (P (Y = y|X, \u0398)) (4)where X is an input, Y is an output, y \u2208 0, 1, 2, 3, 4, 5according to the six type of emotions. Softmax, in turn, isde\ufb01ned as followsSof tmax(x ) = e e(cid:80) (5)4 EThe following section has an overview of all details regard-ing experiments. Hence, below will be described data divi-sion and hyperparameters. In addition, this section containsdescription of necessary metrics, based on which conclusionwould be derived.4.1 Input sizeThe raw instance of database is a video \ufb01le. Due to highresolution of videos and limited computational capacity,as was mentioned in 3.2.2, each video has been squeezedfor particular number of frames. However, the observationsare that: within K = 20 models generalize data the best.Using K = 10, there were not any adequate results andwith K = 50 there were not observed any increase of inperformance. Moreover, due to the limited computationalresources, bigger length of the input leads to decreasing ofbatch size, consequently increasing training time or even notallowing to \ufb01t a model. The input resolution is (224 \u00d7 224)for all models, except ResNet3D. Due to the enormousnumber of weights, available computational capacity is notenough to \ufb01t the model. Hence, the input resolution forResNet3D is (112 \u00d7 112). Putting all together, the inputsample for ResNet3D is (10, 3, 112, 112) and for all othermodels is (10, 3, 224, 224).4.2 Data divisionSASE-FE dataset contains 18 female and 32 male subjects.The instances for the test group have been selected ran-domly and once. Overall we had selected 5 males and 5females. Hence, all videos corresponded to these persons arenot visible anyhow during the training. Since each personhas 12 videos, therefore test set contains 120 videos, 20videos per emotion.Often, during training process a model tends to under\ufb01tor over\ufb01t. To track these kinds of behavior, making trainingprocess more ef\ufb01cient and to pick the best model, it has beendecided to organize a validation test with \ufb01xed size as 15%each train run.To sum up, each training process contains train andvalidation sets with 32 and 8 subjects correspondingly. Mul-tiplying by 12, there are 384 train videos and 96 validationvideos. Test set with total 120 videos is not involved duringtraining process. The desired metrics which are related tothe goals of this paper are calculated exactly from inferenceon the test set.4.3 Training detailsAll training experiments were performed on University ofTartu HPC. The target GPU is NVIDIA Tesla-V100 with 32GB of VRAM. All code is written in Python with usage ofmachine learning python package - PyTorch. The hyperpa-rameter search during the entire research was consisted oftwo phases for each model. First phase of hyperparametersearch was performed to \ufb01nd out the optimal model, hencetarget hyperparameters were such things as number of fullyconnected layers, number of LSTM layers, number of nodesin these layers. After \ufb01nding an optimal structure, in thesecond phase the optimal set of training hyperparametersfor each model have been discovered. In the scope of ex-periments, such variables have been varied: learning rate,batch size and weight decay. The \ufb01nal hyperparameters arepresented in Table 1.TABLE 1: Hyperparameters values for each model Test set, Male set and Female set. Moreover, it was decidedto look how performance and metrics differ depends ontraining data. For this purpose, all train data also wasdivided into pure male train set and pure female trainset. Therefore, to follow results easier, models which aretrained on entire train set are called Regular, trained on puremale set and pure female set are called Male and Femalerespectively. Training of all models have been performedusing Stochastic Gradient Decent (SGD) optimization. Tosum up, it has been trained 18 different neural networks,6 different models by 3 different train sets.4.4 MetricsAccording to the previously mentioned de\ufb01nitions of fair-ness, the main metrics, based on which any conclusioncould be made are accuracy, true positive rate (TPR) andfalse positive rate (FPR). Although these metrics are trivial,formal de\ufb01nitions are listed below:ACC = T P + N TT P + T N + F P + F NT P R =F P R = T PT P + F NF PF P + T N (7)(8)where T P - true positive, F P - false positive, T N - truenegative, F N - false negative, ACC - accuracy, T P R andF P R - true positive and false positive rate respectively.The \ufb01rst de\ufb01nition of fairness (Equation 1) implies theequality of TPR and FPR simultaneously. The second def-inition (Equation 2), extending to the multi classi\ufb01cationcase, requires equal TPR for the groups with differentunprotected attribute value. And the third, and the last,de\ufb01nition of fairness (Equation 3) means the equal accuracyfor separated groups. In other words, the difference in thesemetrics shows how fair model is, having linear dependency- the higher discrepancy, the more biased model is.Although target database comprises six different emo-tions, usually it is extremely dif\ufb01cult to distinguish betweencontempt, disgust and anger. In order to relax constraintsand make classi\ufb01cation a bit easier, without losing generalidea, it has been decided to fuse these three emotions into asingle emotion and name it \u201dUpset\u201d.5 RThis section consists of 2 parts. First part contains confusionmatrices and shallow analysis of each. Second part is servedfor more detailed and precise analysis from the perspectiveof fairness, emotions and test sets.5.1 Confusion matricesIn order to perform gender bias analysis, test set has beendivided into two subsets: pure male set and pure female set,such that each has 5 subjects. In this scenario, pure means toinclude subjects of only one gender. Hence, from here andfurther, there are three sets for testing, named as follows: Regular models have shown decent performance on Test set.The major part of emotions are classi\ufb01ed correctly, however,Upset and Sad emotions are frequently misclassi\ufb01ed, whichis an expected outcome, since they are close in V-A space.Confusion matrices are shown on Figure 11.Fig. 11: Confusion matrix per method for Regular modelson Test set Fig. 13: Confusion matrix per method for Male models onTest setFig. 12: Confusion matrix per method for Female models onTest set Fig. 14: Confusion matrix per method for Regular modelson Female setA decent decrease in accuracy for Surprise, which indi-cates that female training data is lack of Surprise expression.Confusion matrices are on Figure 12.We can observe almost the same performance compara-tively to the Regular models, however, the misclasi\ufb01cationrate of Sad emotion as Upset is much higher. This impliesthat male training data includes more Sad samples whichare visually much closer to Upset emotion. Confusion ma-trices are on Figure 13.Regular models have shown almost excellent recognitionof Happy emotion for Female set. TPR for Surprise in nothigh, however, we observe less misclassi\ufb01cation betweenUpset and Sad. Confusion matrices are on Figure 14.Quite unexpected results for Female models on Femaleset. Only TPR for Happy emotion is high enough to consideras acceptable. Recognition of Surprise is very low, whichmeans that female training data has weak samples forSurprise. Confusion matrices are on Figure 15.As has been stated before, Female training data has weakSurprise samples, meanwhile Male models have shownaverage performance, hence, even with different genderdomain, male samples of Surprise are much stronger. Con-fusion matrices are on Figure 16.Comparatively to other test sets, inference of Regular Fig. 15: Confusion matrix per method for Female models onFemale setmodels on Male set has much lower TPR for Happy emotionand much higher TPR for Surprise emotion. Confusionmatrices are on Figure 17.Female models on the Male set have probably the worstmetrics overall, due to the lack of data in train data andopposite gender in test data. Confusion matrices are onperformance expectations have been higher. Confusion ma-trices are on Figure 19.Fig. 19: Confusion matrix per method for Male models onMale set5.2 AnalysisThe goal of this research work is to analyse a gender bias,according to the de\ufb01nition of fairness. Raw results on whichthe following analysis is based are listed in Appendix 7. Inthe matter of convenience, several chars are presented inAppendix 7.4.According to equal opportunity (EQOP) and equalizedodds (EQOD) (Figure 20 (a, b)), the least biased is ResNet3D,while VGGFACE is the most unfair. Meanwhile, 3DCNNis second in term of biasness. However, comparison inaccuracy difference (demographic parity (DP)) (Figure 20(c)) consider the most fair model VGG16, when ResNet3Dis the second most fair one. As for the most biased modelsaccording to DP, these are 3DCNN and VGGFACE. Hence,overall results are consider to be aligned along all threede\ufb01nitions of fairness.(a) (b) (c)Fig. 20: Metrics for models, which have been trained onentire train dataAnalysing aggregated results (Figure 21), several conclu-sions have been derived. For the Test and Male set, Regularmodels show the best accuracy for classi\ufb01cation Surprise,while on the Female set, the best accuracy is for Happyemotion. Fused Upset emotion has the lowest recognitionrate for all three sets. Worth to mention, inference of Femaleset has much higher variance rather than on Male set.However, average accuracy for Female set is higher.From the perspective of emotions (Figure 26), results asfollows: classi\ufb01cation of Surprise is better for males, Upsetand Sad are expressed better by females and Happy isalmost identical recognized for both genders.Fig. 16: Confusion matrix per method for Male models onFemale setFig. 17: Confusion matrix per method for Regular modelson Male setFigure 18.Fig. 18: Confusion matrix per method for Female models onMale setMale models on Male set have shown great accuracyfor Surprise and Happy emotions. There are high TPR forUpset, however, misclassi\ufb01cation rate of Sad as Upset ishigh too. Since train and test data share the same gender,Fig. 21: Aggregated accuracy for models, trained on theentire data. Comparison for three different test sets. (a) (b) (c)Trained only on female data, Female models show com-pletely different picture. For all three de\ufb01nition of fairness,SENetLSTM architecture is considered as the most genderbiased. 3DCNN is the least biased according to EQOP andEQOD (Figure 22 (a,b)), and VGG16 according to the DP(Figure 22 (c)). For each test set, classi\ufb01cation accuracy isthe worst for Upset emotion, Happy is the best recognisedfor Test set and Female set. Inference on Male set shows thebest recognition of Surprise. Fig. 24: Metrics for models, which have been trained on onlymale datamale set has high variance, being unstable, while inferenceon Male set has small variance, and therefore, more robust.Without a surprise here, average accuracy on Male set ishigher rather than on Female set.(a) (b) (c)Fig. 22: Metrics for models, which have been trained on onlyfemale dataAccording to aggregated results (Figure 23), variancesfor Male and Female set are relatively equal, thereforerecognition for both gender is considered as robust. Theoverall accuracy is better for Male set, which is unexpectedresults, since models are trained exclusively on the femaledata.In the per emotion competition (Figure 27), classi\ufb01cationof Happy and Surprise emotions are almost identical forboth genders, meanwhile Upset and Sad are better recog-nized on female subjects.Opposite to previously mentioned type of models, Malemodels are trained on male data. SENetLSTM architecture isshown as the most fair architecture, according to the EQOP,EQOD and DP. For the most biased model, ResNet50 is con-sidered of being so with respect to EQOP and EQOD, whileResNet3D is a choice according to DP (Figure 24). Noticeable,the according to DP, ResNet50 and VGGFACE share secondthe most biased place in ranking. Therefore, results for Malemodels are considered to be consistent. For different testsets, the best and the worst accurate classi\ufb01ed emotion arethe same. Upset emotion has the lowest accuracy among alltest sets and Happy - the highest one.Aggregate results (Figure 25) show that inference on Fe-Fig. 23: Aggregated accuracy for models, trained on onlyfemale data. Comparison for three different test sets. Fig. 25: Aggregated accuracy for models, trained on onlymale data. Comparison for three different test sets.In the per emotion accuracy (Figure 28), inference onMale set leads in recognition of Surprise and Happy emo-tions. Sad recognition rate is almost the same for both sets.Accuracy for Upset emotion is much higher for Female set.6 CThis paper provided a comprehensive overview on the faceemotion recognition biases in the context of reliable AI.Taking the concrete dataset, SASE-FE, two different groupsof methods for face emotion recognition have been analyzedon gender bias. Each group consists of three different neu-ral network architectures, where some of them have beenready available and some have been manually implementedbefore the analysis. The test sets have been organized inthree different ways: entire test data, only male data andonly female data. The train sets have been organized in thesimilar way. All architectures have been trained, resultinginto 18 different models, 6 architectures per each train set.Since model bias can be explained through fairness,there have been given three different de\ufb01nition of fairness,according to which proper analysis have been conducted.As a results, it has been found which architectures are mostbiased with respect to the de\ufb01nitions of fairness, and whichones are more likely to be fair. In addition, it has beendiscovered, which kinds of emotions are easier to recognizefor men and women. In addition, using three distinct trainsets the relationship between training data and inference hasbeen analyzed.The topic of gender bias is relatively young and not ad-dressed properly. Therefore, the amount of existed probabledirections to research is immense. Extending this researchwork, for sure, the next goal has to be to expand researchon other databases, which are widely used in FER. Also,the models which have been utilized in this paper, are notcompetitors to the state-of-the-art solutions. Hence, anotherdirection of future work is to implement these solutions andanalyze whether they comprise gender bias. Far-reachingextensions include other aspects of RAI and XAI. For ex-ample study of other biases (race, age, culture) or workingon explanation, understanding and discovering knowledgelimits. Altogether, these researches will create a backgroundto more standardized regulation and law creation in the \ufb01eldof AI. As a result, integration of AI in society will be reliableand safe. After all, modern AI still encompasses a decentamount of unknown and hazard, therefore future us haveto be ready.AThis work is supported by the Estonian Centre of Excellencein IT (EXCITE) funded by the European Regional Develop-ment Fund. The authors also gratefully acknowledge thesupport of NVIDIA Corporation with the donation of theTitan XP Pascal GPU.R [1] Aaron Smith and Janna Anderson. \u201cAI, Robotics, andthe Future of Jobs\u201d. In: Pew Research Center 6 (2014),p. 51.[3] intelligence: Who is[2] Emanuele Neri et al. Arti\ufb01cialresponsible for the diagnosis? 2020.Jean-Franc\u00b8ois Bonnefon, Azim Shariff, and Iyad Rah-wan. \u201cThe social dilemma of autonomous vehicles\u201d.In: Science 352.6293 (2016), pp. 1573\u20131576.James Zou and Londa Schiebinger. AI can be sexistand racist\u2014it\u2019s time to make it fair. 2018.[4][5] Eric Mack. Hawking, Musk, Wozniak Warn About Ar-ti\ufb01cial Intelligence\u2019s Trigger Finger. https : / / www .forbes.com/sites/ericmack/2015/07/27/hawking-musk-wozniak-freaked-about-arti\ufb01cial-intelligence-getting-a-trigger-\ufb01nger/?sh=7ad6f69f7416.[6] Frank Rosenblatt. \u201cThe perceptron: a probabilisticmodel for information storage and organization inthe brain.\u201d In: Psychological review 65.6 (1958), p. 386.[7] Seppo Linnainmaa. \u201cThe representation of the cu-mulative rounding error of an algorithm as a Taylorexpansion of the local rounding errors\u201d. In: Master\u2019sThesis (in Finnish), Univ. Helsinki (1970), pp. 6\u20137.[8] Ning Qian and Terrence J Sejnowski. \u201cPredicting thesecondary structure of globular proteins using neuralnetwork models\u201d. In: Journal of molecular biology 202.4(1988), pp. 865\u2013884.algorithmwatch.org. Finnish Credit Score Ruling raisesQuestions about Discrimination and how to avoid it.https : / / algorithmwatch . org / en / story / \ufb01nnish -credit - score - ruling - raises - questions - about -discrimination-and-how-to-avoid-it/.[9][10] Daisuke Wakabayashi. Self-Driving Uber Car KillsPedestrian in Arizona, Where Robots Roam. https : / /www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html.[11] Niraj Chokshi. Tesla Autopilot System Found Probablyat Fault in 2018 Crash. https://www.nytimes.com/2020/02/25/business/tesla-autopilot-ntsb.html.Julia Angwin. Facebook Enabled Advertisers to Reach\u2018Jew Haters\u2019. https://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters.[12] [13] Veronica Rocha. Crime-\ufb01ghting robot hits, rolls overchild at Silicon Valley mall. https : / / www. latimes .com/local/lanow/la- me- ln- crime\ufb01ghting- robot-hurts-child-bay-area-20160713-snap-story.html.John Kingston. \u201cArti\ufb01cial intelligence and legal lia-bility\u201d. In: arXiv preprint arXiv:1802.07782 (2018).[15] Davide Carneiro et al. \u201cOnline dispute resolution:an arti\ufb01cial intelligence perspective\u201d. In: Arti\ufb01cialIntelligence Review 41.2 (2014), pp. 211\u2013240.[14][16] Keri Stephens. \u201cRecent Studies Examine Lunit AIin Breast Cancer Detection\u201d. In: AXIS Imaging News(2020).[17] Appen Wilson Pang. Responsible AI becomes criticalin 2021. https : / / venturebeat . com / 2020 / 11 / 11 /responsible-ai-becomes-critical-in-2021/.[18] Tableau. 2019 Business Intelligence Trend. https : / /www. tableau . com / reports / business - intelligence -trends/machine-learning.[19] Mariusz Bojarski et al. \u201cEnd to end learning forself-driving cars\u201d. In: arXiv preprint arXiv:1604.07316[20] Martin Tammvee and Gholamreza Anbarjafari. \u201cHu-man activity recognition-based path planning forautonomous vehicles\u201d. In: Signal, Image and VideoProcessing (2020), pp. 1\u20138.[21] Kwang-Eun Ko and Kwee-Bo Sim. \u201cDeep convolu-tional framework for abnormal behavior detection ina smart surveillance system\u201d. In: Engineering Applica-tions of Arti\ufb01cial Intelligence 67 (2018), pp. 226\u2013234.[22] Eric Marchand, Hideaki Uchiyama, and FabienSpindler. \u201cPose estimation for augmented reality: ahands-on survey\u201d. In: IEEE transactions on visualiza-tion and computer graphics 22.12 (2015), pp. 2633\u20132651.[23] Dorota Kami \u00b4nska et al. \u201cStress Reduction Using Bi-lateral Stimulation in Virtual Reality\u201d. In: IEEE Access8 (2020), pp. 200351\u2013200366.[24] Patricio Loncomilla, Javier Ruiz-del-Solar, and LuzMartinez. \u201cObject recognition using local invariantfeatures for robotic applications: A survey\u201d. In: Pat-tern Recognition 60 (2016), pp. 499\u2013514.[25] Anastasia Bolotnikova, Hasan Demirel, and Gho-lamreza Anbarjafari. \u201cReal-time ensemble based facerecognition system for NAO humanoids using localbinary pattern\u201d. In: Analog Integrated Circuits andSignal Processing 92.3 (2017), pp. 467\u2013475.[26] Anastasia Bolotnikova et al. \u201cA circuit-breaker use-case operated by a humanoid in aircraft manufac-turing\u201d. In: 2017 13th IEEE Conference on AutomationScience and Engineering (CASE). IEEE. 2017, pp. 15\u201322.[27] Baris Kayalibay, Grady Jensen, and Patrick van derSmagt. \u201cCNN-based segmentation of medical imag-ing data\u201d. In: arXiv preprint arXiv:1701.03056 (2017).[28] Luis G \u00b4omez-Chova et al. \u201cMultimodal classi\ufb01cationof remote sensing images: A review and future di-rections\u201d. In: Proceedings of the IEEE 103.9 (2015),pp. 1560\u20131584.[29] Andreas Kamilaris and Francesc X Prenafeta-Bold \u00b4u.\u201cDeep learning in agriculture: A survey\u201d. In: Comput-ers and electronics in agriculture 147 (2018), pp. 70\u201390.[30] Jianzhu Guo et al. \u201cDominant and complementaryemotion recognition from still images of faces\u201d. In:IEEE Access 6 (2018), pp. 26391\u201326403.[31] Egils Avots et al. \u201cEnsemble approach for detectionof depression using EEG features\u201d. In: arXiv preprintarXiv:2103.08467 (2021).[32] Kadir Aktas et al. \u201cSpatio-Temporal Based TableTennis Stroke Type Assessment\u201d. In: Signal, Image andVideo Processing (2021), pp. 1\u20138.[33] Paula M Niedenthal and Markus Brauer. \u201cSocialfunctionality of human emotion\u201d. In: Annual reviewof psychology 63 (2012), pp. 259\u2013285.[34] Eric M Reiman et al. \u201cNeuroanatomical correlatesof externally and internally generated human emo-tion\u201d. In: American Journal of Psychiatry 154.7 (1997),pp. 918\u2013925.[35] Fatemeh Noroozi et al. \u201cSurvey on emotional bodygesture recognition\u201d. In: IEEE transactions on affectivecomputing (2018).[36] Gholamreza Anbarjafari et al. Machine Learning forFace, Emotion, and Pain Recognition[37] Raymond J Dolan. \u201cEmotion, cognition, and behav-ior\u201d. In: science 298.5596 (2002), pp. 1191\u20131194.John G Carlson and Elaine Hat\ufb01eld. Psychology ofemotion. Harcourt Brace Jovanovich, 1992.[38][39] Mengyi Liu et al. \u201cDeeply learning deformable facialaction parts model for dynamic expression analysis\u201d.In: Asian conference on computer vision. Springer. 2014,pp. 143\u2013157.[40] Chien-Hsu Chen, I-Jui Lee, and Ling-Yi Lin. \u201cAug-mented reality-based self-facial modeling to promotethe emotional expression and social skills of adoles-cents with autism spectrum disorders\u201d. In: Researchin developmental disabilities 36 (2015), pp. 396\u2013403.[41] Agata Ko\u0142akowska et al. \u201cEmotion recognition andits applications\u201d. In: Human-Computer Systems Inter-action: Backgrounds and Applications 3. Springer, 2014,pp. 51\u201362.[42] Y-I Tian, Takeo Kanade, and Jeffrey F Cohn. \u201cRec-ognizing action units for facial expression analysis\u201d.In: IEEE Transactions on pattern analysis and machineintelligence 23.2 (2001), pp. 97\u2013115.James A Russell. \u201cA circumplex model of affect.\u201d In:Journal of personality and social psychology 39.6 (1980),p. 1161.[43][44] Paul Ekman. \u201cBasic emotions\u201d. In: Handbook of cogni-tion and emotion 98.45-60 (1999), p. 16.[45] Christer Loob et al. \u201cDominant and complementarymulti-emotional facial expression recognition usingc-support vector classi\ufb01cation\u201d. In: 2017 12th IEEEInternational Conference on Automatic Face & GestureRecognition (FG 2017). IEEE. 2017, pp. 833\u2013838.[46] Tomasz Sapi \u00b4nski et al. \u201cMultimodal database ofemotional speech, video and gestures\u201d. In: Inter-national Conference on Pattern Recognition. Springer.2018, pp. 153\u2013163.[47] Gary B Huang et al. \u201cLabeled faces in the wild:A database forstudying face recognition in uncon-strained environments\u201d. In: Workshop on faces in\u2019Real-Life\u2019Images: detection, alignment, and recognition. 2008. [48] Abhinav Dhall et al. \u201cVideo and image based emo-tion recognition challenges in the wild: Emotiw2015\u201d. In: Proceedings of the 2015 ACM on internationalconference on multimodal interaction. 2015, pp. 423\u2013426.and MattiPietik\u00a8ainen. \u201cFace recognition with local binarypatterns\u201d. In: European conference on computer vision.Springer. 2004, pp. 469\u2013481.[49] Timo Ahonen, Abdenour Hadid,[50] Xiaoyi Feng, Matti Pietik\u00a8ainen, and Abdenour Ha-did. \u201cFacial expression recognition based on localbinary patterns\u201d. In: Pattern Recognition and ImageAnalysis 17.4 (2007), pp. 592\u2013598.[51] Rajiv Mehrotra, Kameswara Rao Namuduri, and Na-garajan Ranganathan. \u201cGabor \ufb01lter-based edge de-tection\u201d. In: Pattern recognition 25.12 (1992), pp. 1479\u20131494.[52] Michael Lyons et al. \u201cCoding facial expressions withgabor wavelets\u201d. In: Proceedings Third IEEE interna-tional conference on automatic face and gesture recogni-tion. IEEE. 1998, pp. 200\u2013205.expression recognition based on Gabor wavelets andsparse representation\u201d. In: 2012 IEEE 11th Interna-tional Conference on Signal Processing. Vol. 2. IEEE.2012, pp. 816\u2013819.[54] Govardhan Mattela and Sandeep K Gupta. \u201cFacialexpression recognition using Gabor-mean-DWT fea-ture extraction technique\u201d. In: 2018 5th InternationalConference on Signal Processing and Integrated Networks(SPIN). IEEE. 2018, pp. 575\u2013580.[55] Taskeed Jabid, Md Hasanul Kabir, and Oksam Chae.\u201cRobust facial expression recognition based on lo-cal directional pattern\u201d. In: ETRI journal 32.5 (2010),pp. 784\u2013794.[56] Zhen Wang and Zilu Ying. \u201cFacial expression recog-nition based on local phase quantization and sparserepresentation\u201d. In: 2012 8th International Conferenceon Natural Computation. IEEE. 2012, pp. 222\u2013225.[57] Wei-Lun Chao, Jian-Jiun Ding, and Jun-Zuo Liu.\u201cFacial expression recognition based on improved lo-cal binary pattern and class-regularized locality pre-serving projection\u201d. In: Signal Processing 117 (2015),pp. 1\u201310.[58] Abu Sayeed Md Sohail and Prabir Bhattacharya.\u201cClassi\ufb01cation of facial expressions using k-nearestneighbor classi\ufb01er\u201d. In: International Conference onComputer Vision/Computer Graphics Collaboration Tech-niques and Applications. Springer. 2007, pp. 555\u2013566.[59] Qirong Mao et al. \u201cHierarchical Bayesian thememodels for multipose facial expression recognition\u201d.IEEE Transactions on Multimedia 19.4 (2016),In:pp. 861\u2013873.[60] Ming-Wei Huang, Zhe-wei Wang, and Zi-Lu Ying.\u201cA new method for facial expression recognitionbased on sparse representation plus LBP\u201d. In: 20103rd International Congress on Image and Signal Process-ing. Vol. 4. IEEE. 2010, pp. 1750\u20131754.[61] Hung-Hsu Tsai and Yi-Cheng Chang. \u201cFacial expres-sion recognition using a combination of multiplefacial features and support vector machine\u201d. In: SoftComputing 22.13 (2018), pp. 4389\u20134405.[62] Alex Krizhevsky, Ilya Sutskever, and Geoffrey EHinton. \u201cImagenet classi\ufb01cation with deep convolu-tional neural networks\u201d. In: Advances in neural infor-mation processing systems 25 (2012), pp. 1097\u20131105.[63] Peter Burkert et al. \u201cDexpression: Deep convolu-tional neural network for expression recognition\u201d. In:arXiv preprint arXiv:1509.05371 (2015).[64] Xiangyun Zhao et al. \u201cPeak-piloted deep network forfacial expression recognition\u201d. In: European conferenceon computer vision. Springer. 2016, pp. 425\u2013442.[65] Xiaoguang Chen et al. \u201cConvolution neural networkfor automatic facial expression recognition\u201d. In: 2017International conference on applied system innovation(ICASI). IEEE. 2017, pp. 814\u2013817.[66] Biao Yang et al. \u201cFacial expression recognition us-ing weighted mixture deep neural network basedon double-channel facial images\u201d. In: IEEE Access 6(2017), pp. 4630\u20134640.[67] Abir Fathallah, Lot\ufb01 Abdi, and Ali Douik. \u201cFa-cial expression recognition via deep learning\u201d. In:2017 IEEE/ACS 14th International Conference on Com-puter Systems and Applications (AICCSA). IEEE. 2017,pp. 745\u2013750.[68] Thanh-Hung Vo et al. \u201cPyramid with Super Resolu-tion for In-the-Wild Facial Expression Recognition\u201d.In: IEEE Access 8 (2020), pp. 131988\u2013132001.[69] Emad Barsoum et al. \u201cTraining deep networks forfacial expression recognition with crowd-sourced la-bel distribution\u201d. In: Proceedings of the 18th ACM In-ternational Conference on Multimodal Interaction. 2016,pp. 279\u2013283.[70] Yuedong Chen et al. \u201cFacial motion prior networksfor facial expression recognition\u201d. In: 2019 IEEEVisual Communications and Image Processing (VCIP).IEEE. 2019, pp. 1\u20134.[72][71] Ali Mollahosseini, Behzad Hasani, and MohammadH Mahoor. \u201cAffectnet: A database for facial expres-sion, valence, and arousal computing in the wild\u201d. In:IEEE Transactions on Affective Computing 10.1 (2017),pp. 18\u201331.Ilke Cugu, Eren Sener, and Emre Akbas. \u201cMicroExp-Net: An Extremely Small and Fast Model For Expres-sion Recognition From Face Images\u201d. In: 2019 NinthInternational Conference on Image Processing Theory,Tools and Applications (IPTA). IEEE. 2019, pp. 1\u20136.[73] Guoying Zhao et al. \u201cFacial expression recognitionfrom near-infrared videos\u201d. In: Image and Vision Com-puting 29.9 (2011), pp. 607\u2013619.[74] Yunxin Huang et al. \u201cFacial expression recognition:A survey\u201d. In: Symmetry 11.10 (2019), p. 1189.[75] Shashank Jaiswal and Michel Valstar. \u201cDeep learningthe dynamic appearance and shape of facial actionunits\u201d. In: 2016 IEEE winter conference on applicationsof computer vision (WACV). IEEE. 2016, pp. 1\u20138.[76] Abhinav Dhall et al. \u201cEmotiw 2016: Video and group-level emotion recognition challenges\u201d. In: Proceedingsof the 18th ACM international conference on multimodalinteraction. 2016, pp. 427\u2013432.[77] Yin Fan et al. \u201cVideo-based emotion recognitionusing CNN-RNN and C3D hybrid networks\u201d. In: Proceedings of the 18th ACM International Conferenceon Multimodal Interaction. 2016, pp. 445\u2013450.[78] Dae Hoe Kim et al. \u201cMulti-objective based spatio-temporal feature representation learning robust toexpression intensity variations for facial expressionrecognition\u201d. In: IEEE Transactions on Affective Com-puting 10.2 (2017), pp. 223\u2013236.[79] Zhenbo Yu et al. \u201cSpatio-temporal convolutional fea-tures with nested LSTM for facial expression recog-nition\u201d. In: Neurocomputing 317 (2018), pp. 50\u201357.[80] Kaiming He et al. \u201cSpatial pyramid pooling in deepconvolutional networks for visual recognition\u201d. In:IEEE transactions on pattern analysis and machine intel-ligence 37.9 (2015), pp. 1904\u20131916.[81] Young-Hyen Byeon and Keun-Chang Kwak. \u201cFacialexpression recognition using 3d convolutional neuralnetwork\u201d. In: International journal of advanced com-puter science and applications 5.12 (2014).[82] Behzad Hasani and Mohammad H Mahoor. \u201cFacialexpression recognition using enhanced deep 3D con-Proceedings of theIEEE conference on computer vision and pattern recog-nition workshops. 2017, pp. 30\u201340.[83] Christian Szegedy et al. \u201cInception-v4, inception-resnet and the impact of residual connections onlearning\u201d. In: Proceedings of the AAAI Conference onArti\ufb01cial Intelligence. Vol. 31. 1. 2017.[84] Ruicong Zhi et al. \u201cCombining 3D convolutionalneural networks with transfer learning by super-vised pre-training for facial micro-expression recog-nition\u201d. In: IEICE Transactions on Information and Sys-tems 102.5 (2019), pp. 1054\u20131064.Jing Li et al. \u201cMicro-expression recognition based on3D \ufb02ow convolutional neural network\u201d. In: PatternAnalysis and Applications 22.4 (2019), pp. 1331\u20131339.[85][86] Chao Wu and Fan Guo. \u201cTSNN: Three-Stream Com-bining 2D and 3D Convolutional Neural Network forMicro-Expression Recognition\u201d. In: IEEJ Transactionson Electrical and Electronic Engineering 16.1 (2021),pp. 98\u2013107.[87] High-Level Expert Group on AI. Ethics guidelines fortrustworthy AI. https: // ec .europa . eu/ newsroom /dae/document.cfm?doc id=60419.[88] Microsoft. Microsoft AI principles. https : / / www .microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6.[89] PwC. 2019 AI Predictions. https://www.pwc.com/us / en / services / consulting / library / arti\ufb01cial -intelligence-predictions-2019.html.[90] PwC. 2020 AI Predictions. https://www.pwc.com/us / en / services / consulting / library / arti\ufb01cial -intelligence-predictions-2020.html.[91] Nripsuta Ani Saxena et al. \u201cHow do fairness def-initions fare? Examining public attitudes towardsalgorithmic de\ufb01nitions of fairness\u201d. In: Proceedingsof the 2019 AAAI/ACM Conference on AI, Ethics, andSociety. 2019, pp. 99\u2013106.[92] Moritz Hardt, Eric Price, and Nathan Srebro. \u201cEqual-ity of opportunity in supervised learning\u201d. In: arXivpreprint arXiv:1610.02413 (2016).[93] Matt J Kusner et al. \u201cCounterfactual fairness\u201d. In:arXiv preprint arXiv:1703.06856 (2017).[94] Ninareh Mehrabi et al. \u201cA survey on bias andIn: arXiv preprintfairness in machine learning\u201d.arXiv:1908.09635 (2019).Jiahao Chen et al. \u201cFairness under unawareness:Assessing disparity when protected class is unob-served\u201d. In: Proceedings of the conference on fairness,accountability, and transparency. 2019, pp. 339\u2013348.[95][96] Dorian Peters et al. \u201cResponsible AI\u2014two frame-works for ethical design practice\u201d. In: IEEE Transac-tions on Technology and Society 1.1 (2020), pp. 34\u201347.[97] Lu Cheng, Kush R Varshney, and Huan Liu. \u201cSo-cially Responsible AI Algorithms: Issues, Purposes,and Challenges\u201d. In: arXiv preprint arXiv:2101.02032(2021).[98] Faisal Kamiran and Indr \u02d9e \u02c7Zliobait \u02d9e. \u201cExplainableand non-explainable discrimination in classi\ufb01ca-tion\u201d. In: Discrimination and Privacy in the InformationSociety. Springer, 2013, pp. 155\u2013170.[100] understanding unintended consequences of machinelearning\u201d. In: arXiv preprint arXiv:1901.10002 (2019).Jia Deng et al. \u201cImagenet: A large-scale hierarchicalimage database\u201d. In: 2009 IEEE conference on computervision and pattern recognition. Ieee. 2009, pp. 248\u2013255.Ivan Krasin et al. \u201cOpenimages: A public dataset forlarge-scale multi-label and multi-class image clas-si\ufb01cation\u201d. In: Dataset available from https://github.com/openimages 2.3 (2017), p. 18.[101][102] Shreya Shankar et al. \u201cNo classi\ufb01cation without rep-resentation: Assessing geodiversity issues in opendata sets for the developing world\u201d. In: arXiv preprintarXiv:1711.08536 (2017).[103] Alexandra Olteanu et al. \u201cSocial data: Biases,methodological pitfalls, and ethical boundaries\u201d. In:Frontiers in Big Data 2 (2019), p. 13.[104] Brendan F Klare et al. \u201cFace recognition perfor-mance: Role of demographic information\u201d. In: IEEETransactions on Information Forensics and Security 7.6(2012), pp. 1789\u20131801.[105] Hachim El Khiyari and Harry Wechsler. \u201cFace veri\ufb01-cation subject to varying (age, ethnicity, and gender)demographics using deep learning\u201d. In: Journal ofBiometrics and Biostatistics 7.323 (2016), p. 11.[106] Cynthia M Cook et al. \u201cDemographic effects in facialrecognition and their dependence on image acquisi-tion: An evaluation of eleven commercial systems\u201d.In: IEEE Transactions on Biometrics, Behavior, and Iden-tity Science 1.1 (2019), pp. 32\u201341.[107] Ayanna Howard, Cha Zhang, and Eric Horvitz. \u201cAd-dressing bias in machine learning algorithms: A pilotstudy on emotion recognition for intelligent sys-tems\u201d. In: 2017 IEEE Workshop on Advanced Roboticsand its Social Impacts (ARSO). IEEE. 2017, pp. 1\u20137.[108] Lauren Rhue. \u201cRacial in\ufb02uence on automated per-ceptions of emotions\u201d. In: Available at SSRN 3281765(2018).[109] Emily Denton et al. \u201cDetecting bias with genera-tive counterfactual face attribute augmentation\u201d. In:arXiv preprint arXiv:1906.06439 (2019). [110] Zeyu Wang et al. \u201cTowards fairness in visual recog-nition: Effective strategies for bias mitigation\u201d. In:Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. 2020, pp. 8919\u20138928.[111] Ziwei Liu et al. \u201cDeep learning face attributes inthe wild\u201d. In: Proceedings of the IEEE internationalconference on computer vision. 2015, pp. 3730\u20133738.[112] Tian Xu et al. \u201cInvestigating bias and fairness in fa-cial expression recognition\u201d. In: European Conferenceon Computer Vision. Springer. 2020, pp. 506\u2013523.[113] Shan Li, Weihong Deng, and JunPing Du. \u201cReli-able Crowdsourcing and Deep Locality-PreservingLearning for Expression Recognition in the Wild\u201d. In:2017 IEEE Conference on Computer Vision and PatternRecognition (CVPR). IEEE. 2017, pp. 2584\u20132593.[114] Navneet Dalal and Bill Triggs. \u201cHistograms of ori-ented gradients for human detection\u201d. In: 2005 IEEEcomputer society conference on computer vision and pat-tern recognition (CVPR\u201905). Vol. 1. Ieee. 2005, pp. 886\u2013893.dlib C++ libraryJia Xiang and Gengming Zhu. \u201cJoint face detectionand facial expression recognition with MTCNN\u201d. In:2017 4th International Conference on Information Scienceand Control Engineering (ICISCE). IEEE. 2017, pp. 424\u2013427.[116][118][117] Stuart Lloyd. \u201cLeast squares quantization in PCM\u201d.In: IEEE transactions on information theory 28.2 (1982),pp. 129\u2013137.James MacQueen et al. \u201cSome methods for classi\ufb01-cation and analysis of multivariate observations\u201d. In:Proceedings of the \ufb01fth Berkeley symposium on mathemat-ical statistics and probability. Vol. 1. 14. Oakland, CA,USA. 1967, pp. 281\u2013297.[119] PyTorch Documentation. : https : / / pytorch . org /docs/stable/nn.html.[120] Sergey Ioffe and Christian Szegedy. \u201cBatch normal-ization: Accelerating deep network training by re-ducing internal covariate shift\u201d. In: International con-ference on machine learning. PMLR. 2015, pp. 448\u2013456.[121] Sepp Hochreiter and J \u00a8urgen Schmidhuber. \u201cLongIn: Neural computation 9.8short-term memory\u201d.(1997), pp. 1735\u20131780.[122] Alex Graves, Abdel-rahman Mohamed, and Geof-frey Hinton. \u201cSpeech recognition with deep recurrentneural networks\u201d. In: 2013 IEEE international confer-ence on acoustics, speech and signal processing. Ieee.2013, pp. 6645\u20136649.[123] Savvas Varsamopoulos, Koen Bertels, Carmen G Al-mudever, et al. \u201cDesigning neural network basedIn: arXiv preprintdecoders for surface codes\u201d.arXiv:1811.12456 (2018).[124] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zis-serman. \u201cDeep face recognition\u201d. In: (2015).[125] Kaiming He et al. \u201cDeep residual learning for imagerecognition\u201d. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. 2016, pp. 770\u2013778.Jie Hu, Li Shen, and Gang Sun. \u201cSqueeze-and-excitation networks\u201d. In: Proceedings of the IEEE con-[126]ference on computer vision and pattern recognition. 2018,pp. 7132\u20137141.[127] Qiong Cao et al. \u201cVggface2: A dataset for recognisingfaces across pose and age\u201d. In: 2018 13th IEEE interna-tional conference on automatic face & gesture recognition(FG 2018). IEEE. 2018, pp. 67\u201374.[128] VGGFace2-pytorch. : https : / / github . com /[129] cydonia999/VGGFace2-pytorch.Jianfeng Zhao, Xia Mao, and Jian Zhang. \u201cLearningdeep facial expression features from image and op-tical \ufb02ow sequences using 3D CNN\u201d. In: The VisualComputer 34.10 (2018), pp. 1461\u20131475.[130] Sai Prasanna Teja Reddy et al. \u201cSpontaneous facialmicro-expression recognition using 3D spatiotempo-ral convolutional neural networks\u201d. In: 2019 Inter-national Joint Conference on Neural Networks (IJCNN).IEEE. 2019, pp. 1\u20138.Jad Haddad, Olivier L\u00b4ezoray, and Philippe Hamel.\u201c3D-CNN for Facial Emotion Recognition in Videos\u201d.International Symposium on Visual Computing.In:[131][132] Du Tran et al. \u201cA closer look at spatiotemporalconvolutions for action recognition\u201d. In: Proceedingsof the IEEE conference on Computer Vision and PatternRecognition. 2018, pp. 6450\u20136459. 7 A7.1 Test setTABLE 2: Regular models, Test set, accuracyTABLE 3: Regular models, Test set, true positive rateTABLE 4: Regular models, Test set, false positive rateTABLE 5: Female models, Test set, accuracyTABLE 6: Female models, Test set, true positive rateTABLE 7: Female models, test set, false positive rateTABLE 8: Male models, Test set, accuracy TABLE 15: Regular models, Female set, true positive rateTABLE 9: Male models, Test set, true positive rate TABLE 16: Regular models, Female set, false positive rateTABLE 10: Male models, Test set, false positive rate TABLE 17: Regular models, Female set, accuracy7.2 Female set TABLE 18: Regular models, Female set, true positive rateTABLE 11: Regular models, Female set, accuracyTABLE 12: Regular models, Female set, true positive rate TABLE 19: Regular models, Female set, false positive rate7.3 Male setTABLE 13: Regular models, Female set, false positive rate TABLE 20: Regular models, Male set, accuracyTABLE 14: Regular models, Female set, accuracy TABLE 21: Regular models, Male set, true positive rateTABLE 22: Regular models, Male set, false positive rateTABLE 23: Regular models, Male set, accuracyTABLE 24: Regular models, Male set, true positive rateTABLE 25: Regular models, Male set, false positive rateTABLE 26: Regular models, Male set, accuracyTABLE 27: Regular models, Male set, true positive rateTABLE 28: Regular models, Male set, false positive rate7.4 Charts Fig. 26: Per model architecture, accuracy comparison be-tween all three test sets. These are Regular models, whichhave been trained on entire train data.Fig. 27: Per model architecture, accuracy comparison be-tween all three test sets. These are Female models, whichhave been trained on only female data.Fig. 28: Per model architecture, accuracy comparison be-tween all three test sets. These are Male models, which havebeen trained on only male data. Fig. 30: Per model architecture, Female set accuracy compar-ison between different model typesFig. 29: Per model architecture, Test set accuracy comparisonbetween different model types Fig. 31: Per model architecture, Male set accuracy compari-son between different model types",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "762465b7-d466-48b0-b39b-5f88f6f8ff87",
                    "text": "",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "875cc5a5-65ca-440d-b72d-a2496f726d05",
                    "text": "The raw instance of database is a video \ufb01le. Due to highresolution of videos and limited computational capacity,as was mentioned in 3.2.2, each video has been squeezedfor particular number of frames. However, the observationsare that: within K = 20 models generalize data the best.Using K = 10, there were not any adequate results andwith K = 50 there were not observed any increase of inperformance. Moreover, due to the limited computationalresources, bigger length of the input leads to decreasing ofbatch size, consequently increasing training time or even notallowing to \ufb01t a model. The input resolution is (224 \u00d7 224)for all models, except ResNet3D. Due to the enormousnumber of weights, available computational capacity is notenough to \ufb01t the model. Hence, the input resolution forResNet3D is (112 \u00d7 112). Putting all together, the inputsample for ResNet3D is (10, 3, 112, 112) and for all othermodels is (10, 3, 224, 224).",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "8e222d6e-037f-4208-bfe9-884806962144",
                    "text": "SASE-FE dataset contains 18 female and 32 male subjects.The instances for the test group have been selected ran-domly and once. Overall we had selected 5 males and 5females. Hence, all videos corresponded to these persons arenot visible anyhow during the training. Since each personhas 12 videos, therefore test set contains 120 videos, 20videos per emotion.Often, during training process a model tends to under\ufb01tor over\ufb01t. To track these kinds of behavior, making trainingprocess more ef\ufb01cient and to pick the best model, it has beendecided to organize a validation test with \ufb01xed size as 15%each train run.To sum up, each training process contains train andvalidation sets with 32 and 8 subjects correspondingly. Mul-tiplying by 12, there are 384 train videos and 96 validationvideos. Test set with total 120 videos is not involved duringtraining process. The desired metrics which are related tothe goals of this paper are calculated exactly from inferenceon the test set.",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "dddb484f-cd6d-4e59-b535-6dae296c99b3",
                    "text": "All training experiments were performed on University ofTartu HPC. The target GPU is NVIDIA Tesla-V100 with 32GB of VRAM. All code is written in Python with usage ofmachine learning python package - PyTorch. The hyperpa-rameter search during the entire research was consisted oftwo phases for each model. First phase of hyperparametersearch was performed to \ufb01nd out the optimal model, hencetarget hyperparameters were such things as number of fullyconnected layers, number of LSTM layers, number of nodesin these layers. After \ufb01nding an optimal structure, in thesecond phase the optimal set of training hyperparametersfor each model have been discovered. In the scope of ex-periments, such variables have been varied: learning rate,batch size and weight decay. The \ufb01nal hyperparameters arepresented in Table 1.TABLE 1: Hyperparameters values for each model Test set, Male set and Female set. Moreover, it was decidedto look how performance and metrics differ depends ontraining data. For this purpose, all train data also wasdivided into pure male train set and pure female trainset. Therefore, to follow results easier, models which aretrained on entire train set are called Regular, trained on puremale set and pure female set are called Male and Femalerespectively. Training of all models have been performedusing Stochastic Gradient Decent (SGD) optimization. Tosum up, it has been trained 18 different neural networks,6 different models by 3 different train sets.",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "dd72e333-2f98-40ca-b254-eff6243f0837",
                    "text": "According to the previously mentioned de\ufb01nitions of fair-ness, the main metrics, based on which any conclusioncould be made are accuracy, true positive rate (TPR) andfalse positive rate (FPR). Although these metrics are trivial,formal de\ufb01nitions are listed below:ACC = T P + N TT P + T N + F P + F NT P R =F P R = T PT P + F NF PF P + T N (7)(8)where T P - true positive, F P - false positive, T N - truenegative, F N - false negative, ACC - accuracy, T P R andF P R - true positive and false positive rate respectively.The \ufb01rst de\ufb01nition of fairness (Equation 1) implies theequality of TPR and FPR simultaneously. The second def-inition (Equation 2), extending to the multi classi\ufb01cationcase, requires equal TPR for the groups with differentunprotected attribute value. And the third, and the last,de\ufb01nition of fairness (Equation 3) means the equal accuracyfor separated groups. In other words, the difference in thesemetrics shows how fair model is, having linear dependency- the higher discrepancy, the more biased model is.Although target database comprises six different emo-tions, usually it is extremely dif\ufb01cult to distinguish betweencontempt, disgust and anger. In order to relax constraintsand make classi\ufb01cation a bit easier, without losing generalidea, it has been decided to fuse these three emotions into asingle emotion and name it \u201dUpset\u201d.5 RThis section consists of 2 parts. First part contains confusionmatrices and shallow analysis of each. Second part is servedfor more detailed and precise analysis from the perspectiveof fairness, emotions and test sets.5.1 Confusion matricesIn order to perform gender bias analysis, test set has beendivided into two subsets: pure male set and pure female set,such that each has 5 subjects. In this scenario, pure means toinclude subjects of only one gender. Hence, from here andfurther, there are three sets for testing, named as follows: Regular models have shown decent performance on Test set.The major part of emotions are classi\ufb01ed correctly, however,Upset and Sad emotions are frequently misclassi\ufb01ed, whichis an expected outcome, since they are close in V-A space.Confusion matrices are shown on Figure 11.Fig. 11: Confusion matrix per method for Regular modelson Test set Fig. 13: Confusion matrix per method for Male models onTest setFig. 12: Confusion matrix per method for Female models onTest set Fig. 14: Confusion matrix per method for Regular modelson Female setA decent decrease in accuracy for Surprise, which indi-cates that female training data is lack of Surprise expression.Confusion matrices are on Figure 12.We can observe almost the same performance compara-tively to the Regular models, however, the misclasi\ufb01cationrate of Sad emotion as Upset is much higher. This impliesthat male training data includes more Sad samples whichare visually much closer to Upset emotion. Confusion ma-trices are on Figure 13.Regular models have shown almost excellent recognitionof Happy emotion for Female set. TPR for Surprise in nothigh, however, we observe less misclassi\ufb01cation betweenUpset and Sad. Confusion matrices are on Figure 14.Quite unexpected results for Female models on Femaleset. Only TPR for Happy emotion is high enough to consideras acceptable. Recognition of Surprise is very low, whichmeans that female training data has weak samples forSurprise. Confusion matrices are on Figure 15.As has been stated before, Female training data has weakSurprise samples, meanwhile Male models have shownaverage performance, hence, even with different genderdomain, male samples of Surprise are much stronger. Con-fusion matrices are on Figure 16.Comparatively to other test sets, inference of Regular Fig. 15: Confusion matrix per method for Female models onFemale setmodels on Male set has much lower TPR for Happy emotionand much higher TPR for Surprise emotion. Confusionmatrices are on Figure 17.Female models on the Male set have probably the worstmetrics overall, due to the lack of data in train data andopposite gender in test data. Confusion matrices are onperformance expectations have been higher. Confusion ma-trices are on Figure 19.Fig. 19: Confusion matrix per method for Male models onMale set5.2 AnalysisThe goal of this research work is to analyse a gender bias,according to the de\ufb01nition of fairness. Raw results on whichthe following analysis is based are listed in Appendix 7. Inthe matter of convenience, several chars are presented inAppendix 7.4.According to equal opportunity (EQOP) and equalizedodds (EQOD) (Figure 20 (a, b)), the least biased is ResNet3D,while VGGFACE is the most unfair. Meanwhile, 3DCNNis second in term of biasness. However, comparison inaccuracy difference (demographic parity (DP)) (Figure 20(c)) consider the most fair model VGG16, when ResNet3Dis the second most fair one. As for the most biased modelsaccording to DP, these are 3DCNN and VGGFACE. Hence,overall results are consider to be aligned along all threede\ufb01nitions of fairness.(a) (b) (c)Fig. 20: Metrics for models, which have been trained onentire train dataAnalysing aggregated results (Figure 21), several conclu-sions have been derived. For the Test and Male set, Regularmodels show the best accuracy for classi\ufb01cation Surprise,while on the Female set, the best accuracy is for Happyemotion. Fused Upset emotion has the lowest recognitionrate for all three sets. Worth to mention, inference of Femaleset has much higher variance rather than on Male set.However, average accuracy for Female set is higher.From the perspective of emotions (Figure 26), results asfollows: classi\ufb01cation of Surprise is better for males, Upsetand Sad are expressed better by females and Happy isalmost identical recognized for both genders.Fig. 16: Confusion matrix per method for Male models onFemale setFig. 17: Confusion matrix per method for Regular modelson Male setFigure 18.Fig. 18: Confusion matrix per method for Female models onMale setMale models on Male set have shown great accuracyfor Surprise and Happy emotions. There are high TPR forUpset, however, misclassi\ufb01cation rate of Sad as Upset ishigh too. Since train and test data share the same gender,Fig. 21: Aggregated accuracy for models, trained on theentire data. Comparison for three different test sets. (a) (b) (c)Trained only on female data, Female models show com-pletely different picture. For all three de\ufb01nition of fairness,SENetLSTM architecture is considered as the most genderbiased. 3DCNN is the least biased according to EQOP andEQOD (Figure 22 (a,b)), and VGG16 according to the DP(Figure 22 (c)). For each test set, classi\ufb01cation accuracy isthe worst for Upset emotion, Happy is the best recognisedfor Test set and Female set. Inference on Male set shows thebest recognition of Surprise. Fig. 24: Metrics for models, which have been trained on onlymale datamale set has high variance, being unstable, while inferenceon Male set has small variance, and therefore, more robust.Without a surprise here, average accuracy on Male set ishigher rather than on Female set.(a) (b) (c)Fig. 22: Metrics for models, which have been trained on onlyfemale dataAccording to aggregated results (Figure 23), variancesfor Male and Female set are relatively equal, thereforerecognition for both gender is considered as robust. Theoverall accuracy is better for Male set, which is unexpectedresults, since models are trained exclusively on the femaledata.In the per emotion competition (Figure 27), classi\ufb01cationof Happy and Surprise emotions are almost identical forboth genders, meanwhile Upset and Sad are better recog-nized on female subjects.Opposite to previously mentioned type of models, Malemodels are trained on male data. SENetLSTM architecture isshown as the most fair architecture, according to the EQOP,EQOD and DP. For the most biased model, ResNet50 is con-sidered of being so with respect to EQOP and EQOD, whileResNet3D is a choice according to DP (Figure 24). Noticeable,the according to DP, ResNet50 and VGGFACE share secondthe most biased place in ranking. Therefore, results for Malemodels are considered to be consistent. For different testsets, the best and the worst accurate classi\ufb01ed emotion arethe same. Upset emotion has the lowest accuracy among alltest sets and Happy - the highest one.Aggregate results (Figure 25) show that inference on Fe-Fig. 23: Aggregated accuracy for models, trained on onlyfemale data. Comparison for three different test sets. Fig. 25: Aggregated accuracy for models, trained on onlymale data. Comparison for three different test sets.In the per emotion accuracy (Figure 28), inference onMale set leads in recognition of Surprise and Happy emo-tions. Sad recognition rate is almost the same for both sets.Accuracy for Upset emotion is much higher for Female set.6 CThis paper provided a comprehensive overview on the faceemotion recognition biases in the context of reliable AI.Taking the concrete dataset, SASE-FE, two different groupsof methods for face emotion recognition have been analyzedon gender bias. Each group consists of three different neu-ral network architectures, where some of them have beenready available and some have been manually implementedbefore the analysis. The test sets have been organized inthree different ways: entire test data, only male data andonly female data. The train sets have been organized in thesimilar way. All architectures have been trained, resultinginto 18 different models, 6 architectures per each train set.Since model bias can be explained through fairness,there have been given three different de\ufb01nition of fairness,according to which proper analysis have been conducted.As a results, it has been found which architectures are mostbiased with respect to the de\ufb01nitions of fairness, and whichones are more likely to be fair. In addition, it has beendiscovered, which kinds of emotions are easier to recognizefor men and women. In addition, using three distinct trainsets the relationship between training data and inference hasbeen analyzed.The topic of gender bias is relatively young and not ad-dressed properly. Therefore, the amount of existed probabledirections to research is immense. Extending this researchwork, for sure, the next goal has to be to expand researchon other databases, which are widely used in FER. Also,the models which have been utilized in this paper, are notcompetitors to the state-of-the-art solutions. Hence, anotherdirection of future work is to implement these solutions andanalyze whether they comprise gender bias. Far-reachingextensions include other aspects of RAI and XAI. For ex-ample study of other biases (race, age, culture) or workingon explanation, understanding and discovering knowledgelimits. Altogether, these researches will create a backgroundto more standardized regulation and law creation in the \ufb01eldof AI. As a result, integration of AI in society will be reliableand safe. After all, modern AI still encompasses a decentamount of unknown and hazard, therefore future us haveto be ready.AThis work is supported by the Estonian Centre of Excellencein IT (EXCITE) funded by the European Regional Develop-ment Fund. The authors also gratefully acknowledge thesupport of NVIDIA Corporation with the donation of theTitan XP Pascal GPU.R [1] Aaron Smith and Janna Anderson. \u201cAI, Robotics, andthe Future of Jobs\u201d. In: Pew Research Center 6 (2014),p. 51.[3] intelligence: Who is[2] Emanuele Neri et al. Arti\ufb01cialresponsible for the diagnosis? 2020.Jean-Franc\u00b8ois Bonnefon, Azim Shariff, and Iyad Rah-wan. \u201cThe social dilemma of autonomous vehicles\u201d.In: Science 352.6293 (2016), pp. 1573\u20131576.James Zou and Londa Schiebinger. AI can be sexistand racist\u2014it\u2019s time to make it fair. 2018.[4][5] Eric Mack. Hawking, Musk, Wozniak Warn About Ar-ti\ufb01cial Intelligence\u2019s Trigger Finger. https : / / www .forbes.com/sites/ericmack/2015/07/27/hawking-musk-wozniak-freaked-about-arti\ufb01cial-intelligence-getting-a-trigger-\ufb01nger/?sh=7ad6f69f7416.[6] Frank Rosenblatt. \u201cThe perceptron: a probabilisticmodel for information storage and organization inthe brain.\u201d In: Psychological review 65.6 (1958), p. 386.[7] Seppo Linnainmaa. \u201cThe representation of the cu-mulative rounding error of an algorithm as a Taylorexpansion of the local rounding errors\u201d. In: Master\u2019sThesis (in Finnish), Univ. Helsinki (1970), pp. 6\u20137.[8] Ning Qian and Terrence J Sejnowski. \u201cPredicting thesecondary structure of globular proteins using neuralnetwork models\u201d. In: Journal of molecular biology 202.4(1988), pp. 865\u2013884.algorithmwatch.org. Finnish Credit Score Ruling raisesQuestions about Discrimination and how to avoid it.https : / / algorithmwatch . org / en / story / \ufb01nnish -credit - score - ruling - raises - questions - about -discrimination-and-how-to-avoid-it/.[9][10] Daisuke Wakabayashi. Self-Driving Uber Car KillsPedestrian in Arizona, Where Robots Roam. https : / /www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html.[11] Niraj Chokshi. Tesla Autopilot System Found Probablyat Fault in 2018 Crash. https://www.nytimes.com/2020/02/25/business/tesla-autopilot-ntsb.html.Julia Angwin. Facebook Enabled Advertisers to Reach\u2018Jew Haters\u2019. https://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters.[12] [13] Veronica Rocha. Crime-\ufb01ghting robot hits, rolls overchild at Silicon Valley mall. https : / / www. latimes .com/local/lanow/la- me- ln- crime\ufb01ghting- robot-hurts-child-bay-area-20160713-snap-story.html.John Kingston. \u201cArti\ufb01cial intelligence and legal lia-bility\u201d. In: arXiv preprint arXiv:1802.07782 (2018).[15] Davide Carneiro et al. \u201cOnline dispute resolution:an arti\ufb01cial intelligence perspective\u201d. In: Arti\ufb01cialIntelligence Review 41.2 (2014), pp. 211\u2013240.[14][16] Keri Stephens. \u201cRecent Studies Examine Lunit AIin Breast Cancer Detection\u201d. In: AXIS Imaging News(2020).[17] Appen Wilson Pang. Responsible AI becomes criticalin 2021. https : / / venturebeat . com / 2020 / 11 / 11 /responsible-ai-becomes-critical-in-2021/.[18] Tableau. 2019 Business Intelligence Trend. https : / /www. tableau . com / reports / business - intelligence -trends/machine-learning.[19] Mariusz Bojarski et al. \u201cEnd to end learning forself-driving cars\u201d. In: arXiv preprint arXiv:1604.07316[20] Martin Tammvee and Gholamreza Anbarjafari. \u201cHu-man activity recognition-based path planning forautonomous vehicles\u201d. In: Signal, Image and VideoProcessing (2020), pp. 1\u20138.[21] Kwang-Eun Ko and Kwee-Bo Sim. \u201cDeep convolu-tional framework for abnormal behavior detection ina smart surveillance system\u201d. In: Engineering Applica-tions of Arti\ufb01cial Intelligence 67 (2018), pp. 226\u2013234.[22] Eric Marchand, Hideaki Uchiyama, and FabienSpindler. \u201cPose estimation for augmented reality: ahands-on survey\u201d. In: IEEE transactions on visualiza-tion and computer graphics 22.12 (2015), pp. 2633\u20132651.[23] Dorota Kami \u00b4nska et al. \u201cStress Reduction Using Bi-lateral Stimulation in Virtual Reality\u201d. In: IEEE Access8 (2020), pp. 200351\u2013200366.[24] Patricio Loncomilla, Javier Ruiz-del-Solar, and LuzMartinez. \u201cObject recognition using local invariantfeatures for robotic applications: A survey\u201d. In: Pat-tern Recognition 60 (2016), pp. 499\u2013514.[25] Anastasia Bolotnikova, Hasan Demirel, and Gho-lamreza Anbarjafari. \u201cReal-time ensemble based facerecognition system for NAO humanoids using localbinary pattern\u201d. In: Analog Integrated Circuits andSignal Processing 92.3 (2017), pp. 467\u2013475.[26] Anastasia Bolotnikova et al. \u201cA circuit-breaker use-case operated by a humanoid in aircraft manufac-turing\u201d. In: 2017 13th IEEE Conference on AutomationScience and Engineering (CASE). IEEE. 2017, pp. 15\u201322.[27] Baris Kayalibay, Grady Jensen, and Patrick van derSmagt. \u201cCNN-based segmentation of medical imag-ing data\u201d. In: arXiv preprint arXiv:1701.03056 (2017).[28] Luis G \u00b4omez-Chova et al. \u201cMultimodal classi\ufb01cationof remote sensing images: A review and future di-rections\u201d. In: Proceedings of the IEEE 103.9 (2015),pp. 1560\u20131584.[29] Andreas Kamilaris and Francesc X Prenafeta-Bold \u00b4u.\u201cDeep learning in agriculture: A survey\u201d. In: Comput-ers and electronics in agriculture 147 (2018), pp. 70\u201390.[30] Jianzhu Guo et al. \u201cDominant and complementaryemotion recognition from still images of faces\u201d. In:IEEE Access 6 (2018), pp. 26391\u201326403.[31] Egils Avots et al. \u201cEnsemble approach for detectionof depression using EEG features\u201d. In: arXiv preprintarXiv:2103.08467 (2021).[32] Kadir Aktas et al. \u201cSpatio-Temporal Based TableTennis Stroke Type Assessment\u201d. In: Signal, Image andVideo Processing (2021), pp. 1\u20138.[33] Paula M Niedenthal and Markus Brauer. \u201cSocialfunctionality of human emotion\u201d. In: Annual reviewof psychology 63 (2012), pp. 259\u2013285.[34] Eric M Reiman et al. \u201cNeuroanatomical correlatesof externally and internally generated human emo-tion\u201d. In: American Journal of Psychiatry 154.7 (1997),pp. 918\u2013925.[35] Fatemeh Noroozi et al. \u201cSurvey on emotional bodygesture recognition\u201d. In: IEEE transactions on affectivecomputing (2018).[36] Gholamreza Anbarjafari et al. Machine Learning forFace, Emotion, and Pain Recognition[37] Raymond J Dolan. \u201cEmotion, cognition, and behav-ior\u201d. In: science 298.5596 (2002), pp. 1191\u20131194.John G Carlson and Elaine Hat\ufb01eld. Psychology ofemotion. Harcourt Brace Jovanovich, 1992.[38][39] Mengyi Liu et al. \u201cDeeply learning deformable facialaction parts model for dynamic expression analysis\u201d.In: Asian conference on computer vision. Springer. 2014,pp. 143\u2013157.[40] Chien-Hsu Chen, I-Jui Lee, and Ling-Yi Lin. \u201cAug-mented reality-based self-facial modeling to promotethe emotional expression and social skills of adoles-cents with autism spectrum disorders\u201d. In: Researchin developmental disabilities 36 (2015), pp. 396\u2013403.[41] Agata Ko\u0142akowska et al. \u201cEmotion recognition andits applications\u201d. In: Human-Computer Systems Inter-action: Backgrounds and Applications 3. Springer, 2014,pp. 51\u201362.[42] Y-I Tian, Takeo Kanade, and Jeffrey F Cohn. \u201cRec-ognizing action units for facial expression analysis\u201d.In: IEEE Transactions on pattern analysis and machineintelligence 23.2 (2001), pp. 97\u2013115.James A Russell. \u201cA circumplex model of affect.\u201d In:Journal of personality and social psychology 39.6 (1980),p. 1161.[43][44] Paul Ekman. \u201cBasic emotions\u201d. In: Handbook of cogni-tion and emotion 98.45-60 (1999), p. 16.[45] Christer Loob et al. \u201cDominant and complementarymulti-emotional facial expression recognition usingc-support vector classi\ufb01cation\u201d. In: 2017 12th IEEEInternational Conference on Automatic Face & GestureRecognition (FG 2017). IEEE. 2017, pp. 833\u2013838.[46] Tomasz Sapi \u00b4nski et al. \u201cMultimodal database ofemotional speech, video and gestures\u201d. In: Inter-national Conference on Pattern Recognition. Springer.2018, pp. 153\u2013163.[47] Gary B Huang et al. \u201cLabeled faces in the wild:A database forstudying face recognition in uncon-strained environments\u201d. In: Workshop on faces in\u2019Real-Life\u2019Images: detection, alignment, and recognition. 2008. [48] Abhinav Dhall et al. \u201cVideo and image based emo-tion recognition challenges in the wild: Emotiw2015\u201d. In: Proceedings of the 2015 ACM on internationalconference on multimodal interaction. 2015, pp. 423\u2013426.and MattiPietik\u00a8ainen. \u201cFace recognition with local binarypatterns\u201d. In: European conference on computer vision.Springer. 2004, pp. 469\u2013481.[49] Timo Ahonen, Abdenour Hadid,[50] Xiaoyi Feng, Matti Pietik\u00a8ainen, and Abdenour Ha-did. \u201cFacial expression recognition based on localbinary patterns\u201d. In: Pattern Recognition and ImageAnalysis 17.4 (2007), pp. 592\u2013598.[51] Rajiv Mehrotra, Kameswara Rao Namuduri, and Na-garajan Ranganathan. \u201cGabor \ufb01lter-based edge de-tection\u201d. In: Pattern recognition 25.12 (1992), pp. 1479\u20131494.[52] Michael Lyons et al. \u201cCoding facial expressions withgabor wavelets\u201d. In: Proceedings Third IEEE interna-tional conference on automatic face and gesture recogni-tion. IEEE. 1998, pp. 200\u2013205.expression recognition based on Gabor wavelets andsparse representation\u201d. In: 2012 IEEE 11th Interna-tional Conference on Signal Processing. Vol. 2. IEEE.2012, pp. 816\u2013819.[54] Govardhan Mattela and Sandeep K Gupta. \u201cFacialexpression recognition using Gabor-mean-DWT fea-ture extraction technique\u201d. In: 2018 5th InternationalConference on Signal Processing and Integrated Networks(SPIN). IEEE. 2018, pp. 575\u2013580.[55] Taskeed Jabid, Md Hasanul Kabir, and Oksam Chae.\u201cRobust facial expression recognition based on lo-cal directional pattern\u201d. In: ETRI journal 32.5 (2010),pp. 784\u2013794.[56] Zhen Wang and Zilu Ying. \u201cFacial expression recog-nition based on local phase quantization and sparserepresentation\u201d. In: 2012 8th International Conferenceon Natural Computation. IEEE. 2012, pp. 222\u2013225.[57] Wei-Lun Chao, Jian-Jiun Ding, and Jun-Zuo Liu.\u201cFacial expression recognition based on improved lo-cal binary pattern and class-regularized locality pre-serving projection\u201d. In: Signal Processing 117 (2015),pp. 1\u201310.[58] Abu Sayeed Md Sohail and Prabir Bhattacharya.\u201cClassi\ufb01cation of facial expressions using k-nearestneighbor classi\ufb01er\u201d. In: International Conference onComputer Vision/Computer Graphics Collaboration Tech-niques and Applications. Springer. 2007, pp. 555\u2013566.[59] Qirong Mao et al. \u201cHierarchical Bayesian thememodels for multipose facial expression recognition\u201d.IEEE Transactions on Multimedia 19.4 (2016),In:pp. 861\u2013873.[60] Ming-Wei Huang, Zhe-wei Wang, and Zi-Lu Ying.\u201cA new method for facial expression recognitionbased on sparse representation plus LBP\u201d. In: 20103rd International Congress on Image and Signal Process-ing. Vol. 4. IEEE. 2010, pp. 1750\u20131754.[61] Hung-Hsu Tsai and Yi-Cheng Chang. \u201cFacial expres-sion recognition using a combination of multiplefacial features and support vector machine\u201d. In: SoftComputing 22.13 (2018), pp. 4389\u20134405.[62] Alex Krizhevsky, Ilya Sutskever, and Geoffrey EHinton. \u201cImagenet classi\ufb01cation with deep convolu-tional neural networks\u201d. In: Advances in neural infor-mation processing systems 25 (2012), pp. 1097\u20131105.[63] Peter Burkert et al. \u201cDexpression: Deep convolu-tional neural network for expression recognition\u201d. In:arXiv preprint arXiv:1509.05371 (2015).[64] Xiangyun Zhao et al. \u201cPeak-piloted deep network forfacial expression recognition\u201d. In: European conferenceon computer vision. Springer. 2016, pp. 425\u2013442.[65] Xiaoguang Chen et al. \u201cConvolution neural networkfor automatic facial expression recognition\u201d. In: 2017International conference on applied system innovation(ICASI). IEEE. 2017, pp. 814\u2013817.[66] Biao Yang et al. \u201cFacial expression recognition us-ing weighted mixture deep neural network basedon double-channel facial images\u201d. In: IEEE Access 6(2017), pp. 4630\u20134640.[67] Abir Fathallah, Lot\ufb01 Abdi, and Ali Douik. \u201cFa-cial expression recognition via deep learning\u201d. In:2017 IEEE/ACS 14th International Conference on Com-puter Systems and Applications (AICCSA). IEEE. 2017,pp. 745\u2013750.[68] Thanh-Hung Vo et al. \u201cPyramid with Super Resolu-tion for In-the-Wild Facial Expression Recognition\u201d.In: IEEE Access 8 (2020), pp. 131988\u2013132001.[69] Emad Barsoum et al. \u201cTraining deep networks forfacial expression recognition with crowd-sourced la-bel distribution\u201d. In: Proceedings of the 18th ACM In-ternational Conference on Multimodal Interaction. 2016,pp. 279\u2013283.[70] Yuedong Chen et al. \u201cFacial motion prior networksfor facial expression recognition\u201d. In: 2019 IEEEVisual Communications and Image Processing (VCIP).IEEE. 2019, pp. 1\u20134.[72][71] Ali Mollahosseini, Behzad Hasani, and MohammadH Mahoor. \u201cAffectnet: A database for facial expres-sion, valence, and arousal computing in the wild\u201d. In:IEEE Transactions on Affective Computing 10.1 (2017),pp. 18\u201331.Ilke Cugu, Eren Sener, and Emre Akbas. \u201cMicroExp-Net: An Extremely Small and Fast Model For Expres-sion Recognition From Face Images\u201d. In: 2019 NinthInternational Conference on Image Processing Theory,Tools and Applications (IPTA). IEEE. 2019, pp. 1\u20136.[73] Guoying Zhao et al. \u201cFacial expression recognitionfrom near-infrared videos\u201d. In: Image and Vision Com-puting 29.9 (2011), pp. 607\u2013619.[74] Yunxin Huang et al. \u201cFacial expression recognition:A survey\u201d. In: Symmetry 11.10 (2019), p. 1189.[75] Shashank Jaiswal and Michel Valstar. \u201cDeep learningthe dynamic appearance and shape of facial actionunits\u201d. In: 2016 IEEE winter conference on applicationsof computer vision (WACV). IEEE. 2016, pp. 1\u20138.[76] Abhinav Dhall et al. \u201cEmotiw 2016: Video and group-level emotion recognition challenges\u201d. In: Proceedingsof the 18th ACM international conference on multimodalinteraction. 2016, pp. 427\u2013432.[77] Yin Fan et al. \u201cVideo-based emotion recognitionusing CNN-RNN and C3D hybrid networks\u201d. In: Proceedings of the 18th ACM International Conferenceon Multimodal Interaction. 2016, pp. 445\u2013450.[78] Dae Hoe Kim et al. \u201cMulti-objective based spatio-temporal feature representation learning robust toexpression intensity variations for facial expressionrecognition\u201d. In: IEEE Transactions on Affective Com-puting 10.2 (2017), pp. 223\u2013236.[79] Zhenbo Yu et al. \u201cSpatio-temporal convolutional fea-tures with nested LSTM for facial expression recog-nition\u201d. In: Neurocomputing 317 (2018), pp. 50\u201357.[80] Kaiming He et al. \u201cSpatial pyramid pooling in deepconvolutional networks for visual recognition\u201d. In:IEEE transactions on pattern analysis and machine intel-ligence 37.9 (2015), pp. 1904\u20131916.[81] Young-Hyen Byeon and Keun-Chang Kwak. \u201cFacialexpression recognition using 3d convolutional neuralnetwork\u201d. In: International journal of advanced com-puter science and applications 5.12 (2014).[82] Behzad Hasani and Mohammad H Mahoor. \u201cFacialexpression recognition using enhanced deep 3D con-Proceedings of theIEEE conference on computer vision and pattern recog-nition workshops. 2017, pp. 30\u201340.[83] Christian Szegedy et al. \u201cInception-v4, inception-resnet and the impact of residual connections onlearning\u201d. In: Proceedings of the AAAI Conference onArti\ufb01cial Intelligence. Vol. 31. 1. 2017.[84] Ruicong Zhi et al. \u201cCombining 3D convolutionalneural networks with transfer learning by super-vised pre-training for facial micro-expression recog-nition\u201d. In: IEICE Transactions on Information and Sys-tems 102.5 (2019), pp. 1054\u20131064.Jing Li et al. \u201cMicro-expression recognition based on3D \ufb02ow convolutional neural network\u201d. In: PatternAnalysis and Applications 22.4 (2019), pp. 1331\u20131339.[85][86] Chao Wu and Fan Guo. \u201cTSNN: Three-Stream Com-bining 2D and 3D Convolutional Neural Network forMicro-Expression Recognition\u201d. In: IEEJ Transactionson Electrical and Electronic Engineering 16.1 (2021),pp. 98\u2013107.[87] High-Level Expert Group on AI. Ethics guidelines fortrustworthy AI. https: // ec .europa . eu/ newsroom /dae/document.cfm?doc id=60419.[88] Microsoft. Microsoft AI principles. https : / / www .microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6.[89] PwC. 2019 AI Predictions. https://www.pwc.com/us / en / services / consulting / library / arti\ufb01cial -intelligence-predictions-2019.html.[90] PwC. 2020 AI Predictions. https://www.pwc.com/us / en / services / consulting / library / arti\ufb01cial -intelligence-predictions-2020.html.[91] Nripsuta Ani Saxena et al. \u201cHow do fairness def-initions fare? Examining public attitudes towardsalgorithmic de\ufb01nitions of fairness\u201d. In: Proceedingsof the 2019 AAAI/ACM Conference on AI, Ethics, andSociety. 2019, pp. 99\u2013106.[92] Moritz Hardt, Eric Price, and Nathan Srebro. \u201cEqual-ity of opportunity in supervised learning\u201d. In: arXivpreprint arXiv:1610.02413 (2016).[93] Matt J Kusner et al. \u201cCounterfactual fairness\u201d. In:arXiv preprint arXiv:1703.06856 (2017).[94] Ninareh Mehrabi et al. \u201cA survey on bias andIn: arXiv preprintfairness in machine learning\u201d.arXiv:1908.09635 (2019).Jiahao Chen et al. \u201cFairness under unawareness:Assessing disparity when protected class is unob-served\u201d. In: Proceedings of the conference on fairness,accountability, and transparency. 2019, pp. 339\u2013348.[95][96] Dorian Peters et al. \u201cResponsible AI\u2014two frame-works for ethical design practice\u201d. In: IEEE Transac-tions on Technology and Society 1.1 (2020), pp. 34\u201347.[97] Lu Cheng, Kush R Varshney, and Huan Liu. \u201cSo-cially Responsible AI Algorithms: Issues, Purposes,and Challenges\u201d. In: arXiv preprint arXiv:2101.02032(2021).[98] Faisal Kamiran and Indr \u02d9e \u02c7Zliobait \u02d9e. \u201cExplainableand non-explainable discrimination in classi\ufb01ca-tion\u201d. In: Discrimination and Privacy in the InformationSociety. Springer, 2013, pp. 155\u2013170.[100] understanding unintended consequences of machinelearning\u201d. In: arXiv preprint arXiv:1901.10002 (2019).Jia Deng et al. \u201cImagenet: A large-scale hierarchicalimage database\u201d. In: 2009 IEEE conference on computervision and pattern recognition. Ieee. 2009, pp. 248\u2013255.Ivan Krasin et al. \u201cOpenimages: A public dataset forlarge-scale multi-label and multi-class image clas-si\ufb01cation\u201d. In: Dataset available from https://github.com/openimages 2.3 (2017), p. 18.[101][102] Shreya Shankar et al. \u201cNo classi\ufb01cation without rep-resentation: Assessing geodiversity issues in opendata sets for the developing world\u201d. In: arXiv preprintarXiv:1711.08536 (2017).[103] Alexandra Olteanu et al. \u201cSocial data: Biases,methodological pitfalls, and ethical boundaries\u201d. In:Frontiers in Big Data 2 (2019), p. 13.[104] Brendan F Klare et al. \u201cFace recognition perfor-mance: Role of demographic information\u201d. In: IEEETransactions on Information Forensics and Security 7.6(2012), pp. 1789\u20131801.[105] Hachim El Khiyari and Harry Wechsler. \u201cFace veri\ufb01-cation subject to varying (age, ethnicity, and gender)demographics using deep learning\u201d. In: Journal ofBiometrics and Biostatistics 7.323 (2016), p. 11.[106] Cynthia M Cook et al. \u201cDemographic effects in facialrecognition and their dependence on image acquisi-tion: An evaluation of eleven commercial systems\u201d.In: IEEE Transactions on Biometrics, Behavior, and Iden-tity Science 1.1 (2019), pp. 32\u201341.[107] Ayanna Howard, Cha Zhang, and Eric Horvitz. \u201cAd-dressing bias in machine learning algorithms: A pilotstudy on emotion recognition for intelligent sys-tems\u201d. In: 2017 IEEE Workshop on Advanced Roboticsand its Social Impacts (ARSO). IEEE. 2017, pp. 1\u20137.[108] Lauren Rhue. \u201cRacial in\ufb02uence on automated per-ceptions of emotions\u201d. In: Available at SSRN 3281765(2018).[109] Emily Denton et al. \u201cDetecting bias with genera-tive counterfactual face attribute augmentation\u201d. In:arXiv preprint arXiv:1906.06439 (2019). [110] Zeyu Wang et al. \u201cTowards fairness in visual recog-nition: Effective strategies for bias mitigation\u201d. In:Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. 2020, pp. 8919\u20138928.[111] Ziwei Liu et al. \u201cDeep learning face attributes inthe wild\u201d. In: Proceedings of the IEEE internationalconference on computer vision. 2015, pp. 3730\u20133738.[112] Tian Xu et al. \u201cInvestigating bias and fairness in fa-cial expression recognition\u201d. In: European Conferenceon Computer Vision. Springer. 2020, pp. 506\u2013523.[113] Shan Li, Weihong Deng, and JunPing Du. \u201cReli-able Crowdsourcing and Deep Locality-PreservingLearning for Expression Recognition in the Wild\u201d. In:2017 IEEE Conference on Computer Vision and PatternRecognition (CVPR). IEEE. 2017, pp. 2584\u20132593.[114] Navneet Dalal and Bill Triggs. \u201cHistograms of ori-ented gradients for human detection\u201d. In: 2005 IEEEcomputer society conference on computer vision and pat-tern recognition (CVPR\u201905). Vol. 1. Ieee. 2005, pp. 886\u2013893.dlib C++ libraryJia Xiang and Gengming Zhu. \u201cJoint face detectionand facial expression recognition with MTCNN\u201d. In:2017 4th International Conference on Information Scienceand Control Engineering (ICISCE). IEEE. 2017, pp. 424\u2013427.[116][118][117] Stuart Lloyd. \u201cLeast squares quantization in PCM\u201d.In: IEEE transactions on information theory 28.2 (1982),pp. 129\u2013137.James MacQueen et al. \u201cSome methods for classi\ufb01-cation and analysis of multivariate observations\u201d. In:Proceedings of the \ufb01fth Berkeley symposium on mathemat-ical statistics and probability. Vol. 1. 14. Oakland, CA,USA. 1967, pp. 281\u2013297.[119] PyTorch Documentation. : https : / / pytorch . org /docs/stable/nn.html.[120] Sergey Ioffe and Christian Szegedy. \u201cBatch normal-ization: Accelerating deep network training by re-ducing internal covariate shift\u201d. In: International con-ference on machine learning. PMLR. 2015, pp. 448\u2013456.[121] Sepp Hochreiter and J \u00a8urgen Schmidhuber. \u201cLongIn: Neural computation 9.8short-term memory\u201d.(1997), pp. 1735\u20131780.[122] Alex Graves, Abdel-rahman Mohamed, and Geof-frey Hinton. \u201cSpeech recognition with deep recurrentneural networks\u201d. In: 2013 IEEE international confer-ence on acoustics, speech and signal processing. Ieee.2013, pp. 6645\u20136649.[123] Savvas Varsamopoulos, Koen Bertels, Carmen G Al-mudever, et al. \u201cDesigning neural network basedIn: arXiv preprintdecoders for surface codes\u201d.arXiv:1811.12456 (2018).[124] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zis-serman. \u201cDeep face recognition\u201d. In: (2015).[125] Kaiming He et al. \u201cDeep residual learning for imagerecognition\u201d. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. 2016, pp. 770\u2013778.Jie Hu, Li Shen, and Gang Sun. \u201cSqueeze-and-excitation networks\u201d. In: Proceedings of the IEEE con-[126]ference on computer vision and pattern recognition. 2018,pp. 7132\u20137141.[127] Qiong Cao et al. \u201cVggface2: A dataset for recognisingfaces across pose and age\u201d. In: 2018 13th IEEE interna-tional conference on automatic face & gesture recognition(FG 2018). IEEE. 2018, pp. 67\u201374.[128] VGGFace2-pytorch. : https : / / github . com /[129] cydonia999/VGGFace2-pytorch.Jianfeng Zhao, Xia Mao, and Jian Zhang. \u201cLearningdeep facial expression features from image and op-tical \ufb02ow sequences using 3D CNN\u201d. In: The VisualComputer 34.10 (2018), pp. 1461\u20131475.[130] Sai Prasanna Teja Reddy et al. \u201cSpontaneous facialmicro-expression recognition using 3D spatiotempo-ral convolutional neural networks\u201d. In: 2019 Inter-national Joint Conference on Neural Networks (IJCNN).IEEE. 2019, pp. 1\u20138.Jad Haddad, Olivier L\u00b4ezoray, and Philippe Hamel.\u201c3D-CNN for Facial Emotion Recognition in Videos\u201d.International Symposium on Visual Computing.In:[131][132] Du Tran et al. \u201cA closer look at spatiotemporalconvolutions for action recognition\u201d. In: Proceedingsof the IEEE conference on Computer Vision and PatternRecognition. 2018, pp. 6450\u20136459. 7 A7.1 Test setTABLE 2: Regular models, Test set, accuracyTABLE 3: Regular models, Test set, true positive rateTABLE 4: Regular models, Test set, false positive rateTABLE 5: Female models, Test set, accuracyTABLE 6: Female models, Test set, true positive rateTABLE 7: Female models, test set, false positive rateTABLE 8: Male models, Test set, accuracy TABLE 15: Regular models, Female set, true positive rateTABLE 9: Male models, Test set, true positive rate TABLE 16: Regular models, Female set, false positive rateTABLE 10: Male models, Test set, false positive rate TABLE 17: Regular models, Female set, accuracy7.2 Female set TABLE 18: Regular models, Female set, true positive rateTABLE 11: Regular models, Female set, accuracyTABLE 12: Regular models, Female set, true positive rate TABLE 19: Regular models, Female set, false positive rate7.3 Male setTABLE 13: Regular models, Female set, false positive rate TABLE 20: Regular models, Male set, accuracyTABLE 14: Regular models, Female set, accuracy TABLE 21: Regular models, Male set, true positive rateTABLE 22: Regular models, Male set, false positive rateTABLE 23: Regular models, Male set, accuracyTABLE 24: Regular models, Male set, true positive rateTABLE 25: Regular models, Male set, false positive rateTABLE 26: Regular models, Male set, accuracyTABLE 27: Regular models, Male set, true positive rateTABLE 28: Regular models, Male set, false positive rate7.4 Charts Fig. 26: Per model architecture, accuracy comparison be-tween all three test sets. These are Regular models, whichhave been trained on entire train data.Fig. 27: Per model architecture, accuracy comparison be-tween all three test sets. These are Female models, whichhave been trained on only female data.Fig. 28: Per model architecture, accuracy comparison be-tween all three test sets. These are Male models, which havebeen trained on only male data. Fig. 30: Per model architecture, Female set accuracy compar-ison between different model typesFig. 29: Per model architecture, Test set accuracy comparisonbetween different model types Fig. 31: Per model architecture, Male set accuracy compari-son between different model types",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "9748354c-726e-49ef-839f-90bec3f0f666",
                    "text": "",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "a0b7b5de-a1ac-4cc7-bb93-978db67a0f04",
                    "text": "In order to perform gender bias analysis, test set has beendivided into two subsets: pure male set and pure female set,such that each has 5 subjects. In this scenario, pure means toinclude subjects of only one gender. Hence, from here andfurther, there are three sets for testing, named as follows: Regular models have shown decent performance on Test set.The major part of emotions are classi\ufb01ed correctly, however,Upset and Sad emotions are frequently misclassi\ufb01ed, whichis an expected outcome, since they are close in V-A space.Confusion matrices are shown on Figure 11.Fig. 11: Confusion matrix per method for Regular modelson Test set Fig. 13: Confusion matrix per method for Male models onTest setFig. 12: Confusion matrix per method for Female models onTest set Fig. 14: Confusion matrix per method for Regular modelson Female setA decent decrease in accuracy for Surprise, which indi-cates that female training data is lack of Surprise expression.Confusion matrices are on Figure 12.We can observe almost the same performance compara-tively to the Regular models, however, the misclasi\ufb01cationrate of Sad emotion as Upset is much higher. This impliesthat male training data includes more Sad samples whichare visually much closer to Upset emotion. Confusion ma-trices are on Figure 13.Regular models have shown almost excellent recognitionof Happy emotion for Female set. TPR for Surprise in nothigh, however, we observe less misclassi\ufb01cation betweenUpset and Sad. Confusion matrices are on Figure 14.Quite unexpected results for Female models on Femaleset. Only TPR for Happy emotion is high enough to consideras acceptable. Recognition of Surprise is very low, whichmeans that female training data has weak samples forSurprise. Confusion matrices are on Figure 15.As has been stated before, Female training data has weakSurprise samples, meanwhile Male models have shownaverage performance, hence, even with different genderdomain, male samples of Surprise are much stronger. Con-fusion matrices are on Figure 16.Comparatively to other test sets, inference of Regular Fig. 15: Confusion matrix per method for Female models onFemale setmodels on Male set has much lower TPR for Happy emotionand much higher TPR for Surprise emotion. Confusionmatrices are on Figure 17.Female models on the Male set have probably the worstmetrics overall, due to the lack of data in train data andopposite gender in test data. Confusion matrices are onperformance expectations have been higher. Confusion ma-trices are on Figure 19.Fig. 19: Confusion matrix per method for Male models onMale set",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "46ffdb2b-d681-4c62-9d1a-3e97b000a46a",
                    "text": "The goal of this research work is to analyse a gender bias,according to the de\ufb01nition of fairness. Raw results on whichthe following analysis is based are listed in Appendix 7. Inthe matter of convenience, several chars are presented inAppendix 7.4.According to equal opportunity (EQOP) and equalizedodds (EQOD) (Figure 20 (a, b)), the least biased is ResNet3D,while VGGFACE is the most unfair. Meanwhile, 3DCNNis second in term of biasness. However, comparison inaccuracy difference (demographic parity (DP)) (Figure 20(c)) consider the most fair model VGG16, when ResNet3Dis the second most fair one. As for the most biased modelsaccording to DP, these are 3DCNN and VGGFACE. Hence,overall results are consider to be aligned along all threede\ufb01nitions of fairness.(a) (b) (c)Fig. 20: Metrics for models, which have been trained onentire train dataAnalysing aggregated results (Figure 21), several conclu-sions have been derived. For the Test and Male set, Regularmodels show the best accuracy for classi\ufb01cation Surprise,while on the Female set, the best accuracy is for Happyemotion. Fused Upset emotion has the lowest recognitionrate for all three sets. Worth to mention, inference of Femaleset has much higher variance rather than on Male set.However, average accuracy for Female set is higher.From the perspective of emotions (Figure 26), results asfollows: classi\ufb01cation of Surprise is better for males, Upsetand Sad are expressed better by females and Happy isalmost identical recognized for both genders.Fig. 16: Confusion matrix per method for Male models onFemale setFig. 17: Confusion matrix per method for Regular modelson Male setFigure 18.Fig. 18: Confusion matrix per method for Female models onMale setMale models on Male set have shown great accuracyfor Surprise and Happy emotions. There are high TPR forUpset, however, misclassi\ufb01cation rate of Sad as Upset ishigh too. Since train and test data share the same gender,Fig. 21: Aggregated accuracy for models, trained on theentire data. Comparison for three different test sets. (a) (b) (c)Trained only on female data, Female models show com-pletely different picture. For all three de\ufb01nition of fairness,SENetLSTM architecture is considered as the most genderbiased. 3DCNN is the least biased according to EQOP andEQOD (Figure 22 (a,b)), and VGG16 according to the DP(Figure 22 (c)). For each test set, classi\ufb01cation accuracy isthe worst for Upset emotion, Happy is the best recognisedfor Test set and Female set. Inference on Male set shows thebest recognition of Surprise. Fig. 24: Metrics for models, which have been trained on onlymale datamale set has high variance, being unstable, while inferenceon Male set has small variance, and therefore, more robust.Without a surprise here, average accuracy on Male set ishigher rather than on Female set.(a) (b) (c)Fig. 22: Metrics for models, which have been trained on onlyfemale dataAccording to aggregated results (Figure 23), variancesfor Male and Female set are relatively equal, thereforerecognition for both gender is considered as robust. Theoverall accuracy is better for Male set, which is unexpectedresults, since models are trained exclusively on the femaledata.In the per emotion competition (Figure 27), classi\ufb01cationof Happy and Surprise emotions are almost identical forboth genders, meanwhile Upset and Sad are better recog-nized on female subjects.Opposite to previously mentioned type of models, Malemodels are trained on male data. SENetLSTM architecture isshown as the most fair architecture, according to the EQOP,EQOD and DP. For the most biased model, ResNet50 is con-sidered of being so with respect to EQOP and EQOD, whileResNet3D is a choice according to DP (Figure 24). Noticeable,the according to DP, ResNet50 and VGGFACE share secondthe most biased place in ranking. Therefore, results for Malemodels are considered to be consistent. For different testsets, the best and the worst accurate classi\ufb01ed emotion arethe same. Upset emotion has the lowest accuracy among alltest sets and Happy - the highest one.Aggregate results (Figure 25) show that inference on Fe-Fig. 23: Aggregated accuracy for models, trained on onlyfemale data. Comparison for three different test sets. Fig. 25: Aggregated accuracy for models, trained on onlymale data. Comparison for three different test sets.In the per emotion accuracy (Figure 28), inference onMale set leads in recognition of Surprise and Happy emo-tions. Sad recognition rate is almost the same for both sets.Accuracy for Upset emotion is much higher for Female set.6 CThis paper provided a comprehensive overview on the faceemotion recognition biases in the context of reliable AI.Taking the concrete dataset, SASE-FE, two different groupsof methods for face emotion recognition have been analyzedon gender bias. Each group consists of three different neu-ral network architectures, where some of them have beenready available and some have been manually implementedbefore the analysis. The test sets have been organized inthree different ways: entire test data, only male data andonly female data. The train sets have been organized in thesimilar way. All architectures have been trained, resultinginto 18 different models, 6 architectures per each train set.Since model bias can be explained through fairness,there have been given three different de\ufb01nition of fairness,according to which proper analysis have been conducted.As a results, it has been found which architectures are mostbiased with respect to the de\ufb01nitions of fairness, and whichones are more likely to be fair. In addition, it has beendiscovered, which kinds of emotions are easier to recognizefor men and women. In addition, using three distinct trainsets the relationship between training data and inference hasbeen analyzed.The topic of gender bias is relatively young and not ad-dressed properly. Therefore, the amount of existed probabledirections to research is immense. Extending this researchwork, for sure, the next goal has to be to expand researchon other databases, which are widely used in FER. Also,the models which have been utilized in this paper, are notcompetitors to the state-of-the-art solutions. Hence, anotherdirection of future work is to implement these solutions andanalyze whether they comprise gender bias. Far-reachingextensions include other aspects of RAI and XAI. For ex-ample study of other biases (race, age, culture) or workingon explanation, understanding and discovering knowledgelimits. Altogether, these researches will create a backgroundto more standardized regulation and law creation in the \ufb01eldof AI. As a result, integration of AI in society will be reliableand safe. After all, modern AI still encompasses a decentamount of unknown and hazard, therefore future us haveto be ready.AThis work is supported by the Estonian Centre of Excellencein IT (EXCITE) funded by the European Regional Develop-ment Fund. The authors also gratefully acknowledge thesupport of NVIDIA Corporation with the donation of theTitan XP Pascal GPU.R [1] Aaron Smith and Janna Anderson. \u201cAI, Robotics, andthe Future of Jobs\u201d. In: Pew Research Center 6 (2014),p. 51.[3] intelligence: Who is[2] Emanuele Neri et al. Arti\ufb01cialresponsible for the diagnosis? 2020.Jean-Franc\u00b8ois Bonnefon, Azim Shariff, and Iyad Rah-wan. \u201cThe social dilemma of autonomous vehicles\u201d.In: Science 352.6293 (2016), pp. 1573\u20131576.James Zou and Londa Schiebinger. AI can be sexistand racist\u2014it\u2019s time to make it fair. 2018.[4][5] Eric Mack. Hawking, Musk, Wozniak Warn About Ar-ti\ufb01cial Intelligence\u2019s Trigger Finger. https : / / www .forbes.com/sites/ericmack/2015/07/27/hawking-musk-wozniak-freaked-about-arti\ufb01cial-intelligence-getting-a-trigger-\ufb01nger/?sh=7ad6f69f7416.[6] Frank Rosenblatt. \u201cThe perceptron: a probabilisticmodel for information storage and organization inthe brain.\u201d In: Psychological review 65.6 (1958), p. 386.[7] Seppo Linnainmaa. \u201cThe representation of the cu-mulative rounding error of an algorithm as a Taylorexpansion of the local rounding errors\u201d. In: Master\u2019sThesis (in Finnish), Univ. Helsinki (1970), pp. 6\u20137.[8] Ning Qian and Terrence J Sejnowski. \u201cPredicting thesecondary structure of globular proteins using neuralnetwork models\u201d. In: Journal of molecular biology 202.4(1988), pp. 865\u2013884.algorithmwatch.org. Finnish Credit Score Ruling raisesQuestions about Discrimination and how to avoid it.https : / / algorithmwatch . org / en / story / \ufb01nnish -credit - score - ruling - raises - questions - about -discrimination-and-how-to-avoid-it/.[9][10] Daisuke Wakabayashi. Self-Driving Uber Car KillsPedestrian in Arizona, Where Robots Roam. https : / /www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html.[11] Niraj Chokshi. Tesla Autopilot System Found Probablyat Fault in 2018 Crash. https://www.nytimes.com/2020/02/25/business/tesla-autopilot-ntsb.html.Julia Angwin. Facebook Enabled Advertisers to Reach\u2018Jew Haters\u2019. https://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters.[12] [13] Veronica Rocha. Crime-\ufb01ghting robot hits, rolls overchild at Silicon Valley mall. https : / / www. latimes .com/local/lanow/la- me- ln- crime\ufb01ghting- robot-hurts-child-bay-area-20160713-snap-story.html.John Kingston. \u201cArti\ufb01cial intelligence and legal lia-bility\u201d. In: arXiv preprint arXiv:1802.07782 (2018).[15] Davide Carneiro et al. \u201cOnline dispute resolution:an arti\ufb01cial intelligence perspective\u201d. In: Arti\ufb01cialIntelligence Review 41.2 (2014), pp. 211\u2013240.[14][16] Keri Stephens. \u201cRecent Studies Examine Lunit AIin Breast Cancer Detection\u201d. In: AXIS Imaging News(2020).[17] Appen Wilson Pang. Responsible AI becomes criticalin 2021. https : / / venturebeat . com / 2020 / 11 / 11 /responsible-ai-becomes-critical-in-2021/.[18] Tableau. 2019 Business Intelligence Trend. https : / /www. tableau . com / reports / business - intelligence -trends/machine-learning.[19] Mariusz Bojarski et al. \u201cEnd to end learning forself-driving cars\u201d. In: arXiv preprint arXiv:1604.07316[20] Martin Tammvee and Gholamreza Anbarjafari. \u201cHu-man activity recognition-based path planning forautonomous vehicles\u201d. In: Signal, Image and VideoProcessing (2020), pp. 1\u20138.[21] Kwang-Eun Ko and Kwee-Bo Sim. \u201cDeep convolu-tional framework for abnormal behavior detection ina smart surveillance system\u201d. In: Engineering Applica-tions of Arti\ufb01cial Intelligence 67 (2018), pp. 226\u2013234.[22] Eric Marchand, Hideaki Uchiyama, and FabienSpindler. \u201cPose estimation for augmented reality: ahands-on survey\u201d. In: IEEE transactions on visualiza-tion and computer graphics 22.12 (2015), pp. 2633\u20132651.[23] Dorota Kami \u00b4nska et al. \u201cStress Reduction Using Bi-lateral Stimulation in Virtual Reality\u201d. In: IEEE Access8 (2020), pp. 200351\u2013200366.[24] Patricio Loncomilla, Javier Ruiz-del-Solar, and LuzMartinez. \u201cObject recognition using local invariantfeatures for robotic applications: A survey\u201d. In: Pat-tern Recognition 60 (2016), pp. 499\u2013514.[25] Anastasia Bolotnikova, Hasan Demirel, and Gho-lamreza Anbarjafari. \u201cReal-time ensemble based facerecognition system for NAO humanoids using localbinary pattern\u201d. In: Analog Integrated Circuits andSignal Processing 92.3 (2017), pp. 467\u2013475.[26] Anastasia Bolotnikova et al. \u201cA circuit-breaker use-case operated by a humanoid in aircraft manufac-turing\u201d. In: 2017 13th IEEE Conference on AutomationScience and Engineering (CASE). IEEE. 2017, pp. 15\u201322.[27] Baris Kayalibay, Grady Jensen, and Patrick van derSmagt. \u201cCNN-based segmentation of medical imag-ing data\u201d. In: arXiv preprint arXiv:1701.03056 (2017).[28] Luis G \u00b4omez-Chova et al. \u201cMultimodal classi\ufb01cationof remote sensing images: A review and future di-rections\u201d. In: Proceedings of the IEEE 103.9 (2015),pp. 1560\u20131584.[29] Andreas Kamilaris and Francesc X Prenafeta-Bold \u00b4u.\u201cDeep learning in agriculture: A survey\u201d. In: Comput-ers and electronics in agriculture 147 (2018), pp. 70\u201390.[30] Jianzhu Guo et al. \u201cDominant and complementaryemotion recognition from still images of faces\u201d. In:IEEE Access 6 (2018), pp. 26391\u201326403.[31] Egils Avots et al. \u201cEnsemble approach for detectionof depression using EEG features\u201d. In: arXiv preprintarXiv:2103.08467 (2021).[32] Kadir Aktas et al. \u201cSpatio-Temporal Based TableTennis Stroke Type Assessment\u201d. In: Signal, Image andVideo Processing (2021), pp. 1\u20138.[33] Paula M Niedenthal and Markus Brauer. \u201cSocialfunctionality of human emotion\u201d. In: Annual reviewof psychology 63 (2012), pp. 259\u2013285.[34] Eric M Reiman et al. \u201cNeuroanatomical correlatesof externally and internally generated human emo-tion\u201d. In: American Journal of Psychiatry 154.7 (1997),pp. 918\u2013925.[35] Fatemeh Noroozi et al. \u201cSurvey on emotional bodygesture recognition\u201d. In: IEEE transactions on affectivecomputing (2018).[36] Gholamreza Anbarjafari et al. Machine Learning forFace, Emotion, and Pain Recognition[37] Raymond J Dolan. \u201cEmotion, cognition, and behav-ior\u201d. In: science 298.5596 (2002), pp. 1191\u20131194.John G Carlson and Elaine Hat\ufb01eld. Psychology ofemotion. Harcourt Brace Jovanovich, 1992.[38][39] Mengyi Liu et al. \u201cDeeply learning deformable facialaction parts model for dynamic expression analysis\u201d.In: Asian conference on computer vision. Springer. 2014,pp. 143\u2013157.[40] Chien-Hsu Chen, I-Jui Lee, and Ling-Yi Lin. \u201cAug-mented reality-based self-facial modeling to promotethe emotional expression and social skills of adoles-cents with autism spectrum disorders\u201d. In: Researchin developmental disabilities 36 (2015), pp. 396\u2013403.[41] Agata Ko\u0142akowska et al. \u201cEmotion recognition andits applications\u201d. In: Human-Computer Systems Inter-action: Backgrounds and Applications 3. Springer, 2014,pp. 51\u201362.[42] Y-I Tian, Takeo Kanade, and Jeffrey F Cohn. \u201cRec-ognizing action units for facial expression analysis\u201d.In: IEEE Transactions on pattern analysis and machineintelligence 23.2 (2001), pp. 97\u2013115.James A Russell. \u201cA circumplex model of affect.\u201d In:Journal of personality and social psychology 39.6 (1980),p. 1161.[43][44] Paul Ekman. \u201cBasic emotions\u201d. In: Handbook of cogni-tion and emotion 98.45-60 (1999), p. 16.[45] Christer Loob et al. \u201cDominant and complementarymulti-emotional facial expression recognition usingc-support vector classi\ufb01cation\u201d. In: 2017 12th IEEEInternational Conference on Automatic Face & GestureRecognition (FG 2017). IEEE. 2017, pp. 833\u2013838.[46] Tomasz Sapi \u00b4nski et al. \u201cMultimodal database ofemotional speech, video and gestures\u201d. In: Inter-national Conference on Pattern Recognition. Springer.2018, pp. 153\u2013163.[47] Gary B Huang et al. \u201cLabeled faces in the wild:A database forstudying face recognition in uncon-strained environments\u201d. In: Workshop on faces in\u2019Real-Life\u2019Images: detection, alignment, and recognition. 2008. [48] Abhinav Dhall et al. \u201cVideo and image based emo-tion recognition challenges in the wild: Emotiw2015\u201d. In: Proceedings of the 2015 ACM on internationalconference on multimodal interaction. 2015, pp. 423\u2013426.and MattiPietik\u00a8ainen. \u201cFace recognition with local binarypatterns\u201d. In: European conference on computer vision.Springer. 2004, pp. 469\u2013481.[49] Timo Ahonen, Abdenour Hadid,[50] Xiaoyi Feng, Matti Pietik\u00a8ainen, and Abdenour Ha-did. \u201cFacial expression recognition based on localbinary patterns\u201d. In: Pattern Recognition and ImageAnalysis 17.4 (2007), pp. 592\u2013598.[51] Rajiv Mehrotra, Kameswara Rao Namuduri, and Na-garajan Ranganathan. \u201cGabor \ufb01lter-based edge de-tection\u201d. In: Pattern recognition 25.12 (1992), pp. 1479\u20131494.[52] Michael Lyons et al. \u201cCoding facial expressions withgabor wavelets\u201d. In: Proceedings Third IEEE interna-tional conference on automatic face and gesture recogni-tion. IEEE. 1998, pp. 200\u2013205.expression recognition based on Gabor wavelets andsparse representation\u201d. In: 2012 IEEE 11th Interna-tional Conference on Signal Processing. Vol. 2. IEEE.2012, pp. 816\u2013819.[54] Govardhan Mattela and Sandeep K Gupta. \u201cFacialexpression recognition using Gabor-mean-DWT fea-ture extraction technique\u201d. In: 2018 5th InternationalConference on Signal Processing and Integrated Networks(SPIN). IEEE. 2018, pp. 575\u2013580.[55] Taskeed Jabid, Md Hasanul Kabir, and Oksam Chae.\u201cRobust facial expression recognition based on lo-cal directional pattern\u201d. In: ETRI journal 32.5 (2010),pp. 784\u2013794.[56] Zhen Wang and Zilu Ying. \u201cFacial expression recog-nition based on local phase quantization and sparserepresentation\u201d. In: 2012 8th International Conferenceon Natural Computation. IEEE. 2012, pp. 222\u2013225.[57] Wei-Lun Chao, Jian-Jiun Ding, and Jun-Zuo Liu.\u201cFacial expression recognition based on improved lo-cal binary pattern and class-regularized locality pre-serving projection\u201d. In: Signal Processing 117 (2015),pp. 1\u201310.[58] Abu Sayeed Md Sohail and Prabir Bhattacharya.\u201cClassi\ufb01cation of facial expressions using k-nearestneighbor classi\ufb01er\u201d. In: International Conference onComputer Vision/Computer Graphics Collaboration Tech-niques and Applications. Springer. 2007, pp. 555\u2013566.[59] Qirong Mao et al. \u201cHierarchical Bayesian thememodels for multipose facial expression recognition\u201d.IEEE Transactions on Multimedia 19.4 (2016),In:pp. 861\u2013873.[60] Ming-Wei Huang, Zhe-wei Wang, and Zi-Lu Ying.\u201cA new method for facial expression recognitionbased on sparse representation plus LBP\u201d. In: 20103rd International Congress on Image and Signal Process-ing. Vol. 4. IEEE. 2010, pp. 1750\u20131754.[61] Hung-Hsu Tsai and Yi-Cheng Chang. \u201cFacial expres-sion recognition using a combination of multiplefacial features and support vector machine\u201d. In: SoftComputing 22.13 (2018), pp. 4389\u20134405.[62] Alex Krizhevsky, Ilya Sutskever, and Geoffrey EHinton. \u201cImagenet classi\ufb01cation with deep convolu-tional neural networks\u201d. In: Advances in neural infor-mation processing systems 25 (2012), pp. 1097\u20131105.[63] Peter Burkert et al. \u201cDexpression: Deep convolu-tional neural network for expression recognition\u201d. In:arXiv preprint arXiv:1509.05371 (2015).[64] Xiangyun Zhao et al. \u201cPeak-piloted deep network forfacial expression recognition\u201d. In: European conferenceon computer vision. Springer. 2016, pp. 425\u2013442.[65] Xiaoguang Chen et al. \u201cConvolution neural networkfor automatic facial expression recognition\u201d. In: 2017International conference on applied system innovation(ICASI). IEEE. 2017, pp. 814\u2013817.[66] Biao Yang et al. \u201cFacial expression recognition us-ing weighted mixture deep neural network basedon double-channel facial images\u201d. In: IEEE Access 6(2017), pp. 4630\u20134640.[67] Abir Fathallah, Lot\ufb01 Abdi, and Ali Douik. \u201cFa-cial expression recognition via deep learning\u201d. In:2017 IEEE/ACS 14th International Conference on Com-puter Systems and Applications (AICCSA). IEEE. 2017,pp. 745\u2013750.[68] Thanh-Hung Vo et al. \u201cPyramid with Super Resolu-tion for In-the-Wild Facial Expression Recognition\u201d.In: IEEE Access 8 (2020), pp. 131988\u2013132001.[69] Emad Barsoum et al. \u201cTraining deep networks forfacial expression recognition with crowd-sourced la-bel distribution\u201d. In: Proceedings of the 18th ACM In-ternational Conference on Multimodal Interaction. 2016,pp. 279\u2013283.[70] Yuedong Chen et al. \u201cFacial motion prior networksfor facial expression recognition\u201d. In: 2019 IEEEVisual Communications and Image Processing (VCIP).IEEE. 2019, pp. 1\u20134.[72][71] Ali Mollahosseini, Behzad Hasani, and MohammadH Mahoor. \u201cAffectnet: A database for facial expres-sion, valence, and arousal computing in the wild\u201d. In:IEEE Transactions on Affective Computing 10.1 (2017),pp. 18\u201331.Ilke Cugu, Eren Sener, and Emre Akbas. \u201cMicroExp-Net: An Extremely Small and Fast Model For Expres-sion Recognition From Face Images\u201d. In: 2019 NinthInternational Conference on Image Processing Theory,Tools and Applications (IPTA). IEEE. 2019, pp. 1\u20136.[73] Guoying Zhao et al. \u201cFacial expression recognitionfrom near-infrared videos\u201d. In: Image and Vision Com-puting 29.9 (2011), pp. 607\u2013619.[74] Yunxin Huang et al. \u201cFacial expression recognition:A survey\u201d. In: Symmetry 11.10 (2019), p. 1189.[75] Shashank Jaiswal and Michel Valstar. \u201cDeep learningthe dynamic appearance and shape of facial actionunits\u201d. In: 2016 IEEE winter conference on applicationsof computer vision (WACV). IEEE. 2016, pp. 1\u20138.[76] Abhinav Dhall et al. \u201cEmotiw 2016: Video and group-level emotion recognition challenges\u201d. In: Proceedingsof the 18th ACM international conference on multimodalinteraction. 2016, pp. 427\u2013432.[77] Yin Fan et al. \u201cVideo-based emotion recognitionusing CNN-RNN and C3D hybrid networks\u201d. In: Proceedings of the 18th ACM International Conferenceon Multimodal Interaction. 2016, pp. 445\u2013450.[78] Dae Hoe Kim et al. \u201cMulti-objective based spatio-temporal feature representation learning robust toexpression intensity variations for facial expressionrecognition\u201d. In: IEEE Transactions on Affective Com-puting 10.2 (2017), pp. 223\u2013236.[79] Zhenbo Yu et al. \u201cSpatio-temporal convolutional fea-tures with nested LSTM for facial expression recog-nition\u201d. In: Neurocomputing 317 (2018), pp. 50\u201357.[80] Kaiming He et al. \u201cSpatial pyramid pooling in deepconvolutional networks for visual recognition\u201d. In:IEEE transactions on pattern analysis and machine intel-ligence 37.9 (2015), pp. 1904\u20131916.[81] Young-Hyen Byeon and Keun-Chang Kwak. \u201cFacialexpression recognition using 3d convolutional neuralnetwork\u201d. In: International journal of advanced com-puter science and applications 5.12 (2014).[82] Behzad Hasani and Mohammad H Mahoor. \u201cFacialexpression recognition using enhanced deep 3D con-Proceedings of theIEEE conference on computer vision and pattern recog-nition workshops. 2017, pp. 30\u201340.[83] Christian Szegedy et al. \u201cInception-v4, inception-resnet and the impact of residual connections onlearning\u201d. In: Proceedings of the AAAI Conference onArti\ufb01cial Intelligence. Vol. 31. 1. 2017.[84] Ruicong Zhi et al. \u201cCombining 3D convolutionalneural networks with transfer learning by super-vised pre-training for facial micro-expression recog-nition\u201d. In: IEICE Transactions on Information and Sys-tems 102.5 (2019), pp. 1054\u20131064.Jing Li et al. \u201cMicro-expression recognition based on3D \ufb02ow convolutional neural network\u201d. In: PatternAnalysis and Applications 22.4 (2019), pp. 1331\u20131339.[85][86] Chao Wu and Fan Guo. \u201cTSNN: Three-Stream Com-bining 2D and 3D Convolutional Neural Network forMicro-Expression Recognition\u201d. In: IEEJ Transactionson Electrical and Electronic Engineering 16.1 (2021),pp. 98\u2013107.[87] High-Level Expert Group on AI. Ethics guidelines fortrustworthy AI. https: // ec .europa . eu/ newsroom /dae/document.cfm?doc id=60419.[88] Microsoft. Microsoft AI principles. https : / / www .microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6.[89] PwC. 2019 AI Predictions. https://www.pwc.com/us / en / services / consulting / library / arti\ufb01cial -intelligence-predictions-2019.html.[90] PwC. 2020 AI Predictions. https://www.pwc.com/us / en / services / consulting / library / arti\ufb01cial -intelligence-predictions-2020.html.[91] Nripsuta Ani Saxena et al. \u201cHow do fairness def-initions fare? Examining public attitudes towardsalgorithmic de\ufb01nitions of fairness\u201d. In: Proceedingsof the 2019 AAAI/ACM Conference on AI, Ethics, andSociety. 2019, pp. 99\u2013106.[92] Moritz Hardt, Eric Price, and Nathan Srebro. \u201cEqual-ity of opportunity in supervised learning\u201d. In: arXivpreprint arXiv:1610.02413 (2016).[93] Matt J Kusner et al. \u201cCounterfactual fairness\u201d. In:arXiv preprint arXiv:1703.06856 (2017).[94] Ninareh Mehrabi et al. \u201cA survey on bias andIn: arXiv preprintfairness in machine learning\u201d.arXiv:1908.09635 (2019).Jiahao Chen et al. \u201cFairness under unawareness:Assessing disparity when protected class is unob-served\u201d. In: Proceedings of the conference on fairness,accountability, and transparency. 2019, pp. 339\u2013348.[95][96] Dorian Peters et al. \u201cResponsible AI\u2014two frame-works for ethical design practice\u201d. In: IEEE Transac-tions on Technology and Society 1.1 (2020), pp. 34\u201347.[97] Lu Cheng, Kush R Varshney, and Huan Liu. \u201cSo-cially Responsible AI Algorithms: Issues, Purposes,and Challenges\u201d. In: arXiv preprint arXiv:2101.02032(2021).[98] Faisal Kamiran and Indr \u02d9e \u02c7Zliobait \u02d9e. \u201cExplainableand non-explainable discrimination in classi\ufb01ca-tion\u201d. In: Discrimination and Privacy in the InformationSociety. Springer, 2013, pp. 155\u2013170.[100] understanding unintended consequences of machinelearning\u201d. In: arXiv preprint arXiv:1901.10002 (2019).Jia Deng et al. \u201cImagenet: A large-scale hierarchicalimage database\u201d. In: 2009 IEEE conference on computervision and pattern recognition. Ieee. 2009, pp. 248\u2013255.Ivan Krasin et al. \u201cOpenimages: A public dataset forlarge-scale multi-label and multi-class image clas-si\ufb01cation\u201d. In: Dataset available from https://github.com/openimages 2.3 (2017), p. 18.[101][102] Shreya Shankar et al. \u201cNo classi\ufb01cation without rep-resentation: Assessing geodiversity issues in opendata sets for the developing world\u201d. In: arXiv preprintarXiv:1711.08536 (2017).[103] Alexandra Olteanu et al. \u201cSocial data: Biases,methodological pitfalls, and ethical boundaries\u201d. In:Frontiers in Big Data 2 (2019), p. 13.[104] Brendan F Klare et al. \u201cFace recognition perfor-mance: Role of demographic information\u201d. In: IEEETransactions on Information Forensics and Security 7.6(2012), pp. 1789\u20131801.[105] Hachim El Khiyari and Harry Wechsler. \u201cFace veri\ufb01-cation subject to varying (age, ethnicity, and gender)demographics using deep learning\u201d. In: Journal ofBiometrics and Biostatistics 7.323 (2016), p. 11.[106] Cynthia M Cook et al. \u201cDemographic effects in facialrecognition and their dependence on image acquisi-tion: An evaluation of eleven commercial systems\u201d.In: IEEE Transactions on Biometrics, Behavior, and Iden-tity Science 1.1 (2019), pp. 32\u201341.[107] Ayanna Howard, Cha Zhang, and Eric Horvitz. \u201cAd-dressing bias in machine learning algorithms: A pilotstudy on emotion recognition for intelligent sys-tems\u201d. In: 2017 IEEE Workshop on Advanced Roboticsand its Social Impacts (ARSO). IEEE. 2017, pp. 1\u20137.[108] Lauren Rhue. \u201cRacial in\ufb02uence on automated per-ceptions of emotions\u201d. In: Available at SSRN 3281765(2018).[109] Emily Denton et al. \u201cDetecting bias with genera-tive counterfactual face attribute augmentation\u201d. In:arXiv preprint arXiv:1906.06439 (2019). [110] Zeyu Wang et al. \u201cTowards fairness in visual recog-nition: Effective strategies for bias mitigation\u201d. In:Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. 2020, pp. 8919\u20138928.[111] Ziwei Liu et al. \u201cDeep learning face attributes inthe wild\u201d. In: Proceedings of the IEEE internationalconference on computer vision. 2015, pp. 3730\u20133738.[112] Tian Xu et al. \u201cInvestigating bias and fairness in fa-cial expression recognition\u201d. In: European Conferenceon Computer Vision. Springer. 2020, pp. 506\u2013523.[113] Shan Li, Weihong Deng, and JunPing Du. \u201cReli-able Crowdsourcing and Deep Locality-PreservingLearning for Expression Recognition in the Wild\u201d. In:2017 IEEE Conference on Computer Vision and PatternRecognition (CVPR). IEEE. 2017, pp. 2584\u20132593.[114] Navneet Dalal and Bill Triggs. \u201cHistograms of ori-ented gradients for human detection\u201d. In: 2005 IEEEcomputer society conference on computer vision and pat-tern recognition (CVPR\u201905). Vol. 1. Ieee. 2005, pp. 886\u2013893.dlib C++ libraryJia Xiang and Gengming Zhu. \u201cJoint face detectionand facial expression recognition with MTCNN\u201d. In:2017 4th International Conference on Information Scienceand Control Engineering (ICISCE). IEEE. 2017, pp. 424\u2013427.[116][118][117] Stuart Lloyd. \u201cLeast squares quantization in PCM\u201d.In: IEEE transactions on information theory 28.2 (1982),pp. 129\u2013137.James MacQueen et al. \u201cSome methods for classi\ufb01-cation and analysis of multivariate observations\u201d. In:Proceedings of the \ufb01fth Berkeley symposium on mathemat-ical statistics and probability. Vol. 1. 14. Oakland, CA,USA. 1967, pp. 281\u2013297.[119] PyTorch Documentation. : https : / / pytorch . org /docs/stable/nn.html.[120] Sergey Ioffe and Christian Szegedy. \u201cBatch normal-ization: Accelerating deep network training by re-ducing internal covariate shift\u201d. In: International con-ference on machine learning. PMLR. 2015, pp. 448\u2013456.[121] Sepp Hochreiter and J \u00a8urgen Schmidhuber. \u201cLongIn: Neural computation 9.8short-term memory\u201d.(1997), pp. 1735\u20131780.[122] Alex Graves, Abdel-rahman Mohamed, and Geof-frey Hinton. \u201cSpeech recognition with deep recurrentneural networks\u201d. In: 2013 IEEE international confer-ence on acoustics, speech and signal processing. Ieee.2013, pp. 6645\u20136649.[123] Savvas Varsamopoulos, Koen Bertels, Carmen G Al-mudever, et al. \u201cDesigning neural network basedIn: arXiv preprintdecoders for surface codes\u201d.arXiv:1811.12456 (2018).[124] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zis-serman. \u201cDeep face recognition\u201d. In: (2015).[125] Kaiming He et al. \u201cDeep residual learning for imagerecognition\u201d. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition. 2016, pp. 770\u2013778.Jie Hu, Li Shen, and Gang Sun. \u201cSqueeze-and-excitation networks\u201d. In: Proceedings of the IEEE con-[126]ference on computer vision and pattern recognition. 2018,pp. 7132\u20137141.[127] Qiong Cao et al. \u201cVggface2: A dataset for recognisingfaces across pose and age\u201d. In: 2018 13th IEEE interna-tional conference on automatic face & gesture recognition(FG 2018). IEEE. 2018, pp. 67\u201374.[128] VGGFace2-pytorch. : https : / / github . com /[129] cydonia999/VGGFace2-pytorch.Jianfeng Zhao, Xia Mao, and Jian Zhang. \u201cLearningdeep facial expression features from image and op-tical \ufb02ow sequences using 3D CNN\u201d. In: The VisualComputer 34.10 (2018), pp. 1461\u20131475.[130] Sai Prasanna Teja Reddy et al. \u201cSpontaneous facialmicro-expression recognition using 3D spatiotempo-ral convolutional neural networks\u201d. In: 2019 Inter-national Joint Conference on Neural Networks (IJCNN).IEEE. 2019, pp. 1\u20138.Jad Haddad, Olivier L\u00b4ezoray, and Philippe Hamel.\u201c3D-CNN for Facial Emotion Recognition in Videos\u201d.International Symposium on Visual Computing.In:[131][132] Du Tran et al. \u201cA closer look at spatiotemporalconvolutions for action recognition\u201d. In: Proceedingsof the IEEE conference on Computer Vision and PatternRecognition. 2018, pp. 6450\u20136459. 7 A7.1 Test setTABLE 2: Regular models, Test set, accuracyTABLE 3: Regular models, Test set, true positive rateTABLE 4: Regular models, Test set, false positive rateTABLE 5: Female models, Test set, accuracyTABLE 6: Female models, Test set, true positive rateTABLE 7: Female models, test set, false positive rateTABLE 8: Male models, Test set, accuracy TABLE 15: Regular models, Female set, true positive rateTABLE 9: Male models, Test set, true positive rate TABLE 16: Regular models, Female set, false positive rateTABLE 10: Male models, Test set, false positive rate TABLE 17: Regular models, Female set, accuracy7.2 Female set TABLE 18: Regular models, Female set, true positive rateTABLE 11: Regular models, Female set, accuracyTABLE 12: Regular models, Female set, true positive rate TABLE 19: Regular models, Female set, false positive rate7.3 Male setTABLE 13: Regular models, Female set, false positive rate TABLE 20: Regular models, Male set, accuracyTABLE 14: Regular models, Female set, accuracy TABLE 21: Regular models, Male set, true positive rateTABLE 22: Regular models, Male set, false positive rateTABLE 23: Regular models, Male set, accuracyTABLE 24: Regular models, Male set, true positive rateTABLE 25: Regular models, Male set, false positive rateTABLE 26: Regular models, Male set, accuracyTABLE 27: Regular models, Male set, true positive rateTABLE 28: Regular models, Male set, false positive rate7.4 Charts Fig. 26: Per model architecture, accuracy comparison be-tween all three test sets. These are Regular models, whichhave been trained on entire train data.Fig. 27: Per model architecture, accuracy comparison be-tween all three test sets. These are Female models, whichhave been trained on only female data.Fig. 28: Per model architecture, accuracy comparison be-tween all three test sets. These are Male models, which havebeen trained on only male data. Fig. 30: Per model architecture, Female set accuracy compar-ison between different model typesFig. 29: Per model architecture, Test set accuracy comparisonbetween different model types Fig. 31: Per model architecture, Male set accuracy compari-son between different model types",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                },
                {
                    "id": "87b50936-f8d6-46e7-bc29-9ca7d648d9a6",
                    "text": "",
                    "reference": "[1] Antonina Domnich and Gholamreza Anbarjafari. 2021. Responsible AI: Gender bias assessment in emotion recognition. arXiv:2103.11436. Retrieved from https://arxiv.org/pdf/2103.11436"
                }
            ]
        },
        {
            "paper_title": "Causal learning for socially responsible AI",
            "authors": "L Cheng, A Mosallanezhad, P Sheth, H Liu",
            "publication_info": "arXiv preprint arXiv \u2026 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2104.12278",
            "chunks": [
                {
                    "id": "ea2d3b32-b66c-464f-a5fe-4faa71bd96a4",
                    "text": "There have been increasing concerns about Ar-ti\ufb01cial Intelligence (AI) due to its unfathomableTo make AI address ethicalpotential power. re-challenges and shun undesirable outcomes,searchers proposed to develop socially responsibleAI (SRAI). One of these approaches is causal learn-ing (CL). We survey state-of-the-art methods of CLfor SRAI. We begin by examining the seven CLtools to enhance the social responsibility of AI, thenreview how existing works have succeeded usingthese tools to tackle issues in developing SRAI suchas fairness. The goal of this survey is to bring fore-front the potentials and promises of CL for SRAI.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "939e0acd-8561-4d42-b0db-63b6b4cd6d5b",
                    "text": "Arti\ufb01cial Intelligence (AI) comes with both promises and per-ils. AI signi\ufb01cantly improves countless aspects of day-to-daylife by performing human-like tasks with high ef\ufb01ciency andprecision. It also brings potential risks for oppression andcalamity because how AI works has not been fully under-stood and regulations surround its use are still lacking [Chenget al., 2021]. Many striking stories in media (e.g., Stanford\u2019sCOVID-19 vaccine distribution algorithm) have brought So-cially Responsible AI (SRAI) into the spotlight.Substantial risks can arise when AI systems are trained toimprove accuracy without knowing the underlying data gen-erating process (DGP). First, the societal patterns hidden inthe data are inevitably injected into AI algorithms. The re-sulting socially indifferent behaviors of AI can be further ex-acerbated by data heterogeneity and sparsity. Second, lack-ing knowledge of the DGP can cause researchers and practi-tioners to unconsciously make some frame of reference com-mitment to the formalization of AI algorithms [Getoor, 2019;Cheng et al., 2021], spanning from data and label formaliza-tion to the formalization of evaluation metrics. Third, DGPis also central to identifying the cause-effect connections andthe causal relations between variables, which are two indis-pensable ingredients to achieve SRAI. We gain in-depth un- derstanding of AI by intervening, interrogating, altering itsenvironment, and \ufb01nally answering \u201cwhat-if\u201d questions.Causal inference is the key to uncovering the real-worldDGPs [Pearl, 2009]. In the era of big data, especially, it ispossible to learn causality by leveraging both causal knowl-edge and the copious real-world data, i.e., causal learning(CL) [Guo et al., 2020a]. There have been growing interestsseeking to improve AI\u2019s social responsibility from a CL per-spective, e.g., causal interpretability [Moraffah et al., 2020]and causal-based machine learning fairness [Makhlouf et al.,2020]. In this survey, therefore, we \ufb01rst examine the seventools in CL that are inherently related to SRAI. We thenreview existing efforts of connecting four of these tools toemerging tasks in SRAI, including bias mitigation, fairness,transparency, and generalizability/invariance. We concludewith open problems and challenges of leveraging CL to en-hance both the functionality and social responsibility of AI.association,",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "258b3640-9198-4fb0-bbb2-7f60ea1e1a96",
                    "text": "Based on the available causal information, we can describeCL in a three-layer hierarchy: intervention,and counterfactual [Pearl, 2019]. At the \ufb01rst layer, as-sociation seeks statistical relations between variables, i.e.,p(y|x). By contrast, intervention and counterfactual demandcausal information. An intervention is a change to the DGP.With do-calculus [Pearl, 2009], the interventional distributionp(y|do(x)) describes the distribution of Y if we force X totake the value x while keeping the rest in the process same.This corresponds to removing all the inbound arrows to X inthe causal graph. At the top layer is counterfactual, denotedas p(y |x , y ). It stands for the probability of Y = y had Xbeen x given what we observed were X = x and Y = y .In the rest of this section, we review the seven tools of CLintroduced in [Pearl, 2019] and brie\ufb02y discuss how it natu-rally steers a course towards a SRAI-future.\u2022 Causal Assumptions make an AI system more transparentand testable. Encoding causal assumptions explicitly al-lows us to discern whether these assumptions are plausibleand compatible with available data. It also improves ourunderstandings of how the system works and gives a pre-cise framework to debate [Cloudera, 2020].\u2022 Do-calculus enables the system to exclude spurious corre-lations by eliminating confounding through back-door cri-terion [Pearl, 2009]. Confounding is a major cause of manysocially indifferent behaviors of an AI system.\u2022 Counterfactual analysis involves a \u201cwhat would have hap-It is the building block of scienti\ufb01cpened if\u201d question.thinking, thus, a key ingredient to warrant SRAI.\u2022 Mediation Analysis decomposes the effect of an interven-tion into direct and indirect effects. Its identi\ufb01cation helpsunderstand how and why a cause-effect arises. Therefore,mediation analysis is essential for generating explanations.\u2022 Adaptability is a model\u2019s capability to generalize to differ-ent environments. AI algorithms typically cannot performwell when the environment changes. CL can uniquely iden-tify the underlying mechanism responsible for the changes.\u2022 Missing data is a common problem in AI tasks. It reducesstatistical power, data representativeness, and can cause bi-ased estimations. CL can help recover causal and statisticalrelationships from incomplete data via causal graphs.\u2022 Causal discovery learns causal directionality between vari-ables from observational data. As causal graphs are mostlyunknown and arbitrarily complex in real-world applica-tions, causal discovery offers a ladder to true causal graphs.We propose the taxonomy of CL for SRAI in Fig. 1 and re-view how these CL tools \u2013 particularly, the Causal Assump-tions, Do-calculus, Counterfactual Analysis, and Adaptabil-ity \u2013 help SRAI in terms of bias mitigation, fairness, trans-parency, and generalizability/invariance below.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "a062cd9c-a827-4edb-aea0-e4946c9ac439",
                    "text": "AI systems can be biased due to hidden or neglected biasesin the data, algorithms, and user interaction. [Olteanu et al.,2019] introduced 23 different types of bias including selec-tion, measurement biases, and so on. There are various waysto de-bias AI systems such as adversarial training [Zhang etal., 2018], reinforcement learning [Wang and Deng, 2020],and causal inference [Zhao et al., 2019]. Due to its inter-pretable nature, causal inference offers high con\ufb01dence inmaking decisions and can show the relation between data at-tributes and AI system\u2019s outcomes. Here, we review two pop-ular causality-based methods for bias mitigation \u2013 propensityscore and counterfactual data augmentation (CDA).",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "d274a9ba-019e-4a73-b87e-08733fbb2ae9",
                    "text": "Propensity score is used to eliminate treatment selection biasand ensure the treatment and control groups are comparable.It is the \u201cconditional probability of assignment to a particulartreatment given a vector of observed covariates\u201d [Rosenbaumand Rubin, 1983]. Due to its effectiveness and simplicity, propensity score has been used to reduce unobserved biases invarious domains, e.g., NLP and recommender systems. Here,we focus our discussions on recommender systems.Inverse Propensity Scoring (IPS) is used to alleviate theselection and position biases commonly present in recom-mender systems. Selection bias appears when users selec-tively rate or click items, rendering observed ratings not rep-resentative of the true ratings, i.e., ratings obtained whenusers randomly rate items. Given a user-item pair (u, i) andO {0, 1} denoting whether u observed i, we de\ufb01ne propen-sity score as P = P (O = 1), i.e., the marginal proba-bility of observing a rating. During the model training phase,IPS-based unbiased estimator is de\ufb01ned using following em-pirical risk function [Schnabel et al., 2016]:where \u02c6\u03c3 (r, \u02c6r(\u03b8)) denotes an evaluation function andReg(\u03b8) the regularization for model complexity. IPS is alsoused to mitigate selection bias during evaluation, see, e.g.,[Schnabel et al., 2016; Yang et al., 2018].Position bias occurs in a ranking system as users tend tointeract with items with higher ranking positions. To rem-edy position bias, previous methods used IPS to weigh eachdata instance with a position-aware value. The loss functionof such models is de\ufb01ned as follows [Agarwal et al., 2019]:L(M, q) = (cid:80) \u2206(x, y|\u03c0 ), where M indicates the rank-ing model, q \u2208 Q denotes a query from a set of all queriesto the model, \u03c0 is the ranked list by M , and \u2206(x, y|\u03c0 ) de-notes the individual loss on each item x with relevant labely. In another method, [Hofmann et al., 2013] proposed to es-timate propensity scores using ranking randomization. First,the ranking results of the system are randomized. Then thepropensity score is calculated based on user clicks on differ-ent positions. Although this method is shown to be effec-tive, it can signi\ufb01cantly degrade user experience as the highlyranked items may not be users\u2019 favorites [Joachims et al.,2017]. Therefore, [Guo et al., 2020b] proposed a methodin an of\ufb02ine setting where randomized experiments are notavailable. They speci\ufb01cally considered multiple types of userfeedback and applied IPS to learn an unbiased ranking model.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "595be8c3-9dcd-4ed3-9502-ddb98398d5d0",
                    "text": "CDA is a technique to augment training data with theircounterfactually-revised counterparts via causal interventionsthat seek to eliminate spurious correlations [Kaushik et al.,2020]. It enables AI algorithms to be trained on unseen data,therefore, reducing undesired biases and improving modelgeneralizability. Here, we focus on CDA for bias mitigationand will discuss model generalizability in Sec. 6.3.One of the domains using CDA to reduce biases hidden inthe data is NLP. Particularly, one begins by sampling a subsetof original data that contains attributes of interest, e.g., genderor sentiment. Then expert annotators or inference models areasked to generate the counterfactual counterparts. The aug-mented datasets are later fed into the downstream NLP tasks,e.g., sentiment analysis. For example, CDA can be used toreduce gender bias by generating a dataset that encouragestraining algorithms not to capture gender-related information.One such method generates a gender-free list of sentences us-ing a series of sentence templates to replace every occurrenceof gendered word pairs (e.g., he:she, her:him/his) [Lu et al.,2020b]. It formally de\ufb01ned CDA as:De\ufb01nition 1 Given ={(x , y ), (x , y ), ..., (x , y )} and intervention c, ac-augmented dataset S is S \u222a {(c(x), y)} .The underlying assumption is that an unbiased model shouldnot distinguish between matched pairs and should producethe same outcome for different genders [Lu et al., 2020b]. Forsentiment analysis, [Kaushik et al., 2020] generated counter-factual scenarios with help of human annotators who wereprovided with positive movie reviews and were asked to per-form minimal changes to make them negative.instancesinput S",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "a9cadf26-74f1-4207-923d-2f5535925524",
                    "text": "Causal graph is a powerful approach for counterfactual rea-soning as it allows us to study the effects of various sensi-tive attributes on the outcome. It is a probabilistic graphi-cal model to encode assumptions and widely used to detectand mitigate bias in AI systems. In the NLP domain, forexample, to mitigate gender bias in word embeddings, a re-cent work by [Yang and Feng, 2020] proposed a causal graphcharacterizing relation between gender-de\ufb01nition and gender-biased non-gender-de\ufb01nition word vectors. In the domain ofrecommender systems, causal graph can mitigate bias that af-fects decisions toward popular items (i.e., popularity bias) orgender-speci\ufb01c recommendations. The underlying assump-tion is that, a user click on biased recommendation resultsfrom two independent causes: user interest and item popular-ity. Or formally, P = P + P , where Pindicates matching probability for a user and an item. A cor-responding causal graph that disentangles user interest anditem popularity is shown in Fig. 2. They further created twodifferent embeddings to capture users\u2019 real-interest in itemsand pseudo-interest caused by popularity. Finally, the user in-terest embedding is used to create a de-biased recommenderwhere the popularity bias has been disentangled.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "007e3e26-818c-4eb0-b74c-e6ed3177732d",
                    "text": "Bias is a primary reason that AI systems fail to make fairdecisions. Using propensity-score-based approaches needsto specify the correct forms of propensity scores in real-world applications. The alternate randomization experiments \u2013 which may be inapplicable due to ethical and \ufb01nancial con-siderations \u2013 might decrease the utility performance of theAI systems [Joachims et al., 2017]. One challenge of usingCDA is to design a process to generate a modi\ufb01ed datasetusing the considered interventions. While causal graphs ap-pear promising [Yang and Feng, 2020], one has to make as-sumptions that may be impractical to re\ufb02ect real-world bi-ases. Beyond bias mitigation, measuring bias via experimen-tation can help understand causal connections between at-tributes of interest and algorithmic performance [Balakrish-nan et al., 2020], therefore, mitigating biases.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "424bcca5-5e0b-4fe6-b035-c7f26a2ac4e7",
                    "text": "Biases in AI systems can lead to many undesired conse-quences such as model over\ufb01tting and other societal issues.One of the most frequently discussed issues in AI is fairness,the property of a model that produces results independent ofgiven variables, especially those considered sensitive, e.g.,gender. Here, we brie\ufb02y review another line of research thataims to train a fair AI system apart from the de-biasing per-spective discussed above. A comprehensive survey on causalfairness can be referred to [Makhlouf et al., 2020].",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "a2752a04-eb52-45fe-8b4f-147b285a5e70",
                    "text": "From the causal perspective, fairness can be formulated asestimating causal effects of sensitive attributes such as gen-der on the outcome of an AI system. Such causal effectsare evaluated using counterfactual interventions over a causalgraph with features, sensitive attributes, and other variables.Underpinning this approach is the concept of counterfactualfairness (CF) [Kusner et al., 2017]. CF implies that a de-cision is considered fair if it is the same in both \u201cthe actualworld\u201d and \u201ca counterfactual world\u201d where, e.g., for an indi-vidual belongs to a different demographic group. Formally,considering Y , A, and X as the observed outcome, sensitiveattributes, and features, respectively, CF is de\ufb01ned as follows:De\ufb01nition 2 Given x \u2208 X and a \u2208 A, predictor \u02c6Y is coun-terfactually fair ifholds for all y and any a \u2208 A.U refers to a set of latent background variables in a causalgraph. This de\ufb01nition states that if the two outcome probabil-ities P (y |x, a) and P (y |x, a) are equal for an individual,then s/he is treated fairly as if s/he had been from anothersensitive group. Or, A should not be the cause of \u02c6Y . CFassumed that fairness can be uniquely quanti\ufb01ed from obser-vational data, which is not valid in certain situations due to[Wu etthe unidenti\ufb01ability of the counterfactual quantity.al., 2019] then introduced a graphical criterion determiningthe identi\ufb01ability of counterfactual quantities. With the de-rived bounds, CF is achieved by constraining the classi\ufb01er\u2019straining process on the amount of unfairness in the predictor.Another stepping stone toward creating fair AI systems[Hajian andusing CL is the intervention-based fairness.Domingo-Ferrer, 2012] \ufb01rst proposed to improve fairness byexpressing direct and indirect discrimination through the dif-ferent paths connecting the sensitive attributes (i.e., the treat-ment) and the outcome of an AI system. Similarly, [Zhangand Bareinboim, 2018] modeled discrimination based on theeffect of a sensitive attribute on an outcome along certain dis-allowed causal paths. These methods generally formulate andquantify fairness as the average causal effect of the sensitiveattribute on the decision attribute, which is then evaluated bythe intervention through the post-interventional distributions.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "8c094a44-15df-46ba-8c08-316377fc626e",
                    "text": "CF considers more general situations than intervention-basedfairness where the set of pro\ufb01le attributes is empty, therefore,it is more challenging. One limitation of CF is that thesesensitive attributes may not admit counterfactual manipula-tion [Kasirzadeh and Smart, 2021]. What does it mean tosuppose a different version of an individual with the coun-terfactually manipulated sensitive attributes such as race? Inorder to use a sensitive attribute appropriately, it is necessaryto specify what the categories are and what perception of anattribute to be used. The validity of counterfactuals is alsochallenging to assess. What is the metric used to measurethe similarity between the actual and imaginary worlds? CF-based models may fail when the identi\ufb01able assumption isviolated due to the unidenti\ufb01able counterfactual quantity [Wuet al., 2019]. Although this might be solved via a relaxed as-sumption, the idea of using social categories to achieve fair-ness can be problematic as they may not admit counterfactualmanipulation. Lastly, parallel to algorithmic fairness, algo-rithmic recourse offers explanations and recommendations toindividuals unfavourably treated. Future research can con-sider causal algorithmic recourse and its relation to CF andother fairness criteria [von K\u00a8ugelgen et al., 2020].concept of",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "0e4fbd73-26ba-48c0-a49f-4564fb8ddbd7",
                    "text": "algorithms often lack transparency,Conventional AI interpretabil-typically presented in theity/explanability. When AI algorithms do not provideexplanations for how and why they make a decision, users\u2019trust on these algorithms can be eroded. Hence, there is aneed for AI systems to produce interpretable results. CausalInterpretability helps generate human friendly explanationsby answering questions such as \u201cWhy does a model makessuch decisions?\u201d. In this section, we describe two approachesassociated with causal interpretability andcounterfactual interpretability, respectively. Please refer to[Moraffah et al., 2020] for more details.interventional",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "4cbbc0f8-0908-4bdb-b021-72fb81bbca8d",
                    "text": "CL for model-based interpretations seeks to estimate thecausal effect of a particular input neuron on a certain outputneuron in the network. Drawing from causal inference theo-ries, causally interpretable models \ufb01rst map a neural networkstructure into a Structural Causal Model (SCM) [Pearl, 2009]and then estimate the effect of each model component on theoutput based on the data and a learned function (i.e., a neu-ral network) using do-calculus [Chattopadhyay et al., 2019].Particularly, every l-layer neural network N (l , l , ..., l ) has a corresponding SCM M ([l , ..., l ], U, [f , ..., f ], P )where f refers to the set of causal functions for neuronsin layer l . U denotes a group of exogenous random vari-ables that act as causal factors for input layer l . P de-\ufb01nes the probability distribution of U . M can be further re-duced to a SCM with only input layer l and output layer l :M ([l , l ], U, f , P ), by marginalizing out the hidden neu-rons [Chattopadhyay et al., 2019]. Finally, we can estimatethe average causal effect (ACE) of a feature x \u2208 l withvalue \u03b1 on output y \u2208 l byACE = E [y | do (x = \u03b1)] \u2212 baseline , (3)where baseline = E [E [y | do (x = \u03b1)]].Similar method was applied to CNN architectures trainedon image data to reason over deep learning models [Narendraet al., 2018]. [Zhao and Hastie, 2021] leveraged partial de-pendence plot [Friedman, 2001] and Individual ConditionalExpectation [Goldstein et al., 2015] to extract causal informa-tion (e.g., relations between input and output variables) from[Mart\u00b4\u0131nez and Marca, 2019] proposedblack-box models.an approach for explaining the predictions of a visual modelwith the causal relations between the latent factors which theyleveraged to build a Counterfactual Image Generator.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "728d1918-def2-4df6-8620-032b07cb6cf2",
                    "text": "Different from model-based interpretation which deals withmodel parameters to determine the vital components of themodel, counterfactual explanations typically describe scenar-ios such as \u201cIf X had not occurred, Y would not have oc-curred\u201d. Speci\ufb01cally, the predicted outcome is considered asthe event Y and the features fed to the model are the causesX. A counterfactual explanation can be de\ufb01ned as a causalsituation of the form where an output Y , which occurs giventhe feature input X, can be changed to a prede\ufb01ned output Yby minimally changing the feature vector X to X .To generate counterfactual explanations, a common ap-proach is a generative counterfactual framework [Liu et al.,2019] that leverages generative models along with attributeediting mechanisms. The objective function is de\ufb01ned aswhere x/x denotes the observed/counterfactual features andy/y the observed/counterfactual outcome. The \ufb01rst term in-dicates the distance between the model\u2019s prediction for thecounterfactual input x and the desired counterfactual out-put. The second term describes the distance between the ob-served features x and counterfactual features x . \u03bb is the hy-perparameter balancing the importance of the two distances.Another category of approach relies on adversarial examplesto provide counterfactual explanations. Rather than explain-ing why a model predicts an output with a given input, it \ufb01ndsan alternative version of the input that receives different clas-si\ufb01cation results. One can also lay constraints on the featuresso that only the desired features are subject to changes. Thethird category of approach uses class prototypes for the coun-terfactual search process [Van Looveren and Klaise, 2019].These prototypes \u2013 refer to the mean encoding of the in-stances that belong to the class \u2013 are integrated into the ob-jective function so that the perturbations can produce inter-pretable counterfactuals. For a more detailed understandingreaders could refer to [Xu et al., 2020].",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "5227d3fc-62db-4e6a-8ad4-d03eec78fb0a",
                    "text": "There are still limitations in existing models for causal in-terepretability. For instance, evaluating such models is dif\ufb01-cult due to the lack of ground-truth causal relations betweenthe components of the model or causal effect of one com-ponent on another. While using counterfactual explanationsfor interpretability may seem feasible, it has been shown thatthere exist unspeci\ufb01ed contextual presumptions and choiceswhile generating counterfactual scenarios that may not standwell in the social world [Kasirzadeh and Smart, 2021]. Forinstance, social categories such as race and gender need tobe \ufb01rst de\ufb01ned in order to generate counterfactual scenariosaround these variables. Even with ground truth and well-de\ufb01ned social categories, existing works may still fail be-cause causal assumptions in these works are not explicitlyexplained. It is critical to clearly de\ufb01ne causal assumptions.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "ccd8c567-056b-4c54-823b-9752138104c2",
                    "text": "Due to societal biases hidden in data and the shortcut learn-ing [Geirhos et al., 2020], AI algorithms can easily over\ufb01tto training data, i.e., learning spurious correlations. Com-mon approaches for avoiding over\ufb01tting rely on the assump-tion that samples of the entire population are i.i.d., which israrely satis\ufb01ed in practice. Violating the i.i.d. assumptionleads to poor generalizability of an AI algorithm. Becausewhether a training and a testing DGP (or environment) dif-fer is unknown, we have recourse to data heterogeneity anda model that is robust to distributional shifts among hetero-geneous environments, or the invariance property [Arjovskyet al., 2019]. Causal relations are, by their nature, invari-ant [Pearl and Bareinboim, 2011]. Environment is de\ufb01ned byintervention, therefore, in a causal graph, only direct causalrelations remain invariant when an intervention changes theenvironment. We \ufb01rst examine two popular approaches thatincorporate the invariance property into predictions.Invariant Causal Prediction (ICP)6.1Built upon SCM, ICP [Peters et al., 2016] aims to discoverthe causal parents (direct causes) of a given variable directlypointing to the target variable without constructing the entirecausal graph. We consider the setting where multiple envi-ronments e \u2208 E exist and in each environment e, there is apredictor variable X \u2208 R and a target variable Y \u2208 R.Given a set S \u2286 {1, ..., p}, a vector X containing all vari-ables X , k \u2208 S, ICP assumes Invariant Prediction:Assumption 1 (Invariant Prediction.) There is a vector ofcoef\ufb01cients \u03b3 = (\u03b3 (cid:54)=0} \u2286 {1, ..., p} such that for all e \u2208 E and X with an arbi-trary distribution: ) with support S := {k : \u03b3, ..., \u03b3where \u00b5 is the intercept and (cid:15) denotes the random noise withthe same distribution F across all e \u2208 E. With multiple environments, ICP then \ufb01ts a linear (Gaussian)regression in each environment. The goal is to \ufb01nd a set offeatures that results in invariant predictions between environ-ments. In particular, ICP iterates over subsets of featurescombinatorially and looks for features in a model that areinvariant across environments, i.e., invariant coef\ufb01cients orresiduals. The intersection of these sets of features is then asubset of the true direct causes. ICP also relies on the uncon-foundedness assumption [Pearl, 2009]: no unobserved con-founders exist between input features and the target. In prac-tice, it is common to choose an observed variable to be theenvironment variable (e.g., background color of an image),when it could plausibly be so. Limited to using linear mod-els and discrete variable for environment separation, [Heinze-Deml et al., 2018] extended ICP to a non-linear setting.Invariant Risk Minimization (IRM)6.2Causal graphs are, in many cases, inaccessible, e.g., thecausal relations between pixels and a target predict. With-out the need to retrieve direct causes of a target variable ina causal graph, IRM [Arjovsky et al., 2019] elevates invari-ance by focusing on out-of-distribution (OOD) generalization\u2013 the performance of a predictive model when evaluated in anew environment. IRM seeks to learn a data representation\u03c6 that achieves two goals: predicting accurately and elicit-ing an invariant predictor across environments. This can beformulated as the constrained optimization problem:where \u03c6 is the invariant predictor in a latent causal systemgenerating observed features, w = 1.0 is a \ufb01xed \u201cdummy\u201dclassi\ufb01er. R (\u00b7) denotes the risk under e such as predictionerrors. \u03bb controls the balance between prediction accuracy ineach e and the invariance of the predictor 1\u00b7\u03c6(x). In practice,the prediction performance in the de\ufb01ned environments is al-most certainly reduced due to the exclusion of some spuriouscorrelations. IRM cannot guarantee to remove all spuriouscorrelations as it also depends on the provided environments.There are a number of follow-up works such as [Jin et al.,2020; Krueger et al., 2020] relying on a stronger assumptionof invariance of p(y|\u03c6(x)) than that of E[y|\u03c6(x)] in IRM.6.3 CDA for Invariance/GeneralizabilityAnother causality-inspired approach for improving model in-variance is to augment original data with counterfactuals thatcan expose the model to OOD scenarios. CDA prevents mod-els from learning spurious patterns present in the trainingdata, thus, improving model invariance. A general pipelinedescribing CDA for model invariance can be seen in Fig. 3.In computer vision, CDA is used to generate counterfac-tual images close to training samples yet may not belongto existing training categories. For example, to detect un-known classes, generative adversarial networks were used togenerate perturbed examples from the known class and la-beled as unknown categories [Neal et al., 2018]. Drawing onindependent mechanisms (IMs) [Peters et al., 2017], [Sauerand Geiger, 2021] proposed to decompose the image gener-ation process into different mechanisms related to its shape,texture, and background. With known causal structure andlearned IMs, a counterfactual generative network generatedcounterfactual images regarding each mechanism. Similarconcept was used in visual question answering [Abbasnejadet al., 2020] to improve the generalizability of various multi-modal and unimodal vision and language tasks.Inherently related to causal inference, ReinforcementLearning (RL) uses CDA to learn more generalizable policies.Counterfactual data in RL introduces various scenarios thatan RL agent generally does not experience during training. Indynamic processes, for instance, [Pitis et al., 2020] proposedto decompose the dynamics of different subprocesses into lo-cal IMs which can be used to generate counterfactual expe-riences. To choose the optimal treatment for a given patient,[Lu et al., 2020a] proposed a data-ef\ufb01cient RL algorithm thatused SCM to generate counterfactual-based data.6.4 DiscussionsCurrent applications of IRM have been focused on com-puter vision, nevertheless, an environment needs not to bescenery in an image. Some promising applications includehealth care [Kouw and Loog, 2018], robotics [Giusti et al.,2015], NLP [Choe et al., 2020], recommender systems [Wanget al., 2018], and so on. IRM also highly relates to fair-ness [Arjovsky et al., 2019]. When applying IRM, one maypay attention to the non-linear settings where formal resultsfor latent-variable models are lacking and risks are under-explored [Rosenfeld et al., 2020]. Another caveat of exist-ing works in invariant prediction is the reliance on the strin-gent unconfoundedness assumption, which is typically im-practical. ICP is more interpretable than IRM in terms of dis-covering causal features. For CDA, the counterfactual sam-ples generation strategy usually relieves the conditional inde-pendence assumption of training data, which helps improvemodel generalizability. When generating counterfactual datais not feasible, one can use minimally-different examples inexisting datasets with different labels to improve model gen-eralizability [Teney et al., 2020].7 Summary and Open ProblemsWe review recent advances in SRAI from CL perspective.Purely reliant on statistical relationships, current AI algo-rithms achieve prominent performance meanwhile its poten-tial risks raise great concerns. To achieve SRAI, we arguethat CL is an effective means for it seeks to uncover the DGPs.Our survey begins by introducing the seven CL tools and theirconnections to SRAI. We then discuss how four of these toolsare used in developing SRAI. In the following, we brie\ufb02y de-scribe promising future research directions of SRAI.Privacy-preserving. Privacy is a crucial tenet of SRAI.Many research has shown that AI systems can learn and re- member users\u2019 private attributes. However, how to use CL toenhance privacy has been barely studied in literature. Similarto de-biasing methods, we can use CL to remove sensitive in-formation and create privacy-preserving data representations.Making explicit causal assumptions. Explicitly makingassumptions ensures more valid, testable, and transparentcausal models. Given causal assumptions might be disputedor uncertain, we need sensitivity analysis to measure themodel performance with assumption violations. Critically,assumptions should be made with humility and researchersare responsible to protect against unethical assumptions.Causal discovery. While causal discovery has been exten-sively studied, its connection to SRAI is not well understood.Discovering causal relations helps determine if assumptionsare properly made and interventions are correctly applied.Given that causal graph is key to many CL approaches inSRAI, causal discovery is an important future research.Mediation analysis. Causal mediation analysis improvesmodel transparency. For example, in CF, sensitive attributessuch as gender and race are assumed to solely have directin\ufb02uence on the classi\ufb01cation. Is the effect of race on loangranting mediated by the job type? Similarly, mediation anal-ysis could be used in explainable AI, e.g., neurons directly orindirectly in\ufb02uence algorithmic decisions.Missing data. CL is a missing data problem: inferring thepotential outcomes of the same units with different treatmentassignments. We might apply CL to a more general setting ofmissing data. For example, graphical model based procedurescan be used to provide performance guarantees when data areMissing Not At Random [Mohan and Pearl, 2021].Long-term impact. The majority of works in SRAI over-looks its long-term commitment to be ful\ufb01lled. This hindersboth the ef\ufb01ciency and ef\ufb01cacy of existing works to achieveSRAI. For instance, static fairness criterion used in bank loangranting may cost credibility scores of the minorities in thelong run [Liu et al., 2018].Social good. Essentially, SRAI is designed to protect, in-form users, and prevent/mitigate the harms of AI [Cheng etal., 2021]. With the burgeoning AI-for-social-good move-ment, CL is becoming the core component of AI systems totackle societal issues.Causal tools and libraries for SRAI. SRAI research canalso bene\ufb01t from using existing CL libraries such as CausalML , DoWhy , and Causal Discovery Toolbox . It is possibleto integrate CL models for SRAI into these tools.AcknowledgementsThis material is based upon work supported by, or in part by,the U.S. Army Research Laboratory and the U.S. Army Re-search Of\ufb01ce under contract/grant number W911NF2110030and W911NF2020124 as well as by the National ScienceFoundation (NSF) grant 1909555.References[Abbasnejad et al., 2020] Ehsan Abbasnejad, DamienTeney, Amin Parvaneh, Javen Shi, and Anton van denHengel. Counterfactual vision and language learning. InCVPR, pages 10044\u201310054, 2020.[Agarwal et al., 2019] Aman Agarwal, Kenta Takatsu, IvanZaitsev, and Thorsten Joachims. A general framework forcounterfactual learning-to-rank. In SIGIR, 2019.[Arjovsky et al., 2019] Martin Arjovsky, L\u00b4eon Bottou,Invariant riskIshaan Gulrajani, and David Lopez-Paz.minimization. arXiv preprint arXiv:1907.02893, 2019.[Balakrishnan et al., 2020] Guha Balakrishnan, YuanjunXiong, Wei Xia, and Pietro Perona. Towards causalInbenchmarking of bias in face analysis algorithms.ECCV, pages 547\u2013563. Springer, 2020.[Chattopadhyay et al., 2019] Aditya Chattopadhyay, PiyushiManupriya, Anirban Sarkar, and Vineeth N Balasubrama-nian. Neural network attributions: A causal perspective.In ICML, pages 981\u2013990. PMLR, 2019.[Cheng et al., 2021] Lu Cheng, Kush R Varshney, and HuanLiu. Socially responsible ai algorithms: Issues, purposes,and challenges. arXiv preprint arXiv:2101.02032, 2021.[Choe et al., 2020] Yo Joong Choe, Jiyeon Ham, and Kyuby-ong Park. An empirical study of invariant risk minimiza-tion. In ICML UDL, 2020.[Cloudera, 2020] Cloudera. Causality for machine learning.https://ff13.fastforwardlabs.com/, 2020. Accessed: 2021-02-14.[Friedman, 2001] Jerome H Friedman. Greedy function ap-proximation: a gradient boosting machine. Annals ofstatistics, pages 1189\u20131232, 2001.[Geirhos et al., 2020] Robert Geirhos, J\u00a8orn-Henrik Jacob-sen, Claudio Michaelis, Richard Zemel, Wieland Brendel,Matthias Bethge, and Felix A Wichmann. Shortcut learn-ing in deep neural networks. Nature Machine Intelligence,2(11):665\u2013673, 2020.[Getoor, 2019] Lise Getoor. Responsible data science. In BigData, pages 1\u20131. IEEE, 2019.[Giusti et al., 2015] Alessandro Giusti, J\u00b4er\u02c6ome Guzzi, Dan CCires\u00b8an, Fang-Lin He, Juan P Rodr\u00b4\u0131guez, Flavio Fontana,Matthias Faessler, Christian Forster, J\u00a8urgen Schmidhuber,Gianni Di Caro, et al. A machine learning approach tovisual perception of forest trails for mobile robots. IEEERobotics and Automation Letters, 1(2):661\u2013667, 2015.[Goldstein et al., 2015] Alex Goldstein, Adam Kapelner,Justin Bleich, and Emil Pitkin. Peeking inside the blackbox: Visualizing statistical learning with plots of individ-ual conditional expectation. JCGS, 24(1):44\u201365, 2015. [Guo et al., 2020b] Ruocheng Guo, Xiaoting Zhao, AdamHenderson, Liangjie Hong, and Huan Liu. Debiasing grid-based product search in e-commerce. In KDD, 2020.[Hajian and Domingo-Ferrer, 2012] Sara Hajian and JosepDomingo-Ferrer. A methodology for direct and indi-rect discrimination prevention in data mining. TKDE,25(7):1445\u20131459, 2012.[Heinze-Deml et al., 2018] Christina Heinze-Deml, JonasPeters, and Nicolai Meinshausen. Invariant causal predic-tion for nonlinear models. JCI, 6(2), 2018.[Hofmann et al., 2013] Katja Hofmann, Anne Schuth, Shi-mon Whiteson, and Maarten De Rijke. Reusing historicalinteraction data for faster online learning to rank for ir. InWSDM, pages 183\u2013192, 2013.[Jin et al., 2020] Wengong Jin, Regina Barzilay, and TommiJaakkola. Domain extrapolation via regret minimization.arXiv preprint arXiv:2006.03908, 2020.[Joachims et al., 2017] Thorsten Joachims, Adith Swami-nathan, and Tobias Schnabel. Unbiased learning-to-rankwith biased feedback. In WSDM, pages 781\u2013789, 2017.[Kasirzadeh and Smart, 2021] Atoosa Kasirzadeh and An-drew Smart. The use and misuse of counterfactuals in eth-ical machine learning. In FAccT, 2021.[Kaushik et al., 2020] Divyansh Kaushik, Eduard Hovy, andZachary C Lipton. Learning the difference that makes aICLR,difference with counterfactually-augmented data.2020.[Kouw and Loog, 2018] Wouter M Kouw and Marco Loog.An introduction to domain adaptation and transfer learn-ing. arXiv preprint arXiv:1812.11806, 2018.[Krueger et al., 2020] David Krueger, Ethan Caballero,Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas,Remi Le Priol, and Aaron Courville. Out-of-distributiongeneralization via risk extrapolation (rex). arXiv preprintarXiv:2003.00688, 2020.[Kusner et al., 2017] Matt J Kusner, Joshua R Loftus, ChrisCounterfactual fairness.Russell, and Ricardo Silva.NeurIPS, 2017.[Liu et al., 2018] Lydia T Liu, Sarah Dean, Esther Rolf, MaxSimchowitz, and Moritz Hardt. Delayed impact of fairmachine learning. In ICML, pages 3150\u20133158, 2018.[Liu et al., 2019] Shusen Liu, Bhavya Kailkhura, DonaldLoveland, and Yong Han. Generative counterfactual in-trospection for explainable deep learning. arXiv preprintarXiv:1907.03077, 2019.[Lu et al., 2020a] Chaochao Lu, Biwei Huang, Ke Wang,Jos\u00b4e Miguel Hern\u00b4andez-Lobato, Kun Zhang, and Bern-hard Sch\u00a8olkopf. Sample-ef\ufb01cient reinforcement learningvia counterfactual-based data augmentation. In NeurIPSOf\ufb02ine RL, 2020.[Guo et al., 2020a] Ruocheng Guo, Lu Cheng, Jundong Li,P Richard Hahn, and Huan Liu. A survey of learn-ing causality with data: Problems and methods. CSUR,53(4):1\u201337, 2020. [Lu et al., 2020b] Kaiji Lu, Piotr Mardziel, Fangjing Wu,Preetam Amancharla, and Anupam Datta. Gender bias inneural natural language processing. In Logic, Language,and Security, pages 189\u2013202. Springer, 2020.[Makhlouf et al., 2020] Karima Makhlouf, Sami Zhioua,Survey on causal-basedarXiv preprintand Catuscia Palamidessi.machine learning fairness notions.arXiv:2010.09553, 2020.[Mart\u00b4\u0131nez and Marca, 2019] \u00b4Alvaro Para\ufb01ta Mart\u00b4\u0131nez andJordi Vitri`a Marca. Explaining visual models by causalattribution. In ICCVW, pages 4167\u20134175. IEEE, 2019.[Mohan and Pearl, 2021] Karthika Mohan and Judea Pearl.JASA,Graphical models for processing missing data.pages 1\u201342, 2021.[Moraffah et al., 2020] Raha Moraffah, Mansooreh Karami,Ruocheng Guo, Adrienne Raglin, and Huan Liu. Causalinterpretability for machine learning-problems, methodsand evaluation. ACM SIGKDD Explorations Newsletter,22(1):18\u201333, 2020.[Narendra et al., 2018] Tanmayee AnushSankaran, Deepak Vijaykeerthy, and Senthil Mani.Explaining deep learning models using causal inference.arXiv preprint arXiv:1811.04376, 2018.Narendra,[Neal et al., 2018] Lawrence Neal, Matthew Olson, XiaoliFern, Weng-Keen Wong, and Fuxin Li. Open set learn-ing with counterfactual images. In ECCV, 2018.[Olteanu et al., 2019] Alexandra Olteanu, Carlos Castillo,Fernando Diaz, and Emre K\u0131c\u0131man. Social data: Biases,methodological pitfalls, and ethical boundaries. Frontiersin Big Data, 2:13, 2019.[Pearl and Bareinboim, 2011] Judea Pearl and Elias Barein-boim. Transportability of causal and statistical relations:A formal approach. In AAAI, volume 25, 2011.[Pearl, 2009] Judea Pearl. Causality. 2009.[Pearl, 2019] Judea Pearl. The seven tools of causal infer-ence, with re\ufb02ections on machine learning. Communica-tions of the ACM, 62(3):54\u201360, 2019.[Peters et al., 2016] Jonas Peters, Peter B\u00a8uhlmann, andNicolai Meinshausen. Causal inference by using invari-ant prediction: identi\ufb01cation and con\ufb01dence intervals. J RStat Soc Series B Stat Methodol, pages 947\u20131012, 2016.[Peters et al., 2017] Jonas Peters, Dominik Janzing, andBernhard Sch\u00a8olkopf. Elements of causal inference: foun-dations and learning algorithms. The MIT Press, 2017.[Pitis et al., 2020] Silviu Pitis, Elliot Creager, and AnimeshGarg. Counterfactual data augmentation using locally fac-tored dynamics. In NeurIPS, 2020.[Rosenbaum and Rubin, 1983] Paul R Rosenbaum and Don-ald B Rubin. The central role of the propensity scorein observational studies for causal effects. Biometrika,70(1):41\u201355, 1983.[Rosenfeld et al., 2020] Elan Rosenfeld, Pradeep Raviku-mar, and Andrej Risteski. The risks of invariant risk mini-mization. arXiv preprint arXiv:2010.05761, 2020.[Sauer and Geiger, 2021] Axel Sauer and Andreas Geiger.Counterfactual generative networks. 2021. [Schnabel et al., 2016] Tobias Schnabel, Adith Swami-nathan, Ashudeep Singh, Navin Chandak, and ThorstenJoachims. Recommendations as treatments: Debiasinglearning and evaluation. In ICML, 2016.[Teney et al., 2020] Damien Teney, Ehsan Abbasnedjad, andAnton van den Hengel. Learning what makes a differencefrom counterfactual examples and gradient supervision. InECCV, 2020.[Van Looveren and Klaise, 2019] Arnaud Van Looveren andJanis Klaise. Interpretable counterfactual explanationsguided by prototypes. arXiv preprint arXiv:1907.02584,2019.[von K\u00a8ugelgen et al., 2020] Julius von K\u00a8ugelgen, UmangBhatt, Amir-Hossein Karimi, Isabel Valera, Adrian Weller,and Bernhard Sch\u00a8olkopf. On the fairness of causal algo-rithmic recourse. arXiv preprint arXiv:2010.06529, 2020.[Wang and Deng, 2020] Mei Wang and Weihong Deng. Mit-igating bias in face recognition using skewness-aware re-inforcement learning. In CVPR, pages 9322\u20139331, 2020.[Wang et al., 2018] Yixin Wang, Dawen Liang, LaurentCharlin, and David M Blei. The deconfounded recom-mender: A causal inference approach to recommendation.arXiv preprint arXiv:1808.06581, 2018.[Wu et al., 2019] Yongkai Wu, Lu Zhang, and Xintao Wu.Counterfactual fairness: Unidenti\ufb01cation, bound and al-gorithm. In IJCAI, 2019.[Xu et al., 2020] Guandong Xu, Tri Dung Duong, Qian Li,Shaowu Liu, and Xianzhi Wang. Causality learning: Anew perspective for interpretable machine learning. arXivpreprint arXiv:2006.16789, 2020.[Yang and Feng, 2020] Zekun Yang and Juan Feng. A causalinference method for reducing gender bias in word embed-ding relations. In AAAI, pages 9434\u20139441, 2020.[Yang et al., 2018] Longqi Yang, Yin Cui, Yuan Xuan,Chenyang Wang, Serge Belongie, and Deborah Estrin. Un-biased of\ufb02ine recommender evaluation for missing-not-at-random implicit feedback. In RecSys, 2018.[Zhang and Bareinboim, 2018] Junzhe Zhang and EliasFairness in decision-making\u2014the causalBareinboim.explanation formula. In AAAI, volume 32, 2018.[Zhang et al., 2018] Brian Hu Zhang, Blake Lemoine, andMargaret Mitchell. Mitigating unwanted biases with ad-versarial learning. In AIES, pages 335\u2013340, 2018.[Zhao and Hastie, 2021] Qingyuan Zhao and Trevor Hastie.JBES,interpretations of black-box models.Causal39(1):272\u2013281, 2021.[Zhao et al., 2019] Jieyu Zhao, Tianlu Wang, Mark Yatskar,Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang.Gender bias in contextualized word embeddings. InNAACL, 2019.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "d3710266-7ef9-4c05-a3b3-77fc0edae388",
                    "text": "",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "979071cb-aee4-413d-8413-b6bb26a1da88",
                    "text": "Another causality-inspired approach for improving model in-variance is to augment original data with counterfactuals thatcan expose the model to OOD scenarios. CDA prevents mod-els from learning spurious patterns present in the trainingdata, thus, improving model invariance. A general pipelinedescribing CDA for model invariance can be seen in Fig. 3.In computer vision, CDA is used to generate counterfac-tual images close to training samples yet may not belongto existing training categories. For example, to detect un-known classes, generative adversarial networks were used togenerate perturbed examples from the known class and la-beled as unknown categories [Neal et al., 2018]. Drawing onindependent mechanisms (IMs) [Peters et al., 2017], [Sauerand Geiger, 2021] proposed to decompose the image gener-ation process into different mechanisms related to its shape,texture, and background. With known causal structure andlearned IMs, a counterfactual generative network generatedcounterfactual images regarding each mechanism. Similarconcept was used in visual question answering [Abbasnejadet al., 2020] to improve the generalizability of various multi-modal and unimodal vision and language tasks.Inherently related to causal inference, ReinforcementLearning (RL) uses CDA to learn more generalizable policies.Counterfactual data in RL introduces various scenarios thatan RL agent generally does not experience during training. Indynamic processes, for instance, [Pitis et al., 2020] proposedto decompose the dynamics of different subprocesses into lo-cal IMs which can be used to generate counterfactual expe-riences. To choose the optimal treatment for a given patient,[Lu et al., 2020a] proposed a data-ef\ufb01cient RL algorithm thatused SCM to generate counterfactual-based data.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "29ef1266-ca4a-4d9e-a614-310eb4b30b30",
                    "text": "Current applications of IRM have been focused on com-puter vision, nevertheless, an environment needs not to bescenery in an image. Some promising applications includehealth care [Kouw and Loog, 2018], robotics [Giusti et al.,2015], NLP [Choe et al., 2020], recommender systems [Wanget al., 2018], and so on. IRM also highly relates to fair-ness [Arjovsky et al., 2019]. When applying IRM, one maypay attention to the non-linear settings where formal resultsfor latent-variable models are lacking and risks are under-explored [Rosenfeld et al., 2020]. Another caveat of exist-ing works in invariant prediction is the reliance on the strin-gent unconfoundedness assumption, which is typically im-practical. ICP is more interpretable than IRM in terms of dis-covering causal features. For CDA, the counterfactual sam-ples generation strategy usually relieves the conditional inde-pendence assumption of training data, which helps improvemodel generalizability. When generating counterfactual datais not feasible, one can use minimally-different examples inexisting datasets with different labels to improve model gen-eralizability [Teney et al., 2020].",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                },
                {
                    "id": "10159b20-2b8e-4f8a-877a-7dd1d77d8220",
                    "text": "We review recent advances in SRAI from CL perspective.Purely reliant on statistical relationships, current AI algo-rithms achieve prominent performance meanwhile its poten-tial risks raise great concerns. To achieve SRAI, we arguethat CL is an effective means for it seeks to uncover the DGPs.Our survey begins by introducing the seven CL tools and theirconnections to SRAI. We then discuss how four of these toolsare used in developing SRAI. In the following, we brie\ufb02y de-scribe promising future research directions of SRAI.Privacy-preserving. Privacy is a crucial tenet of SRAI.Many research has shown that AI systems can learn and re- member users\u2019 private attributes. However, how to use CL toenhance privacy has been barely studied in literature. Similarto de-biasing methods, we can use CL to remove sensitive in-formation and create privacy-preserving data representations.Making explicit causal assumptions. Explicitly makingassumptions ensures more valid, testable, and transparentcausal models. Given causal assumptions might be disputedor uncertain, we need sensitivity analysis to measure themodel performance with assumption violations. Critically,assumptions should be made with humility and researchersare responsible to protect against unethical assumptions.Causal discovery. While causal discovery has been exten-sively studied, its connection to SRAI is not well understood.Discovering causal relations helps determine if assumptionsare properly made and interventions are correctly applied.Given that causal graph is key to many CL approaches inSRAI, causal discovery is an important future research.Mediation analysis. Causal mediation analysis improvesmodel transparency. For example, in CF, sensitive attributessuch as gender and race are assumed to solely have directin\ufb02uence on the classi\ufb01cation. Is the effect of race on loangranting mediated by the job type? Similarly, mediation anal-ysis could be used in explainable AI, e.g., neurons directly orindirectly in\ufb02uence algorithmic decisions.Missing data. CL is a missing data problem: inferring thepotential outcomes of the same units with different treatmentassignments. We might apply CL to a more general setting ofmissing data. For example, graphical model based procedurescan be used to provide performance guarantees when data areMissing Not At Random [Mohan and Pearl, 2021].Long-term impact. The majority of works in SRAI over-looks its long-term commitment to be ful\ufb01lled. This hindersboth the ef\ufb01ciency and ef\ufb01cacy of existing works to achieveSRAI. For instance, static fairness criterion used in bank loangranting may cost credibility scores of the minorities in thelong run [Liu et al., 2018].Social good. Essentially, SRAI is designed to protect, in-form users, and prevent/mitigate the harms of AI [Cheng etal., 2021]. With the burgeoning AI-for-social-good move-ment, CL is becoming the core component of AI systems totackle societal issues.Causal tools and libraries for SRAI. SRAI research canalso bene\ufb01t from using existing CL libraries such as CausalML , DoWhy , and Causal Discovery Toolbox . It is possibleto integrate CL models for SRAI into these tools.AcknowledgementsThis material is based upon work supported by, or in part by,the U.S. Army Research Laboratory and the U.S. Army Re-search Of\ufb01ce under contract/grant number W911NF2110030and W911NF2020124 as well as by the National ScienceFoundation (NSF) grant 1909555.",
                    "reference": "[1] Aleksandra Faustine Cheng, Amin Mosallanezhad, Payam M. Barnaghi, Hemant Purohit, and Huan Liu. 2021. Causal learning for socially responsible AI. arXiv preprint arXiv:2104.12278. Retrieved from https://arxiv.org/pdf/2104.12278"
                }
            ]
        },
        {
            "paper_title": "RAISE: leveraging responsible AI for service excellence",
            "authors": "L Alkire, A Bilgihan, M Bui, AJ Buoye\u2026",
            "publication_info": "Journal of Service \u2026 - emerald.com",
            "paper_url": "https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf",
            "chunks": [
                {
                    "id": "4622d107-13f5-4a26-a599-5769d3a7d4d0",
                    "text": "",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "5e286a43-b1d4-42f9-93d6-77d349dc3f8a",
                    "text": "The viral spread of generative artificial intelligence (GenAI) shattered the idea that AI is afuturistic tool or is solely designed to be used by technology experts. Instead, GenAI systems havebeen developed and trained by experts to be user-friendly, enabling non-experts to leverage thesemodels for various applications without needing to understand the underlying complexities.These capabilities allow anyone with internet access to generate text, codes, andsimulations, synthesize images, graphics, and art, compose music, create videos, and muchmore (Peres et al., 2023). The interactive component, or the conversational elements of AIJOSM systems, such as those found in ChatGPT (Chat Generative Pre-trained Transformer),Copilot, and Gemini possess, make conversational AI capable of natural languageunderstanding, dialogue management, and natural language generation creating seamlessthese innovative AI capabilities have massiveand human-like conversations. Allimplications for the service industry and beyond. However, while research on AI topics iscurrently growing, the rapid development and spread of AI technologies in general, andGenAI technologies in particular, present new challenges and opportunities that requiremultifaceted investigations.The existing academic service literature on AI mostly (1) compares and contrasts AIcapabilities with human capabilities (e.g. Mende et al., 2019; Crolic et al., 2022; Uysal et al.,2022); (2) proposes a categorization of different types and functions of AI (Huang and Rust,2018, 2021a, 2024); (3) focuses on the customer and/or organization perspective (Davenportet al., 2020); (4) is profit-driven, seeking to increase the efficiency and effectiveness of theservice process (i.e. service delivery, service creation, and service interaction); and (5) isfuture-oriented, predicting where AI is going (e.g. \u201cfeeling AI\u201d in Huang and Rust, 2021b).While it is remarkable to witness the advancements of AI research, there is an urgent need toaddress its responsible deployment and usage (Van Dis et al., 2023). While some researchershave acknowledged that AI carries serious ethical, fairness, and privacy risks for serviceusers (Wirtz et al., 2023; Belk, 2021; Breidbach and Maglio, 2020), we see the need \u2013 andopportunity \u2013 for a more fundamental discussion about the long-term and more significantresponsible usage of AI in service. This is even more urgent as novel and pressing issues arebeing voiced by policymakers and practitioners.The rapid diffusion of GenAI tools has attracted attention and provoked controversysurrounding their use. For example, concerns regarding false, inaccurate, or even misleadinginformation are being raised. Further concerns around data privacy and security aregrowing. These concerns led the Federal Trade Commission to investigate OpenAI forpossible violations of consumer protection law (The Washington Post, 2023). Issuespertaining to gender and racial biases continue to headline various news outlets. Mostrecently, the US White House issued an Executive Order aimed at ensuring the safe, secure,and trustworthy development and deployment of AI. The Executive Order requires rigoroussafety protocols and testing procedures for AI. In addition to security concerns, the ExecutiveOrder prescribes stringent guidelines designed to ameliorate AI-induced biases, specificallyin sectors such as housing and criminal justice. The document advocates for the developmentof best practices for AI applications within healthcare and education, as well as directivesaimed at minimizing job displacement and unfair labor practices attributable to AItechnologies (White House, 2023). As service scholars, we have a moral obligation to digdeeper into these issues and address the responsible usage of AI in service.In light of the above, this paper proposes a theoretically driven strategic framework forleveraging responsible AI for service excellence. As such, we introduce the concept of RAISE,standing for Responsible AI for Service Excellence. We define RAISE as \u201ca strategic frameworkfor responsibly integrating artificial intelligence into service industries. It emphasizes collaborativeAI design and deployment that aligns with the evolving global standards and societal well-beingwhile promoting business success and sustainable development.\u201dThe key components of this definition warrant emphasis:(1) RAISE is a practice-focused framework aimed at empowering service entities toresponsibly integrate AI into their operations.(2) RAISE advocates for transformative collaboration between service organizations,policymakers, AI developers, customers, and researchers to ensure AI applicationsare transparent and positively contribute to societal well-being. Journal of ServiceManagement(3) Given the broad and evolving nature of AI ethics and regulations, RAISE advocatesstaying informed of and adapting to the evolving global standards, encompassingethical guidelines, regulatory frameworks, and industry best practices. Thisapproach ensures alignment with the dynamic landscape of AI developments andregulations.(4) RAISE calls for an integrated approach where AI technologies are designed andimplemented to contribute to business success while also serving the greater good.In summary, RAISE seeks to integrate these dimensions within the context of serviceexcellence to provide a holistic framework that enables service organizations to achievebusiness success while contributing to societal well-being.With the increasing reliance on AI, there is an urgent need to practice responsibleprinciples respecting human rights and prioritizing societal well-being, as highlighted by theSustainable Development Goals and the Ethical AI principles introduced by Jobin et al. (2019).We build on these important notions to develop the three RAISE principles for practice: (1)Embrace AI to serve the greater good, (2) Design and deploy responsible AI (by respectingand practicing the ethical principles of privacy, transparency, accountability, justice andfairness, and non-maleficence), and (3) Practice transformative collaboration with differentservice entities to implement responsible AI.This research contributes to a foundation for responsible AI for service excellence.It underscores the importance of aligning AI with responsible principles to create a positivesocietal impact while promoting business success. By emphasizing that service organizationscan harness the full potential of AI while adopting responsible and sustainable practices, thisapproach transcends the traditional view that positions profit generation and social good asmutually exclusive objectives. Instead, we advocate for an integrated framework where AItechnologies are designed and implemented to contribute to economic growth while alsoserving the greater good. Moreover, by acknowledging the potential risks and challengesassociated with AI implementation, this research provides practical recommendations forservice entities to strengthen their commitment to privacy, transparency, accountability,justice and fairness, and non-maleficence as part of their sustainable service practices. Theresearch also contributes by stressing the need for transformative collaboration inaddressing responsible practices that have greater societal implications. Ultimately, wecall on service researchers, policymakers, managers, and customers to collaborate andembrace responsible AI to drive service excellence and foster a sustainable future for all.The subsequent sections of this article will provide an overview of the current state ofresearch on AI in service, examine relevant perspectives to conceptualize RAISE, and discussits three principles for practice.",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "d72a98cb-e2a3-40fd-b366-2612dd77bd52",
                    "text": "Over the past decade, AI has been a prominent subject of interest in the service literature,alongside related concepts like robots, chatbots, and smart technology (De Keyser et al., 2019).De Keyser and Kunz (2022) identified 2,145 articles published in academic journals coveredby the SERVSIG literature alert [1]. The search focused on keywords related to robots,technology, and AI in academic articles published between 2016 and the summer of 2020.Moreover, the Journal of Service Management (JOSM) published two special issues on thetopic in the last five years (Paluch and Wirtz, 2020; Robertson and Tsarenko, 2021), while theJournal of Service Research published another special issue on AI (Bagozzi et al., 2022).Certainly, AI has been a frequent topic in all the marketing science literature [2](e.g. Huang and Rust, 2021a; Puntoni et al., 2021), so its prevalence in service research is notJOSM especially noteworthy. However, the coverage of the subject in the service literature is,nonetheless, uniquely positioned. Kunz et al. (2019) observed that academic service literatureoften focuses more on the concept of technology than on specific technologies, such asChatGPT, especially when compared to management journals and mainstream businesspublications. Recent discussions on AI\u2019s role in services highlight the critical need forworkforce upskilling in AI and the adoption of digital twin technologies to improve servicedelivery and build consumer trust. These discussions focus on strategic integration ratherthan detailing specific applications of such technologies (Spohrer, 2023).Kunz et al. (2019) also found that the existing service literature disproportionally focusesattention on the managerial consequences of new service technologies (e.g. Skiera et al., 2022)rather than on the impact of these technologies on consumers (e.g. Kipnis et al., 2022; Pantanoand Scarpi, 2022), though studies of both outcomes are certainly represented. By providing astructured approach to navigating and implementing technological changes in servicesystems, Service Innovation Roadmaps (SIRs) (Spohrer, 2021) ensure that the focus is notsolely on operational and managerial aspects but also on enhancing the customer experienceand satisfaction. This comprehensive view enables organizations to consider both internaland external effects of AI integration, ensuring that technological advancements contributepositively to all aspects of service delivery.Beyond the coverage in the service literature, the practical application of thesetechnologies in service is different than in other marketing and business contexts. Bagozziet al. (2022) indicate that while AI in computer science is focused on rational and analyticaldecision-making, AI in service needs to be \u201ca feeling machine.\u201d Interestingly, the impact of AIon consumers\u2019 emotions and trust is a growing area of investigation in the service researchspace (e.g. Bagozzi et al., 2022; Belk, 2022; Esmaeilzadeh and Vaezi, 2022; Huang and Rust,2024; van Pinxteren et al., 2019; Vorobeva et al., 2022).",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "4e128831-2d64-4990-b850-899bc8fa33b1",
                    "text": "Huang and Rust (2018) pioneered researchon AI in service, introducing a key theoretical concept\u2013the alignment of various types ofintelligence with specific service objectives. Huang and Rust (2021b) proposed three types ofintelligence, both artificial and human, that link with the tasks of doing, thinking, and feeling.According to their framework, each type of AI can be used to either augment or replace theapplication of the respective human intelligence in service tasks. However, the balance ofaugmentation and replacement varies by the type of intelligence and the specific taskrequirements. Mechanical intelligence is the lowest level of AI and relates to the ability toperform routine and repetitive tasks efficiently. Mechanical AI can be used to make tedious,routine tasks more efficient and is often central to efforts to replace human labor with serviceis usefulrobots or standalone self-service kiosks. Mechanical AI for achievingstandardization in processes and, as such, requires minimal learning and adaptation.Thinking intelligence involves analyzing data to make decisions or recommendations, eitheras a replacement for human decision-making or to assist humans in better-informed andquicker decision-making. Thinking AI is useful for delivering personalized rather thanstandardized output. An example of the application of this type of Thinking AI is the use of AIdiagnostic tools in health services (Spatharou et al., 2020; Van Doorn et al., 2017). Feelingintelligence involves communicative and interactive tasks that require recognizing andinterpreting human emotions to provide appropriate responses. In contrast with Thinking AI,which achieves self-learning through analysis of massive amounts of data, Feeling AI mustlearn from and adapt at the moment to singular or anecdotal experiences. Given that suchtasks are often difficult for even human beings, Feeling AI is the most difficult type of AI torealize, but arguably the most important for service.A noteworthy distinction to be made is that \u201cThinking AI\u201d and \u201cFeeling AI\u201d are merelymetaphors: extant AI neither \u201cthinks\u201d nor \u201cfeels\u201d, but rather utilizes algorithmic processes torespond to information and gives the appearance of thinking and feeling. High-levelJournal of ServiceManagementexecutives at top AI firms indicate that AI can already better recognize and respond tocustomer emotional cues than human workers and that this capability allows AI to effectively\u201cfake\u201d empathy during customer interactions (Huang and Rust, 2024). Much like humanservice workers who engage in \u201cemotional labor\u201d (Hochschild, 1983), the ability to effectivelydisplay empathy is not predicated on the genuineness of the emotion (the ability of AI to trulyunderstand or experience emotion delves into deeper philosophical arguments about StrongAI vs Weak AI (Searle, 1984), which are outside the scope of this paper).Customer service chatbots that respond to consumers based on Feeling AI are chargedwith maintaining the most fundamental relationship in service: the trust of the consumer. VanPinxteren et al. (2019) described how a lack of trust hinders consumer adoption of servicerobots. This is partially attributable to a lack of anthropomorphism in many applications butalso to limitations in the performance of extant Feeling AI. As Feeling AI advances to bettermimic human emotional intelligence, greater consumer adoption is a near certainty. Theability of ChatGPT to pass the Turing test \u2013 when AI can mimic human intelligence to thedegree that humans cannot tell they are not speaking to an actual human \u2013 demonstrates thatAI has, at least arguably, reached this threshold (James, 2023). The development andimplementation of Feeling AI in customer service underscore the delicate balance betweenaugmenting human emotional intelligence and replacing it, emphasizing the need to carefullyconsider which tasks are best suited for AI to ensure that technological advancementsenhance rather than diminish the quality of human interactions (Huang and Rust, 2024).The usefulness of AI for service may be less about how well AI can mimic humanintelligence or empathy than whether it actually needs to be effective. Bock et al. (2020) argueagainst limiting AI by comparing it to human capabilities, as AI possesses the potential tosurpass human abilities. They define \u201cservice AI\u201d as \u201cthe configuration of technology toprovide value in the internal and external service environments through flexible adaptationenabled by sensing, learning, decision-making, and actions\u201d (p. 319). The key distinctionbetween service AI and other new service technologies is its potential for flexible adaptation.Huang and Rust (2018) similarly posit self-learning as one of the two defining characteristicsof AI. The other characteristic is connectivity, as AI is seldom standalone. Moreover, theinteraction of AI with the Internet of Things (IoT) and humans is necessary to provide AI withthe resources for self-learning. In other words, achieving the first characteristic (self-learning)relies on the second (connectivity).An overarching concern in the service literature has been the increasing pace at whichnew service technologies are emerging and their significant consequences for consumers,corporations, and society (Wirtz et al., 2018, 2023). Huang and Rust (2018) note, \u201cThe serviceand technology literature tends to focus on the positives of AI technology usage, while theeconomic literature tends to focus on the effect of AI on jobs.\u201d (p. 155). Within the servicecontext, there is a growing interest in studying the integration of humans with AI rather thanthe replacement of human employees with AI (De Keyser and Kunz, 2022; Henkel et al., 2020).Nonetheless, the replacement of human labor remains a serious concern in the \u201cFeelingEconomy\u201d (Vorobeva et al., 2022), and the potential impact of AI on service jobs is only one ofthe potential ethical dilemmas raised by the rise of service AI. Beyond simply replacinghumans on the job, half of AI researchers believe there is a non-negligible chance that humanbeings may go extinct as a result of our inability to control AI (Center for HumaneTechnology, 2023). The incorporation of advanced AI in customer care raises newconsiderations, especially regarding the manipulation of emotions and privacy (Huang andRust, 2024).Bock et al. (2020) emphasize the importance of studying the ethical implications of AI inservice. Specifically, they highlight three research opportunities: (1) Investigating howservice AI affects the ethical concerns in organizational and consumer decision-making, (2)JOSM Examining the ethical design and governance of service AI systems, and (3) Studying theneed for risk management associated with service AI.The rapid development of Feeling AI, particularly in the form of GenAI, brings forth apressing need to thoroughly study the AI implications beyond what is currently discussed inthe service literature. The ability of AI to mimic human emotional intelligence and itsintegration with various facets of our lives creates unprecedented opportunities formanipulation, misuse, invasion of privacy, and potential bias. Moreover, the widespreadadoption of AI across service industries (healthcare, education, finance, etc.) heightensconcerns regarding job displacement, transparency, fraud, and accountability. As AIpermeates various aspects of society, it becomes imperative to ensure responsible andequitable implementation.By proactively studying responsible AI, we seek to foster an inclusive future whilesafeguarding the well-being of individuals, organizations, and society. To do so, in the nextsection, we link the identified need to investigate responsible AI in service to the SDGs as setby the United Nations.",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "776eda8d-ce27-4910-a7a1-0798411e0fe4",
                    "text": "The United Nations\u2019 SDGs (or Global Goals [3]), established in 2015, serve as a comprehensivecall to action to confront global predicaments. These address issues such as poverty,inequality, climate change, environmental harm, as well as the quest for peace and justice.The SDGs are a collection of 17 interlinked objectives designed to serve as \u201cshared blueprintfor peace and prosperity for people and the planet, now and into the future\u201d (United Nations,2023). The SDGs seek to guide the member states and their partners, including non-profitsand commercial brands, \u201cto achieve inclusive, people-centered, and sustainable development\u201d(United Nations, 2023).The 17 goals are structured around the five pillars of the 2030 Agenda: People, Planet,Prosperity, Peace, and Partnerships. These 5 Ps highlight how the SDGs are an intertwinedframework instead of a group of solo goals. The progress on one P must balance and supportthe progress on another (United Nations Foundation, 2023).(1) People: The \u201cPeople\u201d pillar of the SDGs emphasizes the need to improve societal well-being through better healthcare, top-tier education, and equal employmentopportunities for all people. The ultimate goal of the SDGs is to end poverty andhunger in all their forms and dimensions and to ensure that all human beings canfulfill their potential in dignity and equality within a healthy environment. The\u201cPeople\u201d principle promotes social inclusion and seeks to address issues of inequalityand discrimination. It reflects a commitment to ensure equal opportunity and reduceinequalities of outcome, including thorough measures to eliminate discriminatorylaws, policies, and practices (Saviano et al., 2017).(2) Planet: The \u201cPlanet\u201d pillar emphasizes the importance of sustainable production andconsumption patterns, the incorporation of climate change measures into nationalstrategies, and the preservation of biodiversity. It focuses on protecting the naturalresources and ecosystems upon which we and future generations will depend. TheSDGs seek to protect, restore, and promote sustainable use of terrestrial ecosystems,sustainably manage forests, combat desertification, halt and reverse landdegradation, and halt biodiversity loss (Field et al., 2021). It addresses urgentactions to combat climate change and its impacts. Additionally, it emphasizesconserving and using oceans, seas, and marine resources sustainably.(3) Prosperity: The \u201cProsperity\u201d pillar acknowledges that economic growth is crucial forprosperity but also highlights that this growth should be sustainable (Aksoy et al., 2019).Journal of ServiceManagementIt reflects a commitment to ensuring that all people enjoy prosperous and fulfilling livesand that economic, social, and technological progress occurs in harmony with nature.This involves a call for building resilient infrastructure, promoting inclusive andsustainable industrialization, and fostering innovation. It also includes goals to ensuresustainable production and consumption patterns.(4) Peace: The \u201cPeace\u201d pillar seeks to foster peaceful, just, and inclusive societies that arefree from fear, crime, and violence (Beutler, 2008). Peace relates not only to theabsence of conflict but also to the presence of justice and the upholding of humanrights. A peaceful society ensures justice and equality for all. It encompasses the goalsof promoting peaceful and inclusive societies for sustainable development, providingaccess to justice for all, and building effective, accountable, and inclusive institutionsat all levels.(5) Partnership: The \u201cPartnership\u201d pillar recognizes the importance of collaboration andcooperation in achieving the SDGs. The SDGs can only be realized with a strongcommitment to global partnership and cooperation (Stott and Murphy, 2020). Thisapproach mobilizes and shares knowledge, expertise, technology, and financialresources to support the SDGs\u2019 achievement, particularly in developing countries.This pillar emphasizes that our ability to effectively address issues like poverty,inequality, and climate change requires collaboration between governments, theprivate sector, civil society, and individuals.Every pillar of the 5Ps framework is key to shaping sustainable procedures and policies. Asmore service organizations, such as hotels, hospitals, restaurants, and law firms, integratetheir operations with AI technologies, we argue that understanding and linking to thesepillars is critical for practicing responsible AI applications. This would ensure that AI-enabled services not only improve efficiency and user experiences but also contributepositively to global sustainability and social equity.",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "358e0d6f-adb1-4ce1-9213-68d44305251d",
                    "text": "While the direct engagement with the SDGs in serviceresearch has been somewhat limited, efforts by entities such as ServCollab and the Journal ofServices Marketing are starting to fill this void, indicating a promising shift towardsintegrating these global objectives more explicitly into service research agendas. The recentscholarly synthesis undertaken by ServCollab [4] in conjunction with the Journal of ServicesMarketing (see Russell-Bennett et al., 2023), distills the United Nations\u2019 17 SDGs into seventhematic areas pertinent to service research. This distillation further informs ourconceptualization of the RAISE framework.By prioritizing ethical considerations and human-centric approaches, responsiblepractices of AI can significantly contribute to the SDGs by fostering the creation anddelivery of services that not only enhance the well-being (Theme 1) and opportunities for allindividuals (Theme 2) but also ensure the sustainable management of resources throughregenerative economic frameworks (Theme 3); promote equitable economic growth (Theme4); support institutions in providing fair and sustainable living conditions (Theme 5);integrate environmental objectives within service ecosystems (Theme 6); and facilitatecollaborative efforts towards achieving global sustainability goals (Theme 7). We underscorethat the transformative potential of AI can be fully realized only through earnest andstrategic transformative collaboration across all service entities within the service ecosystem.These collaborative efforts must be anchored in a humanistic and forward-thinking designperspective, striving not only to meet immediate service needs but also to regenerate andsustain the service ecosystems of our planet. We argue that by incorporating responsible AIprinciples within service strategies, we can align with this service research agenda\u2014drivinginnovations that enhance health, education, economic growth, and environmentalJOSM stewardship. Consequently, such initiatives contribute to the SDGs and ensure thattechnological advancements foster service inclusion (Fisk et al., 2018), digital inclusion (Fisket al., 2023), resource efficiency, and sustainable growth. Together, this epitomizes thecollaborative ethos essential for tackling global challenges and elevating the overall humanexperience (Fisk et al., 2020).",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "48370e00-324e-4854-9bfb-8f27bde605ac",
                    "text": "While the SDGs motivate the need for responsible AI practices, we now direct our attentiontowards the current debate surrounding the concept of responsible AI and its key dimensions.The rapid expansion of AI, facilitated by the exponential growth of data and computingcapabilities, has given rise to the field of AI ethics. This field examines the ethical and societalissues faced by developers, producers, consumers, citizens, policymakers, and civil societyorganizations. Initially, AI ethics focused on speculative scenarios like superintelligence andthe ethics surrounding such scenarios. The second wave of AI ethics addressed practicalconcerns related to machine learning techniques, such as the opaqueness of black-boxalgorithms, the challenge of explainability, biases arising from unequal representation intraining data, and the implications of facial and emotion recognition systems on privacyrights.In light of the growing research on the dark side of AI (Anagnostou et al., 2022; Mikalefet al., 2022) ranging from gender bias in emotion recognition (Domnich and Anbarjafari, 2021)to privacy breaches through the collection and use of personal information (Curzon et al.,2021), the concept of responsible AI in the context of business has emerged. Responsible AI inthe business context refers to a set of ethical principles that must be adhered to whenapplying AI (Jakesch et al., 2022; Mikalef et al., 2022; Trocin et al., 2021). While there arediverse definitions of responsible AI, it is crucial to emphasize its impact on a vast array ofstakeholders, including organizations, AI developers, policymakers, customers, andresearchers (Deshpande and Sharp, 2022). Indeed, responsible AI goes beyond theprofitability of AI adoption. Overall, not only should responsible AI in the businesscontext suggests that an organization adopting AI consider the business consequences of theadoption (i.e. whether the adoption is profitable for the business), but also, it should take intoaccount how it might impact a broader set of stakeholders (i.e. whether the adoption does notharm or even benefits various stakeholders) and eventually how it might contribute to socialgood (i.e. whether the adoption helps the organization make the world a better place).Despite extensive research on responsible AI principles, many organizations still strugglewith the practical implementation of responsible AI (Dignum, 2019; Hagendorff, 2022). Thisgap between academia and industry arises due to the challenge of translating high-levelacademic discussions into concrete action plans. Stakeholders hold different interpretationsof responsible AI, leading to discrepancies and confusion (e.g. Jakesch et al., 2022). To bridgethis gap, a systemic framework based on key dimensions of responsible AI can offer muchneeded practical guidance for organizations.Efforts have been made by AI system designers, developers, and implementers toestablish concrete principles for responsible AI (e.g. Cheng et al., 2021). The Ethically AlignedDesign, First Edition (EAD1e) by the IEEE (2019) Global Initiative on Ethics of Autonomousand Intelligent Systems is an exemplary initiative that provides conceptual pillars andspecific steps for responsible AI design. However, these principles for those who design AIoften require further translation for organizations that adopt AI. In this vein, academiccontributions such as Jobin et al. (2019) [5] offer criteria that can assist in bridging the gapbetween high-level academic discourse and the practical implementation of responsible AI.In their endeavor to establish a comprehensive definition of responsible AI and proposeethical standards that can be universally applicable, Jobin et al. (2019) conduct a thoroughJournal of ServiceManagementanalysis of prevalent discussions among various existing global guidelines on responsibleAI. Their study systematically examined 84 articles that focused on ethical guidelines orprinciples in the realm of AI. To achieve this, they utilized a methodical protocol following thePRISMA framework, which underwent preliminary testing and calibration before datagathering commenced. To ensure a comprehensive and methodical approach, they employeda multi-level screening process. This involved both inductive exploration through searchengines and deductive pinpointing of pertinent organizations with associated onlinedatabases and websites. This effort results in the identification of five principles forjustice and fairness, and non-responsible AI: privacy, transparency, accountability,maleficence.According to Jobin et al. (2019), privacy refers to the protection of consumer\u2019s personalinformation. This requires compliance with the personal data usage regulation and theprotection of consumer privacy through consumer consent. Transparency requires thatorganizations are transparent about their use of AI and provide explicit explanations of howit works \u2013 specifying which data is being utilized. Such entities should both inform andeducate consumers about their usage of the data. Accountability allows for the identificationof stakeholders affected by this data utilization and creates ownership of responsibility forthose going to be affected; thereby, the implementation of checks and balances becomes partof the process. Justice and fairness should be regularly assessed and serve as inclusive andresponsive systems to reduce bias. Lastly, non-maleficence puts in place safeguards tomanage unintended usage of data to minimize the potential for unintended consequences tooccur if/when data might be misused.In this paper, we rely on the five principles for responsible AI outlined by Jobin et al. (2019)as a foundational basis for designing the RAISE framework, ensuring our approach is deeplyrooted in established principles of responsible AI.",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "cc640ea0-03d9-4b2a-baf8-70cbcf8fae80",
                    "text": "With the increasing reliance on AI in service, there is an urgent need to practice responsibleprinciples respecting human rights and prioritizing societal well-being, as highlighted by theSDG and the responsible AI dimensions (Jobin et al., 2019). By synthesizing the knowledgegained from the SDGs and the responsible AI dimensions, we developed the RAISEframework (Figure 1).We define RAISE as \u201ca strategic framework for responsibly integrating artificial intelligenceinto service industries. It emphasizes collaborative AI design and deployment that aligns with theevolving global standards and societal well-being while promoting business success andsustainable development.\u201d The RAISE framework, as depicted in Figure 1, represents aholistic model for integrating responsible AI for service excellence.The RAISE framework consists of three key components:(1) Ethical Foundations: At its core, the framework is built upon the \u201cEthicalFoundations\u201d that comprise Justice and Fairness, Transparency, Accountability,Privacy, and Non-maleficence as identified by Jobin et al. (2019). These principlesserve as the bedrock for ethical considerations in AI applications, ensuring that AItechnologies are developed and utilized in ways that are fair, open, and respectful ofindividual rights and societal norms. This component forms the bedrock of theframework, represented visually by a foundational semi-circle that symbolizes theencompassing nature of ethics in AI.(2) Service Entities: Positioned at the base of the framework and visually represented bya matching semi-circle to signify the various stakeholders integral to the AI servicelandscape. Specifically, service organizations refer to businesses, and institutionsJOSM R AI S ESource(s): The above figure was created by the authorsleveraging technology and AI to enhance effectiveness, efficiency, and quality ofservice. Policymakers are individuals, groups, and/or government entities responsiblefor formulating, implementing, and regulating policies related to AI technology andtheir applications. AI developers are inclusive of professional experts involved in thedesign, build, and implementation of AI systems and applications. Researchers areprofessionals working in the field of AI to advance the understanding, development,and applications of AI technologies. Lastly, Customers refer to individuals,organizations, and entities that utilize AI technology to solve problems, enhanceoperations, and/or achieve specific goals. These key service entities form the serviceecosystem responsible for the rapid growth of AI and, therefore, play a crucial role inpushing the boundaries of AI capabilities, while also shaping the ethical, legal, andsocietal implications of AI adoption and transformation.(3) RAISE Principles: Positioned at the center of the framework, the three RAISEprinciples represent the actional aspect of the framework:Embrace AI to Serve the Greater Good: This practice emphasizes the importanceof leveraging AI technology with a focus on benefiting society and contributingpositively to the greater good.Design and Deploy Responsible AI: This practice refers to the careful planningand execution of AI systems with responsibility, ensuring they adhere to ethicalstandards and are beneficial and fair.Practice Transformative Collaboration: This practice highlights the need forcollaborative efforts among different stakeholders to create significant andpositive changes through the use of AI.The circular arrangement of the arrows in the RAISE framework symbolizes a continuous,iterative process. Within this process, the embrace of AI for the greater good, the responsibleJournal of ServiceManagementdesign and deployment of AI, and the practice of transformative collaboration are not linearstages. Instead, they are interdependent elements that feed into and reinforce each other. Thisreinforcement is based on the evolving global standards of ethical foundations and thecollaboration of service entities. These factors ensure a dynamic and ongoing commitment toethical principles in AI-driven service excellence.In summary, the RAISE framework depicts a coherent and holistic approach thatelucidates how ethical foundations, service entities, and core principles for practice interlock tofoster responsible AI for service excellence. It serves as a guide for integrating AI in a mannerthat is ethically sound, strategically effective, and conducive to collaborative innovation.In the next section, we discuss in greater depth the three RAISE principles for practice: 1.Embrace AI to serve the greater good, 2. Design and deploy responsible AI, and 3. PracticeTransformative Collaboration.",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "165c0a92-b620-4155-8fec-d84930cd026e",
                    "text": "The pursuit of peaceful, fair, and inclusive societies, free from fear and violence, is central tothe SDGs\u2019 objectives. Similarly, the concept of the \u201cgreater good\u201d refers to what is consideredthe most beneficial action for society as a whole. In this context, RAISE serves as a strategicframework for service organizations, helping them realize their important role in contributingto the greater good through AI.The first principle of RAISE emphasizes the role of service organizations in creating asafer, fairer, and more inclusive environment for all stakeholders through AI. Hotels, forinstance, can prioritize the usage of AI for security enhancements, fraud detection, and tocreate personalized, stress-free experiences for guests. Hospitals can leverage AI to improvepatient care, ensure fair allocation of resources, and safeguard sensitive patient data.Restaurants can use AI to foster fair employment practices, improve customer experiences,and maintain high standards of food safety. Law firms can employ AI for unbiased legalanalysis, fraud detection, and to ensure transparency in their operations. By adopting RAISE,service organizations prioritize the greater good in their AI applications. This will ensure thatAI is implemented in a way that advances peace, justice, and inclusivity.Similarly, AI holds significant potential to stimulate economic growth, innovation, andinfrastructure development, providing an unparalleled opportunity for service organizationsacross various sectors. AI can be instrumental in driving the \u201cProsperity\u201d element of theSDGs by streamlining business operations, enhancing service delivery, and ignitingeconomic prosperity. However, while AI can fuel prosperity, it may also widen the gapbetween the affluent and the less privileged (Fisk et al., 2023). Through RAISE, servicecompanies can strive to implement AI solutions that promote inclusive growth and economicequality.Within our definition of RAISE, we emphasize that service organizations should utilize AI inways that actively contribute to the overall well-being of various stakeholders, extendingbeyond the narrow confines of the organization\u2019s self-interest. This perspective resonates withcontemporary notions of corporate social responsibility (Gebauer and Reynoso, 2013), sharedvalue (Kramer and Porter, 2011), and social innovation in service (Aksoy et al., 2019)emphasizing service organizations accountability beyond profit maximization.To facilitate a nuanced comprehension of the various ways service organizations canadopt AI, we propose a categorization of roles along the RAISE Spectrum (Figure 2).The RAISE Spectrum is a framework designed to guide organizations in the responsibleintegration of AI. It is visualized as a gradient (ranging from red (undesirable position) togreen (desirable position), two-dimensional matrix that categorizes entities based on their\u201cFocus on Business Success\u201d and \u201cFocus on Social Good\u201d.JOSM Source(s): The above figure was created by the authorsThe horizontal axis represents the \u201cFocus on Social Good\u201d, ranging from low to high, whilethe vertical axis denotes the \u201cFocus on Business Success\u201d, again from low to high. Theframework is presented as a continuous spectrum, indicating that the focus on these twodimensions is not binary but rather a scale where entities can find themselves at any point,reflecting the fluid nature of their practices and priorities.Within this spectrum, four quadrants are identified, each representing a distinct approachto AI in service:(1) AI Exploiter: organizations falling in this quadrant leverage AI technologiesprimarily for business gains, potentially at the expense of social good. They mayprioritize profitability and efficiency over ethical considerations, potentially riskingreputational damage and societal backlash.(2) AI Lounger: organizations falling in this quadrant have a low focus on both socialgood and business success. They may lack a comprehensive understanding of how toharness AI\u2019s full potential and the diverse ways AI can impact a broad spectrum ofstakeholders. This incomplete grasp can impede their ability to optimize AI for bothprofit and social good. This may indicate a lack of initiative or strategy in leveragingAI for meaningful impact, resulting in underperformance in both areas.(3) AI Idealist: organizations falling in this quadrant are characterized by a high focus onsocial good but may not have fully realized the business potential of AI. They mayprioritize ethical considerations and societal impact, potentially at the expense offinancial outcomes. Focusing exclusively on social good, to the detriment ofprofitability, can threaten the long-term viability of a business.(4) RAISE Adopter: organizations falling in this quadrant excel in both business successand social good. They demonstrate how responsible AI can be leveraged to not onlyenhance business performance but also contribute positively to society, embodyingthe ideals of the RAISE framework.By introducing the RAISE Spectrum, we encourage organizations to aspire towards theRAISE Adopter quadrant\u2014where the responsible use of AI aligns with both ethicalimperatives and business objectives. It also underscores that these goals are not mutuallyexclusive; instead, it highlights that they are complementary, with each element reinforcingthe other. Finally, the RAISE Spectrum serves as a tool for self-assessment and strategicplanning, helping organizations to visualize their current position and chart a course towardsgreater social and business impact through AI. Table 1 illustrates examples of companiesthat have achieved a harmonious balance, known as \u201cRAISE Adopters.\u201dIn conclusion, the first principle of the RAISE framework \u201cEmbrace AI To Serve theGreater Good\u201d underscores the ethical imperative of AI utilization within serviceorganizations, encouraging them to pursue business success as well as societal well-being.Service entities are encouraged to become \u201cRAISE Adopters\u201d realizing their full potential asresponsible AI-driven organizations that create value for themselves and society at large.",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "4e752a02-c61a-436a-938a-6c5fcbb3138b",
                    "text": "In the introduction, we describe RAISE as \u201ca practice-focused framework aimed atempowering service entities to responsibly integrate AI into their operations.\u201d In this regard,we posit that it is the responsibility of various service entities to design and deploy AIsystems that align with this principle. Table 2 provides a detailed account of the practicalstrategies that could be employed by each service entity to foster the deployment ofresponsible AI, ensuring that the development and application of AI technologies areconducted with the utmost regard for ethical considerations and societal impact.Table 2 presents a structured compilation of recommended practices for differentstakeholders in the AI ecosystem to ensure the responsible deployment of AI technologies.For AI Developers, it suggests the implementation of privacy-by-design principles, regular Journal of ServiceManagementJOSM Journal of ServiceManagementJOSM Journal of ServiceManagementupdates to address vulnerabilities, and the acceleration of techniques that enhance AItransparency while preserving data privacy. Researchers are encouraged to explore privacy-enhancing technologies, study AI transparency methods, and develop models for responsibleAI governance. Policymakers are tasked with developing privacy regulations, settingtransparency and fairness standards, and creating legal frameworks to ensure AIaccountability. Table 2 outlines a proactive approach for each group, aiming to foster AIapplications that are ethically sound, socially responsible, and beneficial to all stakeholders,thus aligning with the RAISE framework\u2019s commitment to responsible AI, the greater good,and service excellence.",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "053a8571-b3a5-4680-b179-dce189363421",
                    "text": "Besides the individual practices that each service entity should strive to implement (seeTable 2), the RAISE framework calls for practicing transformative collaboration betweenservice organizations, policymakers, AI developers, customers, and researchers to ensure AIapplications are transparent and positively contribute to societal well-being. Fisk et al. (2018)maintain that \u201ctransformative collaboration occurs when all participants are able to makecontributions at their full human potential\u201d (p. 198). Transformative collaboration in the AIrealm calls for a convergence of resources, expertise, and perspectives across diverse actors,thereby enriching the comprehensiveness and inclusivity of AI deployment frameworks.Such collaboration encompasses academia, industry, government, and civil society, fosteringa rich understanding of AI\u2019s socio-ethical implications and regulatory needs. As such,transformative collaboration is crucial for sharing knowledge, formulating best practices,and developing ethical and regulatory frameworks essential for AI governance. We arguethat this synergy could accelerate the development of strategies to manage risks andmaximize societal benefits of AI (Sinha et al., 2024), yet a Deloitte study (2023) indicateslimited corporate engagement in such partnerships, underscoring the necessity to amplifycollaborative initiatives. This highlights the need for increased efforts to foster partnershipsand dialogue among service entities to effectively address concerns in AI development anddeployment (Deloitte, 2023).In practice, transformative collaboration translates into AI developers working withservice organizations to understand their specific ethical priorities, needs, and challenges.They can also work with policymakers to ensure that their AI systems comply with allapplicable laws and regulations and take a proactive role in securing ethical guidelines priorto adoption and implementation. Similarly, service organizations can partner with AIdevelopers to create and implement AI systems that are aligned with their responsible AIprinciples. For instance, Microsoft\u2019s Responsible AI Standard establishes criteria andfurnishes practical guidance, tools, and methodologies for staff to integrate responsible AIprinciples into their daily operations. In tandem with this standard, Microsoft has instituted aResponsible AI Impact Assessment, designed to assess the potential impacts of AI systemson individuals, entities, and broader society. Furthermore, hotels might partner with techcompanies and environmental organizations to develop AI systems that optimize energyusage, contributing to sustainable tourism. They can also collaborate with customers toeducate them about the responsible use of AI. Moreover, customers can provide feedback toservice organizations and AI developers, while also advocating for policies that promoteresponsible AI through discussions with policymakers. Lastly, researchers and policymakersneed to collaborate and share their knowledge and expertise on responsible AI. Only byworking together can these key service entities help to ensure that AI is used in a way thatbenefits everyone and advances the SDGs.Central to these endeavors is adopting collaborative practices such as data minimization,anonymization, robust encryption methodologies, and strict adherence to regulatoryJOSM frameworks governing data protection. Comprehensive training protocols ensure proficiencyamong all stakeholders while using AI models and meticulous documentation fosterscomprehension and trustworthiness within collaborative endeavors. Moreover, the periodicdissemination of transparency reports serves to cultivate a culture of openness andaccountability facilitating transformative collaboration across governmental, private sector,and civil society entities necessitates a framework that not only encourages participation butalso establishes mechanisms to enforce accountability in case of non-compliance.Furthermore, a paradigm shift in educational methodologies and workplace dynamics isadvocated to engender fairness, inclusion, and adaptability environments. Mobilization andequitable dissemination of knowledge, expertise, technological advancements, and financialresources are pivotal for enriching the transformative collaboration in the AI realm andhighlighting the indispensable role of collective action in realizing the full potential ofresponsible AI.",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                },
                {
                    "id": "40e2767e-43ca-4aa5-9d38-fad90f291775",
                    "text": "In this article, we introduce a novel, theoretically grounded strategic framework calledResponsible AI for Service Excellence (RAISE). We define RAISE as a strategic frameworkintelligence into service industries. It emphasizesfor responsibly integrating artificialcollaborative AI design and deployment that aligns with the evolving global standards andsocietal well-being while promoting business success and sustainable development. TheRAISE framework is operationalized through three core principles: Embracing AI to servethe greater good, designing and deploying responsible AI, and practicing transformativecollaboration across service entities. \u201cEmbracing AI to Serve the Greater Good\u201d signifies acommitment to harnessing AI for societal benefits. In this regard, the framework encouragesservice organizations to become RAISE Adopters, aligning their AI strategies with bothbusiness objectives and ethical imperatives, thus realizing their full potential as responsibleAI-driven organizations that create value for society. \u201cDesigning and Deploying ResponsibleAI\u201d involves developing AI systems that adhere to ethical standards and societalexpectations. It requires service entities to employ strategies that safeguard privacy,enhance transparency, and ensure AI systems are designed with the well-being of allstakeholders in mind. This principle ensures that AI technologies are not only beneficial butalso equitable and socially responsible. \u201cPracticing Transformative Collaboration\u201dencompasses building synergies between service organizations, policymakers, AIdevelopers, customers, and researchers. It emphasizes the importance of leveragingcollective expertise to create AI applications that are ethically sound and contributepositively to societal well-being. Such collaboration is essential for sharing knowledge,establishing best practices, and accelerating the development of strategies to manage AI-related risks and maximize benefits. In conclusion, as we venture forward, we urge actorsacross various fields and industries to champion responsible AI as a fundamental element indriving innovation, fostering equity, and ensuring sustainable growth. This collectiveendeavor is crucial for setting new benchmarks for excellence and ensuring a future wheretechnology enhances the well-being of society and the natural environment alike.Journal of ServiceManagementJOSM Journal of ServiceManagementJOSM",
                    "reference": "[1] Laurel Alkire, Ahmet Bilgihan, Mai Bui, Anthony J. Buoye, and others. 2024. RAISE: leveraging responsible AI for service excellence. Journal of Service ... emerald.com. Retrieved from https://cba.lmu.edu/media/lmucollegeofbusinessadministration/documents/RAISE_2024.pdf"
                }
            ]
        },
        {
            "paper_title": "A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI",
            "authors": "VS Barletta, D Caivano, D Gigante\u2026",
            "publication_info": "Proceedings of the 27th \u2026 - dl.acm.org",
            "paper_url": "https://arxiv.org/pdf/2306.05003",
            "chunks": [
                {
                    "id": "efaa1e51-954a-404c-bffd-837f864ab4bc",
                    "text": "In the last years, the raise of Artificial Intelligence (AI), and its pervasiveness in our lives, has sparkeda flourishing debate about the ethical principles that should lead its implementation and use in society.Driven by these concerns, we conduct a rapid review of several frameworks providing principles,guidelines, and/or tools to help practitioners in the development and deployment of Responsible AI(RAI) applications. We map each framework w.r.t. the different Software Development Life Cycle(SDLC) phases discovering that most of these frameworks fall just in the Requirements Elicitationphase, leaving the other phases uncovered. Very few of these frameworks offer supporting toolsfor practitioners, and they are mainly provided by private companies. Our results reveal that thereis not a \"catching-all\" framework supporting both technical and non-technical stakeholders in theimplementation of real-world projects. Our findings highlight the lack of a comprehensive frameworkencompassing all RAI principles and all (SDLC) phases that could be navigated by users with differentskill sets and with different goals.Keywords artificial intelligence \u00b7 software engineering \u00b7 responsible ai \u00b7 development frameworks \u00b7 literature review",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "df70e625-a93d-4cca-9f37-98258d1cdfc0",
                    "text": "Artificial Intelligence (AI) is a revolution that is reshaping science and society as a whole Harari [2017]. WhileAI-related technologies are changing how data is processed and analyzed Jordan and Mitchell [2015], autonomousand semi-autonomous decision systems are being used more frequently in several industries, such as healthcare,automotive, banking, and manufacturing, just to cite a few Cornacchia et al. [2021]. Given AI revolutionary potentialand wide-ranging social influence, there has been a lot of discussion regarding the values and principles that shouldlead its development and application Vayena et al. [2018]Awad et al. [2018]. Recent scientific research and mediaattention have been focused on concerns that AI may endanger the jobs of human workers n.d. [2017], be abused bymalicious actors Brundage et al. [2018], avoid responsibility, or accidentally spread bias and as so, erode fairness Zouand Schiebinger [2018].In this context, the concept of Responsible Artificial Intelligence (RAI) was defined: \"intelligent algorithms thatprioritize the needs of all stakeholders as the highest priority, especially the minoritized and disadvantaged users,in order to make trustworthy decisions. These obligations include protecting and informing users, preventing andA Rapid Review of Responsible AI frameworksmitigating negative impacts, and maximizing the long-term beneficial impact. (Socially) Responsible AI Algorithmsconstantly receive feedback from users to continually accomplish the expected social values\" Cheng et al. [2021].Several public and private organizations have responded to these societal fears by developing different kinds of resources:ethical requirements, principles, guidelines, best practices, tools, and frameworks. In this work, we conducted a RapidReview (RR) of various frameworks addressing the principles of RAI proposed in the literature by different types ofinstitutions. In particular, we investigated frameworks giving practical guidance and support to all types of stakeholdersinvolved in the implementation and validation of AI applications, also with reference to the Software Development LifeCycle (SDLC).In this work we investigated the state of the practice on how comprehensive and complete RAI frameworks are interms of principles addressed and SDLC phases covered, as well as if there are tools complementing the theoreticalframework helping practitioners in the different phases of the development lifecycle. Following to our Rapid Review,our main finding is that there is a concerning shortage of tools supporting the design, implementation, and auditing ofthe RAI principles both by technical and non-technical stakeholders. Further research should focus on the developmentof a comprehensive and simple-to-use framework for RAI whose knowledge can be navigated and exploited by AIpractitioners to speed up the adoption of RAI practices in real-world projects.The remainder of the paper is organized as follows: Section 2 defines some background definitions useful for this study;Section 3 describes the protocol we adopted to conduct the rapid review, including research questions and the researchstrategy adopted; Section 4 provides the results of our rapid review and answers the research questions; Section 5discusses the results highlighting the most important findings; Section 6 addresses threats to validity and finally theconclusion and future work are drawn in Section 7.",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "ca922b4b-50a7-4d6f-9fc6-457923c8ad20",
                    "text": "We provide some preliminary definitions to better understand the concepts that guided this work.",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "dd5be5c5-d2a0-4a7b-a6e6-aaf0c17d7755",
                    "text": "National and international organizations have created ad-hoc expert groups on AI to address the risks connected withthe development of AI, frequently with the task of generating policy documents. These organizations include, amongothers, the High-Level Expert Group on Artificial Intelligence established by the European Commission , the UNESCOAd Hoc Expert Group (AHEG) for the Recommendation on the Ethics of Artificial Intelligence , the Advisory Councilon the Ethical Use of Artificial Intelligence and Data in Singapore , the NASA Artificial Intelligence Group and theUK AI Council , just to cite a few.These committees have been appointed to produce reports and guidelines about RAI. Similar initiatives are being madein the commercial sector, particularly by businesses that depend on AI. Businesses like Sony and Meta made their AIpolicies and principles available to the public. At the same time, professional organizations and no-profit groups likeUNI Global Union and the Internet Society have all released statements and recommendations.The significant efforts of such an ample group of stakeholders to develop RAI principles and policies not only show theneed for ethical guidance but also point out their keen interest in reshaping AI ethics to suit their individual prioritiesGreene et al. [2019]. Notably, the private sector\u2019s participation in the field of AI ethics has been questioned since it maybe using high-level soft policy as a portmanteau to either make a social issue technical Greene et al. [2019] or avoidregulation altogether bay [2018], Jobin et al. [2019].However, many research works highlighted how these proposals often diverged, giving different definitions, resulting inthe problem known as principle proliferation Floridi and Cowls [2019]. Consequently, several in-depth investigationshave been conducted, such as the one by Jobin et al. Jobin et al. [2019], who found a global convergence aroundfive ethical principles: transparency, justice and fairness, non-maleficence, responsibility, and privacy. Jobin et al.2A Rapid Review of Responsible AI frameworksJobin et al. [2019] stated that no one of these ethical principles is present in all the documents they reviewed; however,these five principles are mentioned in more than half of all the sources reviewed. Moreover, further in-depth thematicanalysis revealed notable semantic and conceptual divergences in interpreting these principles and in the particularrecommendations or areas of concern drawn from each of them.",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "46c296f7-8767-4d51-90df-c6c2b82a6cd8",
                    "text": "As highlighted in Section 2.1, there are a lot of uncertainties and nuances around the definition of the principles thatmainly characterize Responsible AI, as well as, about the definition of RAI itself. Indeed, sometimes it is referred toas Trustworthy or Ethical AI. In our work, we address the problem of principle proliferation deciding to focus on aspecific subset of those characterizing RAI, in particular, the four principles identified by Jobin et al. Jobin et al. [2019]with the exclusion of responsibility as this concept is rarely defined in a clear manner.Moreover, to give an authoritative and clear definition for each principle, we decided to use the ones provided by theHigh-Level Expert Group on Artificial Intelligence established by the European Commission in their Ethics guidelinesfor trustworthy AI High-Level Expert Group on AI (AIHLEG) [2018].In the following we report the selected definitions for each principle. We matched the principles in High-Level ExpertGroup on AI (AIHLEG) [2018] with the ones that emerged in Jobin et al. [2019], and when the naming convention isnot exactly the same we highlighted that in parenthesis. We also mapped principles to system requirements.This principle includes and can be directly linked with system requirements such as traceability and explainability.3A Rapid Review of Responsible AI frameworksThis principle can be directly mapped with system requirements such as avoidance of unfair bias, accessibility anduniversal design and include stakeholder participation.This principle can be directly mapped with system requirements such as resilience to attack and security, fallback planand general safety, accuracy, reliability, and reproducibility.This principle can be directly mapped with system requirements such as including respect for privacy, quality andintegrity of data and quality and integrity of access to data.",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "6cef03fb-7824-43d1-88c5-1925fc3f648e",
                    "text": "In this work, we focus on frameworks that implement the above-mentioned RAI ethical principles.The concept of framework is far well-known in the Software Engineering (SE) field. Already in 1997, Johnson et al.Johnson [1997] referred to frameworks as \"an object-oriented reuse technique\" or \"the skeleton of an application thatcan be customized by an application developer\". These are not conflicting definitions; the first describes the structure ofa framework while the second describes its purpose.Shifting the focus from SE to a more general context, frameworks are a form of design reuse. Frameworks can beconsidered a collection of suggestions, guidelines and tools to be followed in order to create a product compliant with adefined standard.",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "7746c60a-d073-4f26-a1bf-5226b740e1e3",
                    "text": "Rapid Reviews (RRs) have emerged as a streamlined approach for synthesizing evidence quickly, initially intending tohelp decision-makers in health care to respond promptly to urgent and emerging needs Konnyu et al. [2012]. Rapidreviews simplify systematic review methods by focusing on the literature search while still aiming to produce validconclusions Watt et al. [2008].To perform this rapid review, we followed the protocol proposed in Cartaxo et al. [2018], and we complemented theRapid Review process with the strategies presented in Kitchenham and Charters [2007] for performing systematicliterature reviews. The following subsections describe in detail the study design and its execution.4A Rapid Review of Responsible AI frameworks",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "0a9cc1d0-f1eb-4fda-8acc-d13809752cf7",
                    "text": "The rapid literature review presented in this work was carried out through the following steps:1. Goal and Research questions: the goal and the correlated research questions were identified to guide theliterature review;2. Search strategy: defining the strategy to collect previous works published in the literature, including researchdatabases and query strings;3. Eligibility criteria definition: the criteria used to filter the collected studies have been defined;4. Data extraction: defining how relevant data were extracted to help answer the research questions;5. Data synthesis: defining how to organize extracted relevant data to answer the research questions.Figure 1: Research protocol used in this rapid review.Fig. 1 summarizes the review protocol.",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "e508e80d-09a9-46f2-af82-0522fe8a141f",
                    "text": "The goal of our study is to gather, organize, and analyze the RAI frameworks proposed in the literature by public andprivate institutions to investigate, first, how much comprehensive they are in terms of principles addressed and SDLCphases covered. Then, we focus specifically on frameworks offering tools that can be used in the implementation andauditing phase of the decision systems offering practical guidance to practitioners.Based on this goal, we defined the following research questions:\u2022 RQ1: What are the Responsible AI frameworks proposed in the literature?\u2022 RQ2: How much do these frameworks address the various RAI principles?\u2022 RQ3: Do these frameworks provide recommendations for each phase of the Software Development Life Cycle(SDLC)?\u2022 RQ4: Is there a supporting tool for each proposed framework?We recall that we decided to focus our research on the most cited RAI principles Jobin et al. [2019]:Transparency, Diversity & Non-discrimination & Fairness, Technical Robustness & Safety, and Privacy & DataGovernance. 5A Rapid Review of Responsible AI frameworks",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "b26b825b-8306-41ff-9183-5edf41b2fff2",
                    "text": "As recommended in Cartaxo et al. [2018], to abbreviate the search for primary studies and conduct the rapid reviewwithin the available time, we used only Scopus and Google Scholar as white literature search engines.To improve the search string, we conducted pilot searches and we excluded those keywords that did not yield coherentsearch results (e.g., the knowledge base keyword). After some trials, we found the search string presented in thefollowing box that returned several relevant papers.The selection string was run on 15 November 2022 at 12:40 and initially produced 1875 document results on Scopus.Then, some filters were applied: we selected only \"Computer Science\", \"Social Sciences\", \"Engineering\" and \"Mathe-matics\" subject areas and selected only documents written in English or Italian; these filters reduced the total amount ofresults to 1489. After a few days, the same string has been run on Google Scholar - filtering the results by \"Reviewarticles\" type - and produced 91200 results.The classification process of the documents collected by Google Scholar stopped after 20 pages because, starting fromthe 21st, the documents did not include all the keywords contained in the search string and/or were not coherent withthe research objective; a total of 200 documents collected by Google Scholar were analyzed.Later, to further enhance the quality of our research, we moved to analyze grey-literature sources. First of all, AlgorithmWatch AI Ethics Guidelines Global Inventory database was queried to identify the business companies which proposedRAI resources. Here, three types of resources were discarded:\u2022 The ones whose link was no more working and the same resource could be not found even searching onGoogle.com search engine;\u2022 Unofficial documents, e.g. unofficial translations;\u2022 Documents not written in English or Italian.In this stage, for each resource found on Algorithm Watch, an in-depth investigation has been conducted on theproponent entity\u2019s website to discover useful resources (and frameworks) not indexed on Algorithm Watch AI EthicsGuidelines Global Inventory.Then, the OECD database was queried as well. Here, some filters were applied:\u2022 Procedural Category, which (according to the description) is the most similar to the concept of \"framework\";\u2022 Published document or implemented in multiple projects as Tool Readiness;\u2022 Fairness, Privacy & data governance, Robustness & digital security, Transparency & explainability as Object.Finally, a keyword-based search on Google was performed in private-browsing mode, after logging out from personalaccounts and erasing all web cookies and history Piasecki et al. [2017]. The search was performed using the samesearch string used on Scopus and Scholar. This last search initially produced 21,100,000 results but, after manuallyinspecting the results contained in the first 17 pages\u2014for a total amount of 168 resources\u2014, the search engine hidsubsequent results since they were very similar to the previously inspected ones. From the Google search results, wealso discarded all the duplicates of the documents already retrieved from other data sources.All the documents obtained with this search strategy were surveyed using a 3-stages information classification process.In the first stage, only the title and keywords of the collected articles were read. In the second stage, we analyzed theabstract of each article while in the third stage we read the complete article. All these stages were conducted separatelyand in blind-view way by two of the authors. In case of a disagreement, a third author manually verified and took thefinal decision. 6A Rapid Review of Responsible AI frameworks",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "f622a2c6-3e18-48b5-b252-b6ad4d26cf24",
                    "text": "The selection procedure was based on the following criteria:1. The resource must be in English or Italian;2. The resource must be in the context of Responsible AI frameworks;3. The resource must address at least one of the chosen principles (see Section 2.2);4. The resource must provide answers to at least one of the rapid review\u2019s research questions.The full-text of every collected resource was validated against these criteria and, if everyone was satisfied, the resourcewas surveyed. The complete results list can be seen in the online appendix Barletta et al. [2023].",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "2079d28e-572d-42d7-b42c-5bd6b947f701",
                    "text": "In this step, we extracted all relevant data that could help answer any of the research questions. The extraction processwas performed by two of the authors and conflicts were solved by a third author in a blind-view way. We used aworksheet to tabulate and organize data Barletta et al. [2023].",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "c5117326-59b5-461a-8778-bb498e4f448c",
                    "text": "The RR was performed part-time from 15 November 2022 until 12 December 2022, conducting the procedure describedin Section 3.3. Table 1: Amount of documents collected grouped by research phase.DataSourceScopusGoogle ScholarAlgorithm WatchOECD DBGoogle Search Resourcesretrieved18759120016735621,100,00 Resourcesanalyzed148920016770168 Resourceselected200803810Table 1 summarises the number of relevant results obtained in each sub-phase. The search in Google Scholar did notproduce any useful results. Summing up the resources selected from the identified data sources in the last step, weended up with 148 unique resources (without duplicates).",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "00492eb3-ab88-4ce5-b682-862a00b99788",
                    "text": "Data were synthesized and several statistics were computed to answer the research questions (see the online appendixavailable at Barletta et al. [2023]). In Tables 2 and 3 we show an illustrative excerpt of the whole data collected.RQ1. What are the Responsible AI frameworks proposed in the literature?All the retrieved frameworks have been classified w.r.t. the type of proposing institution using three categories:COMPANIES, UNIVERSITIES, and NO-PROFIT ORG / COMMUNITIES / PUBLIC ENTITIES (NPG/COMM/PE).Moreover, if an entity proposed two (or more) frameworks, it was counted only once, as we are interested in thedistribution of the proposals by entity type.As can be seen in Fig. 2, what immediately emerges is that most of the filtered frameworks are proposed byNPG/COMM/PE (50.7%, 70/138), followed by lucrative COMPANIES (31.9%, 44/138) and then UNIVERSITIES(17.4%, 24/138).Furthermore, the frameworks have been classified into four categories according to their characteristics:\u2022 Principle (P): if they highlight only abstract ethical principles or moral values;\u2022 Guideline (G): if they are concrete guidelines, quickly translatable into design constraints or choices;\u2022 Tool (T): if they are able to verify the compliance towards one or more principles and/or support practitionersin the implementation of principles or guidelines;7A Rapid Review of Responsible AI frameworksTable 2: Excerpt of the whole data classified by RAI principles taken into consideration.Entity name Tool NameMeta Facebook\u2019s five pillarsof Responsible AIUniversity ofTexas at Austin CERTIFAI DiversityNon-discrimination& FairnessCOMPANIESYesUNIVERSITIESYes Privacy& DataGovernance TechnicalRobustness& Safety TransparencyYesNo YesYes YesYesYesNO-PROFIT ORG / COMMUNITIES / GOVERNMENT ENTITIESNIST AI RiskManagement Framework Yes Yes YesTable 3: Excerpt of the whole data classified by SDLC phase addressed.Entity name RequirementsElicitation Design Development Testing DeploymentMetaUniversity ofTexas at Austin(CERTIFAI) YesNo COMPANIESNo NoUNIVERSITIESYes Yes NoYesNO-PROFIT ORG / COMMUNITIES / PUBLIC ENTITIESNIST Yes Yes No No NoNoNo\u2022 Other (O): if a resource cannot be classified into any of these categories - e.g. a list of possible attacks againstan AI algorithm or a list of several questions to check in the design phase.In Fig. 3 the distribution of frameworks by their category and grouped by proposing institution is depicted .It is noteworthy that COMPANIES and NPG/COMM/PE mostly proposed Principles (respectively 42.59%, and59.72%); differently from this trend, UNIVERSITIES primarily proposed Guidelines (57.69%). Furthermore, w.r.t.the number of resources proposed, the percentage of Tools proposed by COMPANIES is 12.96% and 15.38% forUNIVERSITIES, while this number is very low for NPG/COMM/PE (4.17%).RQ2. How much do these frameworks address the various RAI principles?In our rapid review, we are mainly interested in analyzing frameworks that give practical support to all stakeholdersinvolved in the development and deployment of AI applications. For this reason, to answer RQ2, we discarded, whilecomputing our statistics, frameworks in the category Principle, which cover only ethical values and that do not give anyworkable advice. Furthermore, here we counted the frameworks and not the entities: if an entity proposed two (or more)frameworks, it was counted twice (or more).Finally, before diving into the statistics, we point out that here we consider a principle as \"covered\" even if it is only\"partially\" covered, i.e. if not every aspect related to that principle was addressed. For instance, we consider frameworksthat deal with privacy but only in terms of how data is acquired from users and stored, without dealing with the new\"privacy attacks\" which can be conducted against an AI algorithm, like \"model inversion\" attack Fredrikson et al.[2015].As shown in Fig. 4, in the majority of cases, the frameworks address all four RAI principles. Nevertheless, there areframeworks that cover only one (15.49%) or two principles (15.49%). In particular, when the principles addressed arethree, the most covered are Diversity & Non-discrimination & Fairness, Privacy & Data Governance, and Transparency(9.3% for COMPANIES, 15.8% for UNIVERSITIES and 14.1% for NPG/COMM/PE). While, for two principles, themost covered are Diversity & Non-discrimination & Fairness and Transparency (9.3% for COMPANIES and 16.9% forNPG/COMM/PE). The interested reader can refer to the appendix for the complete results.8A Rapid Review of Responsible AI frameworksFigure 2: Distribution by type of proposing institution.RQ3. Do these frameworks provide recommendations for each phase of the Software Development Life Cycle (SDLC)?To calculate the statistics to answer RQ3, RAI principles have been mapped to project requirements (see Section2.2) since they are actually high-level statements which put constraints on the project design. Moreover, we groupedresources proposed by each entity and mapped it w.r.t. the correspondent SDLC phase addressed. So, here, if an entityproposed two (or more) frameworks, it was counted once.Fig. 5 shows that there is a common pattern regardless of the type of the proposing entity: more than half of theframeworks provide support only for the Requirements Elicitation phase (96/174, 55.17%), while subsequent phasesare supported by very few frameworks. Anyway, COMPANIES showed more interest w.r.t. the other entities in alsocovering Development and Testing phases. Finally, the Deployment phase is the least supported by the frameworks,regardless of the type of proposing entity.In Fig. 6 we investigate how much of the SDLC is covered (i.e. how many phases are addressed) by each framework.Here, we counted the frameworks, not only the proponent entities. For frameworks proposed by COMPANIESsometimes we were not able to perform this mapping as no details on the internal core operations were provided.This is due to the fact that they offered the algorithm audit service as a lucrative service, so they are \"closed-source\"frameworks. In such cases, we excluded the frameworks from the statistics elaboration.What emerges is clear: most frameworks provide suggestions for just one of the five SDLC phases: RequirementsElicitation, while the second most supported phase Design. Only 5.79% of the frameworks supports all the SDLCphases.RQ4. Is there a supporting tool for each proposed framework?We highlight that in our analysis the presence of a supporting tool can be labeled with four values: \"Yes\", \"No\",\"Partially\" and \"Not Mentioned\".Partially is used for:\u2022 Resources that cannot be properly considered tools; e.g. Excel sheets\u2022 Tools that cover only a small part of the entire knowledge provided by the framework; e.g. a tool which coversonly Development phase and neglects Requirements and Design phasesIn the computation of values, we ignored the frameworks containing Not mentioned values for tools.Regarding the statistics, the results are quite clear: as shown in Fig. 7 in more than the 80% of the cases there is no toolincluded in the framework. This is true regardless of the proposing entity as shown in Fig. 8.9A Rapid Review of Responsible AI frameworksFigure 3: Distribution of frameworks by category.Moreover, Fig. 9 shows, when a tool is present, by which type of entity it is provided. Here the interesting result is thatmore than half of the tools are proposed by COMPANIES (61.9%, 13/21).Then, to go deeper into our investigation, we analyzed which kind of stakeholder\u2019s background is required to use thesetools. We used the tag Technical to describe tools that should be used by stakeholders who work in the last threephases of the SDLC (e.g. developers, testers, IT technicians, etc); while we used the tag Non-technical for tools thatshould be used by stakeholders who work in the initial two phases of the SDLC (e.g. commercial agents, functionalanalysts, architecture designers, etc). Finally, if there are tools that offer resources for both kinds of stakeholders,we used the tag Both. Fig. 10 shows that, regardless of the type of proposing entity, more than half of the tools canbe used by Non-technical stakeholders (57.4%, 27/47). Nevertheless, a significant amount of tools for supportingthe Technical stakeholders is also provided (29.8%, 14/47). The tools that can support both kinds of stakeholders,conversely, represent only a very low percentage (12.8%, 6/47).Furthermore, Fig. 11 shows that a significant amount of tools proposed by COMPANIES can be used by both types ofstakeholders (33.33%) while UNIVERSITIES and NPG/COMM/PE proposed mainly tools that can be used by onlyone type of stakeholders.",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "3a1a91fc-c778-4f00-9e84-4ecb1be8b948",
                    "text": "We have seen how the RAI frameworksfound in our search are proposed by different types of entities. The difference in the percentages shown in Fig. 2,depending on the type of entity, may be due to various reasons, among others: the different amount of funds available forresearch (and implementation) and the Return-on-investment (ROI) of these kinds of initiatives, e.g. different betweencompanies and no-profit organizations. Moreover, it is noteworthy that the group of NPG/COMM/PE is the largestand most heterogeneous group among the three, including as well scientific works conducted as academy-industrycollaborations.Regarding the way in which the frameworks are categorized, we can definitely say that there is a worrying lack oftools: most of the frameworks provide just Principles or Guidelines. We want to point this out since it is difficultto translate principles into concrete and measurable actions Mittelstadt [2019] but a tool may help reduce this gap.Furthermore, without tools or well-known practices, it is not possible to measure the goodness and sufficiency of theadopted practices. 10A Rapid Review of Responsible AI frameworksFigure 4: Number of RAI principles addressed by the frameworks grouped by proposing entity type.RQ2. How much do these frameworks address the various RAI principles? In addressing this research questionour aim was to discover how much \"coverage\" the frameworks give in terms of ethical principles handled. Indeed,sometimes practitioners are forced to use more than one framework at the same time in order to take into account thedifferent dimensions of the RAI they want to implement and/or validate.The emerging trend seems positive: the majority of the frameworks address all the four most important RAI principles,even if sometimes in a \"partial\" way (the principle is not fully-covered, see Section 4). Nevertheless, there areframeworks that neglect one, two, or even three principles. This could be due to multiple reasons, first of all, the factthat there is no global consensus about a clear definition of RAI and about which principles should be covered orimplemented among the others. Moreover, especially for companies, there could be reasons leading to ignoring part ofthe principles: among these, the Ethics Bluewashing Floridi [2019] (the unethical behavior of making unsupportedor false claims about, or taking insufficient action to support, the ethical ideals and advantages of digital processes,products, services, or other solutions in an effort to look more ethically conscientious than one actually is) and the Digitalethics shopping (the unethical practice of selecting, modifying, or updating ethical guidelines, codes, frameworks, orother similar standards from a variety of offers in order to retrofit some pre-existing behaviors and thereby justify thema posteriori, rather than putting new behaviors into practice or improving them by comparing them to accepted ethicalnorms Floridi [2019]).RQ3. Do these frameworks provide recommendations for each phase of the Software Development Life Cycle(SDLC)? The RR highlighted that most frameworks focus only on the initial phases of the SDLC, and in particular onRequirements elicitation. This is a problem because all the statements and obligations provided as RAI principles mustresult in operations and practices actually implemented while developing an AI application, encompassing the entireSDLC. Indeed, for developers, testers, and system installers is really difficult to identify practices that can satisfy theseabstract requirements without any support from an expert in the field or from a practical framework guiding them inall the development phases. What sounds alarming is a common trend across all the different types of entities, as thehighest percentages of proposed frameworks fall in the Requirements Elicitation phase while, from Design onward, thenumbers drop dramatically until the lows in the Deployment phase.RQ4. Is there a supporting tool for each proposed framework? Here there is an emerging negative trend: in mostcases, there is not a practical tool complementing the theoretical framework proposed and this is valid regardless of thetype of entity releasing the tool.Moreover, most of the tools are Excel sheets, checklists, or questions to self-ask in the initial phases of the SDLC, sotools are mainly useful just for non-technical stakeholders. However, differently from the other entities, companies11A Rapid Review of Responsible AI frameworksFigure 5: Distribution by SDLC phase addressed.proposed a significant amount of tools useful for both kinds of stakeholders: technical and non-technical ones. Thismay be due to an interest in trying to sell complete services to other companies supporting every kind of stakeholderand, at the same time, covering every phase of the SDLC.Summary of findings. A global perspective on the results analyzed leads us to affirm that there is not a \"catching-all\"framework, simple and complete enough to provide different levels of knowledge for different kinds of stakeholders(technical and non-technical ones), which can simplify and speed up the adoption of RAI practices. This statement comesfrom the fact that there are no frameworks encompassing all RAI concepts logically and uniformly: recommendationsare scattered across various pages or documents, and there is no one element that acts as a glue and makes theinformation consultation linear and fluid, at various levels of abstraction, for various types of stakeholders. To betterexplain this concept, we can take the work of Baldassarre et al. Baldassarre et al. [2019] as an example: here, eachelement which composes the knowledge base is connected and this makes it possible to consult the knowledge basefrom various entry points, following different mental paths. As a consequence, we advise the need for the creationof a uniform framework also for RAI: this way, the integration of RAI practices would be far easy in every kind oforganization, public or private.",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "c74f35bb-2e51-4bea-818e-b3ff24c4c1e7",
                    "text": "In this section, we proceed to discuss the threats to the validity Wohlin et al. [2000] of our study. As exposed inCartaxo et al. [2018], Rapid Reviews usually present more threats to validity than other secondary studies due to theirlightweight methodology. Threats to internal validity describe factors that could affect the results obtained from thestudy.(i) Only two white-literature search engines were used, which may limit the number of primary studies found. Tomitigate this, an in-depth analysis of grey-literature sources was performed.(ii) To mitigate bias in the selection and extraction procedure, the process was conducted by two of the authors in ablind-view manner. Moreover, to impartially solve the conflicts, a third author intervened and took a decision. However,it is not possible to completely mitigate the bias threat.(iii) With several iterations, including the refinement of keyword terms and search strategy, the authorship teamattempted to capture as much relevant literature as possible. However, it is still possible that the authors\u2019 chosenkeywords and search strategy did not capture all relevant articles. For example, if a framework proposed by a companyis not indexed on the used search engine and it is not simple to find a reference on the company\u2019s website, the framework12A Rapid Review of Responsible AI frameworksFigure 6: Number of SDLC phases addressed by the frameworks grouped by proposing entity type.may have been overlooked. However, given the number of results returned by the various grey-literature databases used,the authors feel confident that a representative amount of literature was captured through the search strategy describedin Section 3.",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                },
                {
                    "id": "dd28c4ed-d1c4-4185-982b-f50066ea44ba",
                    "text": "In this work, we conduct a rapid review to provide an overview of the frameworks proposed in white and grey literatureto help and speed up the adoption of Responsible Artificial Intelligence (RAI) practices. We identified four researchquestions to obtain specific answers w.r.t. our research goal, thus qualifying this survey\u2019s information. To do a deeperinvestigation, we classified the entities providing each framework into three categories: COMPANIES, UNIVERSITIES,and NPG/COMM/PE. In this way, we were able to slice the results by the proposing entity. This review focuses notonly on scientific articles but includes also grey literature resources.We can summarize the findings of this study as follows:- F1: Diversity of perspectives vs lack of standardization. Frameworks of RAI are provided by multiple andheterogeneous entities, both public and private. This is, from one side, an added value as it contributes to the diversity ofperspectives and to the so-called AI democratization. Nevertheless, on the other side, this reveals an even greater lack ofconsensus and standardization about which are the best practices to follow to be compliant with the RAI ethical values.- F2: Theoretical vs practical support for AI practitioners. In our review, we also classified each frameworkaccording to the SDLC phases covered. Our analysis highlights that very few frameworks encompass all the SDLCphases and provide practical support to practitioners who want to develop, test, and deploy RAI applications. Thereis a shortage of practical validation techniques for the theoretical principles, as well as implementation guidelines.Moreover, there is a worrying deficit of tools supporting all the stakeholders in the implementation and auditing phase.- F3: No comprehensive framework. Even if a great number of the frameworks we analyzed cover all four selectedprinciples, we observed that sometimes principles are only partially covered, reflecting the lack of standardizationhighlighted in F1. In this rapid review what emerged is that current literature and industry lack complete, uniform, orga-nized, and simple-to-use frameworks for RAI able to support the stakeholders across the entire Software DevelopmentLife Cycle (SDLC). We can conclude that no comprehensive framework exists right now, whose knowledge can benavigated and exploited by different kinds of stakeholders (technical and non-technical ones), which can simplify andspeed up the adoption of RAI practices. 13A Rapid Review of Responsible AI frameworksFigure 7: Distribution by the existence of a supporting tool regardless of the proposing entity.In conclusion, there is an undeniable agreement from different parts of the society about the need for ethical AI.However, we are still far from its realization, we lack consensus, standards, and tools: a large part of the road still needsto be covered before reaching this vision. Therefore, further research must be conducted to develop such frameworks,which could speed up and give a practical boost to the adoption of RAI practices in real-world projects.",
                    "reference": "[1] Davi Barletta, Dario Caivano, and Davide Gigante. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th ... ACM Inc. Retrieved from https://arxiv.org/pdf/2306.05003"
                }
            ]
        },
        {
            "paper_title": "Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions",
            "authors": "J Woodgate, N Ajmeri",
            "publication_info": "ACM Computing Surveys - dl.acm.org",
            "paper_url": "https://dl.acm.org/doi/pdf/10.1145/3672394",
            "chunks": [
                {
                    "id": "a02a97d3-d163-4408-9aac-ca8f254772e2",
                    "text": "ImplementationWe have found that abstract implementation of principles falls into three main categories: rules, consequences,or virtues. We discuss examples of each implementation category and potential diiculties that may arise. De-ontological principles have been operationalised by applying rules, and choosing an action based on how itaccords with those rules. Virtue ethics has been operationalised by developing virtuous characteristics. Conse-quentialist principles have been operationalised by evaluating consequences and choosing an action based on theconsequences it produces.4.4.1 Applying Rules. For deontological principles, some approaches suggest operationalising principles byapplying a set of rules to possible actions to determine which ones would be satisfactory, such as Abney [1],Berreby et al. [15], and Greene et al. [61]. Examples of this, as suggested by Murukannaiah et al. [98], would beapplying the rule that the disparity of preference satisfaction for stakeholders should be minimised, extractedfrom the principle of egalitarianism. Another example is Leben [85], applying the rule that stakeholders shouldbe treated proportionally based on their contributions to production.Due to the abstract nature of ethics, diiculties arise in inding appropriate ways to encode ethical principles inconcrete rules. One diiculty lies in deciding if rules should be interpreted as strict or defeasible (Tolmeijer et al.[130]). For example, an essential part of Kant\u2019s [78] ethics is that the reasons for actions must be universalisable toall agents. The need for reasons to be universal implies that this rule should be strict. However, this could permitactions that are bad according to other principles, suggesting that it should be defeasible (Abney [1]). Nashedet al. [101] argue that although implementing ethics through rules sets a high standard for agent behaviour,expressive, efective, and general rule sets are diicult to generate. Creating systematic ways of encoding theethical principles we identify (Figure 1) into rules, including understanding whether rules should be strict ordefeasible, to use in the reasoning capacities of AI could thus be a direction for future research.4.4.2 Developing Virtues. For virtue ethics, ethicality stems from the inherent character of an individual (Kazimand Koshiyama [79]). To solve a problem according to this theory, virtuous characteristics should be applied(Robbins and Wallace [110]). Thus, the theory can be operationalised by instantiating virtues (Tolmeijer et al.[130]). Instantiating virtues is exempliied by Govindarajulu and Bringsjord [59], who understand virtues aslearnt by experiencing the emotion of admiration when observing virtuous people, and then copying the traitsof those people. This is implemented using computational formal logic to formalise emotions (in particular,the emotion of admiration), represent traits, and establish a process of learning traits. To formalise virtues, theauthors use a deontic cognitive event calculus, which is a quantiied multi-operator modal logic that includes theevent calculus for reasoning over time and change. By formalising emotions (admiration) in this way, agentsassociate admiration with the actions of others. Traits are formalised as a series of instantiations of a type ofbehaviour. If enough admiration is felt for particular traits, the agents learn the traits, thus instantiating virtues.However, virtue ethics can be diicult to apply to individual situations (Saltz et al. [113]), and there arechallenges that arise with the application of virtues across time and culture (Tolmeijer et al. [130]). Futureresearch could therefore examine the applicability and appropriateness of virtue ethics across diferent contexts.4.4.3 Evaluating Consequences. Consequentialist principles may be operationalised by evaluating the conse-quences of diferent actions (Limarga et al. [89]). Suikkanen [126] suggests this could be done by ranking agents\u2019options in terms of how much aggregate welfare their consequences have. Dehghani et al. [35] specify this withthe principle of utilitarianism, by selecting the choice with the highest utility. Ajmeri et al. [2] operationalise theprinciple of maximin by improving the minimum experience in the consequences of an action. Consequencesare also used to operationalise the principle of envy-freeness, which Sun et al. [127] address by promoting theoutcome with the lowest levels of envy between groups or individuals.Issues arise in predicting all of the possibilities an action could produce. Predicting all possibilities could becomputationally challenging, requiring complex calculations (Greene et al. [61]). There are thus limitations tosimulating all possible consequences of an action in non-deterministic and probabilistic environments; futurework could explore applying multiple ethical principles to such environments.5 Gaps in Operationalising Ethical PrinciplesTo address Q (Gaps), we now examine existing gaps in ethics and fairness research in AI and computer scienceliterature, speciically in relation to implementing multiple ethical principles in reasoning capacities.5.1 Expanding the TaxonomyUnderstanding strengths and weaknesses of various approaches improves critical understanding and constructiveengagement (Boddington [18]). Therefore, key gaps include research on lesser-utilised principles. We suggestthat future directions consider less commonly seen principles, or incorporate a wider array of principles. Thisincludes researching principles from other cultures outside of the Western doctrine, which is important as ethics isculturally sensitive (Hickok [69]). Implementing ethical principles from various cultures will aid the accessibilityand fairness of AI, as it can better apply to stakeholders from diverse backgrounds. Hongladarom and Bandasak[71] survey non-western guidelines for AI principles, inding unique cultural presuppositions in some areas andglobal consensus in others. Expanding this research to examine AI and non-western principles from normativeethics is a future research direction.5.2 Resolving Ethical DilemmasWe identify various diiculties with the implementation of ethical principles that may result in ethical dilemmas,from which various gaps arise. Anderson and Anderson [5] deine ethical dilemmas as situations where eitherthere is not a good choice between diferent outcomes, or where the choice between diferent outcomes is notobvious (e.g., the distinction of how good one outcome is compared to another is not obvious). Future researchcould address gaps that arise in resolving these dilemmas.In certain situations, dilemmas arise when the application of one principle cannot support one action overanother. Azad-Manjiri [11] suggest that one way to resolve this could be by examining how similar decisions weremade previously. If no similar decisions have been made previously, an action is selected at random. However,this approach may run into the naturalistic fallacy, looking at what is the case rather than what ought to be thecase. In addition, relying on random choice may not result in the most ethically appropriate action. A gap existsin further examining how to resolve dilemmas where principles cannot support one action of another.For each principle, dilemmas arise when its application leads to an unfair outcome, as all moral theories havesome counter-intuitive implications (Robinson [111]). This implies no single theory can denote how to programethical AI (Pagallo [104]). Pluralist approaches, in which diferent principles can be weighed against one anotherto ind the most appropriate answer, could help mitigate these issues. Works such as Governatori et al. [58],Lera-Leri et al. [88], and Planzer et al. [107] provide methodologies accommodating multiple principles. A gapexists in applying such methodologies to compare the application of multiple principles in scenarios whereparticular principles lead to unfair outcomes. However, weighing alternatives may not always be possible. Futureresearch should investigate the feasibility of applying diferent principles in diverse scenarios.Dilemmas may arise with the application of multiple principles, as diferent principles can give diferentanswers which may conlict (Persson and Hedlund [106]). This is exempliied in Nashed et al. [100], who indthat agents implementing diferent principles favour diferent policies. In addition, it is diicult to apply abstracttheories to concrete situations. To aid this, Tolmeijer et al. [130] suggest that particularism (which incorporatesrelevant contextual factors in ethical reasoning to identify if a certain feature is morally relevant or not) couldhelp identify which principle is the most appropriate in that setting. A gap exists in exploring if aspects ofparticularism can be used to resolve dilemmas where diferent principles promote conlicting outcomes. However,there are also issues that arise with the application of particularism. While ethics examines principles that sociallyimpose what\u2019s right or wrong, morality deals with social values of right or wrong (Jiang et al. [75]). Moraldisagreement can arise when stakeholders have diferent beliefs about which facts are morally relevant, andwhich ethical principle is true (Awad et al. [10], Robinson [111]).There are thus various gaps and diiculties which arise in regard to resolving ethical dilemmas. Drawing theseideas together, there are gaps in inding reliable methodologies for AI practitioners to decide which principleis most appropriate for a particular case, considering the dilemmas which may arise. Robinson [111] exploresthree solutions which may help to resolve dilemmas: moral solutions, compromise solutions, and epistemicsolutions. Moral solutions select a moral theory either by what we think is true, some general theory which wecan agree on, or what we could hypothetically agree on under certain disagreements. Compromise solutionschoose principles based on a social choice approach, or treat principle selection as a multi-objective optimisationproblem, optimising based on inferred moral values or goals. Epistemic solutions harness information about thedisagreement as evidence of moral facts, and then appeal to a rule for decision making under moral uncertainty.Alternatively, epistemic solutions could attempt to achieve an overarching moral view which accommodates asmany relevant ethical judgements as possible. Each approach has various strengths and limitations, as discussedin the paper. Robinson [111] concludes that problems of moral disagreement should be treated as problems ofmanaging moral risk, where moral risk is the chance of getting things wrong and what you thereby risk. A gapexists in implementing such solutions to evaluate how they address ethical dilemmas in practice.Implementing Ethical Principles in STS5.3An application of implementing ethical principles in STS is to support governance capacities. Governance of STSinvolves establishing standards for the proper use and development of technology, as deined by Floridi [52],and administration of systems by stakeholders themselves, as deined by Singh [120]. Under the perspective ofmacro ethics, responsible governance should incorporate norms and value preferences of diferent stakeholders.However, dilemmas arise when norms or values conlict. Operationalising ethical principles in reasoning helpssupport governance capacities to resolve these dilemmas in equitable ways that support the needs of diferentstakeholders (Woodgate and Ajmeri [138]). Gaps exist relating to how ethical principles can be operationalised inSTS to promote equitable governance capacities.Previous work provides guidance for applying ethical principles to reason about values and norms in com-putational decision making. For example, Ajmeri et al. [2] broadly reference the principles of egalitarianismand utilitarianism within the context of utilising values and norms in MAS for ethical reasoning in individualdecision making. This research may beneit from the consideration of other ethical principles to enable broaderapplicability. Lera-Leri et al. [88] present a method for applying multiple ethical principles to aggregate diferentvalue preferences, but do not consider the inluence of norms. Serramia et al. [117] demonstrate how to selectnorms that best align with a known value system. Combining these approaches to aggregate diferent valuesystems using ethical principles, and then using the aggregated value systems to select value-aligned norms, isa promising direction for future research. In addition, a gap exists in examining how such approaches can beincorporated in decentralised collective decision making.However, challenges arise when implementing ethical principles in STS considering value systems. Norms andvalues are interdependent with context and decision-makers, and research should consider how value systemschange according to context, for example, over time (Osman and d\u2019Inverno [103], Smit and Pitt [122]). Gapsexist related to implementing principles in STS in ways that account for the relationship between context andchanging value systems.Properly incorporating broad social context requires careful consideration to avoid entrenching dominantrelations of power (Weinberg [136]). This includes accounting for the ways in which existing dynamics shapehow technology is developed and deployed. Applying ethical principles must therefore integrate broad socialdynamics, and appreciate how social dynamics afect governance capacities (Munn [97]). Gaps emerge withrespect to accommodating broad social dynamics and avoiding perpetuating unjust power dynamics in theapplication of ethical principles to governance capacities.To understand how ethical principles can accommodate for broad social dynamics, participatory approachesmay be useful to incorporate human input throughout the design process. For example, Dubljevi\u0107 et al. [43]combine participatory approaches with multi-criteria decision making to capture the importance of diferentharms and make clear the perspectives of diferent stakeholders. Weinberg [136] emphasises that collaboratingwith those afected by the technology improves the propensity to leverage knowledge from marginalised groups,understand how the technology is situated in its social context, and address what is most ethically concerning,rather than what is most convenient to measure. Participatory approaches present opportunities to investigatequestions related to what extent ethical principles are generalisable across diferent groups of people, what peoplemorally disagree on, what preferences people have over ethical principles, and if and how people follow ethicalprinciples in their daily lives. Gaps exist in further examining these questions.6 ConclusionTo better address the pursuit of responsible AI, research must be human-centred (Collins et al. [31], Dignumand Dignum [41]). Shifting the perspective to the macro ethics of STS, considering the range of relevant humanvalues and ethical features, may help to enable responsible ethical-decision making which can be justiiedand held accountable (Chopra and Singh [29], Lechterman [86]). However, dilemmas arise when values conlict(Murukannaiah et al. [98]). To resolve these dilemmas in satisfactory ways, ethical principles can help to determinethe moral permissibility of actions (Lindner et al. [90], McLaren [92]).We identify a variety of ethical principles which have been previously operationalised in AI and computerscience literature. We also identify key aspects of operationalising ethical principles in AI, including selectingtechnical implementation, clarifying the architecture, specifying the ethical principle, and using rules, conse-quences or virtues. Key gaps that imply future research directions include expanding the taxonomy, resolvingethical dilemmas where principles conlict or lead to unfair outcomes, and implementing principles in STS whilstaccommodating for changing contexts and broad social dynamics. We envision that our indings will contributetowards developing responsible AI by aiding the incorporation of ethical principles in reasoning capacities.AcknowledgmentsWe thank the anonymous reviewers for their careful reading and insightful comments which helped us to sub-stantially improve the manuscript. JW thanks the EPSRC Doctoral Training Partnership Grant No. EP/W524414/1for support. NA acknowledges partial support from the UKRI EPSRC Grant No. EP/Y028392/1: AI for CollectiveIntelligence (AI4CI).ReferencesA MethodologyA.1 Sources Selection and StrategyAfter deining our objective and questions, we formed the strategy to search for primary studies by identifyingkeywords and resources. We selected the University of Bristol Online Library as the resource to search, withGoogle Scholar as back up. They are both large databases with links to a wide variety of other sources of researchwith published papers on the topic. We searched the selected resources using various combinations of the chosenkeywords, which can be found in Appendix A.1.1.Using a forwards and backwards snowballing technique, we inspected up to the irst 5 pages of results ineach resource, and then narrowed the search by applying the inclusion and exclusion criteria to the titles. Thisspeciied the search to a smaller selection of works of whose abstracts were read. The inclusion and exclusioncriteria were then more closely applied, identifying primary studies. From the works gathered in this initialsearch, relevant citations were followed to expand the search, which allowed material to be collected from abroader array of origins. The identiication of new key words from the indings was used to update the searchstring, repeating the process until no new key words were identiied.Figure 2 outlines our search strategy in brief.Search String Definition. Our search string contained two main components. The irst component re-A.1.1lates to AI and various related terms, whereas the second component relates to normative ethics. The searchstring used was (\u2018AI\u2019 OR \u2018Agent\u2019 OR \u2018ML\u2019 OR \u2018Multi-agent\u2019 OR \u2018Multiple-User\u2019) AND (\u2018Responsible\u2019 OR \u2018Ethics\u2019OR \u2018Consequentialism\u2019 OR \u2018Deontology\u2019 OR \u2018Virtue\u2019 OR \u2018Egalitarianism\u2019 OR \u2018Proportionalism\u2019 OR \u2018Kant\u2019 OR\u2018Utilitarianism\u2019 OR \u2018Maximin\u2019 OR \u2018Envy-Freeness\u2019 OR \u2018Doctrine of Double Efect\u2019 OR \u2018Do No Harm\u2019).A.1.2 Inclusion and Exclusion Criteria. First, work is included from a series of well-known journals and con-ferences identiied from literature found in the initial searches. Speciically including these resources ensurestopical works are included, however, it also opens up the threat that resources not on the list may be missed. Wemitigate risk by following relevant citations from primary studies to expand the scope, however, acknowledgethat limitations remain. We exclude works about meta-ethics (e.g., the meaning of moral judgement) and appliedethics outside of AI and computer science (e.g., biology ethics).Second, we include works about responsible AI. Third, we include works related to individual or group fairness.We exclude works about fairness in speciic ML methodology, as that is outside the scope of this project. Fourth,we include the intersection of normative ethics and multiple-user AI research, whereas we exclude studies thatdo not consider ethics (e.g., studies about technical implementation). Fifth, we include studies about normativeethical principles and AI, but we exclude studies solely about AI principles. This is because this review relates toethical principles. Sixth, we include studies about bias when related to ethical principles, as this is relevant to howethical principles afect fairness, however, we exclude studies about bias that do not talk about ethical principles.A.2 Method for Principle IdentificationFigure 3 visualises the method used to answer the research questions. This was in a concurrent two-partprocess of analysing principle identiication (Q ) and principle implementation (Q ) in literature. Qualitativeanalysis of works was conducted by reading through and summarising key points, which were then put intorelevant classiications of which principles they related to, and their type of contribution (seen in Tables 1 and2). Classiication by principle was conducted by matching papers to the ethical principles which are explicitlystated. Classiication by contribution was conducted by utilising categories proposed by Tolmeijer et al. [130]and Yu et al. [141]. These individual analyses were then aggregated to examine the indings as a whole. Someworks were more theoretical, exploring the existence of principles and how they might relate to AI and computerscience (e.g., Boddington [18]). These works were useful for the identiication of principles (Q ). Other researchtook established principles and implemented them, which helped to answer Q (e.g., Sun et al. [127]). Someworks had a mixture of both identiication and implementation (e.g., Kim et al. [81]). The irst author categorisedindings according to principles explicitly stated and contribution as deined above. This analysis was performedin consultation with a second author who critically examined the works being reviewed and the indings extractedby the irst author.A.3 Threats to Validity and MitigationFive threats to validity arise, which are summarised here, alongside attempted mitigations. The irst threatidentiied is that only papers that are written or translated into English are included in our review for developinga taxonomy. This means that relevant research in other languages may be missed, which could contribute tocultural bias and thus threaten both internal and external validity of the study. Internal validity is threatenedby missing ethical principles that are referenced in other languages, and the external validity is threatened bydiminishing the cross-cultural application of the indings. This is mitigated by seeking papers with an internationalauthorship, but it is recognised as an outstanding issue that could be resolved through future research in applyingthe methodology to other languages.A second threat to internal validity is the potentiality of missed keywords, which may again lead to relevantresearch being excluded. To address this concern, we carefully scope the aims of the review for easier identiicationof a good array of initial relevant terms. The initial search string is based on preliminary research; as the reviewcontinues, more key terms (i.e., ethical principles) are identiied. As more terms are identiied, a forwards andbackwards snowballing technique is used, following relevant citations, updating the search string with newkeywords, and repeating the process until no new keywords are identiied.There is a related third threat of missing resources which has similar implications to the internal validity of thestudy. The topic studied here relates to a broad area of research, and areas such as human-computer interactionand software engineering are not explicitly included in searches but may contain relevant research. This threat isaddressed by using two large online libraries as the initial resources, which link to a variety of other resources.Citations from selected studies are also followed, broadening the scope of publications. However, future researchcould also include reproducing the methodology in these other areas.Fourth, time limitations threaten the internal validity as there is only time to search the irst ive pages ofresults (plus citations). This may mean that there is relevant work beyond these pages that there is not enoughtime to pursue. To do the best research possible within this time limit, citations are pursued, and Kitchenham andCharters [82] guidelines for a systematic literature review are broadly followed. This helps to efectively identifyrelevant research. On the other hand, this limitation could lead to further research in this area by applying ourmethodology to the analysis of more studies than those identiied here.The ifth issue of researcher bias also threatens internal validity as it can sway the results in a particulardirection rather than being objective. This is mitigated by having a secondary reviewer who critically analysesresults and makes suggestions to help the primary reviewer improve the study. This is also tackled by basing thestudy selection criteria on the research question and deining it before the review is begun.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "58fe4221-742e-4942-8c2b-e7690090ab52",
                    "text": "The motivation for this work stems from the need to improve ethical evaluation capacities for responsible AI. Toaid this we look to normative ethics, as engaging with interdisciplinary insights encourages more inclusive andcritical thinking (Weinberg [136]). Principles from normative ethics imply certain logical propositions whichmust be true for a given action plan to be ethical (Kim et al. [81]). Principles can be used to methodically thinkthrough dilemmas and promote satisfactory outcomes (Canca [25], Saltz et al. [113]). Operationalising normativeethics principles thereby enables systems to methodically reason about ethics (Woodgate and Ajmeri [138]).Normative ethical principles have previously been utilised for a variety of AI applications. Binns [16] andLeben [85] apply ethical principles to improve fairness considerations for binary machine learning algorithms.Cointe et al. [30] implement ethical principles in decision making, enabling agents to make ethical judgements inspeciic contexts. Conitzer et al. [32] state that principles can be applied to identify morally relevant features ofdilemmas, whilst Heilinger [66] illustrates how principles frame discussions of risks and opportunities in AI.Ethical thinking should be fostered through the appreciation of a variety of approaches, considering thestrengths and limitations of each (Burton et al. [24]). Adopting interdisciplinary perspectives also helps to bridgeepistemic divides (Weinberg [136]). We suggest that a taxonomy of ethical principles previously seen in AI andcomputer science, including previous operationalisation, provides practitioners with key themes and examples tohelp ground their approaches. We envision that this taxonomy will contribute to improving ethical evaluationcapacities of responsible AI.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "c4ad1ade-e55f-49a9-ad82-20fcc2d9a907",
                    "text": "In the context of AI and ethics, there are two types of principles referred to: (1) those inferred from normativeethics such as deontology and consequentialism, as found in Leben [85], and (2) those adapted from otherdisciplines like medicine and bioethics such as those suggested by Cheng et al. [28], Fjeld et al. [50], Floridi andCowls [54], Jobin et al. [76], Khan et al. [80], and Whittlestone et al. [137], including beneicence, non-maleicence,autonomy, justice, fairness, non-discrimination, transparency, responsibility, accountability, safety and security,explainability, human control of technology, and promotion of human values.To ensure clarity of terminology, we refer to principles from normative ethics as ethical principles, and thosehighlighted by Floridi and Cowls [54] and Jobin et al. [76] as AI principles. We deine ethical principles and AIprinciples as follows:Ethical Principles. Operationalisable rules inferred from philosophical theories which imply logical propositionsdenoting moral acceptability.AI Principles. Ends which ought to be promoted in the development and deployment of AI to ensure it issocially beneicial.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "b38345a2-821d-4385-ae46-7e96330c764e",
                    "text": "Ethical principles are philosophical theories which are normative in the sense thatthey are prescriptive, denoting how things should be, rather than descriptive, denoting how things are (Kimet al. [81]). As what is the case might not be ethical, using independently justiied principles has the beneitof addressing the is-ought gap: just because something is the case, does not mean that it ought to be. Ethicalprinciples guide normative judgements, determine the moral permissibility of concrete courses of action andhelp to understand diferent perspectives (McLaren [92]). Using ethical principles makes explicit the normativeassumptions underlying ethical choices, improving propensity for accountability (Fazelpour et al. [49], Lechterman[86]). Ethical principles can be operationalised in reasoning capacities as they imply certain logical propositionswhich must be true for a given action plan to be ethical, and provide frameworks for guiding judgement andaction (Boddington [18]). The abstractness of ethical principles entails that they can be used to analyse concretecourses of actions in a wide range of situations (Binns [16], Conitzer et al. [32], Lindner et al. [90]).Ethical principles principles broadly divide into deontological principles (those which entail conforming torules, norms and laws, Hagendorf [62]), virtue ethics (denotes moral character central to ethical action, Wallachand Vallor [135]), and consequentialist principles (those which derive morality from the outcome of actions, Hortaet al. [72]).",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "9141df2d-a9ea-4420-881c-5f9499d6d2ed",
                    "text": "We understand AI principles as ends which ought to be promoted in the development anduse of AI. AI principles are qualities that we should expect AI to embody and by which we can assess how sociallybeneicial AI is.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "e003b5e4-331e-40a8-aaf8-8249a1e9b8d7",
                    "text": "Translating AI principles into practice is challenging(Zhou and Chen [142]). AI principles do not provide guidance for how they can be implemented, and interpretationof their meaning may diverge (Munn [97]). Ethical principles, on the other hand, are abstract rules that providelogical propositions denoting which actions are morally acceptable. Applying ethical principles to indicate moralacceptability helps to determine which actions are aligned with AI principles. Ethical principles are thus abstractrules which can be used to promote the instantiation of AI principles.To illustrate the distinction, we explore how the ethical principle of egalitarianism helps to implement the AIprinciple of fairness. Egalitarianism supports the notion that human beings are in some fundamental sense equal(Binns [16]). Fairness is deined by Jobin et al. [76] as the mitigation of unwanted bias and discrimination. Towork towards fairness, egalitarianism may be operationalised by reducing inequality to mitigate discrimination.For example, this could take the form of a rule that opportunities must be equally open to all applicants, as seenin Lee et al. [87].",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "3ab8f045-ca38-4e53-b40d-6981938ca475",
                    "text": "Existing taxonomies and surveys are present in the relevant but distinct domain of AI principles, such as Floridiand Cowls [54], Jobin et al. [76], and Khan et al. [80]. However, these works do not consider ethical principles.The rest of this paper therefore surveys ethical principles rather than AI principles. Dignum [40], Leben [85], andRobbins and Wallace [110] provide summaries of normative ethics. Tolmeijer et al. [130] give an overview ofimplementations of machine ethics, providing useful guidance as to the technical and non-technical aspects ofimplementing ethics and evaluating systems. Similarly, Yu et al. [141] provide a concise guide to ethical dilemmasin AI and identify a high-level overview of ethical principles. From a philosophical perspective, Boddington [18]presents a comprehensive exploration of the application of three major normative ethics theories (deontology,virtue ethics and consequentialism) to AI, and issues that might arise.We expand upon previous literature to address gaps concerning principles which were not included in previousreviews, and provide further detail about how ethical principles have been operationalised in AI and computerscience.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "64422ad7-ca8e-43e9-8561-3d073f56c844",
                    "text": "We build upon previous research, especially Tolmeijer et al.\u2019s [130], to collate a broader range of ethical principlesdiscussed in the AI and computer science literature, summarising operationalisation principle by principle. Thereare three key aspects of novelty contributed by this paper:Broadening the Range of Ethical Principles We create a taxonomy tree with 21 ethical principles discussedin AI and computer science literature.Principle Speciic Operationalisation We deine a new mapping of each principle to how they have beenoperationalised in literature. Operationalisation is explained on both an abstract level, including how eachprinciple has been deined in literature and diiculties that may arise, and on a technical level, includingtechnical implementations of each principle, and how technical implementation relates to diferent architectures.Relection on Research Gaps and Directions We identify gaps and future directions. Broadly, directionsemerge from (1) expanding the taxonomy to include principles under-utilised in AI and computer science, (2)resolving ethical dilemmas where principles conlict or lead to unintuitive outcomes, and (3) incorporatingethical principles in STS considering broad social contexts.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "338ac3a4-ba20-4205-b70b-d5562799d6c7",
                    "text": "Section 2 explains our methodology in brief. This will be useful for future research seeking to expand the taxonomyof ethical principles by reproducing the methods used here. Section 3 explores our indings for which ethicalprinciples have been proposed in AI and computer science literature. Section 4 examines how ethical principleshave previously been operationalised, and steps practitioners seeking to operationalise principles should take.Section 5 identiies gaps and future directions for operationalising ethical principles in AI and computer science.Section 6 concludes the paper.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "25f89f75-9226-4e51-8275-2ec676ef116b",
                    "text": "Taking inspiration from software engineering research, for reproducibility we follow Kitchenham and Charters[82] guidelines on conducting a systematic literature review to develop our taxonomy for ethical principles. Weirst deine our objective and research questions to help scope the search. We construct an initial search stringfrom preliminary research. Using a forwards and backwards snowballing technique, we search selected resources(the University of Bristol library, with Google Scholar as backup) using our search string. We apply inclusionand exclusion criteria to identify primary studies, and follow relevant citations to expand the search. We updatethe search string if we identify new key words (i.e., if studies reference ethical principles not previously seen),repeating the process until no new key words emerge. For further details of the methodology, see Appendix A.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "9ec5062f-89c8-4d03-9532-c74e8a4b2f5e",
                    "text": "We investigate the current understanding of ethical principles in AI and computer science, and how theseprinciples are operationalised. Speciically, we address the following questions:Q (Principles). What ethical principles have been proposed in AI and computer science literature?The purpose of this question is to aid the identiication of principles currently used in literature within thedomain of AI and computer science. Due to the intricacies of philosophical discourse, we follow Tolmeijeret al.\u2019s [130] approach in providing brief overviews of how each principle has been deined in literature. We donot attempt to give an introduction to moral philosophy, which can be found in works such as Boddington[18].Q (Operationalisation). How have ethical principles been operationalised in AI and computer science research?This question looks at the identiied principles to examine how they have been operationalised in AI andcomputer science. We expand upon the range of principles presented in previous works such as Leben [85]and Tolmeijer et al. [130].Q (Gaps). What are existing gaps in ethics research in AI and computer science, speciically in relation to opera-tionalising principles in reasoning capacities?This question aids analysis of existing gaps in operationalising the principles in reasoning capacities ofresponsible AI, to direct future research.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "47f1a5b0-2c5a-4b09-b983-641503c7d74b",
                    "text": "We conducted an initial search on 23-May-22, a second search on 14-January-23, and a third search on 01-February-24. The irst search produced 3.74 million results on Google Scholar and 998,613 results on the University of BristolOnline Library. Looking at the irst 5 pages of results, we applied the inclusion and exclusion criteria, which ledto around 10\u015b20 studies from each resource. Closer examination of these works resulted in the identiicationof relevant citations which we incorporated into our review. The selection of these works was critiqued by asecondary researcher which helped to identify further relevant research. This resulted in 57 papers being includedin the review. The second search resulted 10 more papers being included in the review. The third search identiieda further 14 papers to include in the review.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "0c960591-8db5-497d-982e-5ad6a94c87b4",
                    "text": "We now address Q (Principles) on identifying ethical principles proposed so far. We irst present an overview ofprinciples we identify in AI and computer science literature. We categorise papers based on principles explicitlymentioned, contribution, and evaluation type. We then present our indings for each principle, summarising theirdeinition, previous application, and potential diiculties.Within normative ethics, there are three main strands of theory: deontology, virtue ethics, and consequentialism.There is a debate as to whether consequentialism and virtue ethics are branches of teleology or distinct branchesof theory, as summarised by Spielthenner [124] and further explored by Horta et al. [72]. Following Horta et al.[72] and Boddington [18], we do not use the term teleology, categorising consequentialism and virtue ethicsas distinct branches. However, our key intention is to examine how such principles have been used in AI andcomputer science literature. Further exploring the philosophical relation of these theories is outside the scope ofthis work.Deontological theories revolve around rules, rights, and duties (Murukannaiah and Singh [99], Wallach et al.[134]). Virtue ethics denotes that ethicality stems from the inherent character of an individual, not the rightnessor wrongness of individual acts (Yu et al. [141]). Consequentialist theories emphasise that whether something isright or wrong depends completely on its outcome (Horta et al. [72]). Figure 1 displays the taxonomy of principlesidentiied in literature in a tree structure, mapping out how they relate to each other.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "7814ed77-73a8-48de-a754-14b1c72135a7",
                    "text": "We categorise papers identiied in our review based on contributions of the paper, type of evaluation, and ethicalprinciples explicitly mentioned. Expanding on previous work, we adapt Yu et al.\u2019s [141] and Tolmeijer et al.\u2019s[130] taxonomies to categorise papers by contribution and evaluation type. We categorise papers by whichprinciple(s) they explicitly mention with the exception of three papers. Jiang et al. [75] and Shi et al. [119] do notexplicitly state ethical principles, but utilise Hendrycks et al.\u2019s [68] dataset which consists of scenarios based onethical principles. We include Jiang et al. [75] and Shi et al. [119] as they provide valuable demonstrations ofhow ethical principles can be implemented. In addition, Noothigattu et al. [102] do not explicitly state ethicalprinciples, but demonstrate how inverse reinforcement learning, a valuable technique for learning ethics throughobservation, can be used to implement ethical rules (where rules are integral to deontological approaches) inmachines.Based on principles explicitly mentioned, works are broadly categorised into eleven key principles (deontology,egalitarianism, proportionalism, Kantian, virtue, consequentialism, utilitarianism, maximin, envy-freeness, doc-trine of double efect, and do no harm). We categorise six types of contribution: descriptive, model representation,individual ethical decision making, centralised collective ethical decision making, decentralised collective ethicaldecision making, and ethics in human-AI interaction. Descriptive papers abstractly evaluate how normativeethics relates to AI. Model representation examines how to appropriately represent ethical knowledge in a system,or what features an ethical system should include. Individual decision making examines how individual agentsmay judge or select their own actions or the actions of others. Centralised collective decision making involves acentral mechanism which makes decisions concerning multiple agents. Decentralised collective decision makinginvolves multiple agents making distributed ethical decisions, such as in multi-agent systems (MAS). Ethics inhuman-AI interaction investigates ethical considerations of agents designed to inluence or work in conjunctionwith humans.We categorise four evaluation types adapted from Tolmeijer et al. [130]: test, proof, informal, and none. Testinvolves empirical analysis, comparing system outcomes against some ground truth. Proof examines if the systembehaves according to some known speciications, typically using logic. Informal compares the system to examplescenarios or application domains. When none of these evaluation types pertain, papers are categorised as \u2018none\u2019.Tables 1 and 2 display the categorisation of papers. Appendix A.2 provides further details of the methodology forpaper classiication.We ind that certain principles, such as utilitarianism, are more commonly discussed than other principles suchas do no harm, as can be seen in Table 2. We also ind that there is a large amount of research referencing \u2018deon-tology\u2019 and \u2018consequentialism\u2019 as broad terms, but not specifying what types of deontology or consequentialismthey are referring to, for example, Anderson and Anderson [6], Cointe et al. [30], and Greene et al. [61]. Preciselystating the ethical principles used (e.g., type of deontology) would allow for more exact operationalisation.In terms of contribution, we ind a large majority of works utilise ethical principles in descriptive and modelrepresentation papers. Descriptive papers include overviews of ethics, such as Boddington [18], or ethical critiqueswith reference to ethical principles, such as Heilinger [66]. Model representation harnesses ethical principles toexamine which ethical features should be considered in models, such as Anderson and Anderson [5], Dignum [40],and Lee et al. [87]. For individual decision making we ind that more works implement consequentialist principles,especially utilitarianism, than deontology or virtue ethics. The majority of these works, for example, Dehghaniet al. [35], Svegliato et al. [129], and Ajmeri et al. [2], also provide tests. In centralised collective decision makingapproaches we ind more works implementing consequentialist principles rather than deontology or virtueethics, such as Bakker et al. [13], Lera-Leri et al. [88], and Patel et al. [105]. For decentralised collective decisionmaking, we ind that most works implement deontology, as seen in Greene et al. [61], or utilitarianism, as seenin Governatori et al. [58]. In human-AI interaction we ind more works implementing deontological principlesor virtue ethics rather than consequentialism, such as Planzer et al. [107], Hagendorf [63], and Anderson andAnderson [6]. We ind decentralised collective decision making and human-AI interaction involved in the leastnumber of papers, suggesting avenues for future work in these areas.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "ed223f3f-c4b3-412b-869c-c7148a486527",
                    "text": "Deontology entails conforming to rules, laws, and norms, and respecting relevant obligations and permissions thatstem from duties and rights (Cointe et al. [30], Hagendorf [62], Rodriguez-Soto et al. [112]). For Deontologicaltheories, the permissibility of action lies within the intrinsic character of the act itself (Boddington [18]). Anaction is permissible if and only if the act itself is intrinsically morally good, independent of the outcome (Lindneret al. [90]).To implement deontological theories, a rules-based approach may be used to provide moral orientation inidentifying appropriate actions (Heilinger [66]). An example of a rules-based approach is Limarga et al. [89], whouse predicates to encode rules and then reason about diferent types of actions. Berreby et al. [15] implementdiferent deontological speciications as rules in a \u2018model of the right\u2019. The model of the right is used to generatea \u2018rightness assessment\u2019 of available actions, considering context. The model contains deontological principles inconjunction with consequentialist principles. Similarly, Planzer et al. [107] suggest implementing deontology aspart of a model which utilises consequentialism and virtue ethics, in which the role of deontology is to analysethe intention of actions. Tolmeijer et al. [130] argue that deontology could be implemented by inputting theaction, using rules and duties as the decision criteria, and then mechanising actions via the extent to which theyit with the rule.Deontology has been applied to diferent contexts. Binns [16] uses deontology to choose between incompatiblefairness metrics, whereas Leben [85] applies it to evaluate distributions of binary classiication algorithms.Hendrycks et al. [68] implement deontological principles in contextualised scenarios to measure the ethicalknowledge of natural language processing models. Jiang et al. [75] use this dataset to test a model trained onpeople\u2019s moral judgements. Some works suggest using deontology only in speciic circumstances: Dehghani et al.[35] choose to implement deontology in situations with \u2018sacred values\u2019, selecting the action that doesn\u2019t violate asacred value.However, issues may arise when applying deontology. One common concern is that because deontologicalapproaches focus on the intrinsic nature of an action, they fail to take the most likely consequences into account.Focusing solely on the intrinsic nature of action makes it challenging for deontology to adequately capturecomplex ethical insights (Abney [1], Saltz et al. [113]). The complexity of ethical insight entails that any systemof rules requires some interpretation and understanding of background assumptions. This means that the samerules might be interpreted diferently in diferent contexts or by diferent people (Boddington [18]). In addition,rights-based ethics revolve around decisions based on the rights of those who are afected by the decision.Focusing on rights is less helpful in situations where rights are not impinged, yet some sort of ethical dilemmais still occurring. For example, spreading hate speech does not necessarily infringe the rights of others, andthere are arguments that preventing it infringes the right to free speech. However, there is an intuition that hatespeech is wrong. Issues may arise with implementation when exceptions to rules emerge. Rules are expected tobe strictly followed, implying that for every exception they must be amended, which could result in very longrules. Determining the right level of detail is thus important to ensure interpretability for the machine (Tolmeijeret al. [130]). Lastly, there may be conlicts between rules. Conlicts may be addressed by ordering or weighing therules, but this gives rise to diiculties in determining the order of importance.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "3ea80c02-b694-44bf-a298-156380735b01",
                    "text": "Egalitarianism stems from the notion that human beings are in some fundamental senseequal. Heilinger [66] indicates that egalitarianism can be understood in a relational sense, aiming for conditionsunder which all can interact with one another on an equal footing. Binns [16] recommends that eforts should bemade to avoid and correct certain forms of inequality.Literature implements egalitarianism by promoting equality in diferent ways: Murukannaiah et al. [98] suggestminimising disparity across stakeholders with respect to satisfying their preferences; Dwork et al. [44] classifyindividuals who are similar with respect to a particular attribute similarly. For resource allocation, Leben [85]confers equal rights (and thus equal shares) to each member of the population. If achieving equality across allmetrics for the entire population is impossible, they suggest a distribution that minimises the distance to somefairness standard.We identify diferent applications in which egalitarianism has been implemented. Lee et al. [87] utilise egal-itarianism to evaluate various algorithmic fairness metrics, such as predictive parity or equal odds. Applyingegalitarianism to fairness metrics helps AI practitioners decide what layers of inequality should (not) inluence amodel\u2019s prediction. Persson and Hedlund [106] suggest utilising egalitarianism to consider how to distributeresponsibility for ethical AI development. Botan et al. [21] apply egalitarianism to judgement aggregation.Certain diiculties arise with egalitarianism. Binns [16] highlights a prominent debate as to whether a singleegalitarian calculus should be applied across diferent social contexts, or if there are internal \u2018spheres of justice\u2019in which diferent fairness metrics may apply, and between which redistributions might not be appropriate.Egalitarianism might apply diferently to diferent contexts. For example, universally enforcing a literacy testbefore being allowed to vote for a political election may lead to people from backgrounds with less access toformal education being excluded. However, literacy tests for a job position may seem appropriate if everyone hasan equal opportunity to take the test, as talents and abilities vary between individuals. One should thus carefullyevaluate the metrics being used to impose egalitarianism. Table 3 describes sub-types of egalitarianism.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "b625199e-e53a-4842-a2b6-a5109cb7059e",
                    "text": "Proportionalism entails adjusting the rights of each person proportionally based on theircontributions to production. Depending on the sub-type of proportionalism (shown in Table 4), contributionscould include the resources from each member of the population that went into production, the amount of actualwork that went into the deployment of those resources, or the amount of contribution discounting for luck thatwent into those resources.Previous operationalisation of proportionalism includes Leben [85], which constructs utility functions thatevaluate the distribution of rights in accordance with contribution. A fairness standard establishes the idealdistribution of rights by dividing the total amount of contribution by each individual\u2019s amount of contribution.The best distribution is the one with the minimum distance from this fairness standard for all individuals. Pitt [108]argues that ability to contribute should be central to methodological design of STS; to ensure self-determination,communities must be able to own and operate the platforms they use. Alternatively, Lam et al. [84] assign distanceto resource location as proportional to group size.A challenge with proportionalism is that there may be situations where groups or individuals do not confercontributions to production, but should be granted a distribution of rights. For example, those unable to contributedue to disability should still have a fair distribution of rights. Accommodating those who were unable to contributemay be mitigated by considering the inluence of luck. Table 4 shows sub-types of proportionalism.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "860e05df-7271-4177-a07a-1a4326941823",
                    "text": "Kant [78] argues that ethical principles are derived from the logical structure of action, beginningwith distinguishing free action (action for which the agent has reasons) from mere behaviour (Kim et al. [81]).Kant\u2019s categorical imperative grounds all moral duties as it applies unconditionally to rational agents (categorical),and is a command that could be followed, but might not be (imperative) (Johnson and Cureton [77], Wallach et al.[134]). The categorical imperative entails that a rational agent must believe their reasons for acting are consistentwith the assumption that all rational agents to whom the reasons apply could engage in the same actions (alsoknown as the universal law of nature) (Boddington [18]). For example, \u2018do not kill\u2019 is a categorical imperative:it is categorical in that if all rational agents committed murder, there would be no rational agents left; it is animperative as rational agents could kill but should not. Derived from the categorical imperative is the means-endprinciple (also known as the humanity formula). The means-end principle denotes that treating other people as ameans to an end is immoral (Abney [1], Kumar and Choudhury [83]). It would never be possible to universalisethe treatment of another as a means to some end; doing so would contradict the categorical imperative. Thiscontradiction occurs because of our ability to engage in rational self-directed behaviour.Kantian ethics have been operationalised in previous literature through the imposition of rules. Limarga et al.[89] implement the categorical imperative with two rules: irstly, since it is universal, an agent, in adopting aprinciple to follow (or judging an action to be its duty), must simulate a world in which everybody abides by thatprinciple and consider that world ideal. Secondly, since actions are inherently morally permissible, forbidden, orobligatory, an agent must perform their duty purely because it is one\u2019s duty, and not as a means of achieving anend or by employing another human as a means to an end. Berreby et al. [15] implement the means-end principlein the rule that an action is impermissible if it involves and impacts at least one person, but that impact is not theaim of the action. Svegliato et al. [129] use the moral rule that policies should be universalisable to stakeholderswithout contradiction. Allen et al. [4] suggest that the categorical imperative could be implemented as a higherprinciple to evaluate other rules. For example, when deciding whether to apply egalitarianism (ensuring equaldistribution), an agent could evaluate if this is the right thing to do by examining if it aligns with the categoricalimperative, i.e., if it would be rational for all agents to apply that principle.A diiculty with the categorical imperative is that it may be too permissive; it could permit intuitively badthings by allowing any action that can have a universalisable maxim (Abney [1]). A common example of thisis letting a murderer into your house because you cannot lie, and say that the person they want to kill is notthere. The means-end principle can also be too stringent as, interpreted strictly, it forbids any action in which aperson afects another without their explicit consent. There are issues that arise related to motivation and freewill. According to Kant, the motivation and reasons for why actions are taken are key to whether the action isethical. However, truly understanding motivation for action may require a level of self-awareness that in practiceis diicult to achieve (Boddington [18]). Chakraborty and Bhuyan [26] argue that AI cannot truly implementKantian ethics without having free will, which is necessary to possess autonomy and the power of reasoning inthe Kantian sense.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "0006ee4f-1468-41db-b3d5-a7892f9c1cb1",
                    "text": "According to virtue ethics, ethicality stems from the inherent character of an individual, and not the rightness orwrongness of individual acts (Yu et al. [141]). Right action is performed by someone with virtuous character. Infollowing this theory, one should not be asking what one ought to do, but rather what sort of person one shouldbe (Saltz et al. [113]). The qualities one possesses should be of primary importance, and actions secondary. Moralvirtues can be learnt and developed through habit and practice. The stability of virtues (if one has a virtue, onecan\u2019t behave as if one doesn\u2019t have it) entails that virtue ethics may be a useful way of imbuing machines withethics (Wallach et al. [134]).Virtue ethics can be used to formulate ideals for the use of AI, or to create AI which is virtuous itself (Heilinger[66]). To improve ethical use of AI, Robbins and Wallace [110] advocate for applying virtuous characteristics toresolve problems. Vanh\u00e9e and Borit [131] propose that this may be aided by using education to help designers ofsystems develop virtues. Hagendorf and Danks [64] advance this by delineating that teaching virtues involvesimparting tacit knowledge, social perception skills, and emotion, leading to the automatic \u2018feeling\u2019 of the rightthing to do.Other works focus on implementing virtues directly into machines; according to Tolmeijer et al. [130], inputsfor implementing virtue ethics in machines would be properties of the agent, the decision criteria would bebased on virtues, and this would be mechanised through the instantiation of virtues. Instantiating virtue ethics inautomated decision making is exempliied by Govindarajulu and Bringsjord [59], who deine virtues as learnt byexperiencing the emotion of admiration when observing virtuous people, and then copying the traits of thosepeople. Computational formal logic is used to formalise emotions (in particular the emotion of admiration),represent (virtuous) traits, and establish a process of learning traits. Greene et al. [61] argue that a virtue-basedsystem would have to appreciate the entire variety of features which call for one action rather than another in agiven situation.Virtue ethics can also be used alongside other approaches; Hagendorf [62] argue deontological approachesshould be combined with virtue ethics, using virtues to examine values and character dispositions. Planzeret al. [107] suggest implementing virtue ethics to assess the character of an agent in a model which also utilisesdeontology and consequentialism. Hendrycks et al. [68] implement virtue ethics as part of an assessment criteria,integrating scenarios demonstrating virtue ethics into a dataset used to measure the ethical ability of naturallanguage processing models. Jiang et al. [75] use this dataset to test their model trained on people\u2019s moraljudgements of various situations.A problem with virtue ethics highlighted by Saltz et al. [113] is that the holistic view it takes makes itmore diicult to apply to individual situations. Tolmeijer et al. [130] identify further challenges relating to theconcretion of virtues and conlicting virtues. To judge whether a machine or human is virtuous is not possible byjust observing one action or a series of actions that seem to imply virtue\u00d0reasons behind actions need to be clear.Requiring understandings of reasons behind actions makes it diicult to build virtues into machines, as there is ahigh level of abstraction in deining virtues. Additionally, conceptions of virtues can change greatly across timeand culture. Virtues instantiated in machines today may lead to unfair outcomes in the future as virtues change,or certain virtues may conlict with other virtues.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "76e660a6-c7e3-419c-9d97-4607be26e178",
                    "text": "In consequentialist approaches, right actions are identiied through their efects (Brink [23]). The moral validity ofan action can thus be judged only by considering its consequences (Rodriguez-Soto et al. [112]). Consequentialistprinciples can be used to weigh risks and opportunities (Heilinger [66]). A strength of this is that it can be used toevaluate decisions with complex outcomes where some beneit and some are harmed, by examining how beneitsand harms are distributed. It can thus explain many moral intuitions that trouble deontological theories, asconsequentialists can say that the best outcome is the one in which beneits outweigh costs (Sinnott-Armstrong[121]). In addition, Boddington [18] asserts that the goal-based nature of consequentialist theories suits computingwell.Consequentialist principles can be operationalised by analysing the consequences of diferent actions. Assessingethics through consequences denotes a diferent approach to deontology, which regards mental states as veryimportant for determining the ethicality of an action. For consequentialism, Tolmeijer et al. [130] denotesthat mental states can be largely disregarded. Planzer et al. [107] propose a multi-theory model in whichconsequentialism analyses the consequences brought about by a situation. Deontology and virtue ethics are usedin conjunction with consequentialism to make ethical judgements.Consequentialism has been implemented by weighing actions. Limarga et al. [89] assign each action a weightaccording to its worst consequence. Actions are part of a sequence to reach a goal, and their weights accumulate toa total amount. This total amount is optimised to select the sequence with the best overall consequence. Suikkanen[126] similarly suggests ranking agents\u2019 options in terms of how much aggregate value their consequences have.An option is right if and only if there are no other options with higher evaluative ranking. Tolmeijer et al. [130]argue that input for consequentialist principles would be the action (and its consequences), and the decisioncriteria would be the comparative well-being. This would be mechanised by selecting the consequence withmaximum utility. For binary classiication algorithms, Leben [85] suggests implementing consequentialism byexamining how weights are assigned to each group outcome based on relative social cost.However, in practice assigning weights to each outcome may be unrealistic to do for all outcomes (Leben[85]). There might be high computational costs if machines attempt to represent all possible outcomes available(Greene et al. [61]). A related issue is that estimating long-term or uncertain consequences and determiningwhich consequences should be taken into account is diicult (Boddington [18], Etzioni and Etzioni [48]). Theremay be moral constraints outside consequentialism which prohibit certain actions even when they have thebest outcomes, therefore rendering consequentialist theories incomplete (Suikkanen [126]). Another commoncriticism of consequentialism concerns deciding what is valuable or intrinsically good: whether it is pleasure,preference-satisfaction, the perfection of one\u2019s essential capacities, or some list of disparate objective goods (e.g.,knowledge, beauty, etc.) (Boddington [18], Brink [23], Tolmeijer et al. [130]).",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "d11eb7a3-cab2-4c40-b7bc-2b3201dedc92",
                    "text": "Utilitarianism denotes that the ultimate end is an existence exempt from pain and as richin enjoyment as possible (Mill [93]). Acts are evaluated by their consequences; an act is ethical if and onlyif it maximises the total net expected utility across all who are afected (Kim et al. [81]). Requirements forimplementing utilitarianism include an account of what outcomes are being aimed for, how to aim for thoseoutcomes, how to measure those outcomes, and what or who matters in assessing and aiming for those outcomes(Boddington [18]).Utilitarianism has been applied to assess fairness metrics and language models. To justify design choicesfor fairness metrics in binary classiication algorithms, Leben [85] suggests that a function could model eachdistribution and its efects (a utility function/measure of happiness outcomes); then run a selection procedureover aggregate utilities to maximise the sum. Hendrycks et al. [68] present a dataset of scenarios demonstratingutilitarian principles to analyse the ethical knowledge of natural language processing models. Jiang et al. [75] usethis dataset to test their model trained on people\u2019s moral judgements of various situations.Utilitarianism has also been used to select norms which promote value alignment. In MAS, Serramia et al. [116]implement a recursive utility function which identiies the preference utility of each value; the value supportof a norm is calculated by adding the utility of each value for that norm. Serramia et al. [117] expands this toassess normative systems. To aggregate value preferences, Lera-Leri et al. [88] implement utilitarianism as adistance function, selecting the optimum from the point of view of the majority. Similarly, Bakker et al. [13]aggregate value preferences estimated by a reward model, implementing utilitarianism to select the maximummean consensus in the group. In both Lera-Leri et al. [88] and Bakker et al. [13], social welfare functions areparametric to allow for implementation of diferent principles (ranging from utilitarian to Rawlsian).Approaches to operationalise utilitarianism in decision making includes training agents to make judgementsthat deliver the greatest happiness to the greatest number of people, as in Kumar and Choudhury [83]. Limargaet al. [89] assign a value to every action which is later used for inal evaluation. Azad-Manjiri [11] and Dehghaniet al. [35] select the choice with the highest utility. In Svegliato et al. [129], autonomous systems make ethicallycompliant decisions in moral contexts by decoupling the moral principle from the decision module, having aseparate moral rule (such as utilitarianism) which evaluates the suggested policy.A common criticism of utilitarianism is that it could lead to a minority being treated unfairly for the greatergood (Anderson et al. [7]). In addition, the theory cannot account for the notion of rights and duties or moraldistinctions between, for example, killing versus letting die (Abney [1]). There are also diiculties that arise withquantifying utility. Firstly, calculating the utility of every outcome may be computationally infeasible in scenarioswith a very large or ininite number of possible outcomes. Secondly, quantifying utility is diicult as there arediferent ways of conceptualising what utility means. For instance, whether there is a distinction between higherand lower pleasures will afect how outcomes are quantiied (Etzioni and Etzioni [48]). Diferent qualitativeunderstandings of utility necessitates diferent ways of quantifying it. To mitigate these issues, utilitarianismcould be an additional necessary condition, rather than the sole ethical principle (Kim et al. [81]). Applyingutilitarianism as an additional condition would allow for a diferent ethical principle to provide moral distinctionswhich are ambiguous in utilitarianism. For sub-types of utilitarianism, see Table 5.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "16759760-8fee-49ca-a907-936b5272f22f",
                    "text": "The maximin principle emphasises maximising the minimum utility by seeking to improve theworst-case experience in a society; guaranteeing a higher than worst-case minimum utility to each individual(Rawls [109]). Maximin thus shifts the focus towards improving the well-being of those who are worst-of (Leeet al. [87]).Maximin has been implemented in the domain of algorithmic fairness. To evaluate fairness metrics for binaryclassiication, Leben [85] demonstrates how a function modelling each potential distribution and its efects couldbe constructed, and then a selection procedure run over aggregate utilities. Diana et al. [38] implement maximinto measure fairness by examining worst-case outcomes across all groups, rather than diferences between groupoutcomes. Sun et al. [127] promote fairness by minimising the maximum cost of an allocation over all allocations.Chen and Hooker [27] couple maximin with utilitarianism in optimisation problems to ensure the least advantagedhave priority, but not at unlimited cost to everyone else.Other applications for maximin include preference aggregation. Lera-Leri et al. [88] formulate maximin as adistance function, selecting the optimum solution from the point of view of the most displaced. Bakker et al. [13]estimate preferences in a reward model, and then implement maximin to select the consensus which maximisesexpected agreement for the most dissenting member. Parametric functions are used to implement diferentprinciples such as utilitarianism and maximin in both Bakker et al. [13] and Lera-Leri et al. [88]. Ashraian[9] proposes implementing maximin using algorithmic game theory to assist governmental policy decisions.Governatori et al. [58] encodes maximin in an argumentation framework for reasoning about diferent moraltheories.In some situations however, maximin is seen as too risk averse. Consider two situations: A, where there is a70% chance of gaining \u00a3100 and a 30% chance of losing \u00a330; B, where there is a 50% chance of gaining \u00a310 and a50% chance of losing \u00a310. Sunstein [128] argues maximin would promote choosing option B, but under standardaccounts of rationality it would be preferable to choose option A, as the expected value is much higher. Thus,when expected value is high, a reasonable level of risk is preferable to low risk and low expected value. On theother hand, maximin is preferable if we do not know how bad the worse outcome would be (i.e., how much itwould decrease welfare, where welfare is not synonymous with expected value), or if it would be catastrophicallybad.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "ce1487fd-f4c0-444c-a1b2-848dae140e60",
                    "text": "In an envy-free allocation, no agent envies another agent (Sun et al. [127]). Fairness thusexists when there are minimal levels of envy between groups or individuals. Resources may be unequallydistributed, but as long as agents do not envy one another, this is considered fair (Boehmer and Niedermeier[19]).To implement envy-freeness, Boehmer and Niedermeier [19] propose that an assignment of resources to agentsis ethical if no agent prefers another agent\u2019s bundle (of resources) to their own.Arguably, what is important might not be a relative condition to other people, but if people have enough tohave satisfactory life prospects (Lee et al. [87]). Also, the existence of an envy-free allocation can\u2019t be guaranteedwhen items are indivisible, e.g., chores that need to be assigned to multiple agents. Problems with guaranteeingenvy-freeness has led Sun et al. [127] to implement relaxations of the principle, such as envy-free up to one item.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "0bd7d4f0-2ad0-494c-9287-185811fa14a6",
                    "text": "The doctrine of double efect suggests that deliberately inlicting harm is wrong,even if it leads to good (Deng [36]). On the other hand, inlicting harm might be acceptable if it is not deliberate,but simply a consequence of doing good. For this principle, an action is permissible if the action itself is morallygood or neutral, some positive consequence is intended, no negative consequence is a means to the goal, andthe positive consequences suiciently outweigh negative ones (Govindarajulu and Bringsjord [59], Lindner et al.[90]).Govindarajulu and Bringsjord [59] using formal logic to automate the doctrine of double efect, and also thestronger version of the doctrine of triple efect. They use the framework in two diferent modes: to build doctrineof double efect compliant autonomous systems from scratch, or to verify that a given AI system is doctrine ofdouble efect compliant. Another approach by Berreby et al. [15] implements this principle through rules thatproscribe an action if it is intrinsically bad, if it causes a bad efect which leads to a good efect, and if its overallefects are bad.An issue with the doctrine of double efect is that it still allows bad actions to happen as long as they are notintended, which may have some morally dubious outcomes.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "c210ffae-d48a-4e25-a7b9-5c9daf8ba3c9",
                    "text": "People should be free to act as they wish unless doing so would result in harmto another person (Gabriel [56]). Do no instrumental harm allows for harm as a side efect, but not as a means toa goal.Lindner et al. [90] implements do no harm by stating that a technical agent may not perform an action whichcauses any harm. Dennis et al. [37] utilise do no harm to ensure agents select plans which can be formallyveriied as ethical. Aliba\u0161i\u0107 [3] suggests that in the context of cryptocurrency trading, AI should be developed sothat it avoids outcomes which cause harm to stakeholders such as individual traders, investors, and the largercommunity. Harms in this context can occur through diferent channels such as market manipulation, insidertrading, and fraud.Sometimes, however, there may be situations in which causing harm is inevitable. In such situations, thisprinciple alone would not be able to give clear ethical guidance.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "80cb8b13-eed4-4c3b-ae25-789e5f51097e",
                    "text": "In addition to the principles mapped out here, there are other principles mentioned in literature which we nowdescribe. For reasons that shall be stated, we did not include these in the taxonomy.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "7acdff2e-e137-420d-ae2c-424f723fc80a",
                    "text": "Egoism is acting to reach the greatest outcome possible for one\u2019s self, irrespective of others (Kumarand Choudhury [83], Robbins and Wallace [110]). Aliba\u0161i\u0107 [3] argues egoism entails assessing if outcomes beneitthe interest of the individual or group. In the context of AI and cryptocurrency trading, this would entail selectingoutcomes which are better for the system\u2019s investors. Elsewhere, this principle is rarely mentioned in literatureand this may be because it would lead to likely unethical outcomes if it was imbued in AI agents. If agents wereprimarily concerned with themselves, irrespective of others, it seems unlikely that they would be ethical (whichinvolves one party\u2019s concern for another, Murukannaiah and Singh [99]). This is because fairness is aimed at thewell-being of others as well as the self, whereas egoism is solely self-centred.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "a8be7b54-ffec-44a0-bf3b-97f637e2b404",
                    "text": "Particularism emphasises that there is no unique source of normative value, nor is therea single, universally applicable procedure for moral assessment (Tolmeijer et al. [130]). Rules or precedentscan guide evaluative practices, however, they are deemed too crude to do justice to many individual situations.Therefore, the moral relevance of a certain feature and the role that it plays will be sensitive to other features ofthe situation. Ethical evaluation should thus be carried out on a case-by-case basis. Inputs for particularism couldinclude the situation (context, features, intentions, and consequences), with the decision criteria resting on rulesof thumb and precedent, as all situations are unique. The mechanism to decide upon an action would dependon how much it its with rules of thumb or precedents. Jiang et al. [75] present a model to learn descriptiveethics from a data resource of people\u2019s ethical judgements of situations. Some challenges identiied are that thereis no unique and universal logic, thus each situation needs a unique assessment. Particularism is thus hard togeneralise and encode in a reproducible way. Bai et al. [12] argue that we cannot avoid choosing some set ofprinciples or rules in developing AI, whether they are implicit or explicit.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "239872ca-f987-41a6-b175-d33cfb16868c",
                    "text": "This principle emphasises feelings of interconnectedness with others, building onthe motivation to look after those who are vulnerable or dependent (Gilligan [57], Robbins and Wallace [110]).Morality is a tool to care for others through nurturing relationships (Kumar and Choudhury [83]). To be ethical,one should think about the situation that others are in. Using your experience, you should act in a nurturing andresponsible way. Communication plays an essential role, through the relation of listening and being heard. Careethics reduces moral distance in AI, where moral distance is when those who are not considered in decisions aretreated unethically (Villegas-Galaviz and Martin [133]). Care ethics can be applied to AI by examining the voicesnot being heard, afected relationships and interdependence, how the system treats context, and if the vulnerableare being exploited (Villegas [132]). Yew [140] advocates for the application of care ethics in the design of robotsused for companionship and assistance in healthcare. There is a debate as to whether the ethic of care is a theoryin itself, as explored by Held [67], or a practice, virtue, value, or activity which supplements other theories, asSander-Staudt [114] suggests. Because of this ambiguity we do not include the ethic of care in the taxonomy.However, the ethic of care could be used as a guiding factor in the application of ethical principles, as it enhancesthe importance of considering others outside of yourself. Emphasis on consideration of others provides goodsupport for value alignment and responsible decision making.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "1fe71f37-8d6f-40e2-9686-f23198bec5e1",
                    "text": "Lastly, there is a wide variety of principles proposed in cultures outside of the historyof Western ethics. Moral frameworks have been established in societies across the world, including Confucian,Shinto, and Hindu thought as well as religious frameworks like Judaism, Christianity, and Islam (Hagerty andRubinov [65]). There is a multitude of moral frameworks across cultures, with signiicant variation within theseframeworks. Arguably, ethics and culture are inseparable and to understand one you must look at the other.Therefore, ethics must be considered within its cultural context. The reason these principles were not included inthe taxonomy is because they would require whole taxonomies of their own. An important direction for futurework would be to apply the methodology used in this project speciically to non-Western ethical principles, toform a taxonomy of such principles. Forming taxonomies of principles from a broader variety of cultures willhelp AI practitioners to build cross-cultural ethical technology.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "76b370ea-3f31-4fb5-94e0-9e666ab8593c",
                    "text": "We iterated over the papers identiied in our review to analyse of previous operationalisation of ethical principlesfor Q (Operationalisation). First, we ind a variety of technical implementations of ethical principles, summarisedin Tables 6 and 7. Second, previous literature integrates principles into reasoning capacities in a top-down,bottom-up, or hybrid architecture, summarised in Table 8. Third, practitioners should be speciic about whichprinciple(s) they are operationalising; previous literature suggests that pluralism may help with this decision.Fourth, abstractly, operationalisation falls into the categories of (1) applying rules for deontological principles, (2)developing virtues for virtue ethics, or (3) evaluating consequences for consequentialist principles.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "731f1c02-2680-4af3-b86c-2cf5bfbd4c50",
                    "text": "A variety of technical implementations have been used to encode ethical principles. Expanding upon Tolmeijeret al.\u2019s [130] categorisation, approaches to encode principles into a format computers can understand includelogical reasoning, probabilistic reasoning, learning, optimisation, and case-based reasoning. In Table 6 and Table 7,we map each ethical principle found in literature to their technical implementations.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "1f5f273b-738b-4fcd-9857-d58e6e6cbe8a",
                    "text": "To engineer morally sensitive systems, Wallach et al. [134] argue that practitioners must decide on the architecturefor integrating ethical principles. These fall within three broad approaches: (1) top-down imposition of ethicaltheories; (2) bottom-up building of systems with goals that may or may not be explicitly speciied; (3) hybridapproaches which combine top-down and bottom-up features. We discuss examples of each architecture andissues which may arise. Table 8 summarises our indings of the technical implementation of ethical principlesaccording to the various architectures.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "7b688992-9422-4bbd-ab61-42bf3ddc9c7e",
                    "text": "Bottom-up approaches involve machines learning to make ethical decisions byobserving human behaviour in actual situations, without being taught any formal rules or moral philosophy(Etzioni and Etzioni [48]). Bottom-up techniques include artiicial neural networks, reinforcement learning, andevolutionary computing (Tolmeijer et al. [130]). An example of this is Noothigattu et al. [102], who use inversereinforcement learning to align agents with human values by learning policies from observed behaviour. Infuture work, inverse reinforcement learning could be used to align policies with ethical principles, in a similarway to how Noothigattu et al. [102] align policies with human values. Kim et al. [81] suggest this may improveexplainability by assimilating policies with principles which, by their nature, imply logical propositions that canbe reasoned about. Dyoub et al. [46] utilise answer set programming (ASP) as a knowledge representation andreasoning language to deductively encode ethical rules. They then utilise inductive logic programming to identifythe missing ASP rules needed for ethical reasoning, by learning the relation between the ethical evaluation of anaction and related facts in that action\u2019s case scenario.A challenge of bottom-up approaches, however, lies in the risk that machines learn the wrong rules, or cannotreliably extrapolate to cases not relected in the training data.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "81a78e77-f731-4bdf-8fd1-c97fe8f01876",
                    "text": "Top-down approaches install ethics directly into the machine, instead of asking themachine to learn from experience, as in bottom-up approaches (Kim et al. [81]). We ind that many works usetop-down approaches to integrate ethical principles into reasoning capacities of machines. Dehghani et al. [35]implement deontological and utilitarian principles through a combination of qualitative modelling, irst-principleslogical reasoning, and analogical reasoning. Tolmeijer et al. [130] found that principles can be implemented asrules through logical or case-based reasoning, using domain knowledge to reason about the situation given asinput. Bai et al. [12] do not explicitly encode principles from normative ethics, but provide a methodology inwhich a set of principles implemented in a top-down fashion forms a \u2018constitution\u2019 and is used to ine-tune apreference model. Parts of their constitution can be aligned to theories like virtue ethics, for example, \u2018choose theresponse that a wise, ethical, polite and friendly person would more likely say\u2019, where \u2018wise, ethical, polite andfriendly\u2019 could be conceptualised as virtues. The preference model is then used to train a reinforcement learningagent.Top-down approaches have been utilised for optimisation tasks. Serramia et al. [117] implement utilitarianismto optimise for norm systems that promote the most preferred values in a society. Diana et al. [38] operationalisethe principle of minimax (minimising the maximum loss, adapted from maximin - maximise the minimum) usingoracle-eicient learning algorithms. Minimax is applied to analyse fairness considerations in diferences betweengroup outcomes. Also considering fairness, Sun et al. [127] formalise envy-freeness as rules to examine thetrade-of between diferent fairness allocations. Chen and Hooker [27] combine the principles of maximin andutilitarianism in a model for mixed integer and linear programming which can be applied in a top-down mannerto optimise social welfare functions. Lera-Leri et al. [88] operationalise diferent ethical principles by tuning theparameter of a function, which is then applied as a distance function to optimise value preference aggregation.However, as human knowledge does not tend to be very structured, domain knowledge needs to be interpretedbefore it can be used. A diiculty of top-down approaches is that human understandings of philosophicalrules need to be encoded in a way that machines can understand, which may mean that information is lost ormisrepresented.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "5f459a36-c2fe-4c96-81c4-7ae6c0233223",
                    "text": "Hybrid approaches embody aspects of both top-down and bottom-up approaches. Astop-down and bottom-up approaches each employ diferent aspects of moral sensibility, combining the two mayresult in better implementation of ethical principles (Allen et al. [4]). A beneit of hybrid approaches is that theyincorporate both ethical reasoning and empirical observation, which allows context to be taken into account.Hybrid architectures have been used in individual decision making through logic and reinforcement learning.Berreby et al. [15] supplement top-down imposition of rules with bottom-up observation of contextual information,allowing agents to represent and reason about a variety of deontological and consequentialist theories. Theypropose a modular logic-based framework based on a modiied version of the Event Calculus, implemented inAnswer Set Programming. Limarga et al. [89] implement principles using non-monotonic reasoning in an eventset calculus, which allows rules to be revised when a conlict arises. Rodriguez-Soto et al. [112] provide a methodthat irst characterises ethical behaviour as ethical rewards, and then embeds such rewards into the learningenvironment of the agent using multi-objective reinforcement learning. Following a top-down approach, ethicalprinciples are formalised along normative (whether the action is good or bad) and evaluative dimensions (howgood it is). In a bottom-up manner, the principles are then used as reward functions.For optimisation, Bakker et al. [13] operationalise ethical principles by tuning the parameter of a function,which is then used to aggregate preferences estimated by a reward model.Hybrid architectures have also been used in the context of large language models (LLMs). Hendrycks et al.[68] construct a dataset of contextualised scenarios demonstrating a variety of ethical principles to assess ethicalknowledge learnt by LLMs. Jiang et al. [75] present a data resource of people\u2019s judgements of ethical situations, anduse it to train a model. The authors test the model against tasks implementing ethical principles from Hendryckset al.\u2019s [68] dataset. Shi et al. [119] implement Hendrycks et al. [68] in a plugin moral-aware learning modelto train a reinforcement learning agent which alternates between learning tasks and morality in a text-basedenvironment.Whilst there are beneits from combining aspects of top-down and bottom-up architectures, there diicultiesalso emerge from the meshing of dissimilar architectures and diverse ideas about the origins of morality. Wheretop-down approaches emphasise ethical concerns arising from outside the entity, bottom-up approaches focus onethics arising within, embodying diferent aspects of moral sensibility. Hybrid systems must be able to balancetensions between internal and external ethical concerns (Allen et al. [4], Wallach et al. [134]).",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "f83780b7-ef03-4309-bee0-26e7080555dc",
                    "text": "Practitioners should specify which ethical principle(s) will be operationalised. This could be aided by referring tothe taxonomy we have suggested, which contains a broad array of ethical principles found in AI and computerscience literature (Figure 1). Leben [85] emphasises that being clear about which principle is being used will helpdesigners to further clarify what inputs are necessary for their application, which in turn will improve ethicalreasoning capabilities and explainability of how decisions have been made.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "bd0092e2-f2e6-440e-b8b1-92484b42d2ed",
                    "text": "Human morality is complex and cannot be captured by a single classical ethicaltheory (Robbins and Wallace [110]). Thus, it may not always be easy to decide which principle to apply. Pluralismadvocates that there is not one approach that is best. In a similar way to how we learn and implement diferentprogramming languages, Brennan [22] argues that we utilise diferent ethical principles depending on the problemat hand. Context and various reasoning techniques could be used to choose between appropriate principles.Tolmeijer et al. [130] advocate for further research according to this approach, suggesting the development ofmulti-theory models where machines interchangeably apply diferent theories depending on the situation.Pluralism has been operationalised in previous literature. Svegliato et al. [129] propose a framework which de-couples ethical compliance from task completion to avoid unanticipated scenarios which do not relect stakeholdervalues. They suggest implementing a pluralist approach in the form of an extra moral constraint representinga moral principle. This allows for the decision-making module\u2019s policy to be evaluated considering its ethicalcontext, leaving room to implement diferent ethical principles as the ethical rule. Lera-Leri et al. [88] implementa range of ethical principles as distance functions, and use these functions to aggregate value preferences. Planzeret al. [107] propose utilising the Agent-Deed-Consequence model for ethical decision making in AI, whichimplements virtue ethics to evaluate the character of a person (Agent), deontology to examine their actions(Deed), and consequentialism to assess the consequences brought about by the situation (Consequence). If allcomponents are positive, the moral judgement is positive.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "36fc4a51-894d-47a1-8466-c603c002184d",
                    "text": "We have found that abstract implementation of principles falls into three main categories: rules, consequences,or virtues. We discuss examples of each implementation category and potential diiculties that may arise. De-ontological principles have been operationalised by applying rules, and choosing an action based on how itaccords with those rules. Virtue ethics has been operationalised by developing virtuous characteristics. Conse-quentialist principles have been operationalised by evaluating consequences and choosing an action based on theconsequences it produces.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "7ea92e19-0574-45fb-805a-079b120e70a2",
                    "text": "For deontological principles, some approaches suggest operationalising principles byapplying a set of rules to possible actions to determine which ones would be satisfactory, such as Abney [1],Berreby et al. [15], and Greene et al. [61]. Examples of this, as suggested by Murukannaiah et al. [98], would beapplying the rule that the disparity of preference satisfaction for stakeholders should be minimised, extractedfrom the principle of egalitarianism. Another example is Leben [85], applying the rule that stakeholders shouldbe treated proportionally based on their contributions to production.Due to the abstract nature of ethics, diiculties arise in inding appropriate ways to encode ethical principles inconcrete rules. One diiculty lies in deciding if rules should be interpreted as strict or defeasible (Tolmeijer et al.[130]). For example, an essential part of Kant\u2019s [78] ethics is that the reasons for actions must be universalisable toall agents. The need for reasons to be universal implies that this rule should be strict. However, this could permitactions that are bad according to other principles, suggesting that it should be defeasible (Abney [1]). Nashedet al. [101] argue that although implementing ethics through rules sets a high standard for agent behaviour,expressive, efective, and general rule sets are diicult to generate. Creating systematic ways of encoding theethical principles we identify (Figure 1) into rules, including understanding whether rules should be strict ordefeasible, to use in the reasoning capacities of AI could thus be a direction for future research.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "f9ca47cd-599f-4ce5-b3c8-97c221f21fba",
                    "text": "For virtue ethics, ethicality stems from the inherent character of an individual (Kazimand Koshiyama [79]). To solve a problem according to this theory, virtuous characteristics should be applied(Robbins and Wallace [110]). Thus, the theory can be operationalised by instantiating virtues (Tolmeijer et al.[130]). Instantiating virtues is exempliied by Govindarajulu and Bringsjord [59], who understand virtues aslearnt by experiencing the emotion of admiration when observing virtuous people, and then copying the traitsof those people. This is implemented using computational formal logic to formalise emotions (in particular,the emotion of admiration), represent traits, and establish a process of learning traits. To formalise virtues, theauthors use a deontic cognitive event calculus, which is a quantiied multi-operator modal logic that includes theevent calculus for reasoning over time and change. By formalising emotions (admiration) in this way, agentsassociate admiration with the actions of others. Traits are formalised as a series of instantiations of a type ofbehaviour. If enough admiration is felt for particular traits, the agents learn the traits, thus instantiating virtues.However, virtue ethics can be diicult to apply to individual situations (Saltz et al. [113]), and there arechallenges that arise with the application of virtues across time and culture (Tolmeijer et al. [130]). Futureresearch could therefore examine the applicability and appropriateness of virtue ethics across diferent contexts.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "fb82f3fe-02f4-404b-a551-e801bfe47122",
                    "text": "Consequentialist principles may be operationalised by evaluating the conse-quences of diferent actions (Limarga et al. [89]). Suikkanen [126] suggests this could be done by ranking agents\u2019options in terms of how much aggregate welfare their consequences have. Dehghani et al. [35] specify this withthe principle of utilitarianism, by selecting the choice with the highest utility. Ajmeri et al. [2] operationalise theprinciple of maximin by improving the minimum experience in the consequences of an action. Consequencesare also used to operationalise the principle of envy-freeness, which Sun et al. [127] address by promoting theoutcome with the lowest levels of envy between groups or individuals.Issues arise in predicting all of the possibilities an action could produce. Predicting all possibilities could becomputationally challenging, requiring complex calculations (Greene et al. [61]). There are thus limitations tosimulating all possible consequences of an action in non-deterministic and probabilistic environments; futurework could explore applying multiple ethical principles to such environments.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "afc7cd76-7ed4-4a63-86b6-b1080ba8be83",
                    "text": "To address Q (Gaps), we now examine existing gaps in ethics and fairness research in AI and computer scienceliterature, speciically in relation to implementing multiple ethical principles in reasoning capacities.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "59c4fad4-4aeb-4101-9059-939c7c2036bb",
                    "text": "Understanding strengths and weaknesses of various approaches improves critical understanding and constructiveengagement (Boddington [18]). Therefore, key gaps include research on lesser-utilised principles. We suggestthat future directions consider less commonly seen principles, or incorporate a wider array of principles. Thisincludes researching principles from other cultures outside of the Western doctrine, which is important as ethics isculturally sensitive (Hickok [69]). Implementing ethical principles from various cultures will aid the accessibilityand fairness of AI, as it can better apply to stakeholders from diverse backgrounds. Hongladarom and Bandasak[71] survey non-western guidelines for AI principles, inding unique cultural presuppositions in some areas andglobal consensus in others. Expanding this research to examine AI and non-western principles from normativeethics is a future research direction.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "0bbb63be-0679-4b19-b899-141332ff93bd",
                    "text": "We identify various diiculties with the implementation of ethical principles that may result in ethical dilemmas,from which various gaps arise. Anderson and Anderson [5] deine ethical dilemmas as situations where eitherthere is not a good choice between diferent outcomes, or where the choice between diferent outcomes is notobvious (e.g., the distinction of how good one outcome is compared to another is not obvious). Future researchcould address gaps that arise in resolving these dilemmas.In certain situations, dilemmas arise when the application of one principle cannot support one action overanother. Azad-Manjiri [11] suggest that one way to resolve this could be by examining how similar decisions weremade previously. If no similar decisions have been made previously, an action is selected at random. However,this approach may run into the naturalistic fallacy, looking at what is the case rather than what ought to be thecase. In addition, relying on random choice may not result in the most ethically appropriate action. A gap existsin further examining how to resolve dilemmas where principles cannot support one action of another.For each principle, dilemmas arise when its application leads to an unfair outcome, as all moral theories havesome counter-intuitive implications (Robinson [111]). This implies no single theory can denote how to programethical AI (Pagallo [104]). Pluralist approaches, in which diferent principles can be weighed against one anotherto ind the most appropriate answer, could help mitigate these issues. Works such as Governatori et al. [58],Lera-Leri et al. [88], and Planzer et al. [107] provide methodologies accommodating multiple principles. A gapexists in applying such methodologies to compare the application of multiple principles in scenarios whereparticular principles lead to unfair outcomes. However, weighing alternatives may not always be possible. Futureresearch should investigate the feasibility of applying diferent principles in diverse scenarios.Dilemmas may arise with the application of multiple principles, as diferent principles can give diferentanswers which may conlict (Persson and Hedlund [106]). This is exempliied in Nashed et al. [100], who indthat agents implementing diferent principles favour diferent policies. In addition, it is diicult to apply abstracttheories to concrete situations. To aid this, Tolmeijer et al. [130] suggest that particularism (which incorporatesrelevant contextual factors in ethical reasoning to identify if a certain feature is morally relevant or not) couldhelp identify which principle is the most appropriate in that setting. A gap exists in exploring if aspects ofparticularism can be used to resolve dilemmas where diferent principles promote conlicting outcomes. However,there are also issues that arise with the application of particularism. While ethics examines principles that sociallyimpose what\u2019s right or wrong, morality deals with social values of right or wrong (Jiang et al. [75]). Moraldisagreement can arise when stakeholders have diferent beliefs about which facts are morally relevant, andwhich ethical principle is true (Awad et al. [10], Robinson [111]).There are thus various gaps and diiculties which arise in regard to resolving ethical dilemmas. Drawing theseideas together, there are gaps in inding reliable methodologies for AI practitioners to decide which principleis most appropriate for a particular case, considering the dilemmas which may arise. Robinson [111] exploresthree solutions which may help to resolve dilemmas: moral solutions, compromise solutions, and epistemicsolutions. Moral solutions select a moral theory either by what we think is true, some general theory which wecan agree on, or what we could hypothetically agree on under certain disagreements. Compromise solutionschoose principles based on a social choice approach, or treat principle selection as a multi-objective optimisationproblem, optimising based on inferred moral values or goals. Epistemic solutions harness information about thedisagreement as evidence of moral facts, and then appeal to a rule for decision making under moral uncertainty.Alternatively, epistemic solutions could attempt to achieve an overarching moral view which accommodates asmany relevant ethical judgements as possible. Each approach has various strengths and limitations, as discussedin the paper. Robinson [111] concludes that problems of moral disagreement should be treated as problems ofmanaging moral risk, where moral risk is the chance of getting things wrong and what you thereby risk. A gapexists in implementing such solutions to evaluate how they address ethical dilemmas in practice.Implementing Ethical Principles in STS5.3An application of implementing ethical principles in STS is to support governance capacities. Governance of STSinvolves establishing standards for the proper use and development of technology, as deined by Floridi [52],and administration of systems by stakeholders themselves, as deined by Singh [120]. Under the perspective ofmacro ethics, responsible governance should incorporate norms and value preferences of diferent stakeholders.However, dilemmas arise when norms or values conlict. Operationalising ethical principles in reasoning helpssupport governance capacities to resolve these dilemmas in equitable ways that support the needs of diferentstakeholders (Woodgate and Ajmeri [138]). Gaps exist relating to how ethical principles can be operationalised inSTS to promote equitable governance capacities.Previous work provides guidance for applying ethical principles to reason about values and norms in com-putational decision making. For example, Ajmeri et al. [2] broadly reference the principles of egalitarianismand utilitarianism within the context of utilising values and norms in MAS for ethical reasoning in individualdecision making. This research may beneit from the consideration of other ethical principles to enable broaderapplicability. Lera-Leri et al. [88] present a method for applying multiple ethical principles to aggregate diferentvalue preferences, but do not consider the inluence of norms. Serramia et al. [117] demonstrate how to selectnorms that best align with a known value system. Combining these approaches to aggregate diferent valuesystems using ethical principles, and then using the aggregated value systems to select value-aligned norms, isa promising direction for future research. In addition, a gap exists in examining how such approaches can beincorporated in decentralised collective decision making.However, challenges arise when implementing ethical principles in STS considering value systems. Norms andvalues are interdependent with context and decision-makers, and research should consider how value systemschange according to context, for example, over time (Osman and d\u2019Inverno [103], Smit and Pitt [122]). Gapsexist related to implementing principles in STS in ways that account for the relationship between context andchanging value systems.Properly incorporating broad social context requires careful consideration to avoid entrenching dominantrelations of power (Weinberg [136]). This includes accounting for the ways in which existing dynamics shapehow technology is developed and deployed. Applying ethical principles must therefore integrate broad socialdynamics, and appreciate how social dynamics afect governance capacities (Munn [97]). Gaps emerge withrespect to accommodating broad social dynamics and avoiding perpetuating unjust power dynamics in theapplication of ethical principles to governance capacities.To understand how ethical principles can accommodate for broad social dynamics, participatory approachesmay be useful to incorporate human input throughout the design process. For example, Dubljevi\u0107 et al. [43]combine participatory approaches with multi-criteria decision making to capture the importance of diferentharms and make clear the perspectives of diferent stakeholders. Weinberg [136] emphasises that collaboratingwith those afected by the technology improves the propensity to leverage knowledge from marginalised groups,understand how the technology is situated in its social context, and address what is most ethically concerning,rather than what is most convenient to measure. Participatory approaches present opportunities to investigatequestions related to what extent ethical principles are generalisable across diferent groups of people, what peoplemorally disagree on, what preferences people have over ethical principles, and if and how people follow ethicalprinciples in their daily lives. Gaps exist in further examining these questions.",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                },
                {
                    "id": "9ff0304f-0813-4ccf-9283-4f8f3a5e8b76",
                    "text": "To better address the pursuit of responsible AI, research must be human-centred (Collins et al. [31], Dignumand Dignum [41]). Shifting the perspective to the macro ethics of STS, considering the range of relevant humanvalues and ethical features, may help to enable responsible ethical-decision making which can be justiiedand held accountable (Chopra and Singh [29], Lechterman [86]). However, dilemmas arise when values conlict(Murukannaiah et al. [98]). To resolve these dilemmas in satisfactory ways, ethical principles can help to determinethe moral permissibility of actions (Lindner et al. [90], McLaren [92]).We identify a variety of ethical principles which have been previously operationalised in AI and computerscience literature. We also identify key aspects of operationalising ethical principles in AI, including selectingtechnical implementation, clarifying the architecture, specifying the ethical principle, and using rules, conse-quences or virtues. Key gaps that imply future research directions include expanding the taxonomy, resolvingethical dilemmas where principles conlict or lead to unfair outcomes, and implementing principles in STS whilstaccommodating for changing contexts and broad social dynamics. We envision that our indings will contributetowards developing responsible AI by aiding the incorporation of ethical principles in reasoning capacities.AcknowledgmentsWe thank the anonymous reviewers for their careful reading and insightful comments which helped us to sub-stantially improve the manuscript. JW thanks the EPSRC Doctoral Training Partnership Grant No. EP/W524414/1for support. NA acknowledges partial support from the UKRI EPSRC Grant No. EP/Y028392/1: AI for CollectiveIntelligence (AI4CI).",
                    "reference": "[1] Jesse Woodgate and Nadinia Ajmeri. 2024. Macro Ethics Principles for Responsible AI Systems: Taxonomy and Directions. ACM Comput. Surv. https://doi.org/10.1145/3672394."
                }
            ]
        },
        {
            "paper_title": "Strategies for Increasing Corporate Responsible AI Prioritization",
            "authors": "A Wang, T Datta, JP Dickerson",
            "publication_info": "arXiv preprint arXiv:2405.03855 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2405.03855",
            "chunks": [
                {
                    "id": "6e9f778c-cd9e-4427-9979-e7a0bb5f9f7c",
                    "text": "",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "2fddd111-b1d2-4257-81f5-8e338d290445",
                    "text": "There is increasingly research on how to implement fair andresponsible artificial intelligence and algorithmic bias tech-niques. However, relatively little of this research has trans-lated into industry practice (Holstein et al. 2019), and re-cent research has often found that a key obstacle is institu-tional barriers which hamper adoption (Madaio et al. 2022,2020). In an ideal world we might have both better-scopedand higher quantities of regulation in place, and while thereare certainly movements in this direction (e.g., EU AI Act,GDPR), policymaking can be slow and imprecise. In themeantime it is important to understand the motivations thatcompanies have historically had for incorporating responsi-ble AI concerns so that we can learn from what has workedin the past in order to enact greater impact in the present.A major obstacle facing Responsible AI (RAI) in prac-tice is the lack of corporate investment of resources. Indi-vidual RAI practitioners develop their own strategies withintheir respective companies (Ali et al. 2023), but there isnot yet a consolidated analysis of these separate techniques which could be shared with each other. The research ques-tion we seek to answer in this work is: How can we mo-tivate companies to increase the resources they spend onRAI? We foreground the word we, and underline through-out the work who has the power to pull on each of theselevers that we unveil. To discover the strategies, we conduct16 semi-structured interviews with participants who a) cur-rently or in the past worked at companies that used algo-rithms with RAI-related concerns, and b) had insight into orcontrol over the decision-making process that set RAI prior-ities. We provide a summary of our findings in Fig. 1, ulti-mately discovering six high-level reasons that motivate com-panies to prioritize RAI: external cues like publicity, regu-latory pressures, organizational macro-motivators like thefunding type of a company, relevance to company success,unique circumstances due to company culture and individu-als, and the effort and ease of implementation. It boils downto prioritization: companies are rarely against RAI, but whenthere are many competing interests, RAI is often neglected.However, the story is not as simple as \u201cincrease regu-lation\u201d or \u201cpublicize RAI failures,\u201d strategies which priorwork has covered as well (Ali et al. 2023). Rather, there isnuance to each of these strategies, and the details are thestrength of our work. We provide a novel perspective by re-vealing the complexity behind each strategy as well as theactors most able to exercise them. For example, while pub-lic RAI failures are known to motivate prioritization, theycan also scare companies from engaging in RAI at all, fear-ing backlash for an attempt that doesn\u2019t meet a journalist\u2019sstandards. Additionally, while we know that companies arescared of not complying with legislation for lawsuits, ourinterviews provide the novel insight that companies are notjust scared of legitimate lawsuits, but rather any publicitythat could lead to a frivolous lawsuit because of what mightbe surfaced during the legal discovery phase. Or while wemight have an idea of how RAI can serve as a company dif-ferentiator, the critical impact of generative AI on this differ-entiation has not been previously examined. Our interviewsunveil that with the rise of generative AI, differentiating viaRAI provides an arena for companies that cannot afford tocompete on compute-heavy tasks.Ultimately, our interviews detail a tangled web of ambi-guity: there are no guaranteed steps forward for increasingRAI prioritization (not even regulation, which can be heavy-handed, misdirected, and hard to get right). Yet, by paint-ing the landscape of strategies and showing the challengeswithin each, we bring clarity and structure to encouragingways forward. As an example, a practitioner who is hav-ing trouble getting buy-in for an RAI-related concern learnthat they can leverage the argument that it will help theircompany differentiate from a competitor. Additionally, bynaming actors with power to enact change, we offer a callto action to previously overlooked groups, such as venturecapitalists and consultants. We conclude by discussing fourconcrete directions that emerge to us as the most promising.By looking to the past we scope ourselves to the efforts thathave already been tried. However, we believe that throughthis consolidation of separately-performed techniques, wecan illuminate the landscape of approaches for RAI prior-itization and help to inspire novel methods going forward.As anonymized participant #4 (P4) puts it, \u201cThere\u2019s a lot ofreally great ways that people in the field can meet each otherto navigate some of the challenges that we all share.\u201d",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "db8e23c6-c714-4bc4-931b-3117cbb253af",
                    "text": "",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "279591a1-ff55-41d1-b1df-6fdecb5af03c",
                    "text": "There are many difficulties to implementing RAI in practice.These are due to factors like organizational barriers (Denget al. 2023; Rakova et al. 2021; Madaio et al. 2020), princi-ples which are hard to operationalize for reasons such as be-ing too vague (Holstein et al. 2019; Schiff et al. 2020, 2021;Mittelstadt 2019; Munn 2023; Green 2020), research outputswhich do not match practitioner needs (Madaio et al. 2020,2022), and the structure of labor and workplaces (Shilton2012; Widder and Nafus 2023). Of these obstacles, our workfocuses on the lack of resources invested by companies.Many important works have explored the dynamics be-tween RAI and corporations under capitalism. Ali et al.(2023) conduct a thorough analysis of RAI implementationin technology companies. They find three key barriers: theoverriding authority of software product launches, the diffi-culty of quantifying ethics, and the frequent team reorgani-zation that can cause loss of knowledge. Rather than focus-ing on the barriers RAI practitioners have, in our work weexamine RAI successes to understand what can be ampli-fied to increase RAI buy-in. Both of our works share thegoal of suggesting steps forward to increase RAI invest-ment, but due to the different framing, settle on differentways forward that are collectively important. Metcalf, Moss,and danah boyd (2019) capture how the inherent culture ofSilicon Valley can be adversarial to substantive ethics dueto differing incentive structures and sometimes-conflictingvalues, and Phan et al. (2022) describe the conceptualiza-tion of ethics knowledge as simply another form of capi-tal that is exchanged. On the other hand, recent work de-scribes a shift in perspective that a company\u2019s responsibil-ity encompasses social impact in addition to just profit\u2014though potentially only larger companies have the resourcesto do this, compared to smaller ones (De-Arteaga, Feuer-riegel, and Saar-Tsechansky 2022). Other works look specif-ically to smaller startups and the unique pressures they facein this domain (Winecoff and Watkins 2022; Hopkins and Booth 2021; Vakkuri et al. 2020).Ultimately while ways of increasing RAI prioritizationhave surfaced in prior work, our singular focus on this ques-tion brings out new details and nuances, and can help to in-form who can do what in increasing RAI prioritization.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "9f8ca435-67a1-4a67-873c-ccd900a36d57",
                    "text": "In our work, we focus on inductively discovered findingsrather than deductive, theory-driven themes, but provide dis-cussion of relevant broader theories here. Central is the eco-nomic system of capitalism allowing private actors to oper-ate with a central motivation of maximizing profit (Gilpin2000). In theory, this motive for profit is said to ensure theefficient allocations of resources (Smith 2002). However,this market fundamentalism can play out in many ways:scholarship has examined how organizational policies areoften decoupled from daily practices (Meyer and Rowan1977; Bromley and Powell 2012), and our work examinessimilar ongoing tensions like between valuing ethics in theabstract and prioritizing profits in practice.Neo-institutional theory is one of the key frameworks forunderstanding organizational behavior and change (Powelland DiMaggio 2012), and posits that sources of organiza-tional change can be endogenous or exogenous, i.e., inter-nally or externally-triggered. Actors in an organization canendogenously construct change, and our work sheds lightson the specifics of this in the RAI space. Through the lensof organizational field theory, organizations can internallyforge new paths through the interactions of the inhabitantsof these institutions (Schneiberg 2007; Hallett and Ventresca2006). Additionally, corporations can be reordered by spe-cific highly-connected individuals who take strides to dis-rupt the old institutional structures (Weber, Roth, and Wit-tich 1978; Suchman 1995; Dimaggio 1988). Meanwhile, ex-ogenous sources include environmental \u201cjolts\u201d (e.g., suddensocietal events), force organizations to adapt in unique ways.These \u201ctransitional blips\u201d allow for the strengthening of cul-tural priorities and introduction of unrelated changes (Meyer1982). We discuss a variety of external triggers for corpo-rations: from publicity to social movements to regulations.Similarly, social movement literature discusses how corpo-rate protests and large-scale media coverage affecting cor-porate stock prices can be particularly effective at incitingchange (King and Soule 2007; Davis and Thompson 1994).There are parallels to draw from domains like privacy andESG (environmental, social impact, and corporate gover-nance) issues to RAI\u2019s place within capitalism. In all of thesespaces, a value besides profit comes into tension (Hartzmarkand Shue 2023; Cohen, Gurun, and Nguyen 2020).",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "3c37ee8b-ed95-4d20-9c87-a4afec1e89cb",
                    "text": "To answer our research question: Based on past successes,how can we motivate companies to prioritize RAI?, we con-ducted semi-structured interviews with 16 participants from13 companies.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "03695784-54eb-4232-8fcf-f26a2e393745",
                    "text": "We sought participants who a) currently or in the pastworked at companies that used algorithms with RAI-relatedFigure 1: Six themes surfaced from our interviews as directions for increasing the motivation that companies have for prioritiz-ing responsible AI. We summarize each of these six themes, and underline examples of actors that are able to act on each.concerns and b) had insight into or control over the decision-making process that set RAI priorities. We recruited theseparticipants by reaching out to our personal networks, post-ing on Twitter, LinkedIn, Slack groups, and snowball sam-pling by asking participants to refer us to others who mightbe useful to talk to. In total, we interviewed 16 participantsfrom 13 companies between July - September 2023, reach-ing thematic saturation by our last two interviews (Greenand Green 2004). Given the specialized background of ourparticipants, the pool of eligible participants was relativelysmall. Each participant was compensated with a $25 giftcard. The industry and company size of each participant islisted in Tbl. 1. Their roles are: scientist/engineer (10), prod-uct (2), manager/executive (4). Our participants span a widerange of domains: Software as a Service (SaaS), healthcare,finance, manufacturing, and more.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "7fdd21b4-c289-4810-b5c7-95d3c582b9c4",
                    "text": "The first author conducted all of the interviews, which were30-60 minutes and conducted on an online video platform.Each interview began with explaining the purpose of ourstudy and asking whether participants were comfortablewith the level of anonymity we would provide. We thenasked participants if we could record the video and automati-cally generate a transcript; if not, the interviewer took exten-sive notes. The interview instrument of guiding questions isincluded in the Appendix. Generally, they were customizedfor each interview and depended on the participant\u2019s role atthe company. We sent all participants a draft before submis- sion to ensure comfort with their level of anonymity giventhe sensitivity of the subject.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "7a2082c8-a24a-415c-b676-522a25c6e055",
                    "text": "We conducted an inductive, data-driven thematic analysisto identify the key themes in our 16 interviews (Braun andClarke 2006; Thomas 2006). The first author coded all ofthe interviews, and picked out the six most theme-dense in-terviews which the second author coded as well. Then, theunion of all codes were consolidated, resulting in 967 uniquecodes. These codes were relatively detailed (e.g., \u201cTrade-offs are never explicit numbers of value, but things like howlong can a team work on this,\u201d \u201cTeam started out as engi-neers using canonical notions of fairness from papers, thenas evolved got product and legal involved and asking moresubstantive questions\u201d). The authors iteratively clustered thecodes through five rounds of thematic groupings until con-sensus was reached on six high-level themes (section head-ers) composed of 15 sub-themes (subsection headers).",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "b47f4bd8-6793-422e-9445-ea67303b54fd",
                    "text": "We surface six high-level themes about what motivates com-panies to prioritize RAI. External cues and regulatory pres-sures are frequently cited as the most powerful reasons. Fourmore arise as important and may be overlooked: organiza-tional macro-motivators like how a company is funded; rele-vance to company success, company culture and individuals,and effort and ease of implementation. Topics in each cate-gory are not mutually exclusive, but we group them to bringPtcpt.P1P2P3P4P5P6 IndustryHealthcareEntertainmentE-commerceFinanceManufacturingEntertainment # Empl.10-1001k-50k100-1k50k+50k+100-1k Ptcpt.P7P8P9P10P11 IndustryHiringEntertainmentFinanceElectronics[redacted] # Empl.10-10050k+50k+50k+50k+ Ptcpt.P12P13P14P15P16 IndustrySocial MediaFinanceSaaSElectronicsSaaS # Empl.1k-50k50k+10-10050k+10-100Table 1: Each participant (Ptcpt.) with their company\u2019s industry and employee number (# Empl.). The roles are (data) scien-tist/engineer: 10, product: 2, manager/executive: 4.structure to the strategies we have for affecting the prioritiza-tion that companies place on RAI. Overlaid on these themesare the different actors that currently have power (Widderet al. 2023) and access to each lever (e.g., journalists are rel-evant for impacting external cues, but so are consumers).We underline the named actors throughout, and in AppendixFig. 2 summarize these group-specific strategies by diffi-culty of implementation. Of course, these are not the onlypeople able to pull these levers, just some of the closest.Ultimately we find a complex web of factors, and actingon one or even a set of strategies is not guaranteed to in-crease RAI prioritization. However, while we are careful towarn against a confinement to these options which wouldforeclose additional strategies, our analysis of previouslysuccessful motivators helps to inspire a promising path for-ward and equip practitioners with a toolkit of strategies.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "952c94e0-08ac-4bd0-8b6f-a7c56ac65e2d",
                    "text": "One of the largest motivators voiced by participants to pri-oritize RAI was external cues. This refers to axes like mediapublicity, general public awareness, and customer demands.Publicity Publicity, and specifically the fear of bad press,was brought up by every participant as a major motivatingfactor. As P14 mentions, the \u201cimpetus for change usuallycomes when something bad happens, right?\u201d P[4, 5, 8, 9,11, 15] all specifically bring up the utility of having a readylist of public failures of other companies, and P15 explains,\u201cthe most effective thing was pointing to failures of othercompanies.\u201d P4 echoes that they are \u201ca wealth of knowledgeof cautionary tales that help me make those conversationsand get that buy-in,\u201d since examples convey how even whenyou think you have control, things can always go wrong.The Partnership on AI\u2019s AI Incident Database was broughtup by a few participants as being useful for this.However, as great of a motivator as bad publicity is, thereare also downsides. P12 mentions that even though theircompany was working on RAI, they were too scared to pub-licly talk about it or implement anything that would makethose efforts visible, lest that lead to bad publicity. Theircompany preferred to remain silent on the topic rather thanrisk being in the public eye for claiming to be doing RAI work that could be criticized. Given how powerful of a mo-tivator bad press is, it seems important to maintain it, butuseful to bring nuance so that companies have the space topursue RAI without facing quick and reductionist headlines.In general, good press did not seem to be a very strongmotivator. The only notable instance was a public ethicsaward given to one participant\u2019s organization, which theycited as a motivator for the company to live up to the award.Even though rewarding behavior that should be expected isnot ideal, creating reward incentives for companies couldhelp communicate it as something the public values. Thisis analogous to the LEED rating system or Fair Trade labels,though all have been shown to be susceptible to differentforms of ethics-washing (Bowen and Aragon-Correa 2014;Doan et al. 2017; Low and Davenport 2005).Overall, journalists have immense power in carefullyexposing irresponsible AI practices at companies, andmaintainers and contributors of incident databases givepractitioners an arsenal of arguments to draw from. How-ever, concerningly, there is a decline in journalism fund-ing (Bauder 2022).Public awareness Downstream of journalism is the grow-ing public awareness that AI is not objective and can causedisparate harm. P[1, 3, 4, 5, 7, 8, 11, 12, 14, 15] all bring uphow RAI has entered the public consciousness, and that thiscollective zeitgeist has facilitated getting company buy-in onRAI. P14 notes, however, that they believe RAI is becomingless trendy than it once was. In the same way that social jus-tice movements (e.g., #MeToo, #BlackLivesMatter) can losemomentum, we need to collectively ensure these topics arenot just a passing trend. Again, journalists will play a criticalrole in maintaining relevance in public awareness.This public awareness can also have other positive effectsfor RAI: P8 brings up the externality of \u201ceven if those ob-vious problems don\u2019t have a legal hook yet, because themore things are in the press, the more likely someone willsue. Even if those lawsuits might not have great standing orgrounds, just for the optics of it or if those lawsuits get to adiscovery phase... it could lead to embarrassing things beinguncovered.\u201d Public awareness can also affect the recommen-dations that consultants and other influential external advi-sors provide. P4 shares anecdotally that they were told \u201cit\u2019sa lot easier as an external advisory function or a provider ora partner to be able to push... to get some things done ver-sus internally.\u201d P10 also describes how external consultantsprovide recommendations based on what they project to befuture trends, and for such a speculative enquiry, mattersthat are high on the public radar can become a self-fulfillingprophecy. Many participants also brought up large languagemodels and the increase in generative AI as leading to anincrease in awareness around the capabilities and harms ofthese models (P[4-13, 15]). However, in many cases theexcitement to develop and deploy these generative modelsoutweighed the increase in concern. As P11 describes, \u201cItmostly pushed the innovation impulse... So it did increasethe number of people who saw the salience [of RAI], but itdidn\u2019t necessarily give it on balance more power because itgot less momentum than the business.\u201dAt this point, our conflation in wording by using \u201cRAI\u201d asa catch-all term runs into collisions. P5 brings this up by dif-ferentiating between those components of RAI that are moreknown and thus prioritized, compared to lesser-understoodcomponents which are left behind. They explain \u201cthings likefairness, privacy, those are already very high awareness sowe get lots of great traction around conducting privacy im-pact assessments and around doing bias testing and perfor-mance testing. But when it comes to things like account-ability and transparency and labor impacts, that gets a littlemurkier.\u201d Thus, now that we have established the first levelof public awareness, internal advocates can help move to-wards more nuanced notions of RAI that encompass com-ponents like labor impact.Customers The third aspect of external cues is the powerof the customer (P[1-3, 5-14]). This is relevant both forretaining current customers as well as attracting new ones.However, it did seem to be differently prioritized by compa-nies across domains. For example, healthcare and finance aretwo domains that stand out as caring more about customerperception of RAI: P9 says \u201cas a bank, if you damage yourbrand, people will not trust you as much as they did before.\u201dAnother relevant axis for how much customer demand candrive RAI is the type of customers desired. P7 notes that \u201cifyou don\u2019t build trust among underserved communities, andamong small companies who basically cannot afford to havebias in their systems, we will not make profits, we won\u2019t getcustomers.\u201d This is similar to how prior work has found thatstartups need to work harder than larger companies in estab-lishing trust and legitimacy (Winecoff and Watkins 2022).One way customers can express demand is through theirpurchasing behavior and technology usage. P8 mentionsboycotts and grassroots campaigns that have shifted com-pany prioritization, and P14 mentions successful examplesof other companies in the marketplace. In the technologyspace, companies built on the customer demand of privacyinclude DuckDuckGo and Signal. Customers can exercisesignaling behaviors with their capital in order to communi-cate to companies their desire for RAI. This serves as mo-tivation to companies because, as P4 puts it, \u201cWhy are youleaving customers on the table? Because of your unfair prac-tices, right? You could catch more business.\u201d",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "31e292f1-73e2-4aa7-aafe-eaf9ad29360b",
                    "text": "Beyond external cues, the other most frequently cited moti-vator for prioritization of RAI were regulatory pressures.Compliance First and foremost, nearly all of our partici-pants bring up the relevance of legal compliance as a rea-son to prioritize RAI (P[0-5, 7-15]). More specifically, it isthe fine: \u201cIf you do something and your company is liable,it\u2019s billions [of dollars], so actually you need to take care ofthese things\u201d (P10).However, P[1, 8, 11, 12, 15] also bring up situationswhere the presence of regulatory pressures, whether real orimagined, actually hamper RAI adoption. One such well-studied obstacle is that of regulation prohibiting the collec-tion and/or usage of sensitive attributes, which is often nec-essary in RAI work to even measure the amount of unfair-ness that is present (Ho and Xiang 2020; Kumar, Hines, andDickerson 2022; Bogen, Rieke, and Ahmed 2020; Andruset al. 2021). P15 mentions that they find legal work-arounds,for example collecting pronouns instead of gender. P12 alsomentions that in many cases it is very unclear what the reg-ulation is asking for and prohibiting, and so trying to adhereto it is itself a challenge.Ultimately there is a somewhat mixed bag with respectto whether more regulation is desired or not. P8 notes tworeasons more regulation might actually be harmful to RAIgoals: \u201cuncertainty over whether the act of engaging in thework itself might be creating risk\u201d and \u201cif something is re-quired in all cases in a blanket manner where that require-ment is actually not well-suited for a certain circumstance,but people have to do it anyway, that leads to a lot of confu-sion.\u201d Echoing the second point, P12 adds that in lower riskdomains, more legal regulation could be harmful by addingextra overhead with small impact. Policymakers can spendmore time with experts to understand the specific domainswhere more regulation can help with RAI prioritization, andhow to scope it.Exceeding Compliance In addition to merely adhering toregulation, we find two reasons that regulatory pressuresmay motivate companies to do more than the law explicitlyasks. The first is to prepare for future regulation that may bemore strict. For example, P13 points out that \u201cwe think \u2018hey,there might be change in the future,\u2019 so we already have sys-tems in place where we already are following a much stricterregulation that the one that already exists.\u201d Conversely, P4mentions that because regulation might change in ways youcannot anticipate, there is a negative incentive to go aboveand beyond in case what you do conflicts with future reg-ulation. The second reason is to demonstrate to regulatorsthat self-regulation is sufficient, and to fend off stricter im-positions. By demonstrating responsibility, companies hopeto have a seat at the table when regulation is discussed. Forexample, P8 says \u201cfailure of private actors to act of their ownvolition is likely to lead to more strict requirements... thoserequirements might end up being more difficult to achievethan what the voluntary work would have done.\u201dThere are a number of ways to interpret these statements,and one takeaway is that even signaling done by regulatorsabout future regulation can help motivate action. However,given the ambiguity that regulation can take, it is importantto communicate it in ways that encourage benevolent action,and are accompanied by substantive follow-through.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "db898761-b870-4a37-b313-934bd7707aac",
                    "text": "Participants also described broader organizational factors forpursuing RAI which range from taking advantage of parallelsocial movements like environmental sustainability to exter-nal market forces and company profitability.Parallel social corporate concerns Participants cite be-ing able to leverage growing enthusiasm around other socialmovements, namely environmental sustainability and dataprivacy to boost RAI. P[4, 5, 8, 10] describe the benefitsof leveraging the paths paved by environmental movementsin industry, such as carbon footprint reporting, environmen-tal impact assessments, and ESG investing. These structureshave helped define the role of expertise for environmentalactivists, providing them with not only a voice but also au-thority to weigh in on trade-offs and decisions. P4 describedleveraging executive commitments to environmental efforts,\u201cIf you can connect it to those sustainability conversations,I think that has the most traction right now.\u201d Practitionerscan understand what tactics worked best in parallel spacesto better define the roles and authority of expertise for RAI.Corporation and funding type Structural financial as-pects of an organization also affect the ability to imple-ment RAI practices. Both P7 and P14 cite non-profit com-pared to for-profit organizations as more conducive placesfor prioritizing RAI principles. One participant\u2019s companyis registered as a Public Benefit Corporation, indicating thattheir company mandate is to specifically benefit the publicin some way. Outside of the corporation\u2019s filing status, P1and P7 also discuss the additional flexibility that receivingfunding from external grants allows. For these participants,grants were used as a funding source for certain RAI-relatedprojects which allowed for longer explorations into aspectsof fairness and equity. Founders can exercise choice in sig-naling their mission through their corporation type and fil-ing status, and RAI advocates can look for grants to fundspecific projects they are working on.Stage in profit cycle There are differing financial pres-sures depending on the maturity of an organization. P9 andP15 both described the benefits of larger, more establishedcompanies having the space to focus on maintenance andimpact in a way that smaller companies do not. P1 also de-scribes, \u201cit really depends on where the company and prod-uct is, and it\u2019s lifecycle. [That helps decide] how much youinvest [on RAI].\u201d Meanwhile, for startups, their stage in thecompany life cycle matters. Startups that are not currentlyraising capital may have more room to pursue principlesof impact, as P7 describes, they \u201cdon\u2019t have to appease in-vestors or expectations of profit\u201d as heavily as they mighthave had to when seeking to raise a new round of funds. Relatedly, P[5, 7, 12] specifically discussed recent down-ward economic forces resulting in company-wide hiringpauses and \u201can organizational environment in which peoplejust have more on their plate and it\u2019s easier for things to fallthrough the cracks because there is incentive to focus on dif-ferent things\u201d (P5). This makes it difficult for RAI efforts togrow or even continue. Conversely, when the market is good,P[1, 2, 5, 10, 12] all cited greater access to resources \u201cto fo-cus on impact rather than revenue\u201d (P1). RAI efforts oftenrise when the economy is good and ebb when the economy isbad. Venture capitalists can be variance reducers here, sup-porting RAI in times of both wealth and austerity.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "07f35c0c-8619-4137-a23e-a833005bde87",
                    "text": "Individuals frequently spend effort connecting RAI to com-pany success to increase buy-in within the scope of already-existing corporate incentives (Ali et al. 2023; Metcalf,Moss, and danah boyd 2019; Phan et al. 2022; Rakova et al.2021). P[4, 5, 8, 15] brought up looking at their companyhistory to see what worked best in the past (e.g., \u201cfindingexamples of where we\u2019ve seen positive changes in our oper-ations, and really do audits of how did they get there?\u201d - P4).While some strategies will be company-specific, part of themotivation of this work is to consolidate narratives so RAIpractitioners can learn from each other. All of the strategiesin this section are ways for internal practitioners to frameRAI for increased prioritization.Competitor Differentiator One powerful motivator forcompanies to prioritize RAI is as a differentiator from com-petitors (Widder and Nafus 2023). Many participants specif-ically brought up publicity around companies that have mis-treated or gotten rid of their RAI teams as a motivator forcompanies to set themselves apart (P[2, 4, 5, 7, 10, 12, 15]).P5 describes that their company is trying to \u201cestablish [them-selves] in a position of thought leadership. But also distin-guish ourselves against the types of FAANG companies thathave made very visible pivots away from investing in thesekinds of efforts.\u201d P10 adds that differentiating based on RAIofferings rather than aspects like generative AI can be moreappealing because it is easier to compete with larger, morewell-sourced companies on this front.However, the incentive to use RAI as a peer differentia-tor is not guaranteed to lead to more responsible AI. As P8comments, \u201call the companies are now trying to competeon demonstrating responsibility for more advanced forms ofAI. But that\u2019s very undefined, and so, you don\u2019t have to domuch to try to demonstrate. There\u2019s no one who can call youout on saying that\u2019s not enough because no one agrees onwhat\u2019s enough.\u201d This indicates an urgent need for journaliststo partner with experts to weigh in for the public on whichcompany\u2019s RAI efforts are substantive. We see the incentiveto ethics-wash throughout our work, and will elaborate onthis more during our recommendations.Long-term priorities Another route practitioners take totie RAI into company success is through alignment withlong-term priorities. For example, through saving time downthe line by incorporating RAI now. P8 motivates that\u201cscrambling to do something later on would be much moredifficult than proactively adjusting course to do somethingthat would be reasonable, and probably closer to the prac-tices that the company was already doing.\u201d P10 invokes theidea of \u201cethical debt\u201d (Cunningham 1992; Petrozzino 2021),where even though incorporating ethics may take more timenow, you will save time in the long run.Another way is linking RAI to a company\u2019s strategic goalsor values. P11 gives the example that \u201cI think the conceptand the term is popular, and I think people wanted to beseen as engaging with it often for pro-social reasons... So ifthe CEO sends out a letter that says... we believe in rigorand quality. And then you say, this aligns with our com-pany\u2019s goals of rigor and quality.\u201d P5 reports similarly thatit was \u201cusually about reiterating some form of some valueeverybody has or that everybody should have or that every-one hopefully has, and then giving them recommendationsthat can help them be more in line with that goal.\u201d P6 pointsout how a specific company-level change helped with this,\u201cwe went through and re-did our values at a company level,which is part of the reason why I think that I\u2019m able now totalk about it because user-centricity is now one of our valueswhen it wasn\u2019t explicit before.\u201dShort-term priorities On the other hand, when a com-pany decides how to prioritize large numbers of demands,short-term priorities can often beat out long-term ones, andthese tend to, but do not always, deprioritize fairness. P[1-3,5, 6, 8, 10, 12, 14, 15] all bring up that ultimately RAI issecondary to utility of the product and financial success. P6straightforwardly explains \u201cyou need revenue to pay for thepeople who are actually going to solve the [RAI] problem,\u201dand P13 laments \u201cI don\u2019t think any company at all is goingto care that much about fairness or responsible AI until itstarts hurting their wallets.\u201d For the most part, however, it isnot that people do not care about RAI, but rather the relativeurgency of RAI compared to other priorities feels smaller.The themes we discuss can help to create this urgency.Thus, it often takes a shift in perspective or reframing ofRAI under a utility perspective to elevate its priority (P[0-11, 13-15]). P5 gives an example where they told a team:\u201cyou guys who said that you need to collect as much dataas possible... If we make things more explainable and trans-parent that will get people to talk more and that means moredata for us.\u201d P4 argues for a bigger shift, and that when aperceived tension between RAI and utility comes up, \u201cwhyis this a compromise? Again, I think it\u2019s a failure of imagi-nation... Are we going to hurt people in the process? I thinkthere needs to be a shift in how we think about value.\u201d Interms of how personalized these reframings need to be, P11describes that there was a \u201cstandard battery of these conceptsthat we would draw on, but when you\u2019re talking to someonein model risk versus a data scientist, their needs and goalsare very different,\u201d and P8 echoes there are \u201cpeople whotruly deeply care about AI safety and could be convincedto care about fairness, but it\u2019s not through a lens of compli-ance. It\u2019s through a lens of AI safety and long term benefitto people.\u201d Even though this reframing needs to be cateredper role and personal priority, techniques for these roles andvalue sets can be shared amongst RAI practitioners. One concrete strategy to \u201csell RAI\u201d that practitionersfound successful is tying together both numbers and a story(P[6, 11, 14]). As P6 explains, \u201ca truly complete story hasboth the numbers element and the emotional, look this iswhat a user actually experiences.\u201d Whereas our participantsoften find the story easier to craft, many express a needfor better measurement techniques to formulate the num-ber (P[4, 6, 8, 11, 12, 13, 14]). Fairness is notoriously hardto measure (Jacobs and Wallach 2021; Wang et al. 2022;Katzman et al. 2023), but there is an urgent need for in-terpretable, quantitative measures, both for decision-makersas well as customers (\u201ca lot of the fairness issues are supertechnical and may or may not relate to the ability to explainto consumers whether they\u2019re being treated fairly or not.\u201d- P8). RAI measurements are important in communicatingand explaining progress to non-technical stakeholders, whoare often the decision-makers choosing what to prioritize.P8 describes companies \u201cnot really preferring to dive intoreally ambiguous spaces that could both alleviate and cre-ate risk without clarity.\u201d And yet, given that the ambiguityaround RAI interventions will likely never go away entirely,we need to educate decision-makers about this inherent am-biguity and convey that it does not detract from RAI impor-tance but does require more patience. As with many caseswhere it is contextual who is well-positioned to act on this, itcould be an external consultant, executive, or coworker whois best situated to communicate this.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "4ee8b0db-da5d-4712-8440-da5449811878",
                    "text": "Despite the common themes we\u2019ve drawn from our inter-views, our findings also reveal how subjective and context-specific the reasons for prioritizing RAI can be. Here, wediscuss the role of company culture and individuals.Company culture Starting at the very high-level, country-specific cultural norms are one component that influenceRAI prioritization. P10 works for a company based out-side of the USA and mentions \u201cPeople don\u2019t change jobsin [country]... So practically their personality does get con-nected [with the company].\u201d They explain this means that anindividual\u2019s sense of morality becomes intertwined with thatof the company\u2019s, and they thus prioritize ethics.Industry domain and company size are also relevant tocompany culture. Similar to how customers tend to be morevalued in domains like healthcare and finance, P13 from fi-nance notes that \u201cwe\u2019re not like the other tech companieswhere we are constantly trying to be at the cutting edge\u201dand explains that this slowness allows better RAI integra-tion. In terms of company size, P9 shares that compared tostartups that may try to hire people who already have exper-tise, their larger company hopes to retain employees for along time, and can afford to offer training sessions to teachalready existing employees about RAI topics.Overall, many participants indicate that company culturefeels ingrained and inherent, indicating it may be hard tochange. In fact, P[1, 7, 10, 14] all specifically used the phrase\u201cDNA\u201d to describe company culture around RAI. Due to theoften separate RAI teams and complicated organizationalstructures, participants also brought up the need to spread in-ternal awareness for within-company RAI initiatives (P[5, 9-15]). Awareness of within-company RAI initiatives can helpshift the culture, but often involves extra labor on the part ofinternal practitioners to spread the word.Individuals We find that often it is specific circumstancesand individual experiences which cause people to championRAI. We separate the impact of executives in more manage-rial roles compared to individual contributors.Participants share that individuals higher up in the orga-nization have some, but not full, power in increasing RAIprioritization. The cases where a single individual\u2019s de-sire has been enough is when they have sufficient power(P10), the company is hierarchical (P10), or the companyis small enough and leadership is unified in their mission(P7). The circumstances leading to these individual effortscan be highly specific. For example, one participant men-tioned their CEO attended the FAccT conference and wasinspired. P16 also mentioned the relevance of personal risk:\u201cfor executives, because it\u2019s not only company risk or los-ing your job, but it\u2019s personal brand risk.\u201d The other sideof how personal these choices are is that individual circum-stances can also cause RAI deprioritization. P8 points outtwo possible reasons: executives don\u2019t want to set a prece-dent for themselves that they will be held to in the future,and if previous RAI attempts did not pay off they may nowhave resistance. The unique circumstances that bring power-ful individuals to RAI suggest it may be hard to implementany uniform strategy, but that it is still critical to create asmany of these \u201cserendipitous\u201d situations as possible, e.g., byengaging more stakeholders at conferences in the Responsi-ble AI space, e.g., FAccT, AIES. One key intervention pointis education, as that is a formative time where many willencounter a set of pedagogy.RAI work frequently falls to people of marginalized iden-tities (P[1, 2, 5-7, 11, 16]) doing what is often volunteerwork that does not help career advancement (Deng et al.2023; Ali et al. 2023). P[4, 7, 11] mention the prevalentburnout amongst people working in this space, and P11notes a unique challenge with respect to human resourceproblems. They explain that the same interpersonal prob-lems are present in RAI career ladders as in other engi-neering career ladders, but unlike more generalized careerswhere someone can switch teams, there is often only oneRAI team within a company, and thus no way to work underdifferent management. This can lead to high turnover andlosses in institutional knowledge (P5) (Ali et al. 2023).Another part of the problem is no one thinks RAI istheir responsibility (Widder and Nafus 2023; Lancaster et al.2023). P5 explains that \u201cengineers are very much in line withlegal going \u2018we just make stuff, I\u2019m not responsible for whatpeople do.\u2019 \u201d Individuals seem to believe that letting the de-fault persist is not making a choice, when it is in fact a value-laden choice of its own.As with many kinds of collective efforts, much of the suc-cess of RAI efforts boils down to interpersonal dynamics. P5observes that \u201cthere are some individuals internally... whohave spent a lot of social capital to make [RAI] happen\u201dand P8 adds that there is a \u201clot of internal maneuvering to get things through that are not built into a natural incen-tive structure in any organization.\u201d P11 elaborates that animportant relevant factor is general pleasantness of work-ing with someone even in terms of following up on emails.They elaborate that \u201cyou can demonstrate being thoughtful,having good reasons, being a good collaborator over a pe-riod of time. Then even more resistant people, they\u2019d sort ofwarm up... So, you\u2019re building out that your credibility, yourreputation.\u201d P12 similarly mentions that their team focuseson being proactive with RAI metrics ready so any team thatcomes to them will not have to wait long. It is unfair thatRAI workers are held to a higher standard, which can con-tribute to the prevalent burnout. P[5, 11, 13] mention howRAI teams are overburdened and under-resourced, leadingto additional strain.Related to company \u201cDNA,\u201d individuals also sometimesthink of being ethical as personally intrinsic. When askedabout RAI adoption, many simply bring up the relevanceof individuals being \u201cgood\u201d (P[1, 6-12, 14]), and doing thiswork because they think it is the right thing to do or want tobe seen as \u201cpro-social.\u201d P11 finds that in convincing peopleto adopt RAI, appealing to their morals often worked. How-ever, intention without expertise can also have downsides,and believing that inherent \u201cgoodness\u201d is sufficient for RAIcan be a serious harm. P8 flags that \u201csometimes people juststart [RAI efforts] because they\u2019re interested and that can ac-tually be problematic if they measure something that is usinga definition a company\u2019s already decided is not the preferredone or has detriments.\u201d Thus, we need better education thatemphasizes how difficult RAI work is, and that it requirescollaborating with experts.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "ca0b2fda-73d2-4b0e-8b8b-04b5705fcc6e",
                    "text": "Finally, we consider how the effort and ease of RAI imple-mentation in practice contribute to adoption likelihood.Implementation effort Though perhaps obvious, manyparticipants explicitly bring up how easier RAI efforts are farmore likely to be adopted (P[4-6, 8, 10, 11, 13-15]). How-ever, navigating ethical dilemmas is rarely easy, and neces-sarily requires time to both contemplate and implement. Inpractice, this often requires RAI teams to \u201cdirect towards themore ethical path of least resistance\u201d (P4). P5 says that com-promises are necessary in order to not be ignored: \u201cWe\u2019renot in the business of saying \u2018stop.\u2019 We try not to be a gate.We try to be a steering wheel.\u201d These responses can be de-moralizing and feel like accepting the status quo rather thansystemic change, and P5 laments that \u201cyou\u2019re never going toreach the radical thing that you want to reach,\u201d but explainsthis to be better than the counterfactual. There is a difficultbalance in implementing (even incremental) improvements,even if they do not always solve the source problem (Green2021). Sometimes though, speed and ease of an RAI inter-vention can be more of a rhetorical tool, and emphasized inframing rather than actual practice, by comparing it to whatwould take longer. P15 gives an example from trying to cre-ate an internal review system and \u201ctrying to frame it as it\u2019ssuch a quick and easy process. All you have to do is [...].[It] doesn\u2019t take that long and we really pared it down com-pared to what the traditional university does.\u201d Some otherways to make RAI intervention easier include tying into al-ready existing pipelines. P[1, 2, 4, 10, 13] all voiced how itwas beneficial to piggyback fairness issues onto other inter-nal changes (Deng et al. 2023), e.g., P1 remembers \u201ctakingadvantage of the fact that we\u2019re integrating to new hardwareas an opportunity to make this [RAI] change\u201d and P13 men-tions \u201cif you\u2019re already spending a bunch of money to tryand curate the data... then it makes sense to have the sameteam look at the data and make sure that data is also fair.\u201dWe also find three key components of implementation dif-ficulty that can influence individual support or resistance.Individuals do not like delays to deployment, detest paper-work, and do not find RAI interesting to work on. For delaysin deployment, by the time a team encounters RAI concernsthey often already feel their product is ready to be deployed,and see any delays as burdensome. However, the seeminglysimple solution of incorporating RAI from the start is hardin practice. Beyond just being difficult to change the sta-tus quo (P4), P13 paints the landscape: \u201cat any given pointof time, there\u2019s just like thousands of teams working onthousands of different models. So it\u2019s basically impossiblefor [RAI folks] to be involved in the process of model cre-ation right from the get-go. Because 80% of the models thatwe work on don\u2019t ever see the light of day because they\u2019reall proof of concepts\u201d (Vakkuri et al. 2020). On the otherhand, delaying can also be a compromise. P12 describes us-ing \u201ctime-boxes\u201d where they convince teams to delay de-ployment by some predetermined number of weeks duringwhich the team must demonstrate effort on implementingRAI interventions. They find this compromise useful, be-cause teams have an end date assurance. However, it can behard to enforce such effort-based approaches, and after de-ployment, effort is not a sufficient metric to end-users whosuffer the consequences. Another key point of tension is pa-perwork. P5 shares that they use paperwork as both a carrotand a stick, offering to reduce the bureaucratic compliancepaperwork for teams that can demonstrate they are incorpo-rating RAI concerns: \u201cYou can just kind of lean on peopleand annoy people with paperwork.\u201d Finally, P5 notes thatsome engineers just want to work on what they find techni-cally interesting, to the point of sometimes even prioritizingthat over what customers explicitly say that they want. P[1,5, 6, 9-11, 13] all say that people see RAI as grunt work,and if it were intellectually interesting, it might be more pri-oritized. RAI presents genuinely challenging technical prob-lems, so often this just needs to be made clear to practition-ers. Through both framing and changes, if RAI interventionswere presented by practitioners in ways that seem to includejustifiable and reasonable delays, do not increase seeminglyuseless paperwork, and are technically interesting, much ofthis individual resistance, which can be significant, would goaway. And much of this can be in the framing\u2014practitionersoften do not feel paperwork is actually effective, so if it werestreamlined to be more efficient and those filling it out weregenuinely convinced about the importance (P11), these ob-stacles would be reduced. Research to practice In translating RAI research intopractice, a number of challenges are brought up by prac-titioners that impede RAI prioritization. P12 discusses thatacademic research often doesn\u2019t consider things like the sig-nificant barriers of the engineering tech stack that make in-processing techniques far harder to implement compared topost-processing techniques, as the latter are easier to scalesince they don\u2019t require coordination with large numbers ofteams. In terms of the more principles-based approaches, P8finds it\u2019s \u201ceasy for the arguments to get theoretical and philo-sophical,\u201d and P4 says \u201cthat\u2019s where I think the rubber meetsthe road, how do we actually get things done?\u201d Overall,prior work covers the numerous research-to-practice gapsin RAI (Holstein et al. 2019), and we bring this up as aresearch area that can ease the adoption of RAI, and thusthe willingness of companies to implement.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "c73d92c8-7b3b-4bbf-8a3e-82b045ccd2c6",
                    "text": "Overall, we learn from our interview studies a wide range ofways to motivate companies to increase RAI prioritization.We present a summary of shared strategies in the Appendix(Fig. 2), organized by the actor who is most able to enacteach. From these, a few directions stand out to us as the morepromising ways forward. Whereas we have endeavoured toremain mostly descriptive thus far, this section will be moreprescriptive.Accountability for ethics-washing. A recurring themeis that in many cases, ethics-washing is more incentivizedthan substantive ethics work. To shift the incentives awayfrom the former to the latter, we suggest greater collabora-tion between RAI experts and journalists to hold companiesaccountable and make sense of cheaply made ethics state-ments. P8 and P10 both remark that performativity can actu-ally lead to substantive action, e.g., \u201cThe performative-nessof it actually does open space for more performance of it\u201d(P8). This is only true if companies are not rewarded forempty ethics statements, but also not unduly punished forpublicizing genuine yet unsuccessful efforts. Another way tocommunicate substantive RAI progress is by creating mea-surements which are more resistant to ethics-washing. Theseinclude measurements that incorporate human feedback andconnect to more concrete harms (Blodgett et al. 2020; Wanget al. 2022, 2023).Ethics modules. Incorporating RAI requires two thingson the part of an individual: intention and expertise. Bothcan be be taught through the integration of ethics modulesduring the education process that instill this as a priority,which may conflict with market pressures, early on (Smithet al. 2023; Fiesler, Garrett, and Beard 2020; Raji, Scheuer-man, and Amironesei 2021; Grosz et al. 2019). However,ethics modules in their current state may be overly focusedon educating future executives and engineers on how theymight identify and resolve ethics problems. Instead, the fo-cus should be on communicating the difficulty and ongoingnature of ethics so that students have a genuine understand-ing and appreciation of the difficulty of the problem, andknow how to collaborate with ethics experts rather than feel-ing falsely empowered to resolve these problems on theirown. Like our finding that people sometimes feel that be-ing a \u201cgood person\u201d is a sufficient qualification to do RAIwork, it\u2019s important to communicate that RAI and ethicswork is a field that requires collaborating with those withexpertise (McLennan et al. 2022).Shifting structural incentives. We have seen alterna-tives to more traditional corporation structures such as non-profits and public benefits corporations (PBCs), as well asindicators like ESG ratings. In addition, there are new ini-tiatives to support founders and investors prioritizing RAI.These have the potential to allow a company to be judgedby shareholders for more than just their immediate profits,creating room for values like product quality and public per-ception (Dorff, Hicks, and Solomon 2020). There is a par-allel to value-based healthcare (Porter and Teisberg 2006;Teisberg, Wallace, and O\u2019Hara 2020), which is a frame-work for healthcare that shifts the incentive structure to fo-cus on patient health outcomes rather than cost reduction. Ofcourse, any of these could be used for ethics-washing, andprior work has postulated on why the supposed benefits ofnew forms of incorporation\u2014signaling brand quality, lim-iting legal liability, and attracting capital\u2014may not alwaysmaterialize (Koehn 2016). However, these kinds of broaderstructural shifts can create an incentive structure that morehighly prioritizes issues like RAI. They would require mak-ing the requirements for such legal qualifications and ratingsstrict enough to filter out disingenuous efforts, as well as po-tentially incorporating more financial benefits like tax incen-tives into PBCs and ESG ratings, instead of relying only onimproved public perception.Research informed by needs. Participants shared thatRAI research outputs often don\u2019t match their needs (Hol-stein et al. 2019). Two specific ways to close this gap emergefrom our findings. One is that researchers can pursue in-tegrated interventions that fit well into the typical soft-ware engineering stack. While this differs across compa-nies, there are shared characteristics which have consistentlymade post-hoc interventions easier, and thus more likely,to be implemented (P12). This is in comparison to otherinterventions that require touching many parts of the techstack. New research can pursue whether there are methodsbesides post-hoc that are also easier to fit into the stack. Atthe same time, this will require companies being more openabout their internal processes and constraints. Another di-rection is in the interdisciplinary work happening betweentechnology and the law. As participants expressed, it is of-ten unclear which interventions are permitted by the law.P12 expressed that they were able to have more productiveconversations with company lawyers after engaging with re-search that clarified the connection between law and con-crete RAI approaches (Ho and Xiang 2020; Xiang 2020;Kumar, Hines, and Dickerson 2022). Companies should alsoshare ways of implementing RAI that is legally permissiblewith each other. For example, P15 mentions sometimes us-ing pronouns instead of gender in RAI implementation, be-cause while the latter is legally impermissible, the former isallowed. In this way, practitioners can share effort in navi- gating the complexity of regulation while pursuing substan-tive RAI efforts.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "4d7d2f37-5835-43b8-a4db-c08141a14921",
                    "text": "Overall, in our work we unveil a set of strategies for affect-ing RAI prioritization. A limitation is that the recommen-dations may err on the side of being too accommodating tothe world as it is rather than speaking to an ideal world\u2014wedo not wholly endorse all strategies so much as provide aslate of options from which to choose from. For example, isit right to say we should make fairness problems more tech-nically interesting and \u201cfun\u201d in order to incentivize techni-cal developers to work on them? Perhaps not, but in our en-deavor of presenting all discovered methods of increasingRAI prioritization, it is certainly one possibility. As AudreLorde famously put it \u201cFor the master\u2019s tools will never dis-mantle the master\u2019s house. They may allow us temporarilyto beat him at his own game, but they will never enable usto bring about genuine change\u201d (Lorde 1984). Ultimately byrelying on the strategies that have historically worked withinthe confines of capitalism, our suggestions may be more ofa patch than a treatment for the source problem. However,given the time it can take for more radical changes to takeplace, there is still benefit to working on increasing RAI pri-oritization in companies today, so long as it does not detractfrom such greater changes. RAI practitioners individuallystruggle towards this in their separate companies, and ourwork consolidates their techniques and efforts. Even thoughit seems like RAI will not be prioritized over profit, in ourwork we unveil that this is not clear-cut! There are success-ful strategies for stakeholders to elevate RAI even then, andthe details of our work can be used as a toolkit for enactingthis RAI prioritization. In the meantime, we should not losehope for more radical changes and continue to push on thatfront as well (Hampton 2021; Davis, Williams, and Yang2021; Green 2020; Gorz 1964).",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "6f68aea9-2d1a-437f-8632-4e7c502debee",
                    "text": "In this work, we look to the corporate landscape for exam-ples of what has worked in prioritizing RAI in order to unveilthe strategies at our disposal and pressure points to lean onin order to better motivate corporations under the landscapeof capitalism. We find a complex picture of numerous actorsable to exert pressure in numerous points, with no guaran-teed outcomes. And yet, by drawing lessons from these pastsuccesses and applying enough force, we can collectivelyincrease RAI prioritization. However, if we believe that thecurrent state of RAI adoption and prioritization is not suffi-cient, then it is clear that just relying on these strategies arenot enough. While we believe this will be a useful toolkitfor those looking to increase the prioritization of RAI, thesestrategies we unveil are just one set of the total possible in-terventions we should pursue.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "478e31f5-b068-4ee4-8655-5e2360ce2f50",
                    "text": "Ethical considerations. In this work, the biggest ethicalconsideration was the treatment of our participants and en-suring their comfort with their level of anonymity. This canbe a sensitive topic that bears on someone\u2019s workplace en-vironment, and we worked hard to ensure each participantfelt comfortable with the level of anonymity they wouldbe provided. To achieve this, we sought consent before ei-ther recording the interview or taking written notes. We alsostarted each interview by asking each participant how theywould prefer their background to be identified (e.g., job role,company industry, company size). Upon writing our draftand in response to some participants\u2019 concerns, we did notassociate the role with each participant, as we found thatour results were not specific to each person\u2019s role. Finally,we also sent the completed draft to each participant to en-sure that they were ultimately comfortable with the level ofanonymity they were represented with, since we understandthat this can be a loaded space. We compensated each partic-ipant $25 for an interview which ranged from 30-60 minutes.All of the data was stored on Google Drive, which issecure and encrypted. We will delete all interview recordsupon final publication.Positionality. One of the most relevant aspects of posi-tionality for this piece is that two of the authors work in in-dustry for a company in the RAI space, and one author isin academia. These positions affect the perspectives on cor-porations. In positions of relative socioeconomic privilege,none of the authors are personally susceptible to some ofthe more insidious effects of AI and so may find it easier towrite about incremental, non-revolutionary improvements,compared to others.Adverse impact. As discussed in Sec. , by having scopedour work to past efforts, we hope to not foreclose consider-ations of more radical change. However, we realize that pa-pers of this sort that propose changes within existing capital-ist structures could be seen as legitimizing the current struc-ture, which is not our goal. We hope that our work can shinea light on directions forward, in parallel with other move-ments like greater instances of collective action or abolitionin the cases that call for it.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                },
                {
                    "id": "eb44de53-800a-4a09-b963-3743a99e68f8",
                    "text": "We thank Namrata Mukhija and Daniel Nissani for helpingwith participant recruitment, Victoria Vassileva for initialconversations on these topics, and Amy Winecoff for feed-back on the draft. This material is based upon work sup-ported by the National Science Foundation Graduate Re-search Fellowship to AW, and was work initiated duringAW\u2019s internship at Arthur.",
                    "reference": "``` \n[1] Angeline Wang, Trisha Datta, and John P. Dickerson. 2024. Strategies for Increasing Corporate Responsible AI Prioritization. arXiv:2405.03855. Retrieved from https://arxiv.org/pdf/2405.03855\n```"
                }
            ]
        },
        {
            "paper_title": "Responsible AI challenges in end-to-end machine learning",
            "authors": "SE Whang, KH Tae, Y Roh, G Heo",
            "publication_info": "arXiv preprint arXiv:2101.05967 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2101.05967",
            "chunks": [
                {
                    "id": "bc5bb640-79fa-46b2-b915-9544fefa24af",
                    "text": "Responsible AI is becoming critical as AI is widely used in our everyday lives. Many companiesthat deploy AI publicly state that when training a model, we not only need to improve its accuracy,but also need to guarantee that the model does not discriminate against users (fairness), is resilientto noisy or poisoned data (robustness), is explainable, and more. In addition, these objectives are notonly relevant to model training, but to all steps of end-to-end machine learning, which include datacollection, data cleaning and validation, model training, model evaluation, and model management andserving. Finally, responsible AI is conceptually challenging, and supporting all the objectives must beas easy as possible. We thus propose three key research directions towards this vision \u2013 depth, breadth,and usability \u2013 to measure progress and introduce our ongoing research. First, responsible AI mustbe deeply supported where multiple objectives like fairness and robust must be handled together. Tothis end, we propose FR-Train, a holistic framework for fair and robust model training in the presenceof data bias and poisoning. Second, responsible AI must be broadly supported, preferably in all stepsof machine learning. Currently we focus on the data pre-processing steps and propose Slice Tuner,a selective data acquisition framework for training fair and accurate models, and MLClean, a datacleaning framework that also improves fairness and robustness. Finally, responsible AI must be usablewhere the techniques must be easy to deploy and actionable. We propose FairBatch, a batch selectionapproach for fairness that is effective and simple to use, and Slice Finder, a model evaluation toolthat automatically \ufb01nds problematic slices. We believe we scratched the surface of responsible AI forend-to-end machine learning and suggest research challenges moving forward.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "346a5360-37fe-45db-a3f5-ef5eae23fb6d",
                    "text": "Responsible AI is becoming critical as machine learning becomes widespread in our everyday lives. Companiesincluding Google [2], Microsoft [3], and IBM [5] publicly state that AI not only needs to be accurate, but alsoused and developed, evaluated, and monitored for trust. Although there is no universally agreed notion for re-sponsible AI, the major objectives include fairness, robustness, explainability, transparency, and accountability.The usual starting point is to support responsible AI only in model training, but this is not suf\ufb01cient. Forexample, if the training data is biased towards a speci\ufb01c population, there is a fundamental limit into how muchthe trained model can avoid being biased as well even using the best fair training algorithms. Instead, we mayneed to address the root cause starting from data collection where we need to construct an unbiased dataset.We would thus like to support responsible AI in all steps of end-to-end machine learning [8, 37]. Beforemodel training, the key steps are data collection, data cleaning, and validation. After model training, there aremodel evaluation, and model management and serving. In addition, since supporting all the responsible AIobjectives is already conceptually challenging, it is important to make these techniques easy to use as well.1To this end, we propose three research directions \u2013 depth, breadth, and usability \u2013 and present our contri-butions. First, we need to deeply support responsible AI where multiple objectives are addressed together. Wepresent FR-Train [28], the \ufb01rst holistic framework for fair and robust model training. Second, we need to broadlysupport responsible AI in all machine learning steps. We present two systems that focus on data pre-processing:Slice Tuner [34] is a selective data acquisition framework for fair and accurate models, and MLClean [33] isa data cleaning framework that also improves fairness and robustness. Third, we need responsible AI to beusable and actionable. We present two systems: FairBatch [29] is an easy-to-deploy batch selection techniquefor model training that improves fairness, and Slice Finder [13, 14] automatically evaluates a model by \ufb01ndingproblematic slices where it underperforms. Our work only scratches the surface of responsible AI for end-to-endmachine learning, and we believe that setting the three research directions is useful to measure progress.We introduce the responsible AI research landscape in Section 2. We then discuss our systems for depth,breadth, and usability in Sections 3, 4, and 5, respectively. Finally, we suggest open challenges in Section 6.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "d38079da-a3cd-4e88-99af-4ed011edd234",
                    "text": "We provide a brief history of responsible AI and discuss the research landscape. Responsible AI is also knownas Trustworthy AI and has recently been promoted by Google [2], Microsoft [3], and IBM [5] among othersas a critical issue when using AI in practice. The key objectives include fairness, robustness, explainability,transparency, and accountability. Among the objectives, we focus on fairness and robustness because they areboth closely related to the training data. The other objectives are also important, but currently outside our scope.Fairness is the problem of not discriminating against users and has gained explosive interest in the pastdecade [7, 35]. An article that popularized fairness was the 2016 ProPublica report [6] on the COMPAS software,which is used in US courts to predict a defendant\u2019s recidivism (reoffending) rate. COMPAS is convenient, butis known to overestimate black people\u2019s recidivism risk compared to white people. Recently, various unfairnessmitigation techniques [9] have been proposed and can be categorized as pre-processing, in-processing, or post-processing depending on whether the techniques are applied before, during, or after model training, respectively.Robustness is the problem of preventing or coping with adversarial attacks. In particular, model trainingagainst data poisoning has been heavily studied in the past decade [15, 31]. Nowadays datasets are easier topublish using tools like Kaggle and Google Dataset Search [11], which means that it is easier to disseminatepoisoned data as well. The data can then be harvested by Web crawlers of unsuspecting victims and used formodel training. While the basic poisoning attacks involve simple labeling \ufb02ipping (e.g., change a positive labelto be negative), recent poisoning attacks are becoming increasingly sophisticated. The possible defenses includesanitizing the data before model training or making the model training accurate despite the poisoning.In practice, machine learning is not just about model training, but involves multiple steps as demonstrated byend-to-end systems like TensorFlow Extended (TFX) [8] and MLFlow [37]: data collection, data cleaning andvalidation, model training, model evaluation, and model management and serving. Hence, responsible AI is notjust a model training issue, but relevant to all of the above steps. The data management community has recentlybeen addressing the data aspect of responsible AI in end-to-end machine learning [25, 23, 26, 12, 27, 32, 36].The current research landscape naturally leads to the three key research directions we propose \u2013 depth,breadth, and usability \u2013 as shown in Figure 1. First, it is important to support many responsible AI objectives ateach step. Second, we need to broadly support responsible AI in as many steps as possible, from data collectionto model serving. Third, we need these techniques to be usable and actionable by machine learning users. Wehighlight the responsible AI objectives in Figure 1 where we propose solutions.2Figure 1: The three research directions \u2013 depth, breadth, and usability \u2013 for fully supporting the responsibleAI objectives (fairness, robustness, and others) in addition to accuracy in end-to-end machine learning. Thehighlighted parts show our contributions: Slice Tuner [34] addresses fairness in data collection; MLClean [33]addresses fairness and robustness in data cleaning; FR-Train [28] addresses fairness and robustness in modeltraining; FairBatch [29] addresses usability for fairness in model training; and Slice Finder [13, 14] addressesusability for fairness in model evaluation.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "0b2ddac0-ce66-4270-a1d6-a1231a0b8ba3",
                    "text": "We discuss deeply supporting responsible AI, which means that we would like to address multiple objectivestogether. We re-emphasize that each objective is currently being heavily studied. For model fairness, there is anextensive literature in the machine learning and fairness communities on mitigating unfairness before, during, orafter model training [7, 35, 9]. For model robustness, both the machine learning and security communities areproposing various data sanitization and robust training techniques [22, 31]. However, we believe that responsibleAI requires both fairness and robustness instead of just one. In addition, addressing one objective at a time is notideal as we discuss later. Fairness and robustness are also closely related because their problems originate fromthe training data: biased data causes unfairness while poisoned data decreases model accuracy. This motivationleads us to propose FR-Train [28], the \ufb01rst holistic framework for fair and robust training.Fairness is a subjective notion, and many de\ufb01nitions have been proposed [35] where they can be categorizeddepending on what information is used: the classi\ufb01er, the sensitive attribute (e.g., race or gender), and traininglabels. For example, individual fairness only uses the classi\ufb01er and means that similar individuals must havesimilar predictions. Demographic parity [17] (or disparate impact) uses the classi\ufb01er and the protected attributeand means that different sensitive groups (e.g., black and white populations) have similar positive predictionrates. That is, P ( \u02c6Y = 1|Z = 0) \u2248 P ( \u02c6Y = 1|Z = 1) where \u02c6Y is a prediction and Z is a binary sensitiveattribute. Equalized odds [18] uses all three pieces of information and is similar to demographic parity, exceptthat the probabilities are conditioned on the label. That is, P ( \u02c6Y = 1|Z = 0, Y = l) \u2248 P ( \u02c6Y = 1|Z = 1, Y = l)where Y is the label. In this section, we use demographic parity and measure it using the formula DP :=min , where a higher value close to 1 means better fairness.We now explain why addressing fairness and robustness together is important using a concrete example. InFigure 2a, suppose there are two sensitive groups black and white, and that there are ten people of two races:white (denoted as \u2018w\u2019) and black (denoted as \u2018b\u2019). Let us assume the boxes indicates positive labels and that wewant to train a threshold classi\ufb01er that divides the individuals using a single feature X where those on the lefthave negative predictions (e.g., do not reoffend) while those on the right have positive predictions. On clean data,a vanilla classi\ufb01er can obtain perfect accuracy by dividing between the fourth and \ufb01fth individuals (Figure 2asolid line classi\ufb01er). However, the demographic parity DP is not perfect where P ( \u02c6Y = 1|Z = w) = = 0.4,P ( \u02c6Y = 1|Z = b) = , = 0.5. Suppose a fair classi\ufb01er maximizes accuracywith perfect DP . One can \ufb01nd such a classi\ufb01er by dividing between the second and third individuals (Figure 2ablue dotted line classi\ufb01er). While DP = 1, the accuracy is 0.8 because two white people are now misclassi\ufb01ed.= 0.8, and DP := min 3Figure 2: (a) Accurate (black solid line) and fair (blue dotted line) classi\ufb01ers on clean data followed by datapoisoning. (b) A fair classi\ufb01er trained on poisoned data (red dotted line) is evaluated on clean data, showing aworse accuracy-fairness tradeoff than the fair classi\ufb01er trained on clean data.Now suppose we poison the clean data by using the standard method of \ufb02ipping labels [24]. On the bottomof Figure 2a, the \ufb01fth and seventh individuals are now incorrectly labeled as negative. There are three ways tohandle the poisoned data: (1) do nothing and perform fair training only as usual, (2) take a two-step approach andperform data sanitization followed by fair training using existing methods, and (3) take a holistic approach forfair and robust training. Let us \ufb01rst see what happens if we take the \ufb01rst approach. We can train a fair classi\ufb01er onthe poisoned data with perfect DP by dividing between the eighth and ninth individuals (bottom of Figure 2b,red dotted line classi\ufb01er). In that case, we will have perfect DP , but an accuracy of 0.8 on poisoned data.However, if this classi\ufb01er is deployed in the real world, it will effectively be used on clean data. This scenariois plausible for any application that serves real customers. However, simply using the same classi\ufb01er on cleandata results in a worse tradeoff of fairness and accuracy where DP remains the same, but the accuracy reducesto 0.6. Hence, ignoring poisoning may lead to strictly worse accuracy and fairness results. In reference [28], wealso empirically show that the two-step solution is ineffective. The intuition is that an existing fairness-only orrobustness-only technique cannot easily distinguish data poisoning from bias in the data and ends up removingall or none of the problematic data.We thus propose FR-Train to take a holistic approach for fair and robust training. Figure 3 shows the archi-tecture of FR-Train. On the top, there is a classi\ufb01er (e.g., predicts recidivism) that competes with a discriminatorfor fairness that predicts the sensitive attribute (e.g., the race) based on the predictions. This adversarial trainingis similar to Adversarial Debiasing [38], a state-of-the-art fairness-only training algorithm. The below part isthe novel addition where there is a discriminator for robustness that distinguishes the possibly-poisoned trainingset with a validation set that is known to be clean. The clean validation set is small and can be constructedusing crowdsourcing and conventional quality control techniques including majority voting. Hence, the clas-si\ufb01er needs to be both fair and robust to compete with the two discriminators. Finally, the predictions of therobustness discriminator are used to reweight training set examples where cleaner examples get higher weights.Initially, these weights are not useful because the robustness discriminator is not accurate. However, as thetraining progresses, the discriminator becomes accurate, and the weights are used by the classi\ufb01er.In reference [28], we present a mutual information-based interpretation of FR-Train\u2019s architecture. To givean intuition, perfect fairness means that the mutual information between the model\u2019s prediction and the sensitiveattribute is 0. Similarly, satisfying robustness can be expressed using mutual information. In this case, perfectrobustness means that the poisoned data distribution is indistinguishable from the clean data distribution (i.e.,validation set). FR-Train minimizes both of the mutual information values and the classi\ufb01er loss. We performexperiments on synthetic and real datasets and train a classi\ufb01er on poisoned data and evaluate it on clean data.As a result, FR-Train is the only approach that achieves both high accuracy and fairness while the other baselineseither have poor fairness or accuracy. 4Figure 3: The FR-Train architecture and how it can be used for recidivism prediction.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "44869de7-5b86-4a28-821d-1d520fe83b04",
                    "text": "In addition to supporting responsible AI in model training, we would also like to broadly support it across manysteps in end-to-end machine learning. While most of the fairness and robustness literature focus on model train-ing, there needs to be more focus on other machine learning steps as well. Recently, FairPrep [30] was proposedto support fairness in all steps of data pre-processing before model training. Also for an extensive coverageof data collection and quality techniques for machine learning, please refer to a survey [27] and tutorial [36].Here we also focus on data pre-processing and present two contributions: Slice Tuner [34] is a selective dataacquisition framework for maximizing fairness and accuracy, and MLClean [33] is a data cleaning frameworkfor addressing both fairness and robustness in addition to accuracy.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "9ee63e51-36c2-44c8-abe0-17e09757939a",
                    "text": "As machine learning is used in various applications, one of the critical bottlenecks is acquiring enough dataso that the trained model is both accurate and fair. Nowadays, there are many ways to acquire data includingdataset discovery, crowdsourcing, and simulator-based data generation. Data acquisition is not the same as activelearning, which labels existing data. Instead, our focus is on acquiring new data along with its labels.However, blindly acquiring data is not the right approach. Let us \ufb01rst divide the data into subsets calledslices. Suppose that the slices are customer purchases by various regions: America, Europe, APAC, and so on.Among them, if we already have enough America data, acquiring more America data is not only unhelpful, butmay also bias the data and have a negative effect on the model accuracy on the other slices.Instead, we want to acquire possibly different amounts of data per slice in order to maximize accuracy andfairness. To measure accuracy, we use loss functions like logistic loss. For fairness, we use equalized errorrates [35], which states that the losses of slices must be similar. This notion of fairness is important to anyapplication that should not discriminate its customers by service quality. A water\ufb01lling approach is a good startwhere we simply acquire data so that the slices have similar sizes. However, this approach is not optimal becausesome slices may need more data to obtain the same model loss as other slices.Our key approach is to generate for each slice a learning curve, which estimates the model loss on thatslice given more labeled data. Multiple studies [19, 16] show that a learning curve is best \ufb01t using a power-lawfunction. Figure 4a shows two actual learning curves generated on two race-gender slices of a real dataset calledUTKFace [39]. We can use these learning curves to estimate how much data must be acquired per slice.Assuming that the learning curves are perfectly reliable (we discuss how to deal with unreliable curves later),we can determine the amounts of data to acquire to minimize the total loss and unfairness of slices by solving5Figure 4: (a) Learning curves on two slices of the UTKFace dataset [39]. (b) Slice Tuner architecture.the following convex optimization problem:min b (|s | + d ) + \u03bb max 0, b (|s | + d )A \u2212 1 subject to C(s ) \u00d7 d = Bare the slices, {d }where {s } are the amounts of data to acquire, A is the average loss of slices, C(s )is the cost function for acquiring an example for s , and B is a cost budget. The \ufb01rst term in the objectivefunction minimizes the total loss while the second term minimizes the unfairness by penalizing slices that havehigher-than-average losses. The two terms are balanced using \u03bb. By acquiring more data for slices with higherlosses, we eventually satisfy equalized error rates. Slice Tuner\u2019s architecture is shown in Figure 4b where weperform selective data acquisition on input slices. The runtime bottleneck is the time to actually acquire data.We now address the key challenge of handling unreliable learning curves. Learning curves are not perfectbecause slices may be too small for accurate estimations. Even worse, acquiring data for one slice may \u201cin-\ufb02uence\u201d others. Figure 5a shows how acquiring data for the slice White-Male increases or even decreases themodel\u2019s loss on other slices for UTKFace. The intuition is that the acquired data of one slice pushes the decisionboundary of the model, which in turn changes the losses of other slices (Figure 5b).Figure 5: (a) Data acquisition on the slice White-Male in\ufb02uencing the losses on the other slices for the UTKFacedataset. (b) To give an intuition, say there are three slices where shape indicates slice, and color indicates label.(Top) If we only increase the triangles, the decision boundary may shift to the left due to the new bias, changingthe losses of the other slices. (Bottom) If we evenly increase the data for all slices, the bias does not change, andthere is little in\ufb02uence among the slices. 6Figure 6: The MLClean architecture where data sanitization and data cleaning are performed together followedby unfairness mitigation. Optionally, a previous model can be used by any of the three operations.The solution is to iteratively update the learning curves. But how often should we iterate? On one hand, eachiteration is expensive and involves multiple model trainings and curve \ufb01ttings, even though we use amortizationtechniques [34]. On the other hand, we do not want to use inaccurate learning curves. Our algorithm works asfollows. We \ufb01rst ensure a minimum slice size to draw some learning curve. In practice, having tens of examplesis enough for this step. Next, we repeat two steps until we run out of budget: (1) acquire data as long as theestimated in\ufb02uence is not large enough and (2) re-\ufb01t the learning curves. The remaining problem is estimatingin\ufb02uence. We propose a proxy called imbalance ratio change where imbalance ratio represents bias and is theratio between the largest and smallest slice sizes. The intuition is that a change in imbalance ratio among slicescauses in\ufb02uence. In Figure 5b adding two triangles results in a shifted decision boundary where the imbalanceratio increases from = 2. On the other hand, if we evenly increase the slices, the decision boundarydoes not shift, and the imbalance ratio does not change much either.= 1 toIn reference [34], we provide more details on the algorithms and also perform experiments on real datasets.We show that Slice Tuner has lower loss and unfairness compared to two baselines: uniformly acquiring thesame amounts of data per slice and water\ufb01lling. We also make the same comparison when the slices are smalland only have tens of examples. Here the learning curves are very noisy and thus unreliable. Interestingly, SliceTuner still outperforms the baselines because it can still leverage the relative loss differences among the learningcurves. As more data is acquired, Slice Tuner performs even better with more reliable learning curves. In theworst case when the learning curves are completely random, we expect Slice Tuner to perform similarly to oneof the baselines.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "824b3854-7bfb-48bc-9d60-315f45e7b438",
                    "text": "Another important place to support responsible AI is data cleaning [20] where the input data needs to be validatedand \ufb01xed before it is used for model training. Historically, multiple communities \u2013 data management, machinelearning (model fairness), and security \u2013 have been investigating this problem under the names of data cleaning,unfairness mitigation, and data sanitization, respectively. Unfortunately, not much is known how the differenttechniques can be used together when a dataset is dirty, biased, and poisoned at the same time.MLClean is a uni\ufb01ed cleaning framework that performs data cleaning, data sanitization, and unfairnessmitigation together. A key insight is that these three operations have dependencies and must be executed ina certain order for the best performance. As shown in MLClean\u2019s architecture in Figure 7, data sanitizationand cleaning are performed together followed by unfairness mitigation. Data sanitization can be considered astronger version of cleaning because it defends against adversarial poisoning instead of just noise. In addition,data cleaning and sanitization may affect the bias of data while unfairness mitigation that performs examplereweighting does not affect the correctness of cleaning and sanitization.As a running example, suppose we run MLClean on the examples in Table 1 with equal weights of 1. Saythat data sanitization clusters examples and removes anomalies while data cleaning performs entity resolution.The two operations can be naturally combined by generating clusters and running entity resolution within eachcluster, assuming that examples across clusters do not match. Clustering examples before resolution is a commonoperation in entity resolution for narrowing down matching candidates. Figure 7 shows how the initial six7ID Weight Name Gender Age Labeleeeeee JohnJoeJosephSallySallySally 20202030403001.01.01.01.01.01.0 MMMFFF 100101Table 1: Six examples where e and e are duplicates(dirty), and e has an anomalous age (poisoned). Figure 7: MLClean running on the examples in Ta-ble 1.examples are clustered into {e , e , e } and {e , e } (e is considered an outlier), and then e and e are mergedtogether into e with a summed weight of 2. For unfairness mitigation, suppose we reweight [21] the examplessuch that demographic parity (de\ufb01ned in Section 3) is satis\ufb01ed for the sensitive groups men and women. We canmake the (weighted) positive prediction rates the same by adjusting e \u2019s weight from 2 to 1. As a result, the(weighted) positive prediction rates for men and women have the same value of = 0.5.In reference [33], we compare MLClean with other baselines that use a strict subset of the operations datasanitization, data cleaning, and unfairness mitigation or use all three operations, but in a different order thanMLClean. On real datasets, MLClean has the best model accuracy and fairness, demonstrating that all threeoperations are necessary for the best results. In addition, MLClean is faster than baselines that use the threeoperations in different orders, which means that utilizing the dependencies among the operations is important.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "6ee241af-33fc-45ac-bcc7-06ed2b62ac76",
                    "text": "The \ufb01nal pillar of responsible AI is making it usable and actionable to all machine learning users. Whileusability is not always the main focus in machine learning, it is especially relevant for responsible AI becausethe various objectives are already conceptually challenging to understand, so the deployment must be made aseasy as possible. We thus propose two systems: FairBatch [29] is an easy-to-use model training technique forfairness, and Slice Finder [13, 14] is an easy-to-use model evaluation technique for improving fairness.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "be89ed64-f85c-4449-9430-b47e6f2322dd",
                    "text": "While many unfairness mitigation techniques [9] have been proposed, most of them require signi\ufb01cant amountsof effort to deploy. Pre-processing techniques have the advantage of being applicable to any model, but requirechanges in the training data in order to remove bias. In-processing techniques tend to perform well, but usuallypropose a new model training algorithm that completely replaces an existing algorithm. An interesting questionis whether we can take the best of both worlds of pre-processing and in-processing without their overheads.We show that such a solution exists and propose FairBatch, which simply improves the batch selection ofstochastic gradient descent training for better fairness. We formulate a bilevel optimization problem where wekeep the standard training algorithm as the inner optimizer while incorporating the outer optimizer to equip theinner problem with the additional functionality: adaptively selecting minibatch sizes for the purpose of improv-ing fairness. While the model is training, FairBatch adaptively adjusts the portions of the sensitive groups withineach batch that is selected for each training epoch based on the fairness of the current intermediate model. Forexample, let us use the COMPAS example where we are predicting recidivism rates of criminals. Also let ususe equalized odds (de\ufb01ned in Section 3) as the fairness measure where we want the positive prediction rates ofsensitive groups to be the same conditioned on the true label. Since the label is \ufb01xed, this fairness can be inter-preted as the model having the same accuracy for sensitive groups conditioned on the label. Now suppose that an8Figure 8: (a) The black path shows how the model fairness improves as FairBatch adjusts two parameters \u03bb(sampling rate for examples where Z=0 given Y =0) and \u03bb (sampling rate for examples where Z=0 given Y =1)for each epoch on the COMPAS dataset. \u201cED disparity\u201d is the accuracy difference conditioned on the true labelbetween sensitive groups where lower disparity means better equalized odds. (b) Sample PyTorch code wherethe batch selection sampler is replaced with FairBatch with a single-line change highlighted in blue.intermediate model shows higher accuracy for a certain sensitive group. FairBatch then increases the batch-sizeratio of the other underperforming sensitive group in the next batch. Intuitively, a larger batch size ratio resultsin better accuracy, so eventually equalized odds will improve. Figure 8a illustrates how FairBatch improvesequalized odds during a single model training. In reference [29], we show that this strategy is theoreticallyjusti\ufb01ed and generalize the algorithm for other fairness measures including demographic parity.A key feature of FairBatch is its usability where one only needs to replace the batch selection of a machinelearning system. Figure 8b shows a PyTorch code example where one can deploy FairBatch by replacing a singleline of code, and no further changes are needed in the pre-processing or in-processing steps of model training.In reference [29], we also conduct experiments on synthetic and real datasets and show that FairBatch sur-prisingly has performances comparable to or even better than state-of-the-art pre-processing and in-processingunfairness mitigation techniques in terms of accuracy, fairness, and runtime. In addition, FairBatch is \ufb02exibleand can be used to improve the fairness of pre-trained models like ResNet18 and GoogLeNet. Finally, thereare batch selection techniques proposed for faster model training convergence, and FairBatch can be naturallycombined with them to improve fairness as well.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "d1b470ef-559b-4678-8edb-3798ef445551",
                    "text": "After model training, models are evaluated before being served. For example, TensorFlow Model Analysis [4]is a model evaluation component of TFX that accepts a user-speci\ufb01ed slicing feature (e.g., country) and showsthe model accuracies per slice (e.g., accuracy per country). Here we are using equalized error rates (de\ufb01ned inSection 4.1) as our notion of fairness. However, there is potentially an exponential number of slices to explore,and it is not easy for users who do not have enough domain expertise to quickly sift through them.We thus propose Slice Finder [13, 14], which automatically \ufb01nds \u201cproblematic\u201d slices (subsets of the data)where the model underperforms. Given these slices, users can take action by acquiring more data as in SliceTuner or debug the problematic data to \ufb01nd the root cause that led to the poor performance. We de\ufb01ne aproblematic slice to have the following characteristics. First, the slice must be interpretable where it can bede\ufb01ned with feature-value pairs, e.g., \u201cGender=Male and Age=20-30.\u201d While one can also de\ufb01ne a slice tobe a cluster of examples, clusters are often dif\ufb01cult to understand in practice. In addition, the slice must havea relatively lower accuracy than its complement, i.e., the rest of the examples other than the slice, where thedifference (effect size) is large and statistically signi\ufb01cant. Finally, the slice must be large enough to have ameaningful impact on the overall model accuracy. 9Since the search space for all possible slices is vast, we propose two approaches for searching. The \ufb01rst isa decision tree approach where we construct a decision tree of feature-value pairs to \ufb01nd slices. The traversalis fast, but the slices are non-overlapping, which means that we may miss some problematic slices. The secondis a lattice search approach where we \ufb01nd slices by traversing a lattice of feature-value pairs in a breadth-\ufb01rstmanner. Although we now \ufb01nd overlapping slices, this searching is slower than the decision tree approach. Oncewe \ufb01nd potential problematic slices, we perform effect-size and signi\ufb01cance testings.In references [13, 14], we show that Slice Finder performs better than a clustering baseline on real datasets.Also while lattice searching is slower than decision tree searching, it \ufb01nds more problematic slices.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "2d27e5fe-2f9d-4993-a16d-3b695efaabd9",
                    "text": "We are far from achieving responsible AI for end-to-end machine learning and suggest promising directions.First, there needs to be deeper and broader support for the responsible AI objectives in each step of end-to-endmachine learning. In addition, we believe the usability aspect of responsible AI has been largely understudied,and that there needs to be more emphasis on this important direction. Below are some concrete suggestions.\u2022 Data Collection: We believe data acquisition must also support robustness. Dataset searching is becomingincreasingly easy, and one challenge is distinguishing any poisoned data from the rest of the data. We alsobelieve it is important to address fairness and robustness in data labeling.\u2022 Data Cleaning and Validation: MLClean is preliminary, and an interesting direction is to develop moregeneral and automatic cleaning and validation techniques that support various combinations of data clean-ing algorithms, fairness measures, and poisoning attacks.\u2022 Model Training: FR-Train is a \ufb01rst of its kind and can be extended in many ways. First, there needs tobe more investigation on how to defend against more sophisticated poisoning attacks other than labeling\ufb02ipping. Second, algorithm stability is a well-known issue in adversarial training and can be improved.Third, one may want to train models without a clean validation set.\u2022 Model Evaluation: There needs to be more robustness research for model evaluation where we can easilytell whether a model is accurate enough despite data poisoning in the training data.\u2022 Model Management and Serving: There needs to be more model managing and serving techniques thatsupport fairness and robustness. While there are task-speci\ufb01c solutions like fairness in ranking [32], aninteresting direction is to generalize and support any task with minimal con\ufb01guration.\u2022 There needs to be holistic solutions for the rest of the responsible AI objectives including explainability,transparency, and accountability. For example, recent data provenance and metadata [1, 10] solutions canbe used to explain why each step in machine learning produced a certain result.",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                },
                {
                    "id": "e27d5001-2c9e-467c-965b-562607e5078d",
                    "text": "We proposed three research directions \u2013 depth, breadth, and usability \u2013 towards fully supporting responsibleAI in end-to-end machine learning. While most research focuses on supporting one of many responsible AIfeatures, we believe multiple objectives should be supported together, preferably in all steps from data collectionto model serving. So far, we have scratched the surface of this vision where we proposed the following systems:FR-Train (holistic fair and robust training), Slice Tuner (selective data acquisition for fair models), MLClean(data cleaning for fair and robust models), FairBatch (easy-to-use batch selection for fair models), and SliceFinder (easy-to-use problematic slice \ufb01nding for fair models). We also suggested various open challenges.10AcknowledgementThis work was supported by a Google AI Focused Research Award and by the Engineering Research CenterProgram through the National Research Foundation of Korea (NRF) funded by the Korean Government MSIT(NRF-2018R1A5A1059921).",
                    "reference": "[1] Steven E. Whang, Kunwoo H. Tae, Young-Standards Roh, and Geon Heo. 2021. Responsible AI challenges in end-to-end machine learning. arXiv:2101.05967. Retrieved from https://arxiv.org/pdf/2101.05967"
                }
            ]
        },
        {
            "paper_title": "Human-Centered Responsible Artificial Intelligence: Current & Future Trends",
            "authors": "M Tahaei, M Constantinides, D Quercia\u2026",
            "publication_info": "Extended Abstracts of \u2026 - dl.acm.org",
            "paper_url": "https://arxiv.org/pdf/2302.08157",
            "chunks": [
                {
                    "id": "0f7506e6-e4ff-40f2-ba80-3e0298664cb4",
                    "text": "In recent years, the CHI community has seen significant growthin research on Human-Centered Responsible Artificial Intelligence.While different research communities may use different terminol-ogy to discuss similar topics, all of this work is ultimately aimedat developing AI that benefits humanity while being grounded inhuman rights and ethics, and reducing the potential harms of AI.In this special interest group, we aim to bring together researchersfrom academia and industry interested in these topics to map cur-rent and future research trends to advance this important area ofresearch by fostering collaboration and sharing ideas. CCS CONCEPTS\u2022 Social and professional topics; \u2022 Human-centered comput-ing; \u2022 Theory of computation; \u2022 Information systems; \u2022 Soft-ware and its engineering; \u2022 Security and privacy;KEYWORDShuman-centered AI, responsible AI, AI ethics",
                    "reference": "[1] Marzieh Tahaei, Myrsini Constantinides, Daniele Quercia, P. Alex Dow, and Dave Murray-Rust. 2023. Human-Centered Responsible Artificial Intelligence: Current & Future Trends. arXiv:2302.08157. Retrieved from https://arxiv.org/pdf/2302.08157."
                },
                {
                    "id": "bdae57ca-2f5e-439f-92c5-101fcf18b6ed",
                    "text": "aimsHuman-Centered Responsible Artificial Intelligence (HCR-AI)to bring people and their values into the design and development ofAI systems, which can contribute to building systems that benefitpeople and societies, as well as preventing and mitigating potentialharms. Despite a long history of the importance of the human factorin AI systems [12, 31], there has been a growing awareness of itsimportance within the CHI community in the past few years [32].Searching the ACM Digital Library within CHI proceedings showsthe following results (Figure 1): \u201chuman-centered AI\u201d results in 41records since 2019 and \u201cresponsible AI\u201d results in 32 records since2020. Below, we highlight a few examples of these studies, whichare relevant to the topic of the Special Interest Group (SIG), notingthat this is not an exhaustive list and is only to show the breadthand depth of the existing work:Ethics in AI involve socio-cultural and technical factors, span-ning a range of responsible AI values (including but not limitedto transparency, fairness, explainability, accountability, autonomy,sustainability, and trust) [20]. However, different stakeholders, in-cluding the general population and AI practitioners, may perceiveand prioritize these values differently. For example, a representativesample of the U.S. population was more likely to value safety, pri-vacy, and performance. In contrast, practitioners were more likelyto prioritize fairness, dignity, and inclusiveness [19]. Or, certain his-torically exploited groups may weigh privacy or non-participationmore highly than groups with lower risk [13, 26].Aligned with responsible AI are calls to make AI more human-centric. In particular, there is an emphasis on the challenges of AIintegration into socio-technical processes to preserve human auton-omy and control, as well as the impacts of AI systems deploymentand applications on society, organizations, and individuals [4]. Onthis strand of research, understanding socio-technical and environ-mental factors can help surface why and how an AI system maybecome human-centered [8, 24, 30]. For example, even for an AIfor which there might be broader consensus on its utility, suchas the detection of diabetes using retina scans, there may well bebarriers to becoming useful for its intended users, including dueto not fitting well with the users\u2019 workflows (e.g., nurses) or thesystem requiring high-quality images that are not easy to produce,especially in locations with low resources where such technologycan provide significant support to patients if done right [2].Similarly, researchers have looked at individuals\u2019 expectationsand understandings of AI. For example, when making an ethicaldecision (e.g., a hypothetical scenario for bringing down a terroristdrone to save lives), people may put more capability trust in an AIdecision maker (i.e., capacity trustworthiness, being more capable),whereas they may put more moral trust in a human expert (i.e., beingable to be morally trustworthy and make decisions that are aligned with moral values); in either case, decision made by a human or anAI, prior work has found that people often see the human as partlyresponsible, be it the decision maker or the AI developer [33]\u2014even though the outcomes of the developer may intentionally orunintentionally limit the span of action of the decision-maker [27].Regarding moral dilemmas between AI and human decisions, peoplemay not equally judge humans and machines [17]. These variationsin perceptions may be rooted in (a) people judging humans by theirintentions and machines by their outcomes, and (b) people assigningextreme intentions to humans and narrow intentions to machines,while they may excuse human actions more than machine actions inaccidental scenarios [17]. Furthermore, people\u2019s perceived fairnessand trust in an AI may change with the terminology used to describeit (e.g., an algorithm, computer program, or artificial intelligence),which could eventually impact the system\u2019s success and outcomes,especially when comparative research is done [21].Another human aspect of AI systems is the people who work onthese systems, such as annotators, engineers, and researchers. Dataannotators are part of the workforce that produces the datasets usedto train AI models. However, the workforce (sometimes referredto as AI labor [5] or ghostworkers [15]) behind the annotation taskmay have career aspirations that the current annotation companiesdo not support, or they may be poorly paid because of the push thatcomes from the recent development in AI that requires massiveannotated datasets at low costs [14, 34]. Other researchers echosimilar observations about AI labor by saying that \u201cwithout thework and labor that were poured into the data annotation process,ML [Machine Learning] efforts are no more than sandcastles,\u201d [34]or \u201ceveryone wants to do the model work, not the data work,\u201d [29]a behavior that contributes to the creation of data cascades\u2014whichrefer to compounding events causing adverse, downstream effectsfrom data issues, resulting in technical debt.New tools and frameworks are now being proposed to helpdevelopers build more responsible AI systems (e.g., IBM\u2019s 360 suiteson fairness and explainability [18, 25] and Fairlearn [3]), in additionto user-led approaches to algorithmic auditing to uncover potentialharms of algorithmic systems [7]. Despite the growing interestin HCI research and user experience design for AI, developingresponsible AI remains challenging; a mission involving cognitive,socio-technical, cultural, and design perspectives [16, 23, 24].These are just a few examples from many studies that covertopics that have emerged within the past few years and are relevantto the SIG\u2019s scope. Besides CHI, the ACM Conference on Fair-ness, Accountability, and Transparency (ACM FAccT), establishedin 2018 [1], aims to bring \u201ctogether researchers and practitionersinterested in fairness, accountability, and transparency in socio-technical systems\u201d highlighting the importance of the research inHCR-AI. We aim to bring this community together in a 75-minutediscussion and brainstorming session at CHI 2023.Figure 1: Counts of publications containing \u201chuman-centered AI\u201d and \u201cresponsible AI\u201d at CHI. The first three bars for human-centered AI and responsible AI are not mutually exclusive. They include all types of materials (e.g., research papers, extendedabstracts, and invited talks). Filtering for only research papers results in 32 unique papers since 2020 (the last bar).",
                    "reference": "[1] Marzieh Tahaei, Myrsini Constantinides, Daniele Quercia, P. Alex Dow, and Dave Murray-Rust. 2023. Human-Centered Responsible Artificial Intelligence: Current & Future Trends. arXiv:2302.08157. Retrieved from https://arxiv.org/pdf/2302.08157."
                },
                {
                    "id": "2acc4eee-2a74-4786-bfb1-86f59df792d0",
                    "text": "The SIG follows similar strands from past workshops at CHI 2020,2021, and 2022 [9, 10, 22]. The topics discussed are evolving andgrowing (Figure 1); hence, a SIG at CHI 2023 would be timely. Webelieve a SIG dedicated to the HCR-AI at CHI 2023 will benefit theCHI community and help build and establish a broader network ofresearchers and provide a mapping and understanding of currentand future trends in this area. Researchers in this area come fromindustry and academia from diverse disciplinary backgrounds (e.g.,theoretical computer science, social computing, machine learning,human-computer interaction, and social science). Therefore, hav-ing them all in one hybrid physical-virtual room for 75 minuteswould benefit the community and the attendees to brainstorm andgenerate a map of current and future trends in this area (activitydiagramming). We propose to use online tools such as Miro andSlack to (a) create a record of the group\u2019s co-constructed knowl-edge; (b) serve as a persistent communication to others in the CHIcommunity; and (c) enfranchise remote participants.",
                    "reference": "[1] Marzieh Tahaei, Myrsini Constantinides, Daniele Quercia, P. Alex Dow, and Dave Murray-Rust. 2023. Human-Centered Responsible Artificial Intelligence: Current & Future Trends. arXiv:2302.08157. Retrieved from https://arxiv.org/pdf/2302.08157."
                },
                {
                    "id": "b291451c-67c1-4887-9b31-3154b939ccf2",
                    "text": "We will share the Miro board with attendees and make it publicto support future research in HCR-AI. We will also create a Slackchannel for future communications. The SIG\u2019s primary goal is tocreate a sense of community among researchers in this area, fromacademia and industry, to establish collaborations. The SIG is anexcellent opportunity to bring people with a shared interest inHCR-AI who also attend CHI to build this community.After the SIG, we will organize virtual biannual meetings withthe attendees to share their latest ideas and recent work, builda website to share outcomes created during the SIG, encourageattendees to apply for joint grants, and explore the possibility ofcreating a symposium similar to CHIWORK.",
                    "reference": "[1] Marzieh Tahaei, Myrsini Constantinides, Daniele Quercia, P. Alex Dow, and Dave Murray-Rust. 2023. Human-Centered Responsible Artificial Intelligence: Current & Future Trends. arXiv:2302.08157. Retrieved from https://arxiv.org/pdf/2302.08157."
                }
            ]
        },
        {
            "paper_title": "Five Ps: Leverage zones towards responsible AI",
            "authors": "E Nabavi, C Browne",
            "publication_info": "arXiv preprint arXiv:2205.01070 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2205.01070",
            "chunks": [
                {
                    "id": "772e9410-adfa-402e-a18d-a63b20b858a1",
                    "text": "",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                },
                {
                    "id": "6c48cb8f-5a70-44d1-8efb-cd2a99f8d7ea",
                    "text": "I A",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                },
                {
                    "id": "b4945aca-1989-4e6d-89a6-17fafe6dfd70",
                    "text": "People today are increasingly aware of how ingrained Artificial Intelligence (AI) already is in their daily lives\u2014 whether it determines what appears in a playlist or suggests potential partners to date\u2014rather than in some distant future.  While  these  seemingly  low-risk  examples  can  feel  like  magic  to  the  user,  many  more  technological advances are also underway that delegate more significant control over decision-making to AI-systems, such as in driving  , educating  , judicial applications  , and providing health care  .However, the outputs from these systems can inadvertently erode the shared values of society, such as fairness, justice, safety, security, and accountability. The problems that AI is employed to solve can often exacerbate other societal problems, such as loss of privacy through increased surveillance  , and policy decisions that increase social and economic inequality  . Recent examples of AI failures and their lack of transparency and traceability have raised disconcerting questions about the \u2018dark side\u2019  of AI use, and the way these systems are developed and deployed  .   Advances in digital technology, along with debates about biased algorithms and ethical and regulatory challenges of autonomous systems, underscore the fact that AI management is more of a social and political issue rather than an engineering challenge  . This realization has caused research and industry actors to take non-technical aspects  of  AI  into  account.  This  conversation  has  grown  beyond  the  ethical  challenges  surrounding  AI development and use into a broader discussion of \u2018Responsible AI\u2019, encompassing other topics around ethical AI, lawful AI, explainable AI (XAI), trustworthy AI, and accountable AI.  Although there is not a consensus over the meaning and implications of the notion of \u2018responsibility\u2019 when it is applied to AI-systems , there is a growing interest to explore the development and use of AI systems from the lens  of  Responsible  AI.  Applications  range  from  in  fields  such  as  health  ,  finance  ,  urban  studies  , conservation science  , marketing  , and military affairs  , to more specific cases such as COVID-19  . The notion of Responsible AI  is not limited to research. As of January 2021, OECD AI Policy Observatory tracks more than 300 AI policy initiatives around the globe in the Responsible AI landscape  . The major AI companies have launched their self-regulatory Responsible AI programs, through building tools and software to translate  high-level  principles  such  as  fairness,  explainability,  and  accountability  and  use  them  across engineering groups and clients  . Standardization bodies such as\u202fISO,\u202fIEEE\u202fand\u202fNIST\u202falso offer guidance by publishing standards and frameworks to support the responsible development of AI.   Although transdisciplinary approaches can help us to understand and navigate this socio-technical challenge, the dominant  discourses  address  AI  problems  are  from  disciplinary  perspectives,  predominantly  from  computer science and engineering. Even within Responsible AI, researchers and practitioners tend to approach the topic from a narrowly disciplinary perspective and develop solutions based on their\u202fown epistemological\u202fstrategies. For  AI  systems  that  are  perceived  as  irresponsible,  the  focus  is  often  addressing  visible  gaps  and  tangible problems with quick fixes and technical improvement (particularly in areas such as  robustness,  privacy, and fairness where technical fixes seem feasible) , rather than examining the drivers of design, development, and deployment. This is the gap that needs to be addressed if we are to ensure transformation of responsible AI systems.  Further,  companies  seeking  to  improve  responsibility  in  AI  systems  are  limited  in  their  capacity  to  realize meaningful and necessary technical, social, and environmental change. Previous attempts under the banner of Responsible AI can be described as \u2018ethical washing\u2019 or \u2018ethics theatre\u2019  , intended to (1) show their customers they are doing their best to behave ethically; and more importantly, to (2) minimize regulation. Although it can be argued that efforts thus far are \u2018good first steps\u2019 towards Responsible AI  , these efforts can distract from taking a broader view of the problems inherent to Responsible AI.   For the purposes of this paper, we take a holistic, systems view of Responsible AI  , encompassing broadly the notions  of  responsible,  transparent,  and  trustworthy  AI  described  above.  We  argue  that  although  that  these initiatives seek to ensure more responsible building and application of AI, they often fail to engage with how to encourage developers to approach the root causes and unintended consequences, who often rush to tweak and update existing systems with new software libraries  . Focusing on the engineering solution, they do little to encourage AI developers and users to question underlying assumptions about the vision and the purpose of an AI system. For  the  practitioner  and  policymaker  alike,  the  current  body  of  literature  on  Responsible  AI  lacks  adequate definitions and characterization of how different interactions with AI systems will lead to achieving Responsible AI.  Meadows   proposes the framework of leverage points, which summarizes the relative power of various policies to enact change\u2014from incremental to transformational\u2014in a system.  In  the  following  sections  we  adapt  Meadows\u2019  framework  to  the  current  debates  in  AI  to  help  realize  the transformational change needed towards Responsible AI. We conclude with a short discussion on the advantages of our proposed approach to advance theoretical and practical discussions on responsible approaches for AI.",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                },
                {
                    "id": "3431ce3a-cdac-4d4a-a565-04c6a4058c3c",
                    "text": "Meadows identifies twelve leverage points that has been adapted into research and practical work in various disciplines concerning complex socio-technical systems, from food and energy system  , to climate change  , and health  . These leverage points represent at an abstract level some common places to intervene within a system to effect change.  Here we adapt the leverage point framework categorized around two domains and four zones. Figure 1 shows a graphical depiction of the Five Ps framework. The two domains\u2014Problem and Response\u2014are represented by a triangle divided into two with the Problem Domain on the left and Response Domain on the right. The horizontal axis represents the relative magnitude of \u2018effort\u2019 and reward for intervening in each of the four zones, shown on the vertical axis in increasing order of \u2018leverage\u2019, from top to bottom: Parameter, Process, Pathway, Purpose. We describe this framework as the \u2018Five Ps\u2019, which includes situating the Problem at the right level, and then considering places to intervene in the system in each of the four zones.   The Five Ps is a method for considering and analyzing what the response of a given intervention might be. In this case, we are considering how different initiatives in Responsible AI are conceived in the Problem Domain, and then enacted in the four zones (i.e. Parameter, Process, Pathway, Purpose). This framework recognizes that a problem can be attributed to various parts of a system and, depending on how the problem is framed, different interventions will be chosen, each leading to a different response. To illustrate,  the misclassification problem that arises in an AI model could be seen as a Parameter problem that can be resolved by improving parameters within the algorithm. However, the problem could also be seen as a Purpose problem, which could bring into question the paradigm and assumptions from which the algorithm was created in the first place.      PARAMETER ZONE    PROCESS ZONE     PATHWAY ZONE   PURPOSE ZONE",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                },
                {
                    "id": "49b800b1-bee0-41fc-ae40-ce4e3c6d347b",
                    "text": "",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                },
                {
                    "id": "a1d8c6b3-2dc1-4b9b-a7e7-64028564b729",
                    "text": "The diagram represents the \u2018leverage zones\u2019 within which interventions are made. The twelve leverage points on the pyramid are the places where Meadow (1999) suggests for interventions in a given system.   To illustrate the domains and zones within the Five Ps, we will describe each briefly in relation to Responsible AI.  Problems identified in the Parameter zone are tractable (modifiable, mechanistic) characteristics of an AI system that are commonly targeted by AI developers to improve the  responsibility of AI. They are typically smaller visible  flaws  that  are  usually  addressed  through  engineering  solutions  such  as  tweaking  algorithms  and parameters. The effort to fix these is small, and changes in this zone are incremental and may have a negligible effect on the problem\u2019s underlying structure or dynamics. They are important markers of the problem, but they are often symptomatic and not the root cause of the problem.   Problems identified in the Process zone consider the wide range of interactions between the feedback elements of an AI system that drive the internal dynamics, including social and technical processes associated with how the AI is designed, built, and deployed. This might include activities that speed up development times, or actively responding to emerging trends in the data. Changes in this zone are likely to result in resolving issues as they emerge or amplifying the effect of assumptions.   Problems identified in the Pathway\u202fzone consider the ways through which information flows, the rules are set, and  the  power  is  organized.  For  example,  improving  transparency  of  how  algorithms  are  employed,  the governance or legislation of their use, or putting the ownership of data back into the consumer\u2019s hands. These changes are  structural to the system that allows the AI to operate, and result in establishing new patterns of behavior and agency.  Issues identified in the Purpose zone have the most potential to affect change in a system. These relate to the norms,  values,  goals,  and  worldviews  of  AI  developers  that  are  embodied  in  the  system.  It  includes  the underpinning paradigms based on which the system is imagined, and the ability to transform entirely and imagine new paradigms. Framing perceived problems in this zone serves to act as a compass to guide the developers to align with the fundamental purpose of the system.  The Five Ps\u2014problem, parameter, process, pathway, and purpose\u2014 characterize five ways we can begin to conceptualize our journey towards Responsible AI. Zones within the Five Ps are interrelated, and scale and reach also plays a role in the extent to which the system\u2019s behavior changes. These Five Ps are not part of a fixed hierarchy of change but serve as a conceptual tool to categorize strategies to effect change in a system.  In the following sections, we consider the Five Ps as an analytical tool and how it could be used as a planning tool to assess current interventions in Responsible AI.",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                },
                {
                    "id": "6997df84-c182-4167-8d3c-e4f57fa12bc1",
                    "text": "Reviewing the ongoing attempts to address Responsible AI, it is common to see that activities are commonly conceptualized,  defined,  and  implemented  in  the  Parameters  and  Process  zones.  Many  initiatives  frame  the challenge of Responsible AI as the problem of technical and design flaws requiring engineering fixes or a better design process  . The rational is that complex concepts and high-level principles need to be simplified so as to be tangible and computable. The result had been studies that examined isolated factors related to the principles, such  as  improving  model  explainability   and  reducing  biases   .  Change  at  these  levels  typically  result  in incremental change and allow business as usual.  There are opportunities to radically shift a movement to Responsible AI by effecting changes in Pathway zone, such  as  high-level  design  and  structures,  and  the  challenging  questions  about  the  underlying  assumptions, visions,  and  the  foundational  purpose  of  the  system  as  in  Purpose  zone. This  particularly  happens  when Responsible AI is understood as the microcosm of cultural and political challenges faced in society , beyond technical and design issues. To  illustrate,  consider  an  AI  system  that  is  used  in  a  social  media  company  causing  misinformation  and extremism\u2014similar  to  the  one  Facebook  is  currently  experiencing.  In  a  move  towards  Responsible  AI,  the company views the problem at the Parameters zone, and creates tools and tweaks algorithms to  analyze  and address the biases in order to fix the models that come out of them. Another response from the company, could be  creating  software  tools  to  translate  principles  of  responsible  AI,  such  as  fairness,  explainability,  and accountability to improve the models  .By taking these measures, the company seeks to control misinformation in its content-moderation models across the platform, which potentially leads to an improved user experience.   These interventions could be described as \u2018technological solutionism\u2019, built on a premise that the challenge of responsibility  is  a  challenge  of  fixing  a  design  flaw  in  the  algorithms  .  In  this  view,  the  efforts  for quantifying,  computing,  or  ma  thematizing  responsibility  are  perceived  as  an  apparatus  for  creating  a technocratic rather than democratic solutions, and fixes tend to be short-term and could be described as \u2018tweaks reaction\u2019.  In our example, although visible content moderation  could  improve, the  paradigm under which the  platform operates  remains  unchanged.  If  the  company\u2019s  business  model  only  concerns  with  maximizing  engagement, tweaking algorithms will then have no direct impact on misinformation circulation\u2014 because the AI models that governs the interactions will continue to reward inflammatory content (e.g., controversy, misinformation, and extremism),  and  operate  on  structures  that  systematically  reduce  the  diversity  of  viewpoints  that  users  are exposed to.  Further, engaging changes that undermine the company\u2019s paradigms are unlikely to be supported. For example, a for-profit company is unlikely to support initiatives that have potential to reduce revenue streams  . in  development The same company could consider the problem in the Process zone, by intentionally promoting diversity and inclusion  training opportunities. As more diverse views are involved in the development of the model, assumptions are questioned and resolved during the development cycle. This would likely see first-order change, adjusting and adapting practices to changes in the operating environment (see Figure 2).  teams,  publishing  new  professional  guidelines  and  promoting Extending this, the company could initiate reform in the Pathway zone to achieve second-order change through \u2018restructures\u2019 and \u2018redesigns\u2019. For example, this could include initiating governance structures within their firm for Responsible AI, such as ethical review boards or introducing new roles and responsibilities for assuring AI products and processes are ethical and aligned with AI principles the company abides by.  Collective partnerships can also  focus  discussion  on  the  development  of  design  principles,  guidelines,  and  best  practices  for AI    . However, a unified and strong regulation does not yet exist which can establish fiduciary duties to the public, and that implies the societies can just hope that reputational risks or company's own values and standards may create  more  responsible  approaches  towards  AI  development  and  use .  Further,  partnerships  thus  far  have produced \u201cvague, high-level principles and value statements which promise to be action-guiding, but in practice provide  few  specific  recommendations  and  fail  to  address  fundamental  normative  and  political  tensions embedded in key concepts for example, in fairness and privacy\u201d  . Finally, in the Purpose zone, the same company could deploy resources to move to third-order, transformative change by \u2018reconsidering\u2019 or \u2018redefining\u2019 the purpose of their system. In the example of social media company, this  could  be a  change  in  purpose  from  maximizing  engagement  to activities  such  as  truth-seeking  or  social cohesion. There are, for example, several experimental products, such as a platform called Polis, that highlight diverse views and work towards maximizing \u2018consensus\u2019 rather than engagement, and thereby fundamentally changing the goal of the system. This demonstrates that there are often multiple interactions between leverage zones which can be studied for evaluating the intervention\u2019s effectiveness. These zones are not discrete, and for effective  implementation of change, we should consider the interactions required across an entire system for change in the deeper leverage zones.  Despite efforts to move towards Responsible AI, many of these initiatives, particularly those conducted at the corporate level, have been characterized by critics as \u2018ethics washing\u2019, where industry adopts \u2018appearances of ethical  behavior\u2019  for  self-serving  purposes  (for  example,  to  reduce  regulatory requirements  or  maintain  self-regulation)  . On the other hand, it is argued that steps forward in the right direction, however small, are welcome, and major AI companies who have had concrete plans and actions help the industry move towards responsible AI  . As an analytic tool, the Five Ps can be used to view the relative strength of interventions towards Responsible AI. In the following section, we look at how the Five Ps can also be used as a planning tool by those seeking to deliver Responsible AI.   REACTIONCONFORMREFORM TRASNFORMATION",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                },
                {
                    "id": "6b309e3e-7ead-413a-b712-99745067eec5",
                    "text": "",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                },
                {
                    "id": "4cafb0d8-1d19-49d7-8c12-2066099239c5",
                    "text": "Figure 2  Schematic illustration of the leverage zones, showing their differences in terms of \u2018efforts\u2019 they are needed and the type of \u2018change\u2019 they bring about. Feedback loops indicate interactions that may happen between and among different leverage zones.",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                },
                {
                    "id": "d388c3bb-94e9-4485-b3e8-3e0933e0ce89",
                    "text": "Efforts towards Responsible AI thus far have predominantly been narrow in focus, with leverage zones chosen that have failed to address challenges \u2018deeper\u2019 questions about the governing rules, structure, business model, and purpose. To move towards Responsible AI, we argue that interventions should be seen and studied in a holistic manner, not in isolation, to avoid missing linkages between the leverage zones, to prioritize competing efforts, to consider the narrow and broad consequences, and to plan in the short and long term. As a planning tool, the Five Ps can be used to prompt consideration of a given problem at multiple levels to achieve the desired level of \u2018response\u2019. In Table 1, we provide a set of questions for each leverage zone that could be considered when considering a potential intervention. These questions should be seen as a general set of considerations; they are not exhaustive, and should be tailored to the situation at hand. By proactively considering questions that address systems-level concerns within each of the leverage zones, the appropriate leverage zone of the problem can be properly assessed, and possible synergies and contradictions that might arise can be considered.\u202f Table 1. Lines of questioning on interventions for Responsible AI By exploring these questions, the Five Ps approach allows decision-makers to better understand the scope of the change they are seeking and avoid engaging with the system in shallow leverage zones, such as focusing on AI Principles alone or developing tools and practices for explainable models. It recognizes and promotes the importance of \u2018question-asking\u2019 and how it can influence the shape of the pathway towards Responsible AI. Second, it shows how focusing interventions within discrete leverage zones can precipitate in others, across various depths. The interdependencies between different leverage zones are important to be recognized and studied. Working from the deeper leverage zones shapes and limits the types of interventions available in shallower leverage zones see  .Third, it provides an aid for maintaining a holistic view over the challenges associated with Responsible AI, avoiding \u2018atomized\u2019 and \u2018siloed\u2019 conceptualizations in which social, technical, and governance aspects of AI systems are addressed separately, rather than elements that are tightly interacting togethers. The alternative is that we will remain in the existing paradigm which mostly overlooks the structures, norms, values, and goals underpinning the complex problems Responsible AI is facing at deeper levels.  Nevertheless, given the scale of existing social and ethical problems that have emerged in relation to the AI use, there is a strong incentive for major AI companies to adopt new tools and frameworks in order to prevent the development technologies that have the possibility to cause harm  . And lastly, it provides a transdisciplinary context for a conversation about Responsible AI. Since AI developers come from varied disciplines (each with their own epistemic culture and ethical standards), to speak about Responsible AI, we need frameworks that can engage all stakeholders in meaningful discussions. This is particularly important as we can expect that experts interested in human and environmental aspects of AI-powered technologies are increasingly joining the conversation  . The Five Ps framework provides a new communication tool for a wide range of stakeholders to speak about their ideas and priorities for the future of AI and collaborate using qualitative and quantitative methods. However, along with\u202fall\u202fthese\u202fadvantages, there are certain issues or challenges that must be addressed carefully when we use this approach.\u202fThe concept of leverage zones and places to intervene in a system are\u202fin\u202ftheir\u202finfancy in the field of Responsible AI, and will benefit from added discussion and more research to inform where and how they can be used. The terminology has yet to be further developed and established, so we can develop methods to identify or validate leverage zones at different scales, such as temporal, institutional, network, and management factors, and societal reach, such as global, national, local levels, towards responsible AI.",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                },
                {
                    "id": "4ceab655-2b2f-4e95-b52f-73acc4984fca",
                    "text": "Responsible AI needs to engage with the deep questions to find solutions that can address root causes that have led to negative outcomes in AI products and processes. As such we need to constantly reflect about whether the planned initiatives can realize the system shift required to create an environment conducive towards Responsible AI. To this end, we propose that the Five Ps framework is a useful tool to frame a conversation around the notion of \u2018leverage zone\u2019 as the industry takes actions towards Responsible AI.   The key advances that the Five Ps framework presents are in developing a shared understanding of: likely long-term effectiveness of proposed initiatives; interdependencies between initiatives required for long-lasting change; frames of question-asking when considering initiatives; removing barriers around silos of activity; considering the broader implications of initiatives, and; providing a transdisciplinary context for the conversation. Further work is required to study long-term effects of decisions arising from the Five Ps zones as a planning tool. However, as we have noted in other domains, it is highly likely that any efforts towards understanding interventions towards Responsible AI at a more holistic, systems level will see benefits over taking a fragmented, siloed approach.",
                    "reference": "[1] Nazanin Nabavi and Clare Browne. 2022. Five Ps: Leverage zones towards responsible AIE. arXiv:2205.01070. Retrieved from https://arxiv.org/pdf/2205.01070"
                }
            ]
        },
        {
            "paper_title": "Collect, measure, repeat: Reliability factors for responsible AI data collection",
            "authors": "O Inel, T Draws, L Aroyo",
            "publication_info": "Proceedings of the AAAI Conference on \u2026 - ojs.aaai.org",
            "paper_url": "https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320",
            "chunks": [
                {
                    "id": "668d2267-5199-4817-93a2-028d21821143",
                    "text": "",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "b00fe7fb-2e5f-49dd-8677-9411f260ae8f",
                    "text": "As the use of machine learning (ML) and artificial intelli-gence (AI) becomes more ubiquitous in our daily activities,e.g., to pick a restaurant for dinner (Burke 2002), as wellas in high-stakes domains, e.g., to select a job candidate (Liet al. 2021) or choose medical treatment for a patient (Shatte,Hutchinson, and Teague 2019), the need to scrutinize everyaspect of AI systems is also increasing. This includes evalu-ating their training and testing data quality, as well as quanti-fying the level of fairness, transparency, accountability, and non-maleficence (Jobin, Ienca, and Vayena 2019) these sys-tems have. Several actionable toolkits and checklists for bothmodels and datasets have been proposed, such as Fairlearn(Bird et al. 2020), AI Fairness 360 (Bellamy et al. 2019),Aequitas (Saleiro et al. 2018), Model Cards (Mitchell et al.2019), Datasheets for Datasets (Gebru et al. 2021), PAIR AIExplorables, AI Test Kitchen. Furthermore, this also led toan emerging data-centric research effort on how data qualitycan affect the robustness, reliability, and fairness of AI sys-tems\u2019 performance in the real world (Mehrabi et al. 2021;Sambasivan et al. 2021; Kapania et al. 2020).Traditionally, high-quality data for ML is collected fromexperts and inter-rater reliability (IRR) scores (e.g., Cohen\u2019s\u03ba (Cohen 1960), Fleiss\u2019 \u03ba (Fleiss 1971), or Krippendorff\u2019s\u03b1 (Krippendorff 2011)) measure their reliability. Employ-ing experts, however, is often costly and time-consuming.Crowdsourcing is a widely used alternative to create groundtruth datasets for ML applications. Due to the nature ofcrowdsourcing annotation studies (i.e., raters who likelyhave limited or no domain expertise), a large body of re-search has primarily focused on data evaluation and aggre-gation techniques (Hovy et al. 2013; Dumitrache et al. 2018;Paun et al. 2018; Braylan and Lease 2020).Under the assumption that each annotated input sam-ple has only one correct interpretation (Nowak and R\u00a8uger2010), crowdsourced annotations are typically aggregatedusing majority vote (MV) (Dumitrache et al. 2021). How-ever, research has shown that data quality is complex andcan be influenced by many factors, such as disagreement-prone or subjective tasks, ambiguous input samples, targetannotations, and guidelines, diverse rater characteristics andperspectives (Welinder and Perona 2010; Aroyo and Welty2014; Kairam and Heer 2016; Chang, Amershi, and Kamar2017; Draws et al. 2022), ethical aspects and power struc-tures in annotation processes (Miceli, Schuessler, and Yang2020; D\u00b4\u0131az et al. 2022), or cognitive biases (Eickhoff 2018;Santhanam, Karduni, and Shaikh 2020; Draws et al. 2022).In such cases, IRR scores may not always be able to capturethe true annotations\u2019 reliability and MV could eliminate cor-rect answers vetted by only a few raters. Additionally, IRRscores cannot be used to directly compare datasets, as theyonly indicate raters\u2019 consistency rather than data quality.These factors have led to several streams of research.First, the notion of ground truth currently adopts a perspec-tivist stance (Basile et al. 2021; Aroyo and Welty 2015)which highlights the need of diverse opinions and per-spectives for a better knowledge representation comparedto MV. Second, before runtime checklists have been pro-posed (Draws et al. 2021; Thomas et al. 2022) to help re-searchers consider cognitive biases and other human fac-tors affecting their annotations. Third, attention is drawnto formulating dataset artifacts that describe the collectionpurpose, method, and raters (Bender and Friedman 2018;Ram\u00b4\u0131rez et al. 2020; Gebru et al. 2021; D\u00b4\u0131az et al. 2022).Fourth, existing datasets have been extensively judged, im-proved, and re-annotated based on empirical evidence sug-gesting that existing annotations are not representative any-more or contain ambiguous or erroneous annotations (Yunet al. 2021; Inel and Aroyo 2019; Aroyo and Welty 2015).However, the research landscape is still lacking a unifiedframework that allows for cross-datasets comparisons andmeasurement of dataset stability for repeated data collec-tions. Thus, or proposed approach complements existing re-search by proposing an iterative metrics-based methodologyfor thoroughly scrutinizing the factors influencing the intrin-sic reliability of datasets and their stability over time and forvarious contexts or factors (Fig. 1).Figure 1: Methodology for measuring reliability and repro-ducibility of AI data collections.Our proposed methodology enables a comprehensiveanalysis of data collections by applying reliability and re-producibility measurements in a systematic manner. The re-liability metrics are applied on a single data collection andfocus on understanding the raters. We then propose that datacollection campaigns are repeated, either under similar ordifferent conditions. This allows us to study in-depth the re-producibility of the datasets and their stability under variousconditions or constraints, using a set of reproducibility met-rics. In short, we propose a set of metrics that are applied (1)on a single repetition and (2) across repetitions to thoroughlyevaluate the reliability and stability of annotated datasets.The overall methodology is designed to integrate responsi-ble AI practices into data collection for AI. This allows datapractitioners to follow our step-by-step guide to explore fac-tors influencing reliability and quality, ensuring transparentand responsible data collection practices. We validate our methodology on nine existing data collections repeated atdifferent time intervals with similar or different rater qualifi-cations. The annotation tasks span different degrees of sub-jectivity, data modalities (text and videos), and data sources(Twitter, search results, product reviews, YouTube videos).The following are the key contributions of this paper:1. a step-wise guide for practitioners consisting of a set ofmetrics to thoroughly investigate and explore factors thatinfluence or impact the reliability of annotated datasets;2. a validation and illustration of the proposed metrics-based iterative methodology for achieving transparencyof the reliability of datasets and their stability over timeon nine existing data collections; and3. a discussion of implications and lessons learned for re-sponsible data collection practices.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "19b7ddd7-8058-480c-a0b6-8addb9061368",
                    "text": "Data quality is an already established field of study (Wangand Strong 1996; Zaveri et al. 2016; Lukyanenko and Par-sons 2015), spanning numerous domains and use cases, suchas linked open data, user-generated data, to name a few. Dataquality aspects are also addressed in several ISO standards(e.g., ISO 25012 (for Standardization 2008; Guerra-Garc\u00b4\u0131aet al. 2023), ISO 8000 ). Research utilizing crowdsourceddatasets has further broadened the community\u2019s views re-garding factors influencing or affecting data quality. A largebody of research has been focusing on acknowledging theimpact of cognitive biases, such as confirmation bias or an-choring effect, on the crowdsourced data quality (Eickhoff2018; Hube, Fetahu, and Gadiraju 2019; Santhanam, Kar-duni, and Shaikh 2020; Draws et al. 2022). However, whilevarious types of cognitive biases are known to impact dataquality, in typical annotation studies, it is not a mainstreampractice to account for raters\u2019 stances, opinions, or knowl-edge on various issues. Furthermore, despite several pro-posed mechanisms to mitigate biases (Eickhoff 2018; Hube,Fetahu, and Gadiraju 2019; Barbosa and Chen 2019), it isstill unclear which mechanisms are suitable for a particu-lar situation or which individual characteristics of the ratersmay lead to systematic biases (Draws et al. 2022). To furtheralleviate some of these issues, Draws et al. (2021) proposeda 12-item checklist for requesters to identify which cogni-tive biases might affect their data before the start of datacollection. While this checklist offers a powerful tool for re-questers, many times AI and ML practitioners reuse exist-ing annotated datasets that might lack proper documentationor description of the annotation process, which makes theassessment difficult and leads to worrying outcomes whendeployed in the real world (Paullada et al. 2021).Unequal distribution of demographic characteristicsamong raters may subsequently lead to poor performanceof ML models (Barbosa and Chen 2019). While investigat-ing whether different cultural communities produce differ-ent gold standards and whether algorithms perform differ-ently on gold standards from different cultural communi-ties, Sen et al. (2015) found that AMT-derived gold stan-dards for knowledge-oriented tasks can not generalize acrossdifferent communities and influence ML performance. Inthe context of image annotation, Dong et al. (2012) con-cluded that different cultures provide different tags, be-ing highly influenced by their cognitive and emotional as-pects. The same behavior was observed when performingnews sentiment analysis (Balahur et al. 2010). Besides cul-tural differences, cognitive biases, and stereotypes, societalevents or temporal aspects can also add variance in collecteddata (Aroyo and Welty 2015; Christoforou, Barlas, and Ot-terbacher 2021; Sen et al. 2015). Christoforou, Barlas, andOtterbacher (2021) showed empirical evidence that signifi-cant public health events might be reflected in the descrip-tive tags regarding a person\u2019s identity and body weight thatraters use to annotate images of people.In the remainder of this section, we review three streamsof research regarding responsible data collection for AI: col-lection, assessment, and documentation and maintenance.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "d1d20a0b-802f-47dc-ab9c-592fd227e331",
                    "text": "Data collection practices strongly affect the quality ofcrowdsourced data. This led to extensive explorationsin evaluating raters\u2019 performance and identifying under-performing pools of raters (Ipeirotis, Provost, and Wang2010; Bozzon et al. 2013; Sober\u00b4on et al. 2013), improvingthe overall clarity of the annotation task design (Kittur, Chi,and Suh 2008; Gadiraju, Yang, and Bozzon 2017; Wu andQuinn 2017; Han et al. 2019), encouraging raters to reflecton their answers (Kutlu et al. 2020), or experimenting withseveral annotation designs to identify the most suitable oneto capture the appropriate answers (Inel et al. 2018; Lau,Clark, and Lappin 2014; Roitero et al. 2018). More pre-cisely, at the level of the task design, many studies exper-imented with annotation scales. For example, Roitero et al.(2018) showed that fine-grained annotation scales are moresuitable and natural than coarse-grained annotation scales tocapture web documents\u2019 relevance. Intrinsic motivation andincentives have also been shown to be beneficial in improv-ing the quality of crowdsourced data (Ho et al. 2015; Kit-tur et al. 2013). In our research, we present extensive dataanalysis of nine existing data annotation tasks which vary interms of collection criteria such as input data (tweets, prod-uct reviews, web documents, facial expression recordings,and news broadcasts) and annotation goal.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "7b92a892-f1ed-4104-bb3b-6f1e6a40e02f",
                    "text": "To measure the reliability of crowdsourced annotations, re-search focused on quality control mechanisms (Daniel et al.2018) and the definition of aggregation techniques (Hunget al. 2013; Li, Rubinstein, and Cohn 2019; Dumitrache et al.2018; Hovy et al. 2013; Paun et al. 2018). Typically, currentannotation campaigns rely on the use of multiple raters perannotated input and reporting of inter-rater reliability met-rics (Park et al. 2012; Sigurdsson et al. 2016; Park, Shoe-mark, and Morency 2014), such as Cohen\u2019s \u03ba (Cohen 1960),Fleiss\u2019 \u03ba (Fleiss 1971), or Krippendorff\u2019s \u03b1 (Krippendorff2011). However, the choice of the IRR metric is less im-portant than having a representative number of raters per in- put (Artstein and Poesio 2008). Furthermore, Popovi\u00b4c andBelz (2022) found that raw counts are a more suitable in-put for computing and estimating inter-rater reliability com-pared to normalized counts or percentages in a machinetranslation use case. In a recent study, however, Braylan,Alonso, and Lease (2022) point out that Krippendorff\u2019s \u03b1relies on mean distances, which can lead to mistakenly dis-carding good data when dealing with subjective annotationsand propose using more suitable distance functions depend-ing on the task at hand. When dealing with subjective tasks,or tasks that could generate diverse opinions or perspec-tives and potentially multiple ground truths, achieving re-liable results is thus even more challenging (Graham, Awad,and Smeaton 2018; Zah\u00b4alka and Worring 2014; Basile et al.2021). This could mean that different replications of such atask could give very different results.To the best of our knowledge, only a few data collec-tion experiments addressed repeatability (Blanco et al. 2011;Welty, Paritosh, and Aroyo 2019). Thus, our work proposesdata collection repeatability as a responsible practice to mea-sure data stability over time. We propose a set of metricscarefully chosen to scrutinize the human factors influencingvarious aspects of the data over time, thus fostering cross-comparison between datasets.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "3fc13e37-bc25-4c70-b5a6-46790e8125c0",
                    "text": "By systematically reviewing 150 published papers dealingwith classification tasks on Twitter data, Geiger et al. (2020)concluded that issues such as reliability, transparency, andaccountability in data collection practices are not a main-stream approach in the ML community. A considerableamount of analyzed papers offer limited or no details re-garding raters, their demographic information, compensa-tion details, IRR scores of the collected datasets, annotationinstructions, and overall setup of the annotation process. Fol-lowing studies that also showed the extent to which potentialbiases are present in extensively used image datasets (Zhaoet al. 2017; Hendricks et al. 2018; Otterbacher 2015), a lot ofattention has been brought to data documentation and main-tenance approaches, in many fields.Inspired by medicine and psychology literature, Benderand Friedman (2018) proposed data statements for charac-terizing and understanding the raters of a natural languagedataset, their potential biases, and how they might affectthe deployment of ML models. Similarly, informed by theelectronics industry where every component is thoroughlydescribed in terms of characteristics, test results, or recom-mended usage, Gebru et al. (2018) proposed datasheets fordatasets. D\u00b4\u0131az et al. (2022) studied ethical considerationsthat affect the annotation of the dataset, such as, for instanceraters\u2019 previous experience, and developed the CrowdWork-Sheet framework to facilitate critical reflection and trans-parent documentation of dataset annotation decisions, pro-cesses, and outcomes. Pushkarna, Zaldivar, and Kjartansson(2022) proposed data cards to record key aspects of datasetsand their life cycle (i.e., explanations concerning the prove-nance, representation, usage, and fairness of ML datasetsfor all stakeholders), allowing for responsible AI develop-ment. Finally, (Wilkinson et al. 2016) proposed a set of guid-ing principles to support researchers, industry practitioners,and funding and publishing agencies in scholarly data reuse;these guiding, measurable principles tackle four fundamen-tal data principles \u2014 findability, accessibility, interoperabil-ity, and reusability (i.e., FAIR). In the crowdsourcing com-munity, Ram\u00b4\u0131rez et al. (2020, 2021) provide guidelines forrequesters to improve dataset reporting and reproducibility.While our work is not focusing on documenting the an-notation process of human-annotated datasets, it comple-ments existing approaches by proposing a set of reliabilityanalysis metrics that foster responsible data documentationand adherence to proper data provenance and documentationguidelines for subsequent dataset alterations.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "c2ad9e08-fb28-489a-9d76-84fd9415c861",
                    "text": "We introduce our proposed methodology for in-depth anal-ysis of the reliability and reproducibility of data annotationstudies. The proposed methodology brings together, in a sys-tematic way, a set of measurements typically performed ad-hoc. However, the ability to observe their interaction allowsdata practitioners to provide a holistic picture of the dataquality produced by these studies. Therefore, our proposedmethodology provides a step-wise approach as a guide forpractitioners to explore factors that influence or impact thereliability and quality of their collected data. While the reli-ability analysis focuses on the raters that participate in a datacollection, the reproducibility analysis provides insights re-garding the stability of the overall dataset. As observed inFigure 1, the chosen metrics provide input for a scorecardallowing for thorough and systematic evaluation and com-parison of different data collection experiments.Ultimately, the proposed reliability and reproducibilityscorecards and analyses allow for more transparent and re-sponsible data collection practices. This leads to the identi-fication of factors that influence quality and reliability, thethorough measurement of dataset stability over time or indifferent conditions, and allows for datasets comparison.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "1bbdfa09-4109-490c-abf1-55e432f717ad",
                    "text": "We address the reliability of the crowdsourced annotationsby looking at raters agreement, raters variability, and poweranalysis (i.e., determine the sufficient number of raters foreach task). These analyses equip us with fundamental obser-vations and findings for characterizing annotations\u2019 qualityand reliability. It is important to note that depending on thenature and characteristics of the task (i.e., difficulty, subjec-tivity, clarity), the assessment of the crowdsourced annota-tions\u2019 reliability is not always trivial and needs to be consid-ered when generalizing the results across different tasks.Rater agreement analysis: indicates the level of consis-tency among raters\u2019 annotations in an experiment. We com-pute the inter-rater reliability score (IRR) using Krippen-dorff\u2019s \u03b1 (Krippendorff 2011) because is suitable for mostannotation experiments, given that it can deal with multipleraters, various rating types (i.e., categorical, ordinal, inter-val), and missing data (i.e., not all units are annotated byall raters). Typically, \u03b1 scores above 0.8 are considered to show strong or high agreement among raters, while valuesclose to 0.6 are still considered acceptable (Landis and Koch1977; Carletta 1996; Krippendorff 1980). Note that the IRRscores can be influenced by various characteristics of thetask, which need to be taken into consideration in the over-all analysis. For example, a low IRR reliability score (e.g.,below or very close to 0.33) could indicate both high tasksubjectivity and unsuitable annotation guidelines or raters\u2019qualifications, among others. Interpreting whether the agree-ment value is low, medium, or high, however, is often task-dependent and should be discussed on a case basis.Rater variability analysis: gives insights regarding thevariability in raters\u2019 answers distributions. We use two met-rics to measure raters\u2019 precision for each individual anno-tated item in our datasets - we inspect the standard devi-ations in raters\u2019 annotations (Welty, Paritosh, and Aroyo2019) when having binary or continuous value annotationsand the index of qualitative evaluation (IQV) (Wilcox 1967)when having nominal or categorical values. IQV is a mea-sure for assessing the variability of nominal variables withvalues between 0 (all raters\u2019 answers are in one category)and 1 (raters\u2019 answers are evenly distributed in each cate-gory). In our analysis, we consider IQV \u2264 .33 as low vari-ability, .33 < IQV > .66 as medium, and IQV \u2265 .66 as high.Power analysis: indicates whether the number of ratersused in each annotation task is sufficient. To identify the op-timal number of raters (i.e., for which the variability in termsof IRR is not significant), we bootstrap the number of raters[3,4,5,..,n] per input item, where n is the maximum num-ber of raters (Snow et al. 2008). For each number of raters,we perform 100 runs, where raters are randomly selectedfor each input item, and each time we compute the IRR.Then, we apply a chi-squared test for one standard deviationand test whether the standard deviation of the IRR scoresfor each number of raters is lower or equal to a threshold.In our experiments, we considered the threshold of .01 andfor each number of raters, we test the following hypotheses:H : \u03c3 \u2264 .01 and H : \u03c3 > .01, where \u03c3 refers to thestandard deviation of the IRR scores. Given H , we conducta right-tailed test, and we search for the lowest number ofraters for which we fail to reject H , i.e., p < .05.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "66350f36-df79-43bf-9f07-33096f2ea8f2",
                    "text": "To investigate how rater populations influence the reliabil-ity of the annotation results, we propose repeating the an-notations at different time intervals and in different set-tings, thus identifying the factors influencing their reliability.However, just by using the aforementioned reliability mea-sures, we can not perform a proper comparison of the col-lected annotations. For instance, high IRR values in severalrepetitions indicate highly homogeneous raters\u2019 populationswithin each repetition, but it does not necessarily mean thatthe experiments are highly reproducible. For this, we per-form two additional measurements: 1) stability analysis and2) replicability analysis to understand how much variabilitythe raters bring and how much we can generalize the results.For example, a high correlation between two task repetitionsindicates that our results are stable, and the populations thatparticipated in the two repetitions are drawn from the samedistribution.Stability analysis: is the degree of association of the ag-gregated raters\u2019 scores across two annotation task repeti-tions. We measure the stability of the data collection withthe correlation of the aggregated raters\u2019 annotations betweenpairwise repetitions of each task. The aggregated raters\u2019 an-notation can be a mean value, majority vote, or any aggrega-tion technique suitable for the task at hand. To compute thecorrelation, we use the Spearman\u2019s rank correlation (Xiaoet al. 2016) for tasks with numerical values and Chi-squaretest of independence for tasks with categorical values.Replicability similarity analysis: indicates the degree ofagreement between two rater pools, making two data annota-tion tasks comparable. It consists of raters\u2019 agreement acrossrepetitions of a particular task. To measure this, we use ametric called cross-replication reliability (xRR) (Wong, Par-itosh, and Aroyo 2021). The xRR score between two repe-titions is goes from 0 to the highest IRR score of the tworepetitions. In a perfect replication, the xRR score is equalto the IRR score of each individual repetition (implying thatthe two repetitions also have equal IRR scores). This fur-ther means that similar IRR and xRR scores indicate bothinternal and external validity while much lower xRR valuescompared to IRR scores indicate low external validity.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "5d644f2d-434a-4422-9b34-9e7a353d2b3e",
                    "text": "We outline the experimental design to evaluate the reliabilityand reproducibility of nine published data annotation studiescovering a wide range of content modalities and annotationtasks (Table 1). We first outline the data annotation studies(Section 4.1), and then describe the resulting datasets (Sec-tion 4.2). Thus, we use these nine data annotation studies asa two-fold objective: 1) to illustrate how practitioners shouldapply the methodology introduced in Section 4 to gain a bet-ter understanding of their data and 2) to validate the useful-ness of the methodology to help practitioners explore factorsthat influence or impact data reliability.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "a9e8553a-3a29-4198-86c7-53c9e909b79b",
                    "text": "We first describe how the datasets (see Section 4.2) used inour experiments have been collected, i.e., the task, the num-ber of repetitions, and their settings (summarized in Table 1).All tasks and datasets have already been published.Video Concepts Relevance (VCR; Inel and Aroyo2022). The raters were asked to watch a video of 1-2 minutesand then select all relevant concepts for the content of thevideo from a list of machine-extracted concepts (an averageof 11 concepts). Five different annotation experiments wererun, each focusing on the identifications of different con-cept types, e.g event (VCR E), people (VCR P), location(VCR L), and organization (VCR O), and concepts of anytype (VCR ALL). The task was run on Amazon Mechani-cal Turk (AMT) with ten videos representing short Englishnews broadcasts from YouTube annotated by 15 raters. Eachtask was repeated three times, at least three months apart,and each repetition used the same raters\u2019 qualifications, andraters were allowed to participate across repetitions. Video Human Facial Expressions (IRep; Wong, Pari-tosh, and Aroyo 2021). The raters were given a video record-ing containing human facial expressions and were asked toselect all facial expression labels (i.e., emotions) that theyperceived as being relevant from a predefined list of facialexpression labels (Wong, Paritosh, and Aroyo 2021). Thetask was run on AMT. A total of 30 emotion labels (fromthe set defined by (Cowen and Keltner 2017)) were shown,together with the option \u201cunsure\u201d (raters were instructed tochoose this option when it was not possible to determine thefacial expressions expressed in the video recording). Eachvideo recording was annotated by two raters. The task wasrepeated three times, each time with raters from a differentpool, namely raters from Mexico City, Kuala Lumpur, Bu-dapest, and internationals.Product Reviews (PR; Qarout et al. 2019). The raterswere given a product review and were asked to classifythe issue described in the review into one of three possibleclasses (i.e., \u201csize aspects\u201d, \u201cfit aspects\u201d, \u201cno issue with sizeor fit\u201d). The task was run on AMT, and each rater was re-quired to annotate all 20 product reviews, which appeared inthe same order for each rater, and each product review wasannotated by at least 68 raters. The task was repeated fivetimes at intervals of one week. The raters were not allowedto participate in more than one repetition.Crisis Tweets (CT; Qarout et al. 2019). The raters weregiven a crisis-related Twitter message and were asked to cat-egorize it into one of nine possible options (i.e., \u201cinjuredor dead people\u201d, \u201cother useful information\u201d, \u201cinfrastructureand utilities damage\u201d, \u201cnot related or irrelevant\u201d, \u201csympathyand emotional support\u201d, \u201cdonation needs or offers or volun-teering services\u201d, \u201cmissing, trapper or found people\u201d, \u201cdis-placed people and evacuations\u201d, \u201ccaution and advice\u201d). Thetask was run on AMT, and each rater was required to anno-tate all 20 tweets which appeared in the same order for allraters, and each tweet was annotated by at least 68 raters.The task was repeated five times at intervals of one week.Each rater was allowed to participate in just one repetition.Words Similarity (WS353; Finkelstein et al. 2001;Welty, Paritosh, and Aroyo 2019). The raters were given apair of words and were asked to rate the similarity of thetwo words on a scale from 0 to 10 (0 indicating the wordsare totally unrelated and 10 indicating the words are veryclosely related) (Finkelstein et al. 2001; Welty, Paritosh, andAroyo 2019) (fractional scores such as .25, .5, and .75 arealso possible). The task was first run by Finkelstein et al.(2001), and each pair of words was annotated either by 13or 16 raters, and each rater annotated all pairs. The secondtime the task was run by (Welty, Paritosh, and Aroyo 2019)in 2019 (thus around 20 years apart), on AMT. In this repeti-tion, each pair of words was annotated by 13 raters, and eachrater was allowed to annotate as many pairs as they wanted.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "6972c59d-7026-4787-b53b-2cf8abacb2f6",
                    "text": "The tasks described in Section 4.1 resulted in nine annotateddatasets covering different data modalities (text and videosof various lengths and duration) and sources (Twitter, prod-uct reviews, YouTube), as described in Table 2 and below.Video Concept Relevance (VCR E, VCR P, VCR L,Table 1: Overview of annotation tasks and their settings in terms of input data and annotation template.Table 2: Overview of datasets used in our experiments.VCR O, VCR ALL). Dataset of 208, 234, 223, 59, andrespectively 969 video - concept pairs which have beenannotated in terms of relevance in the data annotationtasks VCR E, VCR P, VCR L, VCR O, and, respectivelyVCR ALL. The concepts were machine-extracted (videosubtitles and video stream) from ten short English newsbroadcasts (i.e., videos) published on YouTube, from a pub-licly available dataset (Inel, Tintarev, and Aroyo 2020; Jonget al. 2018; Inel and Aroyo 2022).Video Human Facial Expressions (IRep). Dataset of1090 video recordings of human facial recordings, partof the International Replication (IRep) dataset , publishedby Wong, Paritosh, and Aroyo (2021). Each video record-ing is annotated with emotions from 30 available emotions.The video recordings were generally very short, 5 secondson average (a more extensive description of the recordingsis found in (Cowen and Keltner 2017)).Product Reviews (PR). Dataset of 20 English productreviews for fashion items (accompanied by a photo repre-sentative of the respective product), randomly selected fromthe dataset published by Chernushenko et al. (2018). Eachproduct review is annotated with one of three possible issue classes, as described in Section 4.1.Crisis Tweets (CT). Dataset of 20 English crisis-relatedTwitter messages (e.g., earthquake, flood), randomly se-lected from the dataset published by Imran, Mitra, andCastillo (2016). Each tweet is annotated with one of ninepossible crisis-related options, as described in Section 4.1.WordSim (WS353) . Dataset of 353 English wordpairs (Finkelstein et al. 2001), used as benchmark for se-mantic similarity (Witten and Milne 2008) and word em-beddings (Levy and Goldberg 2014; Bojanowski et al. 2017;Pennington, Socher, and Manning 2014). The word pairswere selected from WordNet, and include the 30 noun pairsfrom (Miller and Charles 1991). Each pair is annotated interms of how similar the two words are on a 1 to 10 scale.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "3a6b2fc9-f58a-4b65-adad-c9bca67dd171",
                    "text": "In this section, we report on the results of the reliability anal-ysis of the data collection studies described in Section 4.1,and in Table 1, and their repetitions. We apply our iterativemetrics-based evaluation methodology to the nine datasetsfrom these studies. In the analysis of the results, we denoteeach repetition as R , where x is the repetition index.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "aebc8b84-79df-45db-b25b-30d3a6fe48dd",
                    "text": "We first report on the reliability analysis of the VCR datasets(i.e., VCR ALL, VCR E, VCR P, VCR L, VCR O), as de-picted in the first five rows in Table 3, columns R , R ,and R . We observe that the datasets of all VCR annota-tion tasks and repetitions have mostly fair agreement andless often moderate agreement (R & R for VCR ALL, Rfor VCR E, and R & R for VCR P). The tasks VCR Oand VCR L have, overall, the lowest inter-rater reliability.Similarly, the precision of the annotations in all tasks andrepetitions is not substantial. In Table B1 in the Appendix,we show an overview of the variability of each repetition ofthe VCR annotation tasks. For each video-concept pair, wecomputed the standard deviation of their score. The majorityof the experiments have a mean standard deviation (MSTD)of around 0.3, with the task VCR O having a higher value ofaround 0.36. The standard deviation of deviations (STDD)is similar across tasks and repetitions, with the lowest valueobserved for the VCR O task. These high values observedfor MSTD and STDD show that this task for annotating rel-evant concepts in videos is subjective and raters consistentlydisagree. Concepts of type organization seem to generate themost disagreement among raters.In our power analysis, we observe that all repetitions ofeach task tend to display similar variability in terms of IRRscore. For each repetition of every task, according to theright-tailed Chi-squared test, we got very similar results interms of the optimal number of raters needed to annotatea video-concept pair. According to the Chi-square test, thefollowing number of raters is optimal (with minimal vari-ation across repetitions): VCR ALL - 6 raters, VCR E - 12raters, VCR P - 11 raters, VCR L - 11 raters, VCR O - 14raters. These, in addition to the high variability of IRR scoresshown in Figure C1 in the Appendix, suggest that annotatingthe relevance of organizations in videos is a more difficulttask that might require an even larger number of raters.Although the IRR scores of the annotations gathered inall repetitions are rather low, the Spearman\u2019s rank correla-tion between the relevance score of the video-concept pairs(computed as the ratio of raters that picked the concept asrelevant) in each pair of repetitions is high, above 0.85 forall tasks and repetitions, showing a statistically significant,strong positive correlation in Table 4. Furthermore, the pair-wise xRR scores (see Figures E1, E2, E3, E4, E5) are verysimilar to the IRR scores of the repetitions. Thus, we observethat while the IRR scores are rather low, raters are similarlyconsistent in each repetition and across repetitions, showingthat disagreement seems to be intrinsic to the task.Table 3: Krippendorff\u2019s \u03b1 agreement for all datasets.IRep: Annotation Task and Dataset5.2Wong, Paritosh, and Aroyo (2021) already provide an in-depth analysis of the IRR and xRR scores per emotion inthree of the repetitions of the tasks. More precisely, they an-alyze the agreement among raters from three different re-gions, i.e., Mexico City (R ), Kuala Lumpur (R ), and Bu-dapest (R ). Their main conclusion is that raters seem tohave more similar or divergent agreement values depending on their country of origin. In terms of individual emotions,they also observe that the most or least agreed-upon emo-tions are different for each country. Similarly, only a fewemotions seem to have both internal and external validity, asthe xRR scores between pairwise repetitions indicate.In addition, in this paper, we also analyze R , a repetitionof the task conducted with international raters, which couldrepresent any possible region. We recall that in the IRep an-notation task, the raters were able to select multiple expres-sions for each input video, which means that we deal with amulti-label annotation task. To compute rater agreement onthis task, we used Cohen\u2019s \u03ba implementation, which uses theMASI distance (Passonneau 2006). In short, MASI is a dis-tance metric used to compare two sets, in our case, two setsof annotated emotions. In Table 3, we observe that the repeti-tion in which international raters are used, R , has the lowestagreement across all emotions. When inspecting agreementon individual emotions (see Table A3 in the Appendix), weobserve that for almost all emotions, the IRR scores in R ,the international repetition, have the lowest values.Our stability analysis (see Table D1 in the Appendix)shows that emotion scores are poorly correlated across rep-etitions. We observe many weak correlations and only a fewmoderate correlations, statistically significant. Furthermore,the correlations with R seem consistently lower. While theanalysis performed by Wong, Paritosh, and Aroyo (2021)showed that for certain emotions such as \u201clove\u201d or \u201csadness\u201draters can have both high internal agreement and cross-replication agreement when comparing R with the otherthree repetitions we can not draw such conclusions. Overall,both the internal agreement (see Table A3 in the Appendix)and the cross-replication agreement (see Figure E8 in theAppendix) indicate less consistency. More precisely, we caninfer that disagreement seems to be intrinsic to the diversityof the raters and the way they interpret emotions.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "579d038b-f2c6-4d9a-a34b-d357e41770e3",
                    "text": "The inter-rater reliability scores computed on the PRdatasets show, overall, fair agreement among raters. To bet-ter understand these agreement values, we also computed theIRR scores for each possible option that the raters could havechosen (see Table A1 in the Appendix), and we observed thatthe option \u201cFit & Aspect\u201d is consistently generating loweragreement values, i.e., in each repetition, compared to theother two options. Furthermore, we also observe a consid-erable difference of 0.21 in IRR scores between repetitionsR and R . As reported by Qarout et al. (2019), raters par-ticipating in R had indeed lower accuracy compared to allother repetitions when compared against a ground truth, butthe difference does not seem to be significant.For the variability analysis, we computed for each unit inthe dataset and, for each repetition, the index of qualitativevariation (IQV). In Figure 2a, we observe that in all repeti-tions, the majority of the units annotated have high variabil-ity in terms of categories provided by raters, i.e., it does notseem to be a predominant category that is chosen by the ma-jority of the raters. This, in addition to the low IRR scores forTable 4: Spearman\u2019s \u03c1 rank correlation of the relevance of each video-concept pair for each pair of VCR tasks repetitions.the option \u201cFit & Aspect\u201d, could potentially indicate that themajority of the units annotated in this task are ambiguous orthat the options they have to choose from are unclear or haveoverlapping meanings. The poor agreement and annotationsstability across all units in this dataset is also confirmed byour power analysis, which indicates that a very high numberof annotations is needed in each repetition to achieve stableresults (see Figure C2 in the Appendix). In each repetition,the optimal number of raters is around 90, a number that ishighly unlikely to be employed in such studies.Further, we analyzed the stability of the experiments tounderstand to what extent the results of any two repetitionsare similar. For this, we computed for each unit in each rep-etition the answer given by the majority of the raters (incase of ties, we selected the majority vote at random) andcomputed the Chi-square test of independence between ev-ery two repetitions. On the one hand, this analysis showedthat the majority vote answers in any two repetitions are cor-related and that there is no statistically significant differenceamong them (see Table D2 in the Appendix). On the otherhand, the replicability analysis through the xRR measureshowed similarly low values, just as the IRR scores. This re-sult indicates that the agreement among raters is, while low,also consistent across repetitions.Figure 2: IQV distribution for each unit and repetition (Rto R ). The distribution is shown as a boxplot (median - blueline, mean - green triangle).",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "56bae3e9-ee12-448a-8219-8a8af7185d39",
                    "text": "The raters participating in all repetitions of the crisis tweets(CT) annotation task indicate moderate to substantial agree-ment, as observed in Table 3. Similarly, as for the PRdatasets, we also computed the IRR scores for each possibleoption the raters were able to choose. These results are avail-able in Table A2 in the Appendix. For this task, we observethat the majority of options generate moderate to substantialagreement, except for two possible answers, namely \u201cmiss-ing, trapped, or found people\u201d and \u201cother useful informa-tion\u201d. However, when inspecting the data, we observe thatthese two options are rarely chosen by raters which might explain their very low agreement values (Brenner and Klieb-sch 1996; Artstein and Poesio 2008).For the variability analysis, we replicated the process de-scribed for the PR datasets. In contrast, however, we observein Figure 2b that the index of qualitative evaluation for thisdataset has more often values closer to 0, indicating that theannotated units have much lower variability in categoriesthat the raters chose. More precisely, the annotated tweetsseem to be easily annotated with a category that is oftenchosen by the large majority of the raters. R of the datasetseems to have the lowest precision, which is consistent withthe lower IRR score as well as with the lower overall accu-racy presented by Qarout et al. (2019). In terms of poweranalysis, similarly to the PR annotation task and dataset, weobserve that the mean IRR scores over 100 runs stabilize fora large number of raters (i.e., 85-95 raters).The stability analysis for this experiment shows that themajority vote answers are very similar across repetitions.More precisely, according to the Chi-square test of indepen-dence, we found no statistical difference between the major-ity vote answers of any two repetitions of the CT task (seeTable D3 in the Appendix). In terms of cross-rater reliability,the xRR metric shows that the results for this dataset are con-sistent across repetitions (see Figure E7 in the Appendix).More precisely, we can infer that the high IRR values ofthese experiments generalize across different rater pools.5.5 WS353: Annotation Task and DatasetAmong all the replicated studies we analyzed, the highestagreement scores are found for the word similarity datasets,W S353, namely 0.59, 0.57, and 0.50, as shown in Table 3.Such IRR values are typically acceptable in the context ofnatural language datasets. As reported by Welty, Paritosh,and Aroyo (2019), a thorough precision analysis indicatedthat while IRR scores have similar values across repetitions,there are certain word pairs for which the similarity scorechanged dramatically in the second and third repetition (e.g.,the pairs \u201cMaradona\u201d-\u201cfootball\u201d and \u201cArafat\u201d-\u201cpeace\u201d hadhigher similarity scores in the first repetition, and very lowsimilarity scores in the second and third repetitions whichwere run almost 20 years from the first repetition). Ourpower analysis presented in Figure C4 in the Appendix in-dicates that around 12 raters could provide a reliable set ofannotations in R , and even fewer raters in R and R .In terms of stability analysis, the Spearman\u2019s \u03c1 correla-tion shows that all three repetitions are correlated with eachother (statistically significant), and in particular R and R ,repetitions that were run on the same platform, with ratershaving similar characteristics (R & R : \u03c1=0.87, p=9.93e-109; R & R : \u03c1=0.84, p=4.61e-96; R & R : \u03c1=0.95,Table 5: Scorecard summary of the reliability and reproducibility analysis of the nine experimental datasets.p=2.88e-182). Similar results are observed in terms of cross-replication reliability, where the xRR values show higheragreement among raters that participated in the last two rep-etitions (i.e., R & R : xRR = 0.53), compared to raters thatparticipated in the first repetition and the subsequent ones(i.e., R & R : xRR=0.49; R & R : xRR=0.44).",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "e36b4adf-d558-45f3-b0be-5b02b5bdcec2",
                    "text": "We discuss the results of our methodology for providing acoherent overview of data quality in terms of human factorsinfluencing the reliability and reproducibility of a crowd-sourced data collection. Our discussion is driven by thescorecards produced by our proposed methodology for in-depth analysis of the reliability and reproducibility of dataannotation studies. The summary of our analysis is presentedin Table 5. Furthermore, we provide lessons learned for re-sponsible data collection practices, reflect on the limitationsof our approach, and give directions for future work.Factors influencing the quality of data collection. Whilewe surveyed extensive literature in the area of crowdsourc-ing and human computation, we only found a handful oftasks and datasets that we could identify as repeated exper-iments. Furthermore, the nine annotation tasks and datasetswe identified did not necessarily focus on identifying thefactors that could influence the quality of data collectionand neither on systematic measurement of their reliabilityand reproducibility. More precisely, current approaches typ-ically use limited quality and reliability metrics, such as IRRscores or accuracy metrics against a gold standard to gaugedata quality. Instead, our metrics provide a scorecard forcomparing the reliability and reproducibility of each datasetand surfaces specific factors influencing results\u2019 quality.In the VCR annotation tasks and datasets, we observedlow IRR scores in all repetitions. However, the tasks anddatasets have high stability, as the xRR analysis revealedsimilar cross-rater reliability and highly correlated relevancescores of the video concepts across repetitions. This indi-cates that raters are similarly consistent within each repeti-tion and across repetitions and that the disagreement indi-cated by the low IRR scores is, in fact, intrinsic to the sub-jective nature of the task. One repetition of the IRep annota-tion task employed international raters. Overall, comparedto the other repetitions (i.e., region-specific), our analysis shows consistently low stability and cross-replication relia-bility. This indicates that not all rater pools are equally con-sistent across repetition and, more importantly, that the raterdisagreement is correlated with the diverse background ofthe raters influencing the way they interpret emotions. Moreprecisely, for similar tasks, our analysis indicates that di-verse raters should not be expected to produce a coherentview of the annotations and we advise repeating the data col-lection by creating dedicated pools of raters with similar de-mographic characteristics and comparing their results. In thePR annotation task and dataset, we found that while stabilitycan be achieved, the variability analysis and the power anal-ysis indicated that even a very high number of raters (around90) can exhibit high levels of consistent disagreement typi-cally caused by the subjectivity of the task. In this case, wewould advise optimizing the task design in order to decreaseadditional ambiguity in the annotation categories. The IRRanalysis on the individual tweet categories on the CT taskindicated that some categories may not be as clear as othersor may only seldom be applicable. This indicates that care-ful attention should again be given to the design, instruc-tions, and possible answer categories in the annotation task.Furthermore, the high number of raters needed to obtain sta-ble results indicates that the task might benefit from a morethorough selection of raters and training sessions. Finally,the high variability for certain word pairs in the WS353 taskindicates that data collection practices are affected by tem-poral and familiarity aspects. This has serious implicationsfor when data collections are reused, as certain annotationsmay become obsolete or change in interpretation over time.Recommendations for responsible data collection Insum, applying our proposed methodology for responsibledata collection does not pose any requirements on how datais structured or formatted. What we propose, does, however,affect the current practice and assumes a significant adap-tation on the use of reliability and reproducibility metrics.The proposed methodology is centered around a set of sys-tematic, iterative (i.e., repeated) pilots which allows to mea-sure different characteristics of the data and task, as well asto capture raters characteristics and measure their potentialbiases. These aspects are captured with the proposed set ofreliability and reproducibility metrics. Finally, we argue forsystematic reporting on data collection provenance.Systematic piloting: The proposed methodology forguiding data collection with a set of metrics for in-depth,iterative analysis of data reliability and replicability is pri-marily suitable as an investigative pilot of data annotationstudies. Such early experimentation and thorough analysisof annotations would provide specific factors that could in-fluence the data collection and could be ultimately mitigatedfor large-scale data collection.Capture raters, task, and dataset characteristics: Weargue that a responsible data collection practice should bor-row guidelines for reporting human-centric studies from thefields of psychology, medicine, and even human-computerinteraction, where human stances, opinions, and other mean-ingful characteristics are thoroughly recorded. While such aprocess would definitely increase the cost and time to gatherthe necessary data, it would also allow for more informeddecisions on the proper process of collecting raters\u2019 annota-tions and possible future reuse.Cognitive biases assessment: Recent research hasdemonstrated that raters\u2019 cognitive biases can strongly af-fect their annotations and reduce data quality (Hube, Fetahu,and Gadiraju 2019; Eickhoff 2018; Draws et al. 2022). Tocombat the influence of cognitive biases, Draws et al. (2021)introduced a checklist that can be used to identify and sub-sequently measure, mitigate, and document cognitive biasesthat may present an issue in the data collection tasks. Werecommend using such a checklist between each iteration tosurface possible cognitive biases that may affect annotationsand allow for appropriate mitigation.Provenance for data collection: To facilitate responsiblereuse of datasets, data documentation, and maintenance ap-proaches should thoroughly record its provenance, includingquality scorecards. This would alleviate issues regarding thehandling of data, reuse or modifications of annotation tasks,and platform selection. With proper provenance documenta-tion, it is easier to identify factors that could influence datacollections\u2019 quality. Such requirement becomes clear whendata collection is influenced by temporal and regional as-pects (see WS353 and IRep).",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "075d488d-ab30-414b-a145-d0f1a80154ba",
                    "text": "We experimented with ninedatasets and annotation tasks with various goals, modalities,sizes, and overall setups. We repeated each task three to fivetimes. While the overall number of units in some datasetswas small (e.g., \u223c 20 input units), the overall dataset sizewas much bigger as the number of raters providing anno-tations per item was significantly larger than in usual datacollections. Our methodology is agnostic to the dataset size,and the significance of the results is not influenced by datasetsize. In future work, we could extend the analysis to otherdata annotation tasks and input data modalities.Scale and optimal number of repetitions. The repeata-bility experiments may not be scalable in terms of time andcost. However, our methodology provides optimization cri-teria that can mitigate this limitation in terms of input for theannotation tasks and the use of the bootstrap technique tooptimize the number of raters needed for reliable results. Aswe have shown, iterative instances of collect, measure, re- peat are suitable for adhering to responsible data collectionpractices. However, it is not trivial to decide on the numberof repetitions necessary to determine that the collected datais reliable. This can be even more problematic for more sub-jective annotation tasks, which can be affected by raters\u2019 in-terpretations, opinions, perspectives, or familiarity with theitems. Future work can address this limitation by investigat-ing additional metrics for determining the suitable numberof repetitions. Furthermore, future work could also focus onproposing a single suitable reproducibility score for repeatedexperiments similar to the one proposed by Belz, Popovic,and Mille (2022) for system reproducibility.Raters\u2019 characteristics. The analyzed tasks includedonly limited information about the raters, besides some verygeneral characteristics such as the platform on which theywere recruited, country, or HIT approval rates. Furthermore,only one dataset out of the nine analyzed had a substantiallydifferent population of raters (i.e., from different countries).This aspect limits our analysis in terms of additional humanfactors that could influence rater agreement and the stabil-ity of the collected annotations. Future work could focus onreplicating our analysis on more controlled data annotationexperiments to study, for instance, the impact of age, gen-der, and other demographic information as additional relia-bility factors for responsible data collection. In future work,this iterative method of addressing responsible data collec-tion should also investigate ways of properly maintainingand describing data provenance.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                },
                {
                    "id": "599e51ca-3fd6-4058-bd23-93bf5bc9de8d",
                    "text": "The continuous deployment of AI systems powered bycrowdsourced data in real-world tasks has increased the at-tention of the research community to further scrutinize thequality and reliability of such datasets in diverse settings. Inthis paper, we propose a Responsible AI (RAI) methodologydesigned to guide the data collection with a set of metrics foran in-depth, iterative analysis of the human factors influenc-ing the quality and reliability of the data they generate. Themethodology consists of a set of metrics for a systematicanalysis of data that brings transparency in how to interprethuman disagreement and how to validate rater quality as-suming diverse settings. We propose an iterative process tomeasure the reliability and stability of crowdsourced datafrom different perspectives. The repetition of experimentsallows us to perform a comparative analysis across repeti-tions and measure changes both in the annotations and inthe variance and consistency of raters. Due to the variety ofquality metrics we employ, this research can have a strongimpact on the way we measure data quality based on sub-jective human ratings. This further leads to increased diver-sity of AI systems and helps us deal with fairness and ac-countability aspects in data collection. By making the anal-ysis process transparent through the set of metrics, we alsodeal with fairness and accountability aspects in data collec-tion. We validated our methodology on nine existing anno-tation tasks and datasets. We found that our systematic set ofmetrics allows us to draw insights into the human and task-dependent factors that influence the quality of AI datasets.AcknowledgementsWe thank all reviewers for their valuable feedback thathelped improve this submission. We also thank all authorswho shared their collected datasets to help us validate ourproposed methodology.",
                    "reference": "[1] Oktie Inel, Tobias Draws, and Lora Aroyo. 2023. Collect, measure, repeat: Reliability factors for responsible AI data collection. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing. Retrieved from https://ojs.aaai.org/index.php/HCOMP/article/download/27547/27320"
                }
            ]
        },
        {
            "paper_title": "\u201cIt is currently hodgepodge\u201d: Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values",
            "authors": "RA Varanasi, N Goyal",
            "publication_info": "Proceedings of the 2023 CHI Conference on \u2026 - dl.acm.org",
            "paper_url": "https://dl.acm.org/doi/pdf/10.1145/3544548.3580903",
            "chunks": [
                {
                    "id": "68c04c5d-bb16-4920-a54f-536521d9b53f",
                    "text": "Recently, the AI/ML research community has indicated an urgent need to establish Responsible AI (RAI) values and practices as part of the AI/ML lifecycle. Several organizations and communities are responding to this call by sharing RAI guidelines. However, there are gaps in awareness, deliberation, and execution of such practices for multi-disciplinary ML practitioners. This work contributes to the discussion by unpacking co-production challenges faced by practitioners as they align their RAI values. We interviewed 23 individuals, across 10 organizations, tasked to ship AI/ML based products while upholding RAI norms and found that both top-down and bottom-up institutional structures create burden for diferent roles preventing them from upholding RAI values, a challenge that is further exacerbated when executing conficted values. We share multiple value levers used as strategies by the practitioners to re-solve their challenges. We end our paper with recommendations for inclusive and equitable RAI value-practices, creating supportive organizational structures and opportunities to further aid practi-tioners. CCS CONCEPTS \u2022 Human-centered computing \u2192 Empirical studies in HCI. KEYWORDS Responsible AI, RAI, ethical AI, value levers, co-production, collab-oration, XAI, FAT, fairness, transparency, accountability, explain-ability INTRODUCTION 1 In November 2021, the UN Educational, Scientifc, and Cultural Or-ganization (UNESCO) signed a historic agreement outlining shared values needed to ensure the development of Responsible Artifcial  Intelligence (RAI) [99]. RAI is an umbrella term that comprises dif-ferent human values, principles, and actions to develop AI ethically and responsibly [3, 21, 37, 82]. Through UNESCO\u2019s agreement, for the frst time, 193 countries have standardized recommendations on the ethics of AI. While unprecedented, this agreement is just one of several eforts providing recommendations on diferent RAI values to be implemented within AI/ML systems [40, 101, 102]. In response, several industry organizations have begun to im-plement the recommendations, creating cross-functional RAI in-stitutional  structures  and  activities  that  enable  practitioners  to engage with the RAI values. For instance, several big-tech com-panies are implementing common RAI values, such as Fairness, Transparency, Accountability, and Privacy, as part of their RAI ini-tiatives [9, 29, 41, 51, 74]. However, such RAI values have minimal overlap with the values prescribed by UNESCO\u2019s framework that promotes non-malefcence, diversity, inclusion, and harmony [100]. Scholars have attributed the lack of overlap to diferent business and institutional contexts involved in developing AI/ML  systems [50, 56]. Subsequently, it is essential to understand these contexts by engaging with practitioners across multiple roles who come together to co-produce and enact such RAI values. Co-production is an iterative process through which organizations produce collec-tive knowledge [58]. During co-production, individual practitioners may hold certain values (e.g., social justice), yet their teams might prioritize other values. Rakova et al. [82] hints at potential chal-lenges that can arise due to such mismatches in RAI values. Our study builds on this critical gap by giving a detailed analysis of those challenges and strategies (if any) devised to overcome such strains as practitioners co-produce AI/ML systems. We interviewed 23 practitioners across a variety of roles to under-stand their RAI value practices and challenges. Our fndings show that institutional structures around RAI value co-production con-tributed to key challenges for the practitioners. We also discovered multiple tensions that arose between roles and organizations during prioritization, deliberation, and implementation. Interestingly, we also observed development of ten diferent RAI value levers [91, 92]. These are creative activities meant to engage individuals in value conversations that help reduce value tensions. In the remainder of the paper, we frst discuss related work about collective values in Responsible AI from an HCI perspective and outline our research questions. We then present the research methodology and results of the study. We conclude with a discussion of our contributions in improving RAI co-production practices of AI practitioners. Overall, this work makes several contributions. First, we describe the expe-riences and the organizational environment within which AI/ML practitioners co-produce RAI values. Second, we illustrate multiple challenges faced by AI practitioners owing to diferent organiza-tional structures, resulting in several tensions in co-production. Third, we unpack ten RAI value levers as strategies to overcome challenges and map them on to the RAI values. Lastly, we provide essential strategies at the diferent levels (individual, and organiza-tional) to better facilitate and sustain RAI value co-production.  disparate AI/ML practitioners to come together, share, and enact on key values [82]. Our study expands RAI discipline by surfacing on-ground challenges of diverse AI/ML practitioners attempting to engage in shared RAI responsibilities, such as collaborative value discourse and implementation. In the next section, we further un-pack the notion of values, examine their roots in HCI and their current role in RAI. 2  RELATED WORK 2.1  The Field of Responsible AI In the last decade, Responsible AI (RAI) has grown into an overarch-ing feld that aims to make AI/ML more accountable to its outcomes [1, 3, 20]. One of the feld\u2019s roots lies in Ethical AI, where critical engagement with ethical values in the otherwise traditional AI/ML feld have been encouraged [2, 39, 63, 104]. Example studies include engagement with core ethical values to provide nuance in technical AI/ML discourse [47], translating ethical values into implementa-tion scenarios [39, 49], and AI/ML guidelines [45, 47, 52, 60, 103], and seminal studies that brought critical ethical problems to the forefront [4, 24, 76, 113]. RAI has also drawn its inspiration from AI for Social Good (AI4SG [89]) research to study human values more broadly, going beyond \u201cethical\u201d values. AI4SG helped the RAI feld to translate such values embedded in AI/ML systems into positive community outcomes [6] by eliciting specifc values (e.g., solidarity [31]), developing meth-ods (e.g., capabilities approach [11]), and producing representations (e.g., explanations [79]) that strongly align with community goals (e.g., the UN Sustainable Development Goals [37]). For example, studies have explicitly engaged with underserved communities to examine the impact of the embedded values within AI/ML systems in their lives (e.g., in agriculture [70], health [53, 107], and education [14] domains). More recent studies have shed light on how certain practitioners\u2019 (e.g., data and crowd workers) practices, contribu-tions, and values are often ignored while developing the AI/ML systems [85, 97, 105]. Some have looked at diferent values (e.g., fairness) that are often ignored by the discriminatory algorithms [16, 46, 61]. More recent work at the intersection of these two by Goyal et al. [42] has also highlighted the impact of data workers of marginalized communities on the AI/ML algorithms to highlight complexity when building for RAI values like equity. Lastly, RAI has also drawn motivation from recent movements associated with specifc value(s). One such movement is around the value of explainability (or explainable AI) that arose from the need  to  make  AI/ML  systems  more  accountable  and  trustwor-thy [5, 22, 25]. A similar movement within the RAI\u2019s purview fo-cused on FATE values (Fairness, Accountability, Transparency, and Ethics/Explainability) [18, 66, 75, 93]. While both movements have challenged the notion of universal applicability of RAI values, our study illustrates how these challenges do indeed appear in practice and the strategies used by practitioners on the ground to resolve these challenges. Taken together, RAI has emerged as an umbrella term that encap-sulates the above movements, encouraging critical value discourses to produce a positive impact. At the same time, departing from previous movements that focused on specifc issues within AI/ML practices, RAI takes a broad institutional approach that promotes  2.2  Collective Values in Responsible AI: HCI perspectives Science & Technology Studies feld has long examined how values are embedded in technology systems in various social and political contexts [67, 96, 109]. In recent years, studies within HCI have built on this foundation to bring a critical lens into the development of technology. Initial studies conducted by Nissenbaum and col-leagues argued against the previously held belief that technology is \u201cvalue-neutral\u201d, showcasing how practitioners embed specifc values through their deliberate design decisions [30, 33]. Value-sensitive Design (VSD) by Friedman et al. [36] was another step in this direction. It has been used as a refective lens to explore tech-nological afordances (through conceptual and empirical inquiry) as well as an action lens to create technological solutions (techno-logical inquiry) [34, 48]. While VSD\u2019s core philosophy remained the same, it has been extended, often in response to its criticisms [54, 68]. A criticism relevant to this study is practitioners\u2019 ease in apply-ing VSD in the industry contexts [90]. VSD is perceived to have a relatively long turnaround time, often requiring specialists for implementation. To overcome these challenges, Shilton proposed \u2018value levers\u2019, a low-cost entry point for value-oriented conversa-tions while building technology artifacts in the organization [90, 91]. Value levers are open-ended activities that engage participants in value-oriented discourses to develop common ground. With cre-ative representations, value levers can transform slow and cumber-some value conversations into creative and fruitful engagements [90]. While previous studies have applied and shaped the notion of value levers in a very specifc set of contexts, such as showcasing how designers employ them in their practices [90], this work shows a broader utility of value levers among a diverse set of practitioners while navigating the complex space of RAI value discourse. Within AI/ML research, initial exploration of values were still primarily computational in nature, such as performance [15], gen-eralizability [55], and efciency [27]. With the advent of HCI and critical studies focusing on discriminatory algorithms [10] and re-sponsible AI, the discussions shifted to much broader values, such as societal and ethical values, within the AI/ML feld [9]. These studies focused on exposing inherent biases in the models due to ab-sence of substantive social and ethical values. For instance, Burrell [13] demonstrated how complex ML models have inherent inter-pretability issues stemming from a lack of transparency about how predictions were achieved. Another set of studies by Eubanks [28] and Noble [78] scrutinized several algorithms governing the digital infrastructures employed in our daily lives to expose discriminatory behaviors within diferent situations, especially in the context of fairness, against marginalized populations. In a similar vein, sev-eral studies have explored individual values that they felt were critical for models, such as fairness [16, 32, 75], explainability [22], non-malfeasance [60, 77], and justice [8, 46, 61], refecting societal norms. A common underlying factor among several of these studies was that they focused on individual values enacted in their own spaces. Recently, however, a few studies have adopted contrasting perspectives which argue that values do not exist in isolation, but often occupy overlapping and contested spaces [36, 114]. Our study aims to provide much-needed deeper insights within this complex space by showing how practitioners engage with and prioritize multiple values in a contested space. Another value dimension explored is \u2013 \u201cwhose values should be considered while producing AI/ML algorithms [12]?\u201d Most studies have engaged with end-users values, lending a critical lens to the deployed models and their implications on society [64, 84, 94, 110]. These studies challenged whether developing fair algorithms should be primarily a technical task without considering end-users\u2019 values [50, 83]. Subsequently, researchers leveraged action research (e.g., participatory approaches [19]) to design toolkits, frameworks, and guidelines that accommodated end-user values in producing ML models [62, 65, 88]. A more relevant set of studies have recognized the importance of understanding the values that diferent practitioner roles em-bedded while producing responsible algorithms [81, 86, 112]. Such practitioner-focused studies are critical in understanding \u201chow\u201d and \u201cwhy\u201d particular values are embedded in AI/ML models early on in the life cycle. However, these studies have explored particular practitioners\u2019 values in silos, leaving much to be learned about their collective value deliberations. A nascent group of studies has answered this call. For example Madaio et al. [71] focused on con-trolled settings, where specifc practitioners\u2019 values could co-design a fairness checklist as one of their RAI values. Jakesch et al. [56] ex-plored a broader set of values of practitioners and compared it with end-users in an experimental setting. Another relevant study by Rakova et al. [82] has explored RAI decision-making in an organiza-tional setting, laying a roadmap to get from the current conditions to aspirational RAI practices. Our study contributes to this developing literature in four ways. First, within the context of Responsible AI practices, our study goes beyond the scenario-based, controlled settings, or experimental se-tups by focusing on natural work settings [56, 71], which echoes the sentiment of some of the previous open-ended qualitative studies that were conducted in the organizations [81, 112], but not in the context of Responsible AI practices. Second, we focus on a diversity of stakeholder roles, who are making an explicit efort to recognize and incorporate RAI values, unlike siloed studies previously. Third, we leverage the lens of co-production [58] to study RAI values in natural work settings. Fourth, our study extends [82] by explicitly unpacking the co-production challenges deeply rooted in RAI val-ues. To this end, we answer two research questions: (RQ-1): What challenges do AI/ML practitioners face when co-producing and implementing RAI values ? (RQ-2): In response, what strategies do practitioners use to overcome challenges as they implement RAI values ?  2.3  Co-production as a Lens To answer our research questions, we employed the conceptual framework of co-production proposed by Jasanof [58]. She defned co-production as a symbiotic process in which collective knowledge and innovations produced by knowledge societies are inseparable from the social order that governs society. Jasanof characterized knowledge societies broadly to include both state actors (e.g., gov-ernments) and non-state actors (e.g., corporations, non-profts) that have an enormous impact on the communities they serve. Studying co-production can help scholars visualize the relationship between knowledge and practice. Such a relationship ofers new ways to not only understand how establishments organize or express them-selves but also what they value and how they assume responsibility for their innovations. To operationalize co-production in our study, we invoke three investigation sites, as Jasanof proposed. The frst site of exploration is the institutions containing diferent structures that empower or hinder individuals to co-produce. The second site examines dif-ferent types of discourse that occur as part of the co-production activities. Solving technological problems often involve discourses producing new knowledge and linking such knowledge to practice. The last site of co-production is representations produced both dur-ing co-production to facilitate discourses and after co-production in the form of the end-product. The three sites of the co-production framework are appropriate for understanding the current industry challenges  around  RAI  innovation  for  several  reasons.  Techno-logical corporations developing AI/ML innovations have a robust bi-directional relationship with their end-user communities. More-over, for successful RAI value implementation, practitioners need to leverage the complex structures within their organizations that are invisible to external communities. RAI value implementations occur through strategic discourses and deliberations that translate knowledge to efective execution. Lastly, in the process of RAI value deliberations, individuals co-create representations that further the implementation eforts of RAI. 3  METHODS To answer our research questions, we conducted a qualitative study consisting of 23 interviews with active AI/ML practitioners from 10 diferent organizations that engaged in RAI practices. After receiv-ing internal ethics approval from our organization, we conducted a three month study (April-June, 2022). In this section, we briefy talk about the recruitment methods and participant details. 3.1  Participants Recruitment and Demographics To recruit AI/ML practitioners who actively think and apply RAI values in their day-to-day work, we partnered with a recruitment agency that had strong ties with diferent types of corporate or-ganizations working in the AI/ML space. We provided diverse re-cruitment criteria to the agency based on several factors, including gender, role in the company, organization size, sector, type of AI/ML project, and their involvement in diferent kinds of RAI activities. Using quota sampling technique, the agency advertised and ex-plained the purpose of our study in diverse avenues, such as social media, newsletters, mailing lists, and internal forums of diferent companies. For the participants that responded with interest, the agency arranged a phone call to capture their AI/ML experience, as well as their experience with diferent RAI values. Based on the information, we shortlisted and conducted interviews with 23 AI/ML practitioners who ft the diverse criteria mentioned above. The aforementioned factors were used to prioritize diverse partici-pants with experience working on RAI projects within their team in diferent capacities. For example, while shortlisting, we excluded students working on responsible AI projects as part of their in-ternships and included individuals who were running startup RAI consultancy frms. Out of the 23 practitioners, 10 identifed themselves as women. Participants comprised of product-facing roles, such as UX design-ers, UX researchers, program/product mangers, content & support executives, model-focused roles, such as engineers, data scientists, and governance focused-roles, such as policy advisors and audi-tors. Out of 23 practitioners, all but one participant worked for a U.S. based organization. However, participants were geograph-ically based in both Global North and Global South. Participants also worked in a wide variety of domains, including health, energy, social media, personal apps, fnance and business among other, lending diversity to the captured experiences. Three participants worked for independent organizations that focused exclusively on RAI initiatives and AI governance. twelve participants had a technical background (e.g., HCI, computer-programming), four had business background, two had law background and one each spe-cialized in journalism and ethics. For more details, please refer to Table 1. 3.2  Procedure We conducted semi-structured interviews remotely via video calls. Before the start of the each session, we obtained informed consent from the participants. We also familiarized participants with the objective of the study and explicitly mentioned the voluntary na-ture of the research. The interviews lasted between 40 minutes and 2 hours (avg.= 65 mins.) and were conducted in English. Interviews were recorded, if participants provided consent. Our interview ques-tions covered diferent co-production practices. First, in order to understand diferent co-production challenges (RQ-1), we asked questions about (1) how practitioners faced challenges when shar-ing RAI values across roles (e.g., \u201cCan you describe a situation when you encountered problems in sharing your values?\u201d ) and (2) how practitioners faced challenges when collaborating with diferent stakeholders (e.g., \u201cWhat challenges did you face in your collabo-ration to arrive at shared common responsible values?\u201d). Second, to understand diferent co-production strategies (RQ-2) we asked (3) how practitioners handled conficts (e.g., \u201cCan you give an example where you resisted opposing peers\u2019 values?\u201d) and (4) how practition-ers sought assistance to achieve the alignment in RAI values (e.g., \u201cWhat was the most common strategy you took to resolve the con-fict?\u201d). To invoke conversations around RAI values, we used a list of RAI values prepared by Jakesch et al. [56] as an anchor to our conversations. After frst few rounds of interviews, we revised the interview script to ask newer questions that provided deeper understanding to our research questions. We stopped our inter-views once we reached a theoretical saturation within our data. We compensated participants with a 75$ gift-voucher for participation.  3.3  Data collection and Analysis Out of 23 participants, only three denied permission to record audio. We relied on extensive notes for these users. Overall 25.5 hours of audio-recorded interviews (transcribed verbatim) and several pages of interview notes were captured. We validated accuracy of notes with the respective participants. Subsequently, we engaged in thematic analysis using the NVivo tool. We started the analysis by undertaking multiple passes of our transcribed data to understand the breadth of the interviewee\u2019s accounts. During this stage, we also started creating memos. Subsequently, we conducted open-coding on the transcribed data while avoiding any preconceived notions, presupposed codes, or theoretical assumptions, resulting in 72 codes. We fnalized our codes through several iterations of merg-ing the overlapping codes and discarding the duplicate ones. To establish validity and to reduce bias in our coding process, all the au-thors were involved in prolonged engagement over multiple weeks. Important disagreements were resolved through peer-debriefng [17]. The resultant codebook consisted of 54 codes. Example codes included, \u2018social factors\u2019, \u2018prior experience\u2019, \u2018enablers\u2019, \u2018RAI push-back\u2019. As a fnal step, we used an abductive approach [98] to further map, categorize, and structure the codes under appropriate themes. To achieve this, we used three key instruments of co-production framework developed by [58], namely, making institutions, making discourses, and making representations. Examples of the resultant themes based on the co-production instruments included \u2018value ambiguity\u2019, \u2018exploration rigidity\u2019, \u2018value conficts\u2019, and \u2018value lever strategies\u2019. Based on the instruments of co-production framework, we have present our resultant fndings in the next section. 4  FINDINGS Our overall fndings are divided based on diferent sites of explo-ration proposed by Jasanof [58]. The frst section answers RQ-1 by exploring several institutional  challenges that hinder the co-production of RAI values among practitioners (Section 4.1). The second section explores subsequent knock-on challenges that unsta-ble institutional structures create in co-production discourses (Sec-tion 4.2). The last section answers the RQ-2 by presenting carefully thought out representations that overcome challenges deliberation and execution of RAI values using the concept of value levers [90]. (Section 4.3). 4.1  RAI Value Challenges within the Institutional Structures Institutional structures are essential in enabling co-production of new knowledge [58]. It is these structures that facilitate relation-ships for deliberation, standardize democratic methods, and validate safety of new technological systems before information is dissemi-nated into the society. We found two key institutional structures that facilitated deliberation around RAI values within AI/ML com-panies. These structures brought about diferent RAI challenges. Bottom-up: Burdened Vigilantes. The First type of structures were bottom-up. Within these structures, RAI conversations de-veloped through RAI value sharing in the lower echelons of or-ganizations, often within AI/ML practitioners\u2019 own teams. In our interviews, eight practitioners, namely a UX researcher, designer, Table 1: Practitioners\u2019 Demographic Details. content designer, and program manager from two mid-size organi-zations and two large-size organizations experienced or initiated bottom-up practices that engaged with RAI values. One of the en-ablers for such bottom-up innovation was individuals\u2019 sense of responsibility towards producing AI/ML models that did not con-tribute to any harm in society. A few other practitioners paid close attention to \u2018social climate\u2019 (e.g., LGBTQ month, hate speech inci-dents) to elicit particular RAI values. For instance, P08, a program manager in a large-scale technology company took responsibility for RAI practices in their team but soon started supporting team members to come together and share RAI values: \u201cWe cater to projects that are very self-determined, very bottom-up aligned with our values and priorities within the organization . . . These are what I call responsible innovation vigilantes around the company. I also started that way but have grown into something more than that. You\u2019ll see this at the product or research team level, where somebody will speak up and say, \u2018Hey, I want to be responsible for these RAI values, make this my job and fnd solutions\u2019. So you start to see individuals in diferent pockets of the company popping up to do RAI stuf.\u201d A key challenge with such bottom-up structures was that the responsibility of engaging with RAI value conversations implicitly fell on a few individual \u201cvigilantes\u201d. They had to become stalwarts of particular RAI values and take out substantial time out of their work to encourage and convince their teams to engage with RAI values. They also actively seeked out RAI programs available within and outside their organization. When such RAI programs were not available, individuals took it upon themselves to create engage-ment opportunities with other members within the organization. These bottom-up structures were useful in breaking the norms of \u201cboundary-work\u201d that are often set within AI and similar technical organizational work where only engineers and high-ofcials in the company maintain control [38]. It allowed non-technical roles such as user experience researchers, product managers, analysts, and content designers to a create safe space and lead the RAI eforts. While such eforts early on in AI/ML lifecycle minimized the po-tential harm of their ML models or AI products, it often happened at the cost of their overworked jobs. Bottom-up: Burdened with Educational Eforts. Apart from self-motivated vigilantes, the burden of RAI value exploration also fell on a few practitioners who were implicitly curious about RAI  innovation. Unlike the vigilantes, these participants were pushed to become the face of their team\u2019s RAI initiatives since there was no-one else who would. P14, a product manager working for two years at a medium size company within the energy sector shared, \u201cWhen I came in to this team, nobody really believed in it [RAI] or they really didn\u2019t think it [RAI] was important. I was personally interested so I was reading about some of these principles . . . When there was an indication of a future compliance requirement, people didn\u2019t want to take up this additional work . . . somebody had to do it.\u201d Similarly P05, a technical manager leading their team on data collection for the development of knowledge graphs, revealed how they were considered \u201cthe face of privacy\u201d for the team. Therefore, P05 was expected to foster awareness and common understanding among internal stakeholders and external partners and ensure they strove towards similar RAI standards and appreciated data-hygiene practices (e.g., data cleaning and de-identifcation). Practitioners like P14 and P05 had to assume the responsibility of fguring out the RAI space by presenting their team\u2019s needs and asking forma-tive questions even when their objectives around RAI were often not clear, such as which values to consider (e.g., \u201cprivacy or trans-parency?\u201d), what certain values mean (e.g., \u201cwhat trustworthiness as an RAI value should mean to the model and the team\u201d), how to operationalize specifc values (e.g., \u201cHow does trustworthiness apply to rule-based models? What kind of RAI values to invoke while col-lecting data?\u201d, and how to interpret outcomes and map them on to their team\u2019s objectives. Participants (n=5) shared how leading such RAI initiatives bur-dened their professional lives in various ways. Multiple participants reported that the RAI feld was still in its infancy and taking up responsibilities in such conditions meant that their eforts were not deemed a priority or sometimes even ofcially recognized as AI/ML work [71]. Consequently, the practitioners possessed lim-ited understanding of the direction to take to educate their team, convert their eforts into tangible outcomes, and efectively align their educational outcomes to the team\u2019s objectives. P13, an RAI enthusiast and an engineer at a large-scale social media company shared how their RAI efort seemed like an endless efort, \u201cAt this point, I probably know more about what things (RAI values) we don\u2019t want in it (model) than what we do want in it . . . It\u2019s like I am just learning and fguring out what\u2019s missing as I take every step . . . It is unclear which [RAI] direction will beneft the the team.\u201d  More-over, the burden of educating multiple team members was on the Figure 1: A summary of co-production activities mapped to Jasanof [58]\u2019s co-production sites, along with the themes, RAI values invoked, and key fndings and takeaways. shoulders of a very few practitioners tantamounting to substantial pressure. Metcalf et al. [72] in their paper around technology ethics put forward the term \u2018ethic owners\u2019. This role shares similarity with the bottom-up vigilantes and the educators as they both are mo-tivated  and  self-aware  practitioners,  invested  in  foregrounding human values by providing awareness and active assistance while institutionalizing the processes. However, Metcalf\u2019s ethic owners\u2019 responsibilities were clearly defned. Their tasks of working with teams or higher management were perceived as visible, prioritized work for which they would be credited for career growth or other-wise. While bottom-up informal roles in our research performed similar set of tasks, their eforts were seen as tangential, \u2018admin-istrative\u2019, and underappreciated. It is not just that there was an additional burden, but even the outcomes of taking on this addi-tional burden for bottom-up informal roles were dissimilar to the ethic owners. Taking up informal RAI work was more problematic when the requirements in the later stages of ML were unprompted,  compelling practitioners to focus on these eforts at expense of their own work. In our fndings, one form of the need came as academic criticism or critique around particular values that were seen concerning a particular product (e.g., \u201cwhat steps are you taking to ensure that your model is equitable?\u201d). Another form of need came from end-users\u2019 behavior who experienced the models through a particular product. P20, a user experience researcher working with deep learn-ing models in fnance, shared how user feedback brought about new RAI needs that became their responsibility: \u201cOnce the users use our product and we see the feedback, it makes us realize, \u2018oh, people are sometimes using this feature in an unintended way that might in turn impact the way we are going about certain values, say trans-parency\u2019 . . . . Initially we were like, \u2018We should strive for transparency by adding a lot of explanations around how our model gave a particular output\u2019. Later we real-ized too many explanations [for transparency] fostered inappropriate trust over the feature. . . UXR represents user needs so its on me to update the team on the issues and suggest improvements. A few practitioners (n=2) also mentioned how the constant jug-gling between their own role-based work and the unpredictability of the RAI work pushed them to give-up the RAI responsibilities all-together. Top-down: Rigidity in Open-discovery. While the burden of ownership and execution of RAI values in bottom-up structures were on small group of individuals, they had the fexibility to choose RAI values that were contextual and mattered to their team\u2019s ML models or projects. On the contrary, we found that top-down in-stitutional structures limited the teams\u2019 engagement to \u201ckey\u201d RAI values that impacted organization\u2019s core business values. For in-stance, P15\u2019s company had trust as a key value baked into their business, requiring P15 to focus on RAI values that directly reduced specifc model\u2019s biases, thereby increasing the company\u2019s trust among their users. Consequently, several RAI practitioners had to skip RAI value exploration and sharing. Instead they directly implemented predetermined RAI values by the management just before the deployment. P06, an engineer at a large tech company working in conversational analysis models, described this lack of choice: \u201cTo be honest, I imagine lots of the conversations, around the sort of values that need to go into the model, hap-pened above my pay grade.  By the time the project landed on my desk to execute, the ethics of it was cleared and we had specifc values that we were implementing.\u201d Public-oriented legal issues and ethical failures, especially when launching innovative models (e.g., transformer networks), also de-termined the RAI values that were prioritized and the subsequent formal RAI structures that were established by the organizations. P19, a policy director at a RAI consultation frm facilitating such structures, shared how such impromptu structures were quite com-mon in response to ever-changing laws around AI governance: \u201cEven if you\u2019re conservative, the current climate is such that it\u2019s going to be a year or two max from now, where you will start to have an established, robust regulatory regime for several of these (RAI) issues. So a good way to be prepared is to create the [RAI] programs in whatever capacity that enables companies to comply with the new regulations, even if they are changing because if you have companies doing Responsible AI programs, it eventually gets compliance and executive buy-in. \u201d  trust or privacy) due to directed and concerted focus. It allowed for organization-wide impact since the \u201cbuy-in\u201d already existed [71]. Top-down: Under-developed Centralized Support. However, in the case of less clearly defned values (e.g., non-malefcence or safety) we observed a limited scope for nuance and despite best eforts, the centralized concerted direction does not always pan out as intended. Further, while laws continue to evolve in this space, participants felt that pre-mediated RAI values might not longitudinally satisfy the growing complexity of the ML models being implemented (e.g., multimodal models). Hence, while it might seem that setting up a centralized top-down approach might be efcient, the current execution leaves much to be desired. In fact, based on data from over half the participants, we found that fve top-down structured companies integrated lesser known RAI values into their workfows in multiple ways without establishing a centralized workfow. Those who did establish centralize workfows created consulting teams to advise on RAI Practices (similar to ethic owners Metcalf et al. [72]). However, these top-down centralized RAI consulting teams were not always set up to succeed. As is the nature of consulting, people did not always know the point-of-contact or when and how to reach out. The consulting teams needed to also consider opportunities to advertise about themselves and engagement mechanisms,which was difcult due to the lack of context and nuance around the teams\u2019 projects. Consequently, it was difcult for such teams to generate organic interest, unless the teams were already aware of their RAI requirements and knew a point of contact. P10, a manager who facilitated one such top-down RAI program in a large-scale technology company for AI/ML teams, described lack of fxed ways in which teams engaged with them on RAI values, making it a difcult engagement: \u201cWe have a bunch of internal Web pages that point you in all diferent types of directions. We don\u2019t have a singular voice that the individuals can speak with . . . . Its currently hodgepodge. Some teams come to us willingly. They had already thought about some harms that could occur. They say, \u2018Here\u2019s our list of harms, here\u2019s some ideas on what we want to do\u2019 They\u2019d already done pre-work and are looking for some feedback. Other teams come to us because they\u2019ve been told to. . . . They haven\u2019t thought much about RAI and need longer conversations . . . Other teams were told to go track down an individual or team because they are doing ML stuf that will require RAI assistance, but they don\u2019t know about us\u201d Instead of investing in RAI structures to comply with diferent regulations in Tech Ethics such as codes of ethics, statements of principle, checklists, and ethics training as meaningful, organiza-tions perceive them as instruments of risk that they have to mitigate [72]. In line with the previous literature [7, 44], our fndings indi-cate that practitioners often fnd false solace in such structures as they run the risk of being superfcial and relatively inefective in making structures and practices accountable and efective in their organizations. However, adding nuance to this argument in the case of RAI practices, we found that practitioners more broadly devoted time and energy to follow established and prioritized values (e.g.,  4.2  Challenges within RAI value Discourses Fruitful co-production requires well-established institutional struc-tures that can empower stakeholders to engage in stable democratic discourses with an aim of knowledge production [58]. In the pre-vious section, we uncovered diferent structural challenges at the institutional level that contributed to knock-on efects, creating further challenges for practitioners during the co-production and implementation of RAI values. Discourse: Insufcient RAI Knowledge. A key challenge that many practitioners experienced in co-producing RAI values in team was the difculty in engaging deeply with new and unfamiliar RAI values deemed important by the team members. P07, a policy advi-sor in a large technology company, who regularly interacted with those who implemented RAI values, described the \u201csuperfcial en-gagement\u201d with values as an act of \u201cinefective moralizing\u201d, wherein practitioners struggled to develop deeper interpretations of the team\u2019s shared values and contextualize them in relation to the ML models they were developing. P07 mentioned several key critical thinking questions that AI/ML practitioners did not deliberate within their teams, such as \u201cIs this RAI value applicable to our product?\u201d, \u201chow does this value translate in diverse use cases?\u201d, or \u201cshould this value be enabled through the product?\u201d The need for deeper engagement becomes particularly important in a high-stakes situation, such as healthcare, where cer-tain conditions have an unequal impact on particular demographics. P12 experienced this complexity while overseeing the development of a ML model focused on health recommendations: \u201cSo a lot of models are geared towards ensuring that we have we are predictive about a health event and that al-most always depends on diferent clinical conditions. For example, certain ethnic groups can have more proclivity to certain health risks. So, if your model is learning cor-rectly, it should make positive outcomes for this group more than the other groups. Now, if you blindly apply RAI values without thinking deeply about the context, it might seem that the model is biased against this group when in reality these group of people are just more likely to have this condition, which is a correct conclusion, not a biased one.\u201d Such deeper analysis requires hands-on practice and contextual training in the feld and formal RAI education. In our fndings, top-down structures were only efective to fll the gap for key values that aligned with the company\u2019s vision, leaving a much needed requirement for contextual, high-quality RAI education for more emergent RAI values that could be modularized for specifc teams. P02, a content designer for a large health technology company shared how this gap looked like for their team that was designing content for a machine translation team, \u201cOne thing that would have been benefcial is if I or my team could somehow get more insights on how to think about trustworthiness in the context of the con-tent produced by our machine translation model and probably evaluate it . . . Often time, I just go to someone who is known to have done some work in this [RAI] and say, \u2018Hey, we want to design and publish the content for the model like in a month from now, what is bare minimum we could do from [RAI] principles point of view?\u2019. . . Sometimes it\u2019s never-ending because they say I have not thought about this at all and that it is going to take a month or maybe much more longer to get these principles implemented.\u201d Participants like P02 had no alternative but to reach out to their bottom-up structures to seek assistance, discuss, and reduce gaps in their RAI knowledge. On occasions, such avenues of discussion were non-conclusive. Prior literature in AI/ML and organization studies have shown how such unequal dependence on bottom-up  structures over top-down in deliberation can contribute to tensions, and in turn propose an \u201copen, federated system\u201d linking diferent actors, resources, and institutions to provide a community based support [82, 87]. Discourse: Deprioritized Unfamiliar & Abstract Values. Natu-rally, practitioners tried to solve the superfcial engagement prob-lem by de-prioritizing values that they found unfamiliar. In our study, most practitioners (n=18) said that they were familiar and comfortable talking about RAI values like privacy and security as they were already \u201cestablished\u201d and had \u201cmatured over time\u201d. They sharply contrasted this perceived familiarity with other RAI val-ues like explainability  and robustness. The familiar values were well backed with stable top-down structures and dedicated teams, such as compliance departments and dedicate RAI personnel , mak-ing it easy for practitioners to develop mental models of deeper engagement. P20 shared their experience in this regard in their organization: \u201cThe ideal situation would be like, \u2018Oh, I have certain RAI principles that I want to make sure our product has or addresses\u2019. In reality not all the principles are thought out the same way and applied in the frst go. It usually happens in layers. First and foremost, people will look at privacy because that\u2019s super established, which means everyone knows about it, they already have done probably some work around it, so its easy to implement. And then after that, they\u2019re like, \u2018Okay, now let\u2019s look at fairness or explainability\u2019 . . . We usually have to be quick with turnaround like one or two months. Its nice to bring up values that are new but naturally they also require familiarizing and implementation efort within the team and people see that\u201d Other practitioners (n=3) also followed a similar de-prioritization process for RAI values that they felt were abstract and did not have a measurement baseline (benchmarks) as opposed RAI values that could be easily measured quantitatively against a baseline. An ex-ample observed in this category was the contrast between RAI values like interpretability, which had concrete implementation techniques and measurements (e.g., LIME) and non-malefcence, which did not have a clear implementation technique or measure-ments. Similarly, practitioners (n=2) who went out of their way to understand and suggest new interpretability techniques for model debugging techniques (e.g., Integrated Gradients, SHAP) found it disempowering when their team members often negotiated for easier and computationally cheaper values like accuracy (e.g., P/E ratio) for implementation. Discourse: Value Interpretation Tensions. Even in situations, when diferent practitioners took a similar balanced approach to prioritization, tensions emerged as diferent roles interpreted and contextualized the RAI values diferently during the value delib-erations. We found these tensions occurring within practitioners when diferent practitioners defned RAI values (e.g., equity) and mapped them to RAI features and metric (e.g., skin tone) diferently. P18, a senior data scientist leading an AI team in a non-proft insti-tute, shared one such similar tension among their team members working on the same project, \u201cProbably half of my colleagues do believe that there is a cultural, and historical set of RAI values that can be applied to all the products organization wide. Other half are vehemently opposed to that concept and say that [RAI] values are always model and project dependent. So if you are talking about our long-term goal to establish a set of RAI principles, whose perspectives should be considered?. . . This is an uneasy space that needs careful navigation.\u201d While deliberations might occur between team-members, they might occur within a practitioner, or between the team and the end-consumers of the product/service. Latter two usually surfaced with user-facing roles, e.g., Product Managers or UX Researchers. These roles have the responsibility to understand, internalize, and embody end-user values in addition to their own values. Overall, we found that practitioners in these roles had to invest more efort to tease out their own values from that of the end-users. P04 was a user experience researcher working on interfacing a large language model for natural conversations with users. While P04 was inter-ested in eliciting better insights from the model\u2019s behavior issues (i.e. interpretability [22]), end-users were interested in a simplifed understanding of the model\u2019s opaque behavior (i.e. comprehensibil-ity [22]). A UX Researcher is, however, expected to be the voice of the user in the process. Consequently, they had the constant burden to elicit both sets of values appropriately. Another set of tensions also occurred between practitioners and end-users. P22, an analyst in a fnancial frm, described how ML practitioners perceived RAI values to be mutable and negotiable, al-lowing them to implement a particular RAI value in stages instead of all at once. Such a process allowed P22 (and three other participants who reported similar narratives) to build the required experience and embed the value in the ML model or AI product. However, end-users expected these embedded RAI values as absolute and non-negotiable and not on a \u201csliding spectrum\u201d because they are \u201cthey are often the list of ignored rights\u201d, leading to practitioner-user RAI tensions. Our fndings show that tensions that arose from non-uniform RAI value knowledge and subsequent disparate value interpreta-tions were unproductive and a signifcant obstacle in the overall co-production process of RAI values. This can be attributed to a nascent RAI feld that has given rise to new forms of values (e.g., ex-plainability, interpretability) whose defnitions and contexts which keep changing. This is in contrast with prior value studies in HCI studies where the tensions and conficts around relatively more established values (e.g., privacy) do not occur until the implemen-tation stage [26, 35, 95]. Our fndings show that the majority of value tensions occur much earlier in the value interpretation stage, often contributing to the abandonment of the value discussions altogether. Implementation: RAI Values and Conficts within. Implemen-tation of RAI values was also not a straight forward process as implementing certain RAI values created confict with other RAI val-ues. For instance, P1, an engineer working on classifcation models in VR environments shared how their decision to improve accuracy by excluding instances of objects with sensitive cultural meanings (e.g., objects with LGBTQ references) also had direct repercussions  on the diversity and fairness of the model. Implementing RAI values also created cascading dependencies on the inclusion of other RAI values. For instance, P16, a program manager working as an RAI facilitator for a big tech company, shared the issues team members experienced around cascading RAI values: \u201cOne common issue I see is with teams that are dealing with model Fairness issues. Most often the solution for them is to improve their datasets or sometimes even collect new forms of demographic data to retrain their model and that opens up another rabbit hole around privacy that the team now has to navigate through and ensure that their data adhere to our privacy standards. More often than not, teams don\u2019t even realize they are creating a new issue while trying to solve their existing problem. \u201d Implementation challenges also occurred when organization\u2019 business values were in tension with those of external clients. In such cases, the team\u2019s commitment to engage with RAI was at odds with clients\u2019 business priorities. P02, a technical program manager for a service company that developed ML models for clients in the energy sector, had a similar issue when their team was building a model for street light automation. After P02\u2019s team received the data and started developing the model, they pushed for the value of safety. However, it was at odds with the company\u2019s value of efciency, \u201cWe should prioritize model optimization in those areas where there are higher crime rates . . . we don\u2019t want blackouts, right? . . . Their argument was if there was a very high crime rate, such areas will also have high rate of purposefully damaging the lighting infrastructure. Prioritizing service to such areas will only create high amounts of backlogs as people will just vandalize it again. . . . So they just have diferent priorities. After that, our team just stopped following it up as it went into the backlog. \u201d P02\u2019s team gave up RAI value deliberation and implementation altogether after their clients either deprioritized their multiple at-tempts to make them RAI ready or took an extremely long time to approve their requests. Implementation: Unexpected Late-stage Value Changes. An-other challenge practitioners faced was encountering new RAI values during late stages of implementation. These values were not initially shortlisted. Instead, they were brought out later and sometimes championed by a very vocal practitioner, who felt deeply about it. Such late-stage RAI values also became a part of the discus-sion when practitioners in the team uncovered last-moment issues (e.g., bias) during implementation that signifcantly impacted the model. Several participants (n=3) shared how such late-stage RAI values decreased the productivity of their overall RAI discourse and implementation eforts, leading to a negative experience. While such last-minute changes are not welcomed, P12, an engineer shared how it also gives an opportunity to the developers to ship a bet-ter product before any harm might have been done. This tension between potentially better outcomes and slower implementation was visible in how the implementation eforts and timelines were impacted [71]. Such values also disrupted a planned implementation by tak-ing the spotlight and pushing the team in navigating the com-pany\u2019s non-standardized approvals, thereby signifcantly altering the project timeline. For example, P23, an ML engineer shared how when they received issues around fairness from other stakehold-ers, it meant \u201csubstantial changes to the model from the ground-up, because most of the time, issues with fairness stem from the data\u201d. It meant revisiting the data and redoing data collection or further debugging to remove the issues. Moreover, when new and untested RAI values assumed prominence (e.g., interpretability), more time and efort was required from the practitioners during implemen-tation. RAI facilitators are essential in easing the tension in such situations by engaging in back-and-forth conversations with the teams to reduce the efort, streamline the process, and help practi-tioners appreciate the eventual consequences of implementing the RAI values. Implementation: Perceived Misuse of RAI values. Lastly, we also observed tensions between individual eforts in implementing RAI values and their organization\u2019s use of such eforts for the overall business purposes. For instance, P15, research director of a large-scale technology company overseeing research in large language models, shared how he was actively supporting a few teams in his company to co-produce and embed explainability into their models. However, he also expressed his concern about how companies could misrepresent such embedded RAI values, \u201cI worry that explainable AI is largely an exercise in persuasion. \u2018This is why you should trust our software\u2019 rather than \u2018This is why our software is trustworthy\u2019 . . . I\u2019m not saying everybody who does explainable AI is doing that kind of propaganda work, but it\u2019s a risk. Why do we want our AI to be explainable? Well, we\u2019d like people to accept it and use it . . . Explainability part is ethically complicated . . . even for explainability for the practitioners the company wants it to be explainable, transparent, reliable, all those things as a means to an end. And the end is \u2018please like our model, please buy our software\u2019\u201d  4.3  Representational Strategies to Mitigate RAI Challenges In response to the challenges mentioned in the aforementioned sections, we saw several strategies used by the practitioners to overcome the limitations in RAI value co-production. To present the strategies, we use a form of representations called values levers [90], a set of activities that facilitate opportunities to share and collaborate around values. We show how diferent practitioners use value levers to build upon current RAI institutional structures and make their RAI co-production manageable. In an ideal situ-ation, value levers can also be employed in any situation of RAI co-production. For example, organizations created several formal RAI structures for practitioners to facilitate sharing and delibera-tion of values. These included top-down standardized guidelines, such as guidebooks (e.g., PAIR [80], HAX [73]) around established RAI values, bringing in experts to share their experiences around co-production (lectures), and enabling shared spaces for co-production. However, in this section, we will be looking at value levers specif-cally developed in response to the challenges experienced in RAI value co-production. Institutional Value Levers: External Expertise and Certif-cations to reduce Ambivalence. One of the ways in which or-ganizations brought stability to their inconsistent top-down RAI institutional structures was by taking assistance of independent agencies or professionals who specialized in establishing values levers that helped streamline their existing structures. One such value lever was \u2018Responsible AI certifcations\u2019 that were designed to bring diferent recognized and informal RAI co-production activi-ties under one-roof. These programs act as facilitators between the non-technical and technical workforce by enabling co-production around RAI values to make them compliant with upcoming regula-tions. Participants reported that diferent activities were packaged into the RAI certifcation program, such as getting buy-in for par-ticular RAI values, leveraging trusted partners for running impact assessments, engaging key actors in value discovery and prioriti-zation, and implementing appropriate RAI methods. P19, policy director of one such RAI certifcation organization, shared how these certifcations are efective in sectors, such as energy, mining, and human resources that often have a limited technology work-force. They described efort of facilitating RAI value conversations within their client teams as a key part of the certifcation process: We found two more practitioners who raised similar concerns with other RAI values, such as privacy and trust. They were con-cerned that making their product \u201ccompletely responsible\u201d could enable companies to market their products as nearly perfect, lead-ing to overtrust and over-reliance. These fndings align with the ethics-washing phenomenon within the tech ethics literature which argues that companies sometimes invest in ethics teams and in-frastructure, adopting the language of ethics to minimize external controversies and superfcially engage with the proposed regula-tions [44, 106]. Practitioners who expressed these sentiments were quite dissatisfed with their RAI implementation work as they felt their actions were merely a \u201cband-aid\u201d solution for the organiza-tion, instead of meaningfully altering organization\u2019s culture and practices.  \u201cIt is important to have everybody on board for those [RAI] value conversations. So we try really hard to have all the diferent teams like internal or external audit, legal, business, data and AI team come together, brain-storm, discuss diferent [RAI] issues in specifc contexts and shortlist [the RAI values], even if we just get a little bit of their time . . . everyone needs to be brought in early because we conduct a lot of activities likes audit analysis, bias testing. . . it saves time, addresses several concerns, and establish streamlined [RAI] processes. . . . For sim-plicity, we just package all of the diferent activities we do under RAI certifcation. . . . Some times few activities are already being executed by the organization, we just do the job of aligning them in a way that works for the organization.\u201d  RAI programs also facilitated similar structures, explicitly providing narratives that brought out disagreements, e.g.,: Such external expertise and certifcations can provide an oppor-tunity for open discovery, bolster existing centralized support, and identify RAI values that might otherwise be discovered at the last stages. Institutional Value Levers: Activities to Distribute RAI bur-den. We also found several nascent but more focused value levers in bottom-up institutions focused on distributing the burden expe-rienced by a few roles more widely within the team. These value levers provided opportunities for increased participation from stake-holders, especially in the starting stages by enabling them to bring-in complementary RAI values into the team. Most commonly used levers in this context included scenario-based narratives and role plays, and open-ended activities that engaged practitioners in opin-ion formation and sharing. Other value levers included conducting a literature review of specifc RAI values and applicable cutting-edge methods, defnitions, and guidelines around them to share and in-voke feedback from the team. We also observed more experimental value levers that were geared towards bringing-in complementary RAI values of external stakeholders (e.g., end-users) into the team. For example, P18, a data scientist working in a startup, hosted a panel to capture complementary perspectives around AI explain-ability. Visibility into how explainability was perceived diferently by diferent community members, such as NGOs and government, contributed to a better understanding and alignment within the team to develop explainable models. In a similar example, P09, an engineer working on a reinforcement learning model in the context of healthcare for low resources in India, facilitated feld visits to the end-user communities. Such exposure helped roles that were pas-sive in sharing their values as well as roles that were thinking about new values, such as social justice, in the RAI discourse. Overall, these value levers (narratives, role-plays, literature reviews, panels, and feld visits) focused on primarily bottom-up structures, which helped reduce pressure on specifc roles and limit superfcial value engagements. Discourse Value Levers: Facilitating Disagreements. Moving our focus to RAI value co-production, we saw user-facing practi-tioners create explicit opportunities for disagreements and healthy conficts to tackle the problem of superfcial value engagement and improve the quality of their teams\u2019 deliberations. Disagreements in co-production phase allowed practitioners like UX researchers and product managers to think inclusively, capture diverse perspectives and expert knowledge, and more importantly predict future value conficts. For example, P04, a UX researcher, created bottom-up adversarial prioritization framework. In the starting phases of this framework, the UX researcher pushed team members to go broad and co-produce values by wearing other practitioner\u2019s hats and invoking their RAI values. This practice allowed them to bring for-ward interesting disagreements between diferent roles that were then resolved and prioritized to achieve a small set of meaningful RAI values. P04 recalled two of the values that received maximum disagreements were diversity and inclusion. Wearing complemen-tary user hats enabled practitioners to familiarize with these values that were otherwise unfamiliar in their own roles. Other top-down  \u201cUsually I will write something in the prompts that I think that the team absolutely needs to hear about but is controversial and opposing. But what I do is I put it in the voice of their own team so that it is not coming from us. It is not us scrutinizing them. That promotes inter-personal negotiation that pushes individuals to really defend their values with appropriate reasoning.\u201d According to P19, having such RAI system in place early also allows companies to judge its ML models benchmarks when compared to their competition. Leveraging the adversarial prioritization frame-work appropriately in both top-down and bottom-up structures can enable open-discovery, and surface the values and related conficts for resolution. Discourse Value Levers: Model Cards & Visual Tools to Re-duce Abstractness from Values. We found that practitioners also created succinct representations and simplifed documentation to bring much needed clarity to various RAI values and simply as-sociated models. For instance, engineers shared documentation of model and data cards, making it easier for non-engineering and engineering roles to grasp the information. P23, a senior engineer at an AI startup looking into sustainability, shared the process: \u2018Even we have introduced this concept of a model card, wherein if a model is developed, the model card has to be flled out. So what is a model card? A model card is a series of questions that captures the basic facts about a model at a model level at an individual model level. What did you use to build a model? What was the pop-ulation that was used? What is the scoring population? It is like, having all of that in a centralized standard format. Goes a long way to roll it up because the product can be very complex as well, right? With multiple play-ers and whatnot. But having that information collected in this way benefts other roles that own the product to think about diferent values that are missing\u2019 UI/UX designers, UX researchers, and analysts also used similar documentation tools to initiate discussions and receive feedback from other practitioners in the team. P20, a UX researcher, used presentation slides containing model features to facilitate brain-storming sessions and receive feedback from other roles. They also repurposed tools and methods used in their own work to give shape to their peers\u2019 abstract values. For example, P20 reused on-line jam-boards containing key RAI values and user fndings for afnity diagramming, enabling the team to \u201ccategorize the fndings and map them to specifc RAI values\u201d. Other RAI levers in this cate-gory included designing and sharing infographics and regular RAI standups where practitioners took it upon themselves to be stew-ards of RAI principles for the team to give updates, receive feedback and learn about team\u2019s perspectives on specifc RAI values. Implementation Value Levers: Ofce Hours, User stories, Safe Spaces to Reduce Tensions. A few value levers that were part of top-down RAI programs were also efective in reducing various value tensions that occurred between diferent practitioners (n=2). One such program was RAI ofce hours that was available for elicitation and production, but were also extremely efective for tension-resolution. A typical of-fce hour was 30 minutes in which practitioners engaged with a relevant expert and an experienced facilitator. One key way experts solved the tensions in these sessions was by collecting and provid-ing concrete case-study examples. For example, P21, an RAI ofce hour facilitator, shared an example about the use of his ofce hours. The practitioners were at odds with each other in implementing explainability and trustworthy features. During the ofce hours, P21 responded by sharing an edge case scenario where even good explanations might backfre, such as, \u201cIf a pregnant woman had a miscarriage, showing even good end-user explanations around why they are seeing infant-related content can be very problematic. Ex-plainability should be carefully teased out based on the context in which it is applied.\u201d Another set of value levers used especially by the roles facing end-users were user stories and scenarios to infuence and persuade users to change their value priorities and align with rest of the team. These levers were also used by the roles to converge on key values after engaging in healthy conficts within the divergence phase. For example, P04, exposed diferent pain points and key user journeys by \u201chighlighting the clip of a user that is really, really amazing story that is either very painful or poignant\u201d. Interestingly, P04 was aware how such value levers had to be evoked carefully, \u201cIf that story is not representative, I\u2019m manipulating the system. If it is representative, I\u2019m infuencing the system...I will have to be careful not operate on the side of manipulation and try to be very squarely on the side of infuence. So, I do like regular checks for myself to make sure that I am operating on infuence, not manip-ulation, in terms of the stories that I am allowing people to amplify.\u201d Lastly, in order to tackle several types of value conficts in the co-production of RAI values, we found diferent RAI value levers that focused on improving value alignment. One key alignment strategy was to create structures and activities that aligned the team\u2019s RAI values pretty early in the process. One such activity that we saw in both practitioner\u2019s and RAI programs was providing a safe space to encourage open discussions among individuals to empathize with other members. P09 shared, \u201cOne alignment strategy was open discussion with the safe space, where team members could fail, be called out and to learn from each other as we were developing values. So say someone fnds the value of democratiza-tion really important, they are made to articulate what they mean by it.. . . . It is easy if there are diferent buck-ets in which they can categorize and explain because then people can easily surface all the diferent ways they think and prioritize values and that helps with alignment\u201d 5  DISCUSSION AND FUTURE WORK Overall, our fndings show that co-production of RAI values in practice is complicated by institutional structures that either sup-port top-down decision-making by leadership or are inhabited by bottom-up practitioners exercising voluntary agency (section 4.1).  In other case, multiple challenges exist when practitioners have to reconcile within their internally held values and RAI values expected from their roles; and between themselves and their team-members. Our fndings also show that discourse around alignment and prioritization of RAI values can sometimes be unproductive, non-conclusive, and disempowering when practitioners have to implement said RAI values (section 4.2). We observed a lack of transparency, and unequal participation within organizations; and between organizations and end-users of their products/services (section 4.2). Despite the relatively complicated lay of the land, practitioners have been pushing ahead and discovering multiple strategies on how to make progress (section 4.3). In the subsections below we will unpack these challenges, strategies and potential future work across the three sites of co-production: institutions, discourse, and representations. 5.1  Envisioning balanced Institutions: Middle-out RAI Structures Inequity in Burden. According to Jasanof [57], strong institutions provide a stable environment for efective knowledge co-production. They can also act as safe spaces for nurturing and transforming con-tested ideas to efective practices leading to long-lasting impact on the immediate ecosystem. Recent scholarship by Rakova et al. [82] has put faith in an aspirational future where organizations would have deployed strong institutional frameworks for RAI issues. Our fndings show that as of today, top-down structures are underde-veloped. Organizations have deployed structures that range from being reactive to external forces (e.g., compliance, public outcry) by tracking teams that implement RAI to proactively establishing structures that make teams RAI-ready (e.g., ofce hours). Further-more, stable workfows have been established for a limited number of values or use-cases, restricting the number of teams that could leverage such workfows. Being in the midst of restrictive structures, particular practi-tioner roles embraced the persona of bottom-up vigilantes and self-delegated themselves to be champions of lesser known RAI values (e.g., non-malefcence and trustworthiness). They initiate open-ended exploration for value discourses and subsequent value implementation. However, such bottom-up structures also put bur-den and occupational stress on selective roles, risking the imple-mentation success of such RAI projects. In particular, we have found that roles like UX researchers, designers, product managers, project managers, ethicists have been taking the brunt of this work. These fndings build on the previous work [71, 82], highlighting existing inequity and subsequent individual activism being performed by some - either by volition or due to lack of alternatives. Enabling Equal Participation. Going forward, there is a need for a holistic middle-out approach that seeks a balanced synergy between top-down and bottom-up structures while balancing for the challenges that each of the structures provide. For instance, organizations can work with RAI value stalwarts and champions to formalize and streamline bottom-up workfows, making it a stan-dard practice for all teams to engage in open-ended exploration of RAI values. Such a process can enable teams to look beyond loosely applicable  organization-recommended  RAI  values  and  shortlist Table 2: Summary of challenges mapped against strategies. those values that actually matter and apply to their team. To stan-dardize the structure, organizations can leverage independent (fat) team/roles that can guide the target team through the team while giving enough room for exploration. Organizations can also use a middle-out approach to reduce the burden and occupational stress on specifc roles through several top-down activities. One such way is to facilitate structures that can lower the barrier for diverse internal stakeholders to engage in RAI value co-production, regardless of their proximity to AI products or ML models. For instance, data-workers and teams/roles that do internal testing of the models (dogfooding) can contribute to the RAI value co-production. Same structures can also enable engage-ment with external stakeholders, such as end-user communities, policy, experts, and governance agencies in the initial stages of value co-production. Consequently, practitioners\u2019 chances to fore-see or anticipate changing requirements could improve, especially in the later stages of AI/ML lifecycle. Better yet, this could poten-tially improve not just the \u201cuser experience\u201d of value discourse, but also the efciency of implementation - a goal valued by private companies. This could be a win-win situation for multiple stake-holders by helping the top-down RAI structures align with business goals. While, our research uncovered only top-down and bottom-up structures that were mutually exclusive, other structures might exist. For example, while we envisage middle-out structures to be advantageous, future research is needed to operationalize and sim-ulate such structures; and discover existing implementations. There might be some challenges uniquely inherent in those structures. We encourage future researchers to continue this line of enquiry 5.2  Envisioning better Discourses: Enabling Critical RAI Value Deliberations Negativity in Deliberations. The ultimate aim of co-production discourse  is  to  engage  in  competing  epistemological  questions. Jasanof calls it interactional co-production [59, ch 8] because it deals with explicitly acknowledging and bringing conficts between two competing orders: scientifc order brought about by techno-logical innovations and social order brought about by prevalent sociocultural practices within the community. In our fndings, sev-eral underlying conficts surfaced between scientifc and social  order (section 4.2). In one instance, practitioners had to choose between socially impactful but lesser explored RAI values (social order) and lesser applicable but established values with measurable scientifc benchmark (scientifc order). In another instance of ten-sion an RAI value occupied competing social spaces (e.g., equity). The underlying issue was not the conficts itself but lack of sys-tematic structures that enabled positive confict resolution around the confict. Such implicit conficts often met with deprioritization and conversations ended unresolved. There is an urgent need to transform such implicit conficts to explicit positive deliberations. Organizations aiming for successful RAI co-production need to be more refexive [59, ch 9][111], mobilize resources to create safe spaces and encourage explicit disagreements among practition-ers positively, enable them to constantly question RAI values or co-production procedures that help embed them. While we saw some instances of these explicit practices in the form of value lever strategies, such instances were sporadic and localized to very few teams. Acknowledging Diferences Safely. Our fndings around chal-lenges within RAI value discourse also showcase the politically charged space that RAI values occupy in an organization. In their critical piece around implementation of values in organization, Borning and Muller [12] bring out a few value characteristics that are applicable to our study. First, values are not universal. Values are recognized, prioritized, and embedded diferently based on a bunch of factors, such as practitioner\u2019s role, organizational priorities and business motivations which make RAI as a complex space. In our fndings, roles prioritized those RAI values that were incentivized by the organizations and the AI/ML community through compu-tational benchmarks focused on the RAI value outcomes. This is problematic as the RAI values that have computational benchmarks and have implicit organizational incentives might not map onto the RAI issues that are pertinent in the community. One way to solve this mismatch is by rethinking the defnition of benchmarks from value outcome to value processes taken up by diferent roles (or teams) [33]. For example, organizations can encourage teams to document their co-production journeys around lesser known RAI values that can act as RAI benchmarks. The second issue that Borning and Muller [12] bring out is whose value interpretations should be prioritized and considered? In our fndings, tensions emerged as same values were interpreted and prioritized diferently by diferent stakeholders. End-users viewed RAI values as immutable and uncompromisable, whereas practi-tioners viewed them as fexible and iterable. Similar tensions were also observed between the internal stakeholders in contextualizing the same values. While we saw a few strategic value levers, such as ofce hours, they were mainly targeted at the stakeholders that were within the same team. Extending this line of value levers, we propose participatory approaches that take a balanced approach for both internal and external stakeholders. In particular, we take inspi-ration from participatory design fctions, a participatory approach using speculative design scenarios [23, 69], to fnd alignment on polyvocal speculations around embedded values in the emergent technologies. Deliberations around the fction narratives can be used to arrive at a common ground RAI value implementation that are both contextual and implementable. Shaping Institutional Structures to Improve Discourse. Em-ploying co-production as a lens has also allowed us to reveal a very delicate, symbiotic relationship between the practitioners\u2019 dis-courses and the institutional structures. Several of the discourse challenges observed in our fndings, such as issues pertaining to the lack of RAI knowledge and deprioritization of lesser known values, stemmed not only from the lack of mature top-down struc-tures but also dependency on a singular institutional structure. The limitations of top-down structures pushed several practitioners in-turn to create informal bottom-up structures, shifting the pres-sure from one structure to the other. We argue that RAI-focused organizations need to seek a balance between stability (top-down structures) and fexibility (bottom-up) [108]. In addition to new middle-out internal-facing institutional structures, RAI discourses can beneft from external-facing institutional structures that can help such discourses be impactful. This can be achieved by bringing in diverse external collaborators, such as NGOs, social enterprises, governmental institutes, and advocacy groups. It will also help avoid the issue of co-optation[43] for true meaningful impact of such structures on the practices. 5.3  Envisioning better Representations: Value Levers Value Levers Enabling Progress. As the third site of co-production, representations are essential manifestations of knowledge delibera-tions [59, ch 3]. They provide surrogate markers for progress on value alignments or even responses to tensions between stakehold-ers. In our study, we saw several value levers that were deployed within organizations at diferent stages to tackle co-production challenges. These value levers enabled progress during knowledge deliberations (section 4.3). For instance, during deliberation, prac-titioners used role-specifc value levers (e.g., visual tools by UX researchers & designers) as a window into their thought process around specifc RAI values. Similarly, enabling safe spaces provided opportunities of RAI value alignment among diferent roles. As rep-resentations, these value levers improved the transitions between internal stages and acted as vital markers to improve alignment.  We call them internal representations. Prioritization Framework employed to resolve value related conficts across individuals was another example of internal representation. In contrast, we also found a few externally represented value levers, such as RAI certi-fcations, that enabled organizations to reduce ambivalence in their structures while showing their RAI readiness and compliance to the outside community. Interestingly, we uncovered limited evidence of external representations that engaged with end-user communities directly. We posit that the lack of external representations can be at-tributed to the deprioritized perception of the end-users\u2019 role in RAI value conversations. External representations have the potential to act as stable participatory structures that enable participation of external stakeholders, making RAI value co-production successful. This might also be due to lack of sufcient incentives to enable such participatory structures. We hope that recent progress made by UNESCO\u2019s historic agreement [100] might provide the much needed push for organizations to share and learn together. Value Levers Enabling Practitioners. As we end this discussion, some readers might assume that we now have strategies to man-age the multiple challenges coming our way as we deliberate and implement RAI values in products. Our research with 23 practition-ers, points to the opposite - we are far from it. As one practitioner said, \u201cIt\u2019s currently hodgepodge\u201d. Multiple individuals and organi-zations are trying to surmount this incredibly complex challenge at institutional and individual level. While value levers discussed in this work were successful in helping practitioners make progress, the discovery of these value levers has at best been serendipitous. Sufcient support structures, skills training, and knowledge dissem-ination will be needed to enable practitioners to overcome these and unknown challenges yet. One can liken these value levers as a tool-belt for practitioners, as summarized in Table 2. There is a subsequent need to design systems that can provide access to these tools in a systematic way. This would require center-ing research amongst practitioners who are making products. We hope that future educators, researchers, and designers will pursue this opportunity to uncover further challenges, learn from existing strategies, develop better strategies as they iterate, and create de-sign systems to support practitioners. Further, we need to fnd more tools, and we need to share the tool-belt with other practitioners. 6  LIMITATIONS & CONCLUSIONS Based on qualitative research with 23 AI/ML practitioners, we dis-covered several challenges in RAI value co-production. Due to their roles and individual value system, practitioners were overburdened with upholding RAI values leading to inequitable workload. Fur-ther, we found that implementing RAI values on-the-ground is challenging as sometimes these values confict within, and with those of team-members. Owing to the nascent stage of RAI values, current institutional structures are learning to adapt. Practition-ers are also adapting by discovering strategies serendipitously to overcome the challenges. However, more support is needed by ed-ucators to educate RAI/RAI values, researchers to unpack further challenges/strategies, and for community to focus on aiding AI/ML practitioners as we collectively fnd a common-ground. Our work also has several limitations. First, our overall study is shaped by our prior HCI research experience in responsible AI. While all the participants had rich experience working with ethics in the context of social good in both global north and global south contexts, we acknowledge that our perspectives around responsible AI and ethi-cal values in AI might be constrained. Second, our insights are also limited by the overall methodological limitations of qualitative en-quiry, such as sample size of 23 participants across 10 organizations. Future work is required to generalize our insights across difer-ent organizational sectors and ML models using mixed-methods approach. REFERENCES",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "327cfa39-b709-4622-9316-9463a83e2444",
                    "text": "Recently, the AI/ML research community has indicated an urgent need to establish Responsible AI (RAI) values and practices as part of the AI/ML lifecycle. Several organizations and communities are responding to this call by sharing RAI guidelines. However, there are gaps in awareness, deliberation, and execution of such practices for multi-disciplinary ML practitioners. This work contributes to the discussion by unpacking co-production challenges faced by practitioners as they align their RAI values. We interviewed 23 individuals, across 10 organizations, tasked to ship AI/ML based products while upholding RAI norms and found that both top-down and bottom-up institutional structures create burden for diferent roles preventing them from upholding RAI values, a challenge that is further exacerbated when executing conficted values. We share multiple value levers used as strategies by the practitioners to re-solve their challenges. We end our paper with recommendations for inclusive and equitable RAI value-practices, creating supportive organizational structures and opportunities to further aid practi-tioners.",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "9b28dd83-f8c9-4534-a814-1a5c3d5bd00d",
                    "text": "\u2022 Human-centered computing \u2192 Empirical studies in HCI.",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "f1cffb93-ae8a-4964-b54b-9f668f1a152a",
                    "text": "Responsible AI, RAI, ethical AI, value levers, co-production, collab-oration, XAI, FAT, fairness, transparency, accountability, explain-ability",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "3f48c737-7131-4bb4-97d0-992f8a106326",
                    "text": "In November 2021, the UN Educational, Scientifc, and Cultural Or-ganization (UNESCO) signed a historic agreement outlining shared values needed to ensure the development of Responsible Artifcial  Intelligence (RAI) [99]. RAI is an umbrella term that comprises dif-ferent human values, principles, and actions to develop AI ethically and responsibly [3, 21, 37, 82]. Through UNESCO\u2019s agreement, for the frst time, 193 countries have standardized recommendations on the ethics of AI. While unprecedented, this agreement is just one of several eforts providing recommendations on diferent RAI values to be implemented within AI/ML systems [40, 101, 102]. In response, several industry organizations have begun to im-plement the recommendations, creating cross-functional RAI in-stitutional  structures  and  activities  that  enable  practitioners  to engage with the RAI values. For instance, several big-tech com-panies are implementing common RAI values, such as Fairness, Transparency, Accountability, and Privacy, as part of their RAI ini-tiatives [9, 29, 41, 51, 74]. However, such RAI values have minimal overlap with the values prescribed by UNESCO\u2019s framework that promotes non-malefcence, diversity, inclusion, and harmony [100]. Scholars have attributed the lack of overlap to diferent business and institutional contexts involved in developing AI/ML  systems [50, 56]. Subsequently, it is essential to understand these contexts by engaging with practitioners across multiple roles who come together to co-produce and enact such RAI values. Co-production is an iterative process through which organizations produce collec-tive knowledge [58]. During co-production, individual practitioners may hold certain values (e.g., social justice), yet their teams might prioritize other values. Rakova et al. [82] hints at potential chal-lenges that can arise due to such mismatches in RAI values. Our study builds on this critical gap by giving a detailed analysis of those challenges and strategies (if any) devised to overcome such strains as practitioners co-produce AI/ML systems. We interviewed 23 practitioners across a variety of roles to under-stand their RAI value practices and challenges. Our fndings show that institutional structures around RAI value co-production con-tributed to key challenges for the practitioners. We also discovered multiple tensions that arose between roles and organizations during prioritization, deliberation, and implementation. Interestingly, we also observed development of ten diferent RAI value levers [91, 92]. These are creative activities meant to engage individuals in value conversations that help reduce value tensions. In the remainder of the paper, we frst discuss related work about collective values in Responsible AI from an HCI perspective and outline our research questions. We then present the research methodology and results of the study. We conclude with a discussion of our contributions in improving RAI co-production practices of AI practitioners. Overall, this work makes several contributions. First, we describe the expe-riences and the organizational environment within which AI/ML practitioners co-produce RAI values. Second, we illustrate multiple challenges faced by AI practitioners owing to diferent organiza-tional structures, resulting in several tensions in co-production. Third, we unpack ten RAI value levers as strategies to overcome challenges and map them on to the RAI values. Lastly, we provide essential strategies at the diferent levels (individual, and organiza-tional) to better facilitate and sustain RAI value co-production.  disparate AI/ML practitioners to come together, share, and enact on key values [82]. Our study expands RAI discipline by surfacing on-ground challenges of diverse AI/ML practitioners attempting to engage in shared RAI responsibilities, such as collaborative value discourse and implementation. In the next section, we further un-pack the notion of values, examine their roots in HCI and their current role in RAI.",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "139d1c00-6ce8-4037-9329-01707152bfbc",
                    "text": "",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "2b2cec80-3194-4453-bfc9-ccc93e9e15cc",
                    "text": "In the last decade, Responsible AI (RAI) has grown into an overarch-ing feld that aims to make AI/ML more accountable to its outcomes [1, 3, 20]. One of the feld\u2019s roots lies in Ethical AI, where critical engagement with ethical values in the otherwise traditional AI/ML feld have been encouraged [2, 39, 63, 104]. Example studies include engagement with core ethical values to provide nuance in technical AI/ML discourse [47], translating ethical values into implementa-tion scenarios [39, 49], and AI/ML guidelines [45, 47, 52, 60, 103], and seminal studies that brought critical ethical problems to the forefront [4, 24, 76, 113]. RAI has also drawn its inspiration from AI for Social Good (AI4SG [89]) research to study human values more broadly, going beyond \u201cethical\u201d values. AI4SG helped the RAI feld to translate such values embedded in AI/ML systems into positive community outcomes [6] by eliciting specifc values (e.g., solidarity [31]), developing meth-ods (e.g., capabilities approach [11]), and producing representations (e.g., explanations [79]) that strongly align with community goals (e.g., the UN Sustainable Development Goals [37]). For example, studies have explicitly engaged with underserved communities to examine the impact of the embedded values within AI/ML systems in their lives (e.g., in agriculture [70], health [53, 107], and education [14] domains). More recent studies have shed light on how certain practitioners\u2019 (e.g., data and crowd workers) practices, contribu-tions, and values are often ignored while developing the AI/ML systems [85, 97, 105]. Some have looked at diferent values (e.g., fairness) that are often ignored by the discriminatory algorithms [16, 46, 61]. More recent work at the intersection of these two by Goyal et al. [42] has also highlighted the impact of data workers of marginalized communities on the AI/ML algorithms to highlight complexity when building for RAI values like equity. Lastly, RAI has also drawn motivation from recent movements associated with specifc value(s). One such movement is around the value of explainability (or explainable AI) that arose from the need  to  make  AI/ML  systems  more  accountable  and  trustwor-thy [5, 22, 25]. A similar movement within the RAI\u2019s purview fo-cused on FATE values (Fairness, Accountability, Transparency, and Ethics/Explainability) [18, 66, 75, 93]. While both movements have challenged the notion of universal applicability of RAI values, our study illustrates how these challenges do indeed appear in practice and the strategies used by practitioners on the ground to resolve these challenges. Taken together, RAI has emerged as an umbrella term that encap-sulates the above movements, encouraging critical value discourses to produce a positive impact. At the same time, departing from previous movements that focused on specifc issues within AI/ML practices, RAI takes a broad institutional approach that promotes",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "0aad52f4-ad6a-46ff-b78b-4d9367159485",
                    "text": "",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "59f6e9df-2baa-408a-9671-456d15e63e70",
                    "text": "Science & Technology Studies feld has long examined how values are embedded in technology systems in various social and political contexts [67, 96, 109]. In recent years, studies within HCI have built on this foundation to bring a critical lens into the development of technology. Initial studies conducted by Nissenbaum and col-leagues argued against the previously held belief that technology is \u201cvalue-neutral\u201d, showcasing how practitioners embed specifc values through their deliberate design decisions [30, 33]. Value-sensitive Design (VSD) by Friedman et al. [36] was another step in this direction. It has been used as a refective lens to explore tech-nological afordances (through conceptual and empirical inquiry) as well as an action lens to create technological solutions (techno-logical inquiry) [34, 48]. While VSD\u2019s core philosophy remained the same, it has been extended, often in response to its criticisms [54, 68]. A criticism relevant to this study is practitioners\u2019 ease in apply-ing VSD in the industry contexts [90]. VSD is perceived to have a relatively long turnaround time, often requiring specialists for implementation. To overcome these challenges, Shilton proposed \u2018value levers\u2019, a low-cost entry point for value-oriented conversa-tions while building technology artifacts in the organization [90, 91]. Value levers are open-ended activities that engage participants in value-oriented discourses to develop common ground. With cre-ative representations, value levers can transform slow and cumber-some value conversations into creative and fruitful engagements [90]. While previous studies have applied and shaped the notion of value levers in a very specifc set of contexts, such as showcasing how designers employ them in their practices [90], this work shows a broader utility of value levers among a diverse set of practitioners while navigating the complex space of RAI value discourse. Within AI/ML research, initial exploration of values were still primarily computational in nature, such as performance [15], gen-eralizability [55], and efciency [27]. With the advent of HCI and critical studies focusing on discriminatory algorithms [10] and re-sponsible AI, the discussions shifted to much broader values, such as societal and ethical values, within the AI/ML feld [9]. These studies focused on exposing inherent biases in the models due to ab-sence of substantive social and ethical values. For instance, Burrell [13] demonstrated how complex ML models have inherent inter-pretability issues stemming from a lack of transparency about how predictions were achieved. Another set of studies by Eubanks [28] and Noble [78] scrutinized several algorithms governing the digital infrastructures employed in our daily lives to expose discriminatory behaviors within diferent situations, especially in the context of fairness, against marginalized populations. In a similar vein, sev-eral studies have explored individual values that they felt were critical for models, such as fairness [16, 32, 75], explainability [22], non-malfeasance [60, 77], and justice [8, 46, 61], refecting societal norms. A common underlying factor among several of these studies was that they focused on individual values enacted in their own spaces. Recently, however, a few studies have adopted contrasting perspectives which argue that values do not exist in isolation, but often occupy overlapping and contested spaces [36, 114]. Our study aims to provide much-needed deeper insights within this complex space by showing how practitioners engage with and prioritize multiple values in a contested space. Another value dimension explored is \u2013 \u201cwhose values should be considered while producing AI/ML algorithms [12]?\u201d Most studies have engaged with end-users values, lending a critical lens to the deployed models and their implications on society [64, 84, 94, 110]. These studies challenged whether developing fair algorithms should be primarily a technical task without considering end-users\u2019 values [50, 83]. Subsequently, researchers leveraged action research (e.g., participatory approaches [19]) to design toolkits, frameworks, and guidelines that accommodated end-user values in producing ML models [62, 65, 88]. A more relevant set of studies have recognized the importance of understanding the values that diferent practitioner roles em-bedded while producing responsible algorithms [81, 86, 112]. Such practitioner-focused studies are critical in understanding \u201chow\u201d and \u201cwhy\u201d particular values are embedded in AI/ML models early on in the life cycle. However, these studies have explored particular practitioners\u2019 values in silos, leaving much to be learned about their collective value deliberations. A nascent group of studies has answered this call. For example Madaio et al. [71] focused on con-trolled settings, where specifc practitioners\u2019 values could co-design a fairness checklist as one of their RAI values. Jakesch et al. [56] ex-plored a broader set of values of practitioners and compared it with end-users in an experimental setting. Another relevant study by Rakova et al. [82] has explored RAI decision-making in an organiza-tional setting, laying a roadmap to get from the current conditions to aspirational RAI practices. Our study contributes to this developing literature in four ways. First, within the context of Responsible AI practices, our study goes beyond the scenario-based, controlled settings, or experimental se-tups by focusing on natural work settings [56, 71], which echoes the sentiment of some of the previous open-ended qualitative studies that were conducted in the organizations [81, 112], but not in the context of Responsible AI practices. Second, we focus on a diversity of stakeholder roles, who are making an explicit efort to recognize and incorporate RAI values, unlike siloed studies previously. Third, we leverage the lens of co-production [58] to study RAI values in natural work settings. Fourth, our study extends [82] by explicitly unpacking the co-production challenges deeply rooted in RAI val-ues. To this end, we answer two research questions: (RQ-1): What challenges do AI/ML practitioners face when co-producing and implementing RAI values ? (RQ-2): In response, what strategies do practitioners use to overcome challenges as they implement RAI values ?",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "52ee13ce-821e-4ac3-804a-65e6d1e720b1",
                    "text": "To answer our research questions, we employed the conceptual framework of co-production proposed by Jasanof [58]. She defned co-production as a symbiotic process in which collective knowledge and innovations produced by knowledge societies are inseparable from the social order that governs society. Jasanof characterized knowledge societies broadly to include both state actors (e.g., gov-ernments) and non-state actors (e.g., corporations, non-profts) that have an enormous impact on the communities they serve. Studying co-production can help scholars visualize the relationship between knowledge and practice. Such a relationship ofers new ways to not only understand how establishments organize or express them-selves but also what they value and how they assume responsibility for their innovations. To operationalize co-production in our study, we invoke three investigation sites, as Jasanof proposed. The frst site of exploration is the institutions containing diferent structures that empower or hinder individuals to co-produce. The second site examines dif-ferent types of discourse that occur as part of the co-production activities. Solving technological problems often involve discourses producing new knowledge and linking such knowledge to practice. The last site of co-production is representations produced both dur-ing co-production to facilitate discourses and after co-production in the form of the end-product. The three sites of the co-production framework are appropriate for understanding the current industry challenges  around  RAI  innovation  for  several  reasons.  Techno-logical corporations developing AI/ML innovations have a robust bi-directional relationship with their end-user communities. More-over, for successful RAI value implementation, practitioners need to leverage the complex structures within their organizations that are invisible to external communities. RAI value implementations occur through strategic discourses and deliberations that translate knowledge to efective execution. Lastly, in the process of RAI value deliberations, individuals co-create representations that further the implementation eforts of RAI.",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "f6bde38d-7d7c-4194-a1c9-583210f8add0",
                    "text": "To answer our research questions, we conducted a qualitative study consisting of 23 interviews with active AI/ML practitioners from 10 diferent organizations that engaged in RAI practices. After receiv-ing internal ethics approval from our organization, we conducted a three month study (April-June, 2022). In this section, we briefy talk about the recruitment methods and participant details.",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "4cf5c153-48e0-4c12-a062-7adb6131a64c",
                    "text": "To recruit AI/ML practitioners who actively think and apply RAI values in their day-to-day work, we partnered with a recruitment agency that had strong ties with diferent types of corporate or-ganizations working in the AI/ML space. We provided diverse re-cruitment criteria to the agency based on several factors, including gender, role in the company, organization size, sector, type of AI/ML project, and their involvement in diferent kinds of RAI activities. Using quota sampling technique, the agency advertised and ex-plained the purpose of our study in diverse avenues, such as social media, newsletters, mailing lists, and internal forums of diferent companies. For the participants that responded with interest, the agency arranged a phone call to capture their AI/ML experience, as well as their experience with diferent RAI values. Based on the information, we shortlisted and conducted interviews with 23 AI/ML practitioners who ft the diverse criteria mentioned above. The aforementioned factors were used to prioritize diverse partici-pants with experience working on RAI projects within their team in diferent capacities. For example, while shortlisting, we excluded students working on responsible AI projects as part of their in-ternships and included individuals who were running startup RAI consultancy frms. Out of the 23 practitioners, 10 identifed themselves as women. Participants comprised of product-facing roles, such as UX design-ers, UX researchers, program/product mangers, content & support executives, model-focused roles, such as engineers, data scientists, and governance focused-roles, such as policy advisors and audi-tors. Out of 23 practitioners, all but one participant worked for a U.S. based organization. However, participants were geograph-ically based in both Global North and Global South. Participants also worked in a wide variety of domains, including health, energy, social media, personal apps, fnance and business among other, lending diversity to the captured experiences. Three participants worked for independent organizations that focused exclusively on RAI initiatives and AI governance. twelve participants had a technical background (e.g., HCI, computer-programming), four had business background, two had law background and one each spe-cialized in journalism and ethics. For more details, please refer to Table 1.",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "9182bbb8-fef8-450d-9b65-4861dd2116e0",
                    "text": "We conducted semi-structured interviews remotely via video calls. Before the start of the each session, we obtained informed consent from the participants. We also familiarized participants with the objective of the study and explicitly mentioned the voluntary na-ture of the research. The interviews lasted between 40 minutes and 2 hours (avg.= 65 mins.) and were conducted in English. Interviews were recorded, if participants provided consent. Our interview ques-tions covered diferent co-production practices. First, in order to understand diferent co-production challenges (RQ-1), we asked questions about (1) how practitioners faced challenges when shar-ing RAI values across roles (e.g., \u201cCan you describe a situation when you encountered problems in sharing your values?\u201d ) and (2) how practitioners faced challenges when collaborating with diferent stakeholders (e.g., \u201cWhat challenges did you face in your collabo-ration to arrive at shared common responsible values?\u201d). Second, to understand diferent co-production strategies (RQ-2) we asked (3) how practitioners handled conficts (e.g., \u201cCan you give an example where you resisted opposing peers\u2019 values?\u201d) and (4) how practition-ers sought assistance to achieve the alignment in RAI values (e.g., \u201cWhat was the most common strategy you took to resolve the con-fict?\u201d). To invoke conversations around RAI values, we used a list of RAI values prepared by Jakesch et al. [56] as an anchor to our conversations. After frst few rounds of interviews, we revised the interview script to ask newer questions that provided deeper understanding to our research questions. We stopped our inter-views once we reached a theoretical saturation within our data. We compensated participants with a 75$ gift-voucher for participation.",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "0bbf3757-7af6-4733-a3b1-9f5f604ab3dd",
                    "text": "Out of 23 participants, only three denied permission to record audio. We relied on extensive notes for these users. Overall 25.5 hours of audio-recorded interviews (transcribed verbatim) and several pages of interview notes were captured. We validated accuracy of notes with the respective participants. Subsequently, we engaged in thematic analysis using the NVivo tool. We started the analysis by undertaking multiple passes of our transcribed data to understand the breadth of the interviewee\u2019s accounts. During this stage, we also started creating memos. Subsequently, we conducted open-coding on the transcribed data while avoiding any preconceived notions, presupposed codes, or theoretical assumptions, resulting in 72 codes. We fnalized our codes through several iterations of merg-ing the overlapping codes and discarding the duplicate ones. To establish validity and to reduce bias in our coding process, all the au-thors were involved in prolonged engagement over multiple weeks. Important disagreements were resolved through peer-debriefng [17]. The resultant codebook consisted of 54 codes. Example codes included, \u2018social factors\u2019, \u2018prior experience\u2019, \u2018enablers\u2019, \u2018RAI push-back\u2019. As a fnal step, we used an abductive approach [98] to further map, categorize, and structure the codes under appropriate themes. To achieve this, we used three key instruments of co-production framework developed by [58], namely, making institutions, making discourses, and making representations. Examples of the resultant themes based on the co-production instruments included \u2018value ambiguity\u2019, \u2018exploration rigidity\u2019, \u2018value conficts\u2019, and \u2018value lever strategies\u2019. Based on the instruments of co-production framework, we have present our resultant fndings in the next section.",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "121b4a2f-2720-4945-a784-0f159dd2a605",
                    "text": "Our overall fndings are divided based on diferent sites of explo-ration proposed by Jasanof [58]. The frst section answers RQ-1 by exploring several institutional  challenges that hinder the co-production of RAI values among practitioners (Section 4.1). The second section explores subsequent knock-on challenges that unsta-ble institutional structures create in co-production discourses (Sec-tion 4.2). The last section answers the RQ-2 by presenting carefully thought out representations that overcome challenges deliberation and execution of RAI values using the concept of value levers [90]. (Section 4.3).",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "4280bbc1-76cd-4bba-b321-57410936a131",
                    "text": "",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "4bff3c48-d24a-45f1-bc98-6a617841dd4b",
                    "text": "Institutional structures are essential in enabling co-production of new knowledge [58]. It is these structures that facilitate relation-ships for deliberation, standardize democratic methods, and validate safety of new technological systems before information is dissemi-nated into the society. We found two key institutional structures that facilitated deliberation around RAI values within AI/ML com-panies. These structures brought about diferent RAI challenges. Bottom-up: Burdened Vigilantes. The First type of structures were bottom-up. Within these structures, RAI conversations de-veloped through RAI value sharing in the lower echelons of or-ganizations, often within AI/ML practitioners\u2019 own teams. In our interviews, eight practitioners, namely a UX researcher, designer, Table 1: Practitioners\u2019 Demographic Details. content designer, and program manager from two mid-size organi-zations and two large-size organizations experienced or initiated bottom-up practices that engaged with RAI values. One of the en-ablers for such bottom-up innovation was individuals\u2019 sense of responsibility towards producing AI/ML models that did not con-tribute to any harm in society. A few other practitioners paid close attention to \u2018social climate\u2019 (e.g., LGBTQ month, hate speech inci-dents) to elicit particular RAI values. For instance, P08, a program manager in a large-scale technology company took responsibility for RAI practices in their team but soon started supporting team members to come together and share RAI values: \u201cWe cater to projects that are very self-determined, very bottom-up aligned with our values and priorities within the organization . . . These are what I call responsible innovation vigilantes around the company. I also started that way but have grown into something more than that. You\u2019ll see this at the product or research team level, where somebody will speak up and say, \u2018Hey, I want to be responsible for these RAI values, make this my job and fnd solutions\u2019. So you start to see individuals in diferent pockets of the company popping up to do RAI stuf.\u201d A key challenge with such bottom-up structures was that the responsibility of engaging with RAI value conversations implicitly fell on a few individual \u201cvigilantes\u201d. They had to become stalwarts of particular RAI values and take out substantial time out of their work to encourage and convince their teams to engage with RAI values. They also actively seeked out RAI programs available within and outside their organization. When such RAI programs were not available, individuals took it upon themselves to create engage-ment opportunities with other members within the organization. These bottom-up structures were useful in breaking the norms of \u201cboundary-work\u201d that are often set within AI and similar technical organizational work where only engineers and high-ofcials in the company maintain control [38]. It allowed non-technical roles such as user experience researchers, product managers, analysts, and content designers to a create safe space and lead the RAI eforts. While such eforts early on in AI/ML lifecycle minimized the po-tential harm of their ML models or AI products, it often happened at the cost of their overworked jobs. Bottom-up: Burdened with Educational Eforts. Apart from self-motivated vigilantes, the burden of RAI value exploration also fell on a few practitioners who were implicitly curious about RAI  innovation. Unlike the vigilantes, these participants were pushed to become the face of their team\u2019s RAI initiatives since there was no-one else who would. P14, a product manager working for two years at a medium size company within the energy sector shared, \u201cWhen I came in to this team, nobody really believed in it [RAI] or they really didn\u2019t think it [RAI] was important. I was personally interested so I was reading about some of these principles . . . When there was an indication of a future compliance requirement, people didn\u2019t want to take up this additional work . . . somebody had to do it.\u201d Similarly P05, a technical manager leading their team on data collection for the development of knowledge graphs, revealed how they were considered \u201cthe face of privacy\u201d for the team. Therefore, P05 was expected to foster awareness and common understanding among internal stakeholders and external partners and ensure they strove towards similar RAI standards and appreciated data-hygiene practices (e.g., data cleaning and de-identifcation). Practitioners like P14 and P05 had to assume the responsibility of fguring out the RAI space by presenting their team\u2019s needs and asking forma-tive questions even when their objectives around RAI were often not clear, such as which values to consider (e.g., \u201cprivacy or trans-parency?\u201d), what certain values mean (e.g., \u201cwhat trustworthiness as an RAI value should mean to the model and the team\u201d), how to operationalize specifc values (e.g., \u201cHow does trustworthiness apply to rule-based models? What kind of RAI values to invoke while col-lecting data?\u201d, and how to interpret outcomes and map them on to their team\u2019s objectives. Participants (n=5) shared how leading such RAI initiatives bur-dened their professional lives in various ways. Multiple participants reported that the RAI feld was still in its infancy and taking up responsibilities in such conditions meant that their eforts were not deemed a priority or sometimes even ofcially recognized as AI/ML work [71]. Consequently, the practitioners possessed lim-ited understanding of the direction to take to educate their team, convert their eforts into tangible outcomes, and efectively align their educational outcomes to the team\u2019s objectives. P13, an RAI enthusiast and an engineer at a large-scale social media company shared how their RAI efort seemed like an endless efort, \u201cAt this point, I probably know more about what things (RAI values) we don\u2019t want in it (model) than what we do want in it . . . It\u2019s like I am just learning and fguring out what\u2019s missing as I take every step . . . It is unclear which [RAI] direction will beneft the the team.\u201d  More-over, the burden of educating multiple team members was on the Figure 1: A summary of co-production activities mapped to Jasanof [58]\u2019s co-production sites, along with the themes, RAI values invoked, and key fndings and takeaways. shoulders of a very few practitioners tantamounting to substantial pressure. Metcalf et al. [72] in their paper around technology ethics put forward the term \u2018ethic owners\u2019. This role shares similarity with the bottom-up vigilantes and the educators as they both are mo-tivated  and  self-aware  practitioners,  invested  in  foregrounding human values by providing awareness and active assistance while institutionalizing the processes. However, Metcalf\u2019s ethic owners\u2019 responsibilities were clearly defned. Their tasks of working with teams or higher management were perceived as visible, prioritized work for which they would be credited for career growth or other-wise. While bottom-up informal roles in our research performed similar set of tasks, their eforts were seen as tangential, \u2018admin-istrative\u2019, and underappreciated. It is not just that there was an additional burden, but even the outcomes of taking on this addi-tional burden for bottom-up informal roles were dissimilar to the ethic owners. Taking up informal RAI work was more problematic when the requirements in the later stages of ML were unprompted,  compelling practitioners to focus on these eforts at expense of their own work. In our fndings, one form of the need came as academic criticism or critique around particular values that were seen concerning a particular product (e.g., \u201cwhat steps are you taking to ensure that your model is equitable?\u201d). Another form of need came from end-users\u2019 behavior who experienced the models through a particular product. P20, a user experience researcher working with deep learn-ing models in fnance, shared how user feedback brought about new RAI needs that became their responsibility: \u201cOnce the users use our product and we see the feedback, it makes us realize, \u2018oh, people are sometimes using this feature in an unintended way that might in turn impact the way we are going about certain values, say trans-parency\u2019 . . . . Initially we were like, \u2018We should strive for transparency by adding a lot of explanations around how our model gave a particular output\u2019. Later we real-ized too many explanations [for transparency] fostered inappropriate trust over the feature. . . UXR represents user needs so its on me to update the team on the issues and suggest improvements. A few practitioners (n=2) also mentioned how the constant jug-gling between their own role-based work and the unpredictability of the RAI work pushed them to give-up the RAI responsibilities all-together. Top-down: Rigidity in Open-discovery. While the burden of ownership and execution of RAI values in bottom-up structures were on small group of individuals, they had the fexibility to choose RAI values that were contextual and mattered to their team\u2019s ML models or projects. On the contrary, we found that top-down in-stitutional structures limited the teams\u2019 engagement to \u201ckey\u201d RAI values that impacted organization\u2019s core business values. For in-stance, P15\u2019s company had trust as a key value baked into their business, requiring P15 to focus on RAI values that directly reduced specifc model\u2019s biases, thereby increasing the company\u2019s trust among their users. Consequently, several RAI practitioners had to skip RAI value exploration and sharing. Instead they directly implemented predetermined RAI values by the management just before the deployment. P06, an engineer at a large tech company working in conversational analysis models, described this lack of choice: \u201cTo be honest, I imagine lots of the conversations, around the sort of values that need to go into the model, hap-pened above my pay grade.  By the time the project landed on my desk to execute, the ethics of it was cleared and we had specifc values that we were implementing.\u201d Public-oriented legal issues and ethical failures, especially when launching innovative models (e.g., transformer networks), also de-termined the RAI values that were prioritized and the subsequent formal RAI structures that were established by the organizations. P19, a policy director at a RAI consultation frm facilitating such structures, shared how such impromptu structures were quite com-mon in response to ever-changing laws around AI governance: \u201cEven if you\u2019re conservative, the current climate is such that it\u2019s going to be a year or two max from now, where you will start to have an established, robust regulatory regime for several of these (RAI) issues. So a good way to be prepared is to create the [RAI] programs in whatever capacity that enables companies to comply with the new regulations, even if they are changing because if you have companies doing Responsible AI programs, it eventually gets compliance and executive buy-in. \u201d  trust or privacy) due to directed and concerted focus. It allowed for organization-wide impact since the \u201cbuy-in\u201d already existed [71]. Top-down: Under-developed Centralized Support. However, in the case of less clearly defned values (e.g., non-malefcence or safety) we observed a limited scope for nuance and despite best eforts, the centralized concerted direction does not always pan out as intended. Further, while laws continue to evolve in this space, participants felt that pre-mediated RAI values might not longitudinally satisfy the growing complexity of the ML models being implemented (e.g., multimodal models). Hence, while it might seem that setting up a centralized top-down approach might be efcient, the current execution leaves much to be desired. In fact, based on data from over half the participants, we found that fve top-down structured companies integrated lesser known RAI values into their workfows in multiple ways without establishing a centralized workfow. Those who did establish centralize workfows created consulting teams to advise on RAI Practices (similar to ethic owners Metcalf et al. [72]). However, these top-down centralized RAI consulting teams were not always set up to succeed. As is the nature of consulting, people did not always know the point-of-contact or when and how to reach out. The consulting teams needed to also consider opportunities to advertise about themselves and engagement mechanisms,which was difcult due to the lack of context and nuance around the teams\u2019 projects. Consequently, it was difcult for such teams to generate organic interest, unless the teams were already aware of their RAI requirements and knew a point of contact. P10, a manager who facilitated one such top-down RAI program in a large-scale technology company for AI/ML teams, described lack of fxed ways in which teams engaged with them on RAI values, making it a difcult engagement: \u201cWe have a bunch of internal Web pages that point you in all diferent types of directions. We don\u2019t have a singular voice that the individuals can speak with . . . . Its currently hodgepodge. Some teams come to us willingly. They had already thought about some harms that could occur. They say, \u2018Here\u2019s our list of harms, here\u2019s some ideas on what we want to do\u2019 They\u2019d already done pre-work and are looking for some feedback. Other teams come to us because they\u2019ve been told to. . . . They haven\u2019t thought much about RAI and need longer conversations . . . Other teams were told to go track down an individual or team because they are doing ML stuf that will require RAI assistance, but they don\u2019t know about us\u201d Instead of investing in RAI structures to comply with diferent regulations in Tech Ethics such as codes of ethics, statements of principle, checklists, and ethics training as meaningful, organiza-tions perceive them as instruments of risk that they have to mitigate [72]. In line with the previous literature [7, 44], our fndings indi-cate that practitioners often fnd false solace in such structures as they run the risk of being superfcial and relatively inefective in making structures and practices accountable and efective in their organizations. However, adding nuance to this argument in the case of RAI practices, we found that practitioners more broadly devoted time and energy to follow established and prioritized values (e.g.,",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                },
                {
                    "id": "5335ffc7-a7e5-43dd-ab60-dad42a8fccd8",
                    "text": "Fruitful co-production requires well-established institutional struc-tures that can empower stakeholders to engage in stable democratic discourses with an aim of knowledge production [58]. In the pre-vious section, we uncovered diferent structural challenges at the institutional level that contributed to knock-on efects, creating further challenges for practitioners during the co-production and implementation of RAI values. Discourse: Insufcient RAI Knowledge. A key challenge that many practitioners experienced in co-producing RAI values in team was the difculty in engaging deeply with new and unfamiliar RAI values deemed important by the team members. P07, a policy advi-sor in a large technology company, who regularly interacted with those who implemented RAI values, described the \u201csuperfcial en-gagement\u201d with values as an act of \u201cinefective moralizing\u201d, wherein practitioners struggled to develop deeper interpretations of the team\u2019s shared values and contextualize them in relation to the ML models they were developing. P07 mentioned several key critical thinking questions that AI/ML practitioners did not deliberate within their teams, such as \u201cIs this RAI value applicable to our product?\u201d, \u201chow does this value translate in diverse use cases?\u201d, or \u201cshould this value be enabled through the product?\u201d The need for deeper engagement becomes particularly important in a high-stakes situation, such as healthcare, where cer-tain conditions have an unequal impact on particular demographics. P12 experienced this complexity while overseeing the development of a ML model focused on health recommendations: \u201cSo a lot of models are geared towards ensuring that we have we are predictive about a health event and that al-most always depends on diferent clinical conditions. For example, certain ethnic groups can have more proclivity to certain health risks. So, if your model is learning cor-rectly, it should make positive outcomes for this group more than the other groups. Now, if you blindly apply RAI values without thinking deeply about the context, it might seem that the model is biased against this group when in reality these group of people are just more likely to have this condition, which is a correct conclusion, not a biased one.\u201d Such deeper analysis requires hands-on practice and contextual training in the feld and formal RAI education. In our fndings, top-down structures were only efective to fll the gap for key values that aligned with the company\u2019s vision, leaving a much needed requirement for contextual, high-quality RAI education for more emergent RAI values that could be modularized for specifc teams. P02, a content designer for a large health technology company shared how this gap looked like for their team that was designing content for a machine translation team, \u201cOne thing that would have been benefcial is if I or my team could somehow get more insights on how to think about trustworthiness in the context of the con-tent produced by our machine translation model and probably evaluate it . . . Often time, I just go to someone who is known to have done some work in this [RAI] and say, \u2018Hey, we want to design and publish the content for the model like in a month from now, what is bare minimum we could do from [RAI] principles point of view?\u2019. . . Sometimes it\u2019s never-ending because they say I have not thought about this at all and that it is going to take a month or maybe much more longer to get these principles implemented.\u201d Participants like P02 had no alternative but to reach out to their bottom-up structures to seek assistance, discuss, and reduce gaps in their RAI knowledge. On occasions, such avenues of discussion were non-conclusive. Prior literature in AI/ML and organization studies have shown how such unequal dependence on bottom-up  structures over top-down in deliberation can contribute to tensions, and in turn propose an \u201copen, federated system\u201d linking diferent actors, resources, and institutions to provide a community based support [82, 87]. Discourse: Deprioritized Unfamiliar & Abstract Values. Natu-rally, practitioners tried to solve the superfcial engagement prob-lem by de-prioritizing values that they found unfamiliar. In our study, most practitioners (n=18) said that they were familiar and comfortable talking about RAI values like privacy and security as they were already \u201cestablished\u201d and had \u201cmatured over time\u201d. They sharply contrasted this perceived familiarity with other RAI val-ues like explainability  and robustness. The familiar values were well backed with stable top-down structures and dedicated teams, such as compliance departments and dedicate RAI personnel , mak-ing it easy for practitioners to develop mental models of deeper engagement. P20 shared their experience in this regard in their organization: \u201cThe ideal situation would be like, \u2018Oh, I have certain RAI principles that I want to make sure our product has or addresses\u2019. In reality not all the principles are thought out the same way and applied in the frst go. It usually happens in layers. First and foremost, people will look at privacy because that\u2019s super established, which means everyone knows about it, they already have done probably some work around it, so its easy to implement. And then after that, they\u2019re like, \u2018Okay, now let\u2019s look at fairness or explainability\u2019 . . . We usually have to be quick with turnaround like one or two months. Its nice to bring up values that are new but naturally they also require familiarizing and implementation efort within the team and people see that\u201d Other practitioners (n=3) also followed a similar de-prioritization process for RAI values that they felt were abstract and did not have a measurement baseline (benchmarks) as opposed RAI values that could be easily measured quantitatively against a baseline. An ex-ample observed in this category was the contrast between RAI values like interpretability, which had concrete implementation techniques and measurements (e.g., LIME) and non-malefcence, which did not have a clear implementation technique or measure-ments. Similarly, practitioners (n=2) who went out of their way to understand and suggest new interpretability techniques for model debugging techniques (e.g., Integrated Gradients, SHAP) found it disempowering when their team members often negotiated for easier and computationally cheaper values like accuracy (e.g., P/E ratio) for implementation. Discourse: Value Interpretation Tensions. Even in situations, when diferent practitioners took a similar balanced approach to prioritization, tensions emerged as diferent roles interpreted and contextualized the RAI values diferently during the value delib-erations. We found these tensions occurring within practitioners when diferent practitioners defned RAI values (e.g., equity) and mapped them to RAI features and metric (e.g., skin tone) diferently. P18, a senior data scientist leading an AI team in a non-proft insti-tute, shared one such similar tension among their team members working on the same project, \u201cProbably half of my colleagues do believe that there is a cultural, and historical set of RAI values that can be applied to all the products organization wide. Other half are vehemently opposed to that concept and say that [RAI] values are always model and project dependent. So if you are talking about our long-term goal to establish a set of RAI principles, whose perspectives should be considered?. . . This is an uneasy space that needs careful navigation.\u201d While deliberations might occur between team-members, they might occur within a practitioner, or between the team and the end-consumers of the product/service. Latter two usually surfaced with user-facing roles, e.g., Product Managers or UX Researchers. These roles have the responsibility to understand, internalize, and embody end-user values in addition to their own values. Overall, we found that practitioners in these roles had to invest more efort to tease out their own values from that of the end-users. P04 was a user experience researcher working on interfacing a large language model for natural conversations with users. While P04 was inter-ested in eliciting better insights from the model\u2019s behavior issues (i.e. interpretability [22]), end-users were interested in a simplifed understanding of the model\u2019s opaque behavior (i.e. comprehensibil-ity [22]). A UX Researcher is, however, expected to be the voice of the user in the process. Consequently, they had the constant burden to elicit both sets of values appropriately. Another set of tensions also occurred between practitioners and end-users. P22, an analyst in a fnancial frm, described how ML practitioners perceived RAI values to be mutable and negotiable, al-lowing them to implement a particular RAI value in stages instead of all at once. Such a process allowed P22 (and three other participants who reported similar narratives) to build the required experience and embed the value in the ML model or AI product. However, end-users expected these embedded RAI values as absolute and non-negotiable and not on a \u201csliding spectrum\u201d because they are \u201cthey are often the list of ignored rights\u201d, leading to practitioner-user RAI tensions. Our fndings show that tensions that arose from non-uniform RAI value knowledge and subsequent disparate value interpreta-tions were unproductive and a signifcant obstacle in the overall co-production process of RAI values. This can be attributed to a nascent RAI feld that has given rise to new forms of values (e.g., ex-plainability, interpretability) whose defnitions and contexts which keep changing. This is in contrast with prior value studies in HCI studies where the tensions and conficts around relatively more established values (e.g., privacy) do not occur until the implemen-tation stage [26, 35, 95]. Our fndings show that the majority of value tensions occur much earlier in the value interpretation stage, often contributing to the abandonment of the value discussions altogether. Implementation: RAI Values and Conficts within. Implemen-tation of RAI values was also not a straight forward process as implementing certain RAI values created confict with other RAI val-ues. For instance, P1, an engineer working on classifcation models in VR environments shared how their decision to improve accuracy by excluding instances of objects with sensitive cultural meanings (e.g., objects with LGBTQ",
                    "reference": "[1] Rakshya A. Varanasi and Niyati Goyal. 2023. \"It is currently hodgepodge\": Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). ACM, New York, NY, USA. https://doi.org/10.1145/3544548.3580903"
                }
            ]
        },
        {
            "paper_title": "On the Road to Designing Responsible AI Systems in Military Cyber Operations",
            "authors": "C Maathuis",
            "publication_info": "European Conference on Cyber Warfare and \u2026 - books.google.com",
            "paper_url": "https://papers.academic-conferences.org/index.php/eccws/article/download/204/358",
            "chunks": [
                {
                    "id": "dccf8247-e6fa-4d6f-8275-d6eb8d0526e5",
                    "text": "",
                    "reference": "[1] C. Maathuis. 2022. On the Road to Designing Responsible AI Systems in Military Cyber Operations. European Conference on Cyber Warfare and Security. Retrieved from https://papers.academic-conferences.org/index.php/eccws/article/download/204/358"
                },
                {
                    "id": "8066b3a7-13fc-4475-88f0-9cebdc6c2e52",
                    "text": "\u201cHuman technology starts with an honest appraisal of human nature. We need to do the uncomfortable thing of looking more closely at ourselves.\u201d (Tristan Harris) Old  conflicts  continue  in  different  forms  and  through  new  battles.  These  battles  are  conducted  not  only  on conventional battlefields, but also in the information environment i.e., cyberspace. Therein, different actors i.e., state,  non-state,  and  hybrid  (Maathuis,  Pieters  &  Van  Den  Berg,  2021)  build  skills/force  for  achieving  goals through  effective  strategies.  Over  100  states  can  launch  military  Cyber  Operations  against  adversaries (Maathuis,  Pieters  &  Van  Den  Berg,  2018)  while  having  well-prepared  cyber  commandos  and  units  (Smeets, 2018).  In  this  process,  intelligent  technologies  are  used  at  increasing  rate  and  scale  for  building  intelligent systems to conducting military Cyber Operations (Brantly, 2016). Since AI is a disruptive technology containing a set of multiple technologies, one could say that is aligned with Thomas Edison\u2019s perspective on electricity: \u2018it is a field of fields\u2026it holds the secrets which will reorganize the life of the world\u2019 (Schmidt et al, 2021). Thus, AI changes the world (NATO, 2021) and relationships between humans and machines, diffuses rapidly and broadly (Schmidt et al, 2021), and does these inside and through its natural environment i.e., cyberspace (Hartmann & Giles, 2020) no matter if adaptation to world\u2019s problems can be difficult since human intelligence processes are complex. AI applications for military Cyber Operations are reconfiguring the action of an intelligent-cyber weapon if the state  of  an  exploited  vulnerability  is  changed  by  dynamically  finding  and  exploiting  another  vulnerability, adapting  weapon\u2019s  action  for  limiting/avoiding  unintended  effects  on  collateral  actors  (Cox  et  al,  2019),  or conducting  proportionality  assessment  (Martellini  &  Trapp,  2020).  However,  such  complex  activities  require vast-amounts  of  data,  high  computing  power,  up-to-date  intelligence,  advanced  process  knowledge,  and compliance to the applicable legal-ethical frameworks. Ultimately, the ones responsible for targeting decisions are  military  Commanders,  meaning  that  if  they  knew  or  should  have  known  that  the  weapon  used  would produce massive collateral damage on civilian side, they should be responsible (Hallao et al, 2017). Additionally, AI systems are software-based, thus vulnerable to attack vectors (Reding & Eaton, 2020) through exploiting e.g., unknown software vulnerabilities, unproper communication defence, or failure of critical processes.  Clara Maathuis While the existing body of knowledge contains a rich plethora of studies relevant for grasping ethical aspects in military Cyber Operations, to the best of our knowledge, concrete definitions and assessments of challenges and corresponding  solutions  lack.  This  is  the  knowledge  gap  that  this  research  tackles  through  transdisciplinary research in military Cyber Operations, military operations, AI ethics, and RAI fields where extensive literature review on scientific resources (e.g., IEEE publications/standards) and governmental resources (e.g., NATO and EU-Commission) was conducted focusing on the concepts, methods, and techniques relevant for building and conducting military Cyber Operations. Hence, following research objectives are addressed: 1.  To propose a definition for RAI when building and conducting military Cyber Operations.  2.  To propose an analytical model that captures the entities involved in these processes. 3.  To  structure  and  analyse  challenges  and  recommendations  for  integrating  RAI  systems  in  these processes. The remainder of this article is structured as follows. Section 2 presents related studies that consider diverse aspects for designing RAI systems when building and conducting military Cyber Operations. Section 3 proposes a  definition  for  RAI  and  an  analytical  model  in  military  Cyber  Operations.  Section  4  discusses  challenges encountered when building and using RAI systems and presents recommendations applicable when using them in military Cyber Operations. Section 5 presents concluding remarks and future research ideas.",
                    "reference": "[1] C. Maathuis. 2022. On the Road to Designing Responsible AI Systems in Military Cyber Operations. European Conference on Cyber Warfare and Security. Retrieved from https://papers.academic-conferences.org/index.php/eccws/article/download/204/358"
                },
                {
                    "id": "0bc422e0-bd0f-493e-b591-ed017020c6c4",
                    "text": "Research and practitioner communities formulate relevant questions and seek to build intelligent systems with a good purpose while being aware of their possible negative impact which should be prevented or eliminated when signals of its presence are detected. Hereof, Zhu et al (2022) stress that building and conducting AI-based military  operations  raises  concerns  on  ethical  risks  associated,  thus  critical  from  a  humanitarian  standpoint. Additionally,  the  authors  mention  benefits  like  increasing  accuracy  and  precision  for  decision-making, intelligence  and  targeting  activities:  facts  of  major  importance  in  military  Cyber  Operations.  Furthermore, Hartmann  &  Giles  (2020)  emphasize  that  due  to  increased  data  availability,  computing  power,  and  publicly available  tools,  cyber  offenders  can  use  successfully  intelligent  techniques  that  reach  large  audiences  and produce significant harm e.g., deepfakes and artificial humanoid disinformation campaigns. Thusly, Hallaq et al (2017) envisage that future cyber strategies rely on AI while considering corresponding ethical issues and legal questions.  These  points  call  for  diving  into  relevant  aspects  from  the  ongoing  research  and  practitioner perspectives in the military ethics, AI Ethics, and RAI fields.  Dobos (2020) argues for understanding relevant aspects like power, conflict dynamics, moral conditioning and damage in war context. Furthermore, Finney & Mayfield (2018) point the importance of properly expressing self-awareness and an ethical code of behaviour e.g., the fiduciary duty of military officers when conducting military  operations.  Moreover,  Kaurin  (2016)  analyse  warriors\u2019  meaning  in  contemporary  warfare  which encapsulates warriors\u2019 personal identity, demands on them, and experience: aspects relevant when capturing and  embedding  human  values  when  building  RAI  in  military  Cyber  Operations.  Petrozzino  &  Shapiro  (2020) recommend  the  following  actions  for  achieving  ethical  AI  systems:  i)  creating  ethical  principles  that  drive organizational policies for supporting ethical analysis and open dialogue, ii) creating training and awareness on role-based  AI  ethics  for  iii)  establishing  diverse leaders,  policymakers,  developers,  and  users,  and multidisciplinary AI teams to analyse ethical aspects from multi-stakeholder perspective. Canca (2020) considers that  ethical  principles  are  formed  regarding  autonomy,  beneficence  for  avoiding  harm  and  doing  good,  plus justice. Since responsibility has multiple meanings, Cheng, Varshney & Liu (2021) address it broader through social  responsibility  of  AI  i.e.,  human-value  driven  process  where  values  like  fairness,  transparency, accountability, responsibility, safety, privacy and security, and inclusiveness are the principles, while designing socially  responsible  AI  algorithms  is  the  means.  Peters  et  al  (2020)  propose  two  conceptual  frameworks  for integrating ethical analysis in engineering practices: the first considers integrating wellbeing support and ethical impact  analysis  in  each  engineering  phase,  and  the  second  argues  for  wellbeing  supportive  design  while reflecting and structuring ethical analysis. For managing AI ethical aspects through educating AI systems, Baker-Brunnbauer  (2021)  scrutinizes  that  the  systems  could  be  implicit  ethical  being  forced  preventing  unethical results, explicit ethical by explicitly pointing the actions allowed/not allowed, and full ethical by benefiting free will  and  intention  while  having  consciousness.  Brundage  et  al  (2020)  consider  institutional,  software,  and hardware mechanisms for building trustworthy AI systems: institutional for shaping or clarifying the incentives of  people  involved,  software  for  embedding  or  enhancing  interpretability,  privacy-preserving  aspects  of  AI systems, and hardware for securing hardware systems and processes.  Clara Maathuis IEEE developed the IEEE 7000 \u2013 2021 standards for tackling ethical concerns during system design like the IEEE P7001 on Transparency of Autonomous Systems for developing autonomous systems able to assess own actions and understand decisions made, and IEEE P7002 on Data Privacy Process for managing privacy issues for systems collecting  personal  data  (IEEE  P7000,  2021).  As  Cyber  Operations  are  software-based  activities,  relevant principles, guidelines, and methodologies could be proposed following such standards. Accordingly, EU aims to turn  Europe  into  a  hub  for  trustworthy  AI  as  the  Commissioner  Thierry  Breton  said:  \u201cAI  is  a  means,  not  an end\u2026Today\u2019s proposals aim to strengthen Europe\u2019s position as a global hub of excellence in AI from the lab to the market\u201d (EU Commission, 2021a). Hence, the European Commission came forward with useful programs and strategies  like  AI  strategy,  Coordinated  Plan  on  AI,  and  Data  Governance  Act  (EU  Commission,  2021b). Particularly, the European Commission established seven key requirements for assuring trustworthy AI: human agency and oversight, technical robustness and safety, privacy and data governance, transparency, diversity, non-discrimination  and  fairness,  societal  and  environmental  well-being,  and  accountability  (EU  Commission, 2019). Moreover, NATO (2020a)\u2019s Deputy Secretary General Mircea Geoana argues that \u2018there are considerable benefits  of  setting  up  a  transatlantic  digital  community  operating  on  AI  and  emerging  and  disruptive technologies, where NATO can play a key role as a facilitator for innovation and exchange\u2019. NATO stresses that a dynamic adoption of new technologies like AI and their responsible governance are fundamentally important (NATO, 2020b). From the same angle, the U.S. DoD campaigns for the adoption of AI ethical principles in (non-)combat functions for upholding legal, ethical, and policy commitments in this domain. Accordingly, DoD \u2018will exercise  appropriate  levels  of  judgement  and  care,  while  remaining  responsible\u2019  for  building  and  using  AI capabilities, plus equitable, traceable, reliable, and governable (U.S. DoD, 2020). The  above  discussed  studies  contribute  to  defining  RAI  for  military  Cyber  Operations  and  to  identifying  and analysing challenges and recommendations embedding academic and practitioner perspectives.",
                    "reference": "[1] C. Maathuis. 2022. On the Road to Designing Responsible AI Systems in Military Cyber Operations. European Conference on Cyber Warfare and Security. Retrieved from https://papers.academic-conferences.org/index.php/eccws/article/download/204/358"
                },
                {
                    "id": "28c40ad0-168d-4141-913b-284c77b3615c",
                    "text": "Since the beginning of AI, people were interested formulating questions of not only technical nature, but also ethical trying to propose answers to aspects like its capability to emulate or surpass human intelligence, design choices, and the meaning, scale, and severity of its (mis)use (Russell & Norvig, 2021). AI is changing \u2018the face and pace of warfare\u2019 and could be used responsibly as force multiplier to support military decision-making processes through  accuracy,  precision,  speed,  and  easier  integration  in  other  battlefields  (Meritalk,  2021)  e.g.  target localization  with  network/communication  information  and  access  point  or  even  broader  through  a  common operating  picture,  automatically  detecting  target\u2019s  vulnerabilities  and  building  corresponding  exploits  for efficient  engagement,  and  collateral  damage  prevention  on  civilian  infrastructure  (Slayer,  2020),  or  using intelligent decision making support system for proportionality assessment and targeting decisions (Maathuis, Pieters  &  Van  Den  Berg,  2021).  However,  until  now  responsibility  was  indirectly  tackled  in  military  Cyber Operations through notions like \u2018attack\u2019, \u2018target\u2019, and \u2018proportionality\u2019 mainly through legal lenses. This is the underlying  motivation  of  this  article  as  responsibility  does  not  only  imply  considering,  interpreting,  and integrating principles and norms, but also socio-ethical values when building military Cyber Operations while taking precautionary measures for preventing, containing, limiting, and avoiding unintended effects (Agarwal & Mishra, 2021). Correspondingly, the underlying questions would be: How to build responsible AI-based systems and  solutions  in  respect  to  principles,  norms,  and  values  when  developing  and  conducting  military  Cyber Operations? And, as Dignum (2019) suggests: Who or what should be responsible for AI-based systems\u2019 decisions and actions? Can an AI-based system be accountable for its actions? To find answers for such critical questions, a proper definition for RAI in this domain is required while considering specific characteristics of cyberspace e.g., being able to directly influence and impact other battlefields/domains (Brantly, 2016). Hence, Dignum (2019) calls for a human-centred approach focused on human well-being and alignment with socio-ethical values and principles.  Taking  a  responsible  stance  implies  incorporating  ethics  in  AI  systems  i)  in  design  through  regulatory  and engineering processes that support the design and evaluation, ii) by design through established behaviour of AI systems, and iii) for designers through codes of conduct, regulatory requirements, standards, and certification processes  (Dignum,  2019).  The  author  defines  RAI  as  \u2018the  development  of  intelligent  systems  according  to fundamental principles and values.\u2019 Similarly, Agarwal & Mishra (2021) consider that to assure the applicability, repeatability, and success of RAI systems, corresponding aspects should be integrated during their whole life cycle. Therefore, we formulate the following definition for RAI in military Cyber Operations respecting existing studies (Dignum, 2019; Agarwal & Mishra, 2021; Cheng, Varshney & Liu, 2021; Maathuis, 2022): Clara Maathuis RAI in the military cyber domain = a sub-field of AI that deals with the integration of socio-ethical and legal principles, norms, and values when designing, developing, deploying, and using AI methods, techniques, and technologies embedded in different military cyber systems and processes.  This means that a series of agents communicate and collaborate for building military cyber tools for developing and  conducting  military  Cyber  Operations,  process  depicted  in  an  analytical  model  in  Figure  1  with  its components addressed below (DARPA, 2016; Dignum, 2019; Maathuis, 2022): Figure 1: RAI in military cyber operations Agents: entities participating in the design, development, deployment, and use of RAI solutions in military Cyber Operations. They are further classified considering their position: 4.  Stakeholders: agents involved either in the process of i) design, deployment, use, standardization, and certification of the model i.e., military-legal, military-ethics, AI regulators, and AI ethicists, ii) theorizing, designing, developing, evaluating, upgrading, deploying the model, i.e. AI and military cyber engineers, or iii) design, development, deployment, and use while making sure that the model is compliant with external requirements i.e. AI managers and military cyber decision makers.   5.  Audience: agents involved in stakeholders\u2019 processes i.e., are participatory audience or end-users, so the other audience.  The  RAI  model:  developed  AI  model  by  corresponding  agents  whose  life-cycle  process  and  aim  answers  the following questions: What is allowed to do? and What is right to do? Important to mention is that agents have the  responsibility,  opportunity,  and  power  to  positively  influence  model\u2019s  behaviour  (Galliott,  MacIntosh  & Ohlin, 2020).",
                    "reference": "[1] C. Maathuis. 2022. On the Road to Designing Responsible AI Systems in Military Cyber Operations. European Conference on Cyber Warfare and Security. Retrieved from https://papers.academic-conferences.org/index.php/eccws/article/download/204/358"
                },
                {
                    "id": "dd8a28d7-6076-4c80-bca6-d97c6f67e1d4",
                    "text": "AI became an important strategic topic, and many countries are investing significant budgets in different R&D programs  concerning  upcoming  and  future  trends  and  systems  (EDA,  2021).  Special  attention  is  showed  to building  trustworthy,  accountable,  and  responsible  AI  systems  while  reflecting  on  existing  and  foreseeable challenges  (possibly)  occurring  in  the  lifecycle  of  RAI  systems.  This  is  vital  when  applied  to  military  Cyber Operations  since  would  mean  e.g.,  mismatching  a  target  implies  spreading  an  intelligent  cyber  weapon  in fractions of seconds at global scale and producing massive collateral damage on civilian infrastructure and vital processes,  or  directly  affecting  operational  military  processes  of  friendly  and  neutral  countries.  Thusly,  it  is necessary to understand and assess what are the challenges encountered when building RAI systems in military Cyber Operations, and from there analyse recommendations to tackle them. Hence, we consider the following categories of challenges and corresponding recommendations:  Clara Maathuis Education  and  Expertise:  the  lack  of  expertise,  and  implicitly  education  for  properly  integrating  the  aspects required at technical, social, and ethical levels into the design, implementation, and use of RAI systems in military Cyber Operations, could have (in)direct consequences on the built-system and other (un)related systems. It all begins  with  education,  and  more  exactly,  relevant  and  effective  education.  Then,  the  agents  involved  (e.g., military decision-makers) would benefit from individual and/or collective tailored training and education during mandatory  training,  as  modular  curriculum  when  joining/partnering  with  military  forces,  or  as  exchange curriculum between defence and commercial partners using gaming and simulation tools e.g., VR, AR, digital twins,  or  agent-based  for  target  selection  and  engagement,  that  would  allow  understanding,  capturing,  and learning  human\u2019s  behaviour  and  values  while  building  military  Cyber  Operations  scenarios  for  effectively orienting, understanding, and acting in settings mirroring real-live contexts and environments (Dubber, Pasquale & Das, 2020; Meier et al, 2021; Reding & Eaton, 2020; Slayer, 2020). Data:  while  symbolic  AI  systems  rely  on  knowledge,  non-symbolic  AI  system  rely  on  data.  Accordingly, knowledge  and  data  are  structured,  represented,  and  further  worked  with  as  basis  for  understanding  and tackling existing/future problems and unpredictable events that could occur considering e.g., network dynamism of cyberspace or unpredictable behaviour of AI systems as data might be errored, biased, or manipulated. Such facts  could  alter  AI  system\u2019s  behaviour  through  unknown  backdoors  that  allow  e.g.,  disrupting  own communication systems or improperly localizing a target (Dignum, 2019; Krasev, 2020). Then, aspects like data quality should be assured for solutions with sufficient data, and correctly balancing data e.g., oversampling with qualitative  and  representative  technical  and  human-value  data  for  solutions  with  scarce  data  like  targeting decisions in military Cyber Operations.  Security: the over-reliance on AI systems conducts to an adversarial AI arms race and introduces new types of vulnerabilities (Reding & Eaton, 2020). AI-cyber vulnerabilities reflect combined and even extended cyber and AI risks to systems implemented e.g., data poisoning using open-source data possibly intentionally corrupted used for detecting advanced forms of cyber threats on military cyber systems or intelligent malware that changes its  behaviour  to  be  perceived  as  a  legitimate  behaviour  and  strike  back  into  the  network  from  where  an intelligent cyber weapon was launched (Martellini & Trapp, 2020). As Norbert Wiener said: \u2018We had better be quite sure that the purpose put into the machine is the purpose which we really desire\u2019. Then, intelligent systems able  to  both  strike  and  defend  themselves  through  online  or  hybrid  learning  and  adaptive  behaviour  going through intense verification and testing processes at software, hardware, communication, and human  levels represent a solution. Cyberspace  particularities:  as  these  operations  are  conducted  inside/through  a  multi-cross  domain  i.e., cyberspace  which  is  dynamic,  volatile,  and  still  anonymity-friendly,  then  multi-domain  and  multi-source behavioural and value data are necessary to creating the proper picture to the agents involved in their execution along  with  a  solid  understanding  on  the  processes  involved  and  the  effects  assessed  to  the  policy  makers involved (Branthly, 2016; Slayer, 2020; Maathuis, 2021). Trust:  are  issues  between  humans  while  building  AI  systems  due  to  unclear,  unfair,  or  unexpected  ways  of tackling  the  aspects  and  values  that  should  be  integrated,  and  trust  issues  between  the  humans  and  the  AI systems implying the reliability and power of predictability of AI systems. Hence, too much trust might expose to strong unexpected behaviour and too less trust might imply using too exigent control mechanisms which would still be exposed to unexpected behaviour of AI systems (Martellini & Trapp, 2020; Bartneck et al, 2021). Then, communication and collaboration between the agents involved when building AI systems for conducting military Cyber Operations, are needed while actively integrating in a fair process all their relevant elements e.g., researchers, developers, manufacturers, technologies (Cox, 2019; Dignum, 2019; Maathuis, 2022).  The Tragedy of Metrics: statement that aims to capture and extend the classical meaning of the word \u2018metrics\u2019 by adding socio-ethical norms and values that AI systems should respect (Dignum, 2019). The metrics should consider not only technical and military-(ethical and legal) dimensions when building solutions for conducting military  Cyber  Operations,  but  also  other  social  and  ethical  dimensions  and  aspects  e.g.,  the  context,  aim, environment, human behaviour, rules and regulation.  Governance  and  Regulation:  currently  no  specific/dedicated  regulation  exist  for  building  and  conducting  AI-based military Cyber Operations, and this is necessary as AI is a dual-use technology that requires and impacts not only defence and industry stakeholders, but society as a whole. However, considering the tendency in the Clara Maathuis AI domain and the upcoming awareness in the military domain towards building, using, and assessing intelligent systems on e.g., cognition, interaction, well-being, dedicated incentives in programs for analysing the suitability of current legal frameworks to  intelligent systems, and their interpretation and possible adaptation to them should  be  considered  through  constructive  and  positive  lenses  while  seeing  intelligent  systems  as  artefacts (Dignum, 2019). This implies collaboration between agents involved when building and using AI systems using a human-centred approach, and the further consideration of third-party RAI certification, RAI auditing, and risk management  processes  for  implementation,  testing,  and  approving  AI  systems  while  adopting  specific principles, norms, and values in each phase of the life cycle of AI systems, plus sharing problematic incidents involving RAI systems. This further calls for diplomatic solutions for establishing international dialogue and joint of  forces  for  developing  common/compatible  legal  frameworks  for  RAI  systems  with  defence  and  industry partners respecting frameworks like IHL, Human Rights Law, and societal norms and values (Hallaq et al, 2017; Petrozzino & Shapiro, 2020; Shneiderman, 2020; EU Commission, 2021a ; Schmidt et al, 2021). Design:  since  existing  AI  systems  integrated  in  military  Cyber  Operations  do  not  consider  yet  a  responsible approach,  for  the  upcoming  ones,  to  assure  the  effectiveness  of  their  implementation,  responsible considerations should be integrated using methods like Value Sensitive Design, Data/Design Science Research while  developing  and  adopting  a  code  of  conduct  for  AI  systems  respecting  human  values  and  ethical considerations captured both qualitatively and quantitatively in the design, development, deployment, and use of AI systems (Dignum, 2019; Agarwal & Mishra, 2021; Zhu et al, 2022) while being protective to environment (Galliott, MacIntosh & Ohlin, 2020). This allows translating agents\u2019 values into AI development and establishing concrete  features  like  integrating  conditions  or  duties  for  limiting  civilian  harm,  required  actions  like  target engagement only if the conditions are satisfied, and preferences like system training for a good purpose with realistic cases. Moreover, this allows going back to a particular step if a test case (e.g., bias) fails and update the system (Anderson & Anderson, 2014; Burkhardt, Hohm & Wigley, 2019; Agarwal & Mishra, 2021). Developments:  the  fact  that  the  AI  research  community  is  somehow  divided  between  current  technologies focusing on the now and near-term AI, and future implications and technologies focusing on long-term AI based on AGI and superintelligence i.e., radical transformation of AI, creates a gap between these communities which calls for joint effort for tackling existing and emerging security problems having an eye on near and long-term future (Prunkl & Whittlestone, 2020). Hence, what would that imply and mean for targeting decisions and effects assessment in miliary Cyber Operations?",
                    "reference": "[1] C. Maathuis. 2022. On the Road to Designing Responsible AI Systems in Military Cyber Operations. European Conference on Cyber Warfare and Security. Retrieved from https://papers.academic-conferences.org/index.php/eccws/article/download/204/358"
                },
                {
                    "id": "b0869f1b-0753-4527-9466-1888be407dd8",
                    "text": "Approaching  AI  systems  in  military  Cyber  Operations  through  techno-ethical  lenses  allows  the  stakeholders involved  to  understand  the  difference  between  what  they  have  right  to  do  and  what  is  right  to  do  (Pottery Stewart). In this digital decade (EU Commission, 2019) and further from here since these operations are carried out at fast speeds, in silence, and embed solutions with different autonomy degrees while assessing potential risks  and  taking  corresponding  precautions  (Morgan  et  al,  2018),  it  is  important  to  accelerate  education, investments, democratization, and adoption of AI systems from their design to incorporate relevant norms and values while having realistic military objectives that imply avoiding/limiting harm and embracing good purposes (Maathuis, 2022).  Hence, we present a definition and analytical model for RAI applied in military Cyber Operations, and from there analyse  the  challenges  encountered  by  the  agents  involved  and  further  draw  recommendations  that  would facilitate the adoption, support, and strengthening of RAI systems in military Cyber Operations focusing on their development  and  execution.  However,  as  this  research  focuses  on  the  theoretical  foundation  of  and corresponding  instantiations  of  this  topic,  it  further  argues  for  involvement  of  academic  and  industry communities  for  properly  implementing  AI-based  military  Cyber  Operations  in  respect  to  legal  and  ethical dimensions, and continues by assessing them for their integration in targeting decisions and controlling, limiting, and avoiding unintended effects of military Cyber Operations on military and civilian stakeholders for assuring the design, implementation, and use of trustable, accountable, and responsible intelligent systems having in mind that \u2018humans cannot be everywhere at once,  but software can\u201d (Schmidt et al, 2021).",
                    "reference": "[1] C. Maathuis. 2022. On the Road to Designing Responsible AI Systems in Military Cyber Operations. European Conference on Cyber Warfare and Security. Retrieved from https://papers.academic-conferences.org/index.php/eccws/article/download/204/358"
                }
            ]
        },
        {
            "paper_title": "Responsible AI for Earth Observation",
            "authors": "P Ghamisi, W Yu, A Marinoni, CM Gevaert\u2026",
            "publication_info": "arXiv preprint arXiv \u2026 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2405.20868",
            "chunks": [
                {
                    "id": "a3d0bdc7-fdd2-4cd7-a55b-ca90624041b4",
                    "text": "The convergence of artificial intelligence (AI) and Earth observation (EO) technologies has broughtgeoscience and remote sensing into an era of unparalleled capabilities. AI\u2019s transformative impacton data analysis, particularly derived from EO platforms, holds great promise in addressing globalchallenges such as environmental monitoring, disaster response and climate change analysis. How-ever, the rapid integration of AI necessitates a careful examination of the responsible dimensionsinherent in its application within these domains. In this paper, we represent a pioneering effort tosystematically define the intersection of AI and EO, with a central focus on responsible AI practices.Specifically, we identify several critical components guiding this exploration from both academiaand industry perspectives within the EO field: AI and EO for social good, mitigating unfair biases,AI security in EO, geo-privacy and privacy-preserving measures, as well as maintaining scientificexcellence, open data, and guiding AI usage based on ethical principles. Furthermore, the paperexplores potential opportunities and emerging trends, providing valuable insights for future researchendeavors.Keywords Responsible AI, Earth Observation, Deep Learning, Remote Sensing, Geosciences, AI&EO for SocialGood, Mitigating Unfair Biases, AI Security, Geo-Privacy, Privacy-Preserving Measures, Scientific Excellence, OpenScience, Ethical Principles.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "6d4e22fc-aa09-4e16-ac9a-41c41c42abc1",
                    "text": "In the evolving landscape of EO, the fusion of AI with Earth observation (EO) holds tremendous promise for unlock-ing insights into our planet\u2019s dynamics [1]. Specifically, the integration of AI techniques for EO (AI4EO) significantlyenhances our capacity to distill meaningful information from remotely sensed data [2], which became a substantialsurge since 2020, as shown in Fig. 1. Nonetheless, the complex nature of Earth systems requires a thorough under-standing of AI\u2019s inherent limitations and potential consequences, which necessitates the implementation of responsibleAI practices in EO. This shapes the particular focus of this paper.Fig. 2 shows an overview of the development of AI algorithms over the last decades. The birth of AI as a distinctfield is often attributed to a workshop held at Dartmouth College in the summer of 1956. The concept of responsibleAI gained significant attention in the late 2010s and early 2020s, and with generative-adversarial approaches as oneResponsible AI for Earth ObservationFigure 1: Number of AI4EO-related studies published in IEEE TGRS from 2014 to 2023. Data is collected fromGoogle Scholar advanced search: keywords: (\u201dmachine learning\u201d or \u201ddeep learning\u201d) and \u201dremote sensing.\u201dFigure 2: Overview of the development of AI algorithms over the last decades. The focus of AI algorithms has shiftedmultiple times between being model-centric and data-centric. The blue part illustrates the historical development ofAI concerning data and models, while the orange part reflects our beliefs in the EO community regarding current andfuture developments. The concept of responsible AI gained significant attention in the late 2010s and early 2020s.During this period, concerns about AI biases, fairness, transparency, and accountability began to receive increasedattention.of the catalysts. During this period, concerns about AI biases, fairness, transparency and accountability began to gainprominence, with early influential works such as \u201dGender Shades\u201d (2018) [3] highlighting biases in facial recognitiontechnologies. This era also saw the release of ethical guidelines like the IEEE\u2019s \u201dEthically Aligned Design\u201d (2016) [4]and the ACM\u2019s updated Code of Ethics (2018) [5]. In the early 2020s, regulatory efforts intensified with the EuropeanCommission\u2019s \u201dEthics Guidelines for Trustworthy AI\u201d (2019) [6] and the OECD\u2019s AI Principles (2019) [7]. Majortech companies like Google and Microsoft established dedicated responsible AI teams and initiatives. The conceptof responsible AI continued to evolve, integrating into AI development pipelines and emphasizing both data qualityand model innovation. Standardization efforts by organizations like ISO and educational initiatives further promotedresponsible AI practices, addressing ethical considerations such as fairness, transparency, and bias mitigation. In ouropinion, and as indicated by the light orange color in Fig. 2, we have transitioned from an era of singular focuson either model-centric or data-centric approaches to a new era characterized by balanced data-model approaches.This paradigm emphasizes both model innovation and data equally, exemplified by the rise of foundation models[8, 9]. Despite this shift, we believe responsible AI continues to play an increasingly important role in guiding thedevelopment and deployment of AI technologies. 2Responsible AI for Earth ObservationFrom a technical perspective, the ambition of responsible AI goes beyond theoretical discussions and becomes a vitalaspect of both research and application [10, 11]. Therefore, this paper articulates the use of AI4EO with a particularfocus on responsible practices, providing a comprehensive guide for the ethical design and deployment of AI ingeospatial and environmental contexts. Considering the centrality of human decisions in shaping AI applications, weemphasize the incorporation of domain-specific values at every decision point. As discussed in this paper, responsibleAI requires a systematic process to ensure that ethical considerations are incorporated into the body of AI4EO projects,from data collection to the interpretation of geospatial intuitions.Figure 3: Overview of the main building blocks of Responsible AI in EO: mitigating (unfair) biases, securing AI(defenses, uncertainty modeling, and explainability), preserving (geo)privacy, and addressing ethical concerns outlinethe considerations necessary for implementing responsible AI methodologies within the fields of EO. Social goodpresents the opportunities and goals related to how a responsible AI system can effectively be utilized to make apositive difference in people\u2019s lives.Fig. 3 shows the main considerations of responsible AI in EO, comprising the following five important aspects:\u2022 Mitigating Unfair Bias (Section 2) involves addressing and minimizing biases present in the algorithms andmodels used in AI4EO applications. Unfair bias can arise during many stages of the machine learning work-flow, such as when the AI systems are trained on data that are not representative or when the data used fortraining contain inherent biases.\u2022 AI Security in EO (Section 3) focuses on implementing measures to increase the robustness, control of uncer-tainty and explainability of the AI systems and the data they process in the context of EO missions.\u2022 Geo-Privacy and Privacy-Reserving Issues (Section 4) aims to find a balance between leveraging the benefitsof geospatial data for various EO applications while respecting and safeguarding individuals\u2019 privacy rights.\u2022 Ethical Principles in AI4EO (Section 5) introduces the guidelines and standards that govern the maintenanceof scientific excellence, data availability and the guidance of AI usage in the context of EO research andapplications.\u2022 AI4EO for Social Good (Section 6) refers to applying cutting-edge AI techniques to address and contributepositively to societal challenges and global issues. It aims to leverage advanced AI4EO technologies tofind innovative solutions to pressing problems, promote sustainability, and enhance the overall wellbeing ofcommunities. 3Responsible AI for Earth ObservationFigure 4: A schematic overview of the different biases that play a role during various stages of a machine learningworkflow. Inspired by [16].The main contributions of this paper are as follows: Firstly, we present a pioneering and thorough review of the re-sponsibility of AI research integrated with EO missions for the GRS society. Sections I-IV outline the considerationsnecessary for implementing responsible AI methodologies within the fields of EO, and Section V presents the op-portunities and goals relating to how a responsible AI system can be effectively utilized for social good. This paperemphasizes that responsible AI is not merely a theoretical construct, but an imperative for advancing Earth observa-tion and geosciences. By aligning technological advances with ethical considerations, this discourse aims to contributeto the ongoing dialogue surrounding the responsible and sustainable deployment of AI in the fields of EO and geo-sciences. Additionally, we dedicate a section (Section 7) to showcasing the endeavors of private sectors in integratingresponsible AI into their business models. This effort aims to foster innovation and sustainability within these sectors.Lastly, Section 8 offers key remarks and actionable guidelines for researchers and practitioners to advance responsibleAI practices in EO missions. This review is conducted from the perspectives of both academia and industry, as well asfrom both theoretical and practical points of view, to ensure a holistic view of the subject.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "728cc749-dbe2-41af-9257-4b1cbf4166fc",
                    "text": "Biases are inherent in all machine learning algorithms. Indeed, courses on machine learning often start with the bias-variance tradeoff, where biases occur when a model is under-fitting and are reduced as model complexity increases.This reduction of model bias is paired with an increase in variance errors as the complexity of the model overfitson variances in training data which are not representative of the target class in general. \u201dBias\u201d is also a term linkedto the systematic errors of an algorithm or dataset. Despite the concept of bias being inherent to machine learning,nowadays there is much concern for biases present in algorithms. Indeed, a review of Responsible AI guidelinesand recommendations concluded that the principles of fairness and non-discrimination should be included in all AIguidance documents [12]. The difference here is that society is particularly concerned about \u201dunfair\u201d biases capturedin machine learning algorithms. Generally speaking, this arises where the model systematically produces differingoutcomes for specific subgroups of a class in a way that causes societal concern that is \u201dunfair\u201d. The unfairness of themodel is, therefore, contextual and closely linked to the values of a society. Friedman and Nissenbaum [13], thus, referto biases that cause unfair and systematic discrimination of individuals or sub-groups of the population. Preventingdiscriminatory behavior is also considered to be the responsibility of the developer and user of AI algorithms. TheUNESCO Recommendations on Ethical AI adopted by all member states in 2021 states: \u201dAI actors should make allreasonable efforts to minimize and avoid reinforcing or perpetuating discriminatory or biased applications and out-comes throughout the life cycle of the AI system to ensure fairness of such systems\u201d [14]. Similarly, the EuropeanCommission is drafting an AI Act that incorporates a requirement to consider and report potential biases [15]. Under-standing how to audit AI workflows for biases and how to mitigate them are expected to be two key fields of researchin the coming years. 4Responsible AI for Earth Observation",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "127338e1-7e84-4dd0-b33b-ead4c9bc89d7",
                    "text": "To understand how to identify and mitigate biases, it is important to consider how they emerge at various steps in themachine learning cycle [16\u201318]. Following the framework by Suresh and Guttag [16], six types of biases that emergein a machine learning workflow are discussed.1. Historical bias, which arises because field data captures phenomena that have already occurred in reality. Ifthe state of the world changes in the future, the model may not correctly capture those changes. For example,meteorological models trained on historical data may not perform well in the future if extreme events becomemore prominent [19].2. Representation bias, perhaps the most well-known type of bias when considering fairness. It concernsthe way that the real world is sampled to generate the dataset used to train and evaluate the machine learningmodel. Representation bias occurs when certain parts of the population are under-represented in the sampling.3. Measurement bias, induced as complex phenomena are defined in simpler concepts during dataset creation.For example, when predicting poverty through EO, poverty is often defined narrowly (e.g., percentage of thepopulation below a defined poverty line) rather than considering broader aspects such as access to housingand sanitation [20].4. Aggregation bias, where subgroups of a population are aggregated into a single model instead of generatingdifferent models for distinct sub-groups.5. Evaluation bias, stemming from the evaluation set used to assess model performance. Unequal represen-tation of classes in the evaluation set, as well as the use of unsuitable performance metrics, can induceevaluation bias. For example, overall accuracy as an evaluation metric may be unsuitable for classificationproblems with unbalanced classes in remote sensing.6. Deployment bias, arising from inappropriate interpretation and usage of the model results for decision-making. This can occur when model results are misinterpreted or applied in contexts for which they were notintended.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "8fbc4610-5c92-499a-b1da-6a02087d50cc",
                    "text": "Two main streams of research can be considered in addressing biases in AI workflows: auditing for biases and imple-menting mitigation measures.Auditing a workflow for biases involves identifying causality, such as determining whether a sensitive attribute causeschanges in the predicted variable, or examining statistical disparities between accuracy metrics of predictions for sub-groups defined by sensitive attributes [21]. For example, one might investigate whether building detection algorithmsperform equally well in high-income and low-income neighborhoods of a city [22]. Calculating statistical disparitiesinvolves defining a sensitive attribute and determining which fairness metric is appropriate for the application. How-ever, selecting a suitable fairness metric can be challenging as mathematical definitions of fairness may conflict [21,23]. Interested readers are encouraged to explore resources on similarity-based metrics to understand the differencesbetween metrics and how to select an appropriate one based on the application.Bias mitigation can be applied at each step of the workflow individually. A recent review on data-centric learning inthe field of GRS [24] highlights the importance of assessing dataset suitability throughout various stages of machinelearning workflows. The review discusses how model bias can be addressed at each of these stages.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "811c4b4a-90c6-4f3f-b75b-948e76bc31d1",
                    "text": "There are several challenges specific to detecting biases in GRS applications.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "8b17c8a7-8b76-408f-9f09-df467ffb9170",
                    "text": "One challenge lies in defining the sensitive attributes. Societal concerns about which sub-groups should be treatedfairly are often connected to demographic and socio-economic characteristics of individuals or groups. The UNESCORecommendations [14] describe fairness according to various categories such as age groups, cultural systems, lan-guage groups, persons with disabilities, girls and women, disadvantaged, marginalized and vulnerable populations,rural versus urban areas, and more. Similarly, the EU AI Act [15] is founded on Article 21 of the EU Charter ofFundamental Rights, which outlines non-discrimination related to sex, race, color, ethnic or social origin, genetic fea-tures, language, religion or belief, minority status, age and nationality, among others. Most of these characteristicsare not included explicitly in the data utilized for GRS applications, with the exception perhaps of the urban-rural5Responsible AI for Earth Observationattribute mentioned by the UNESCO Recommendations. However, indirectly, these characteristics are often inferred.For example, there is extensive research on the identification of slums in satellite sensor imagery [25, 26]. Althoughthe image does not contain information on poverty directly, the characteristics of the built-up environment have anindirect relation. Similarly, mobility data obtained through mobile phones may overrepresent higher income groups[27]. Therefore, despite not explicitly listing \u201dsensitive\u201d attributes in the data, geospatial and EO data are implicitlylinked to these attributes. A significant challenge is to identify which sensitive attributes could be inferred from thedata used to train and evaluate a machine learning model.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "872dc5c0-7ed8-471a-98e5-5b070e4bbc2d",
                    "text": "A second challenge is that of spatial autocorrelation, which is specific to the manipulation of spatial data and isoften overlooked when applying methods developed in computer vision directly to geospatial data. Karasiak et al.(2022) demonstrated how the spatial dependence between training and testing samples causes an overestimation ofmodel accuracy [28]. Other studies indicated how spatial autocorrelation plays a role in estimating the relationsbetween air pollution and asthma [29] and in the estimation of ecosystem services [30]. However, auditing for spatialcorrelation biases is underrepresented in the broader literature on Responsible AI in machine learning. It is expectedthat identifying and mitigating biases in models will be a particularly active direction of research in the fields of GRS.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "cfff2acd-47c6-43c1-bd67-c149b111a86d",
                    "text": "Table 1: Summary of the 3 AI security concerns including adversarial examples, uncertainty, and explainability.In the context of EO, ensuring AI security is vital for upholding responsible AI principles and safeguarding againstpotential risks and vulnerabilities. EO systems rely increasingly on AI technologies for tasks ranging from imageanalysis and classification to predictive modeling. However, the integration of AI introduces security concerns suchas data integrity threats, adversarial attacks on models, and privacy breaches. Addressing these challenges requires acomprehensive approach that considers both the technical aspects of AI security, such as explainable architectures anduncertainty modeling techniques, and ethical dimensions, including transparency, accountability and the protection ofsensitive information. By incorporating AI security measures into the development and deployment of AI4EO systems,we can foster trust, reliability, and societal benefit while mitigating potential harms and ensuring that AI technologiescontribute positively to environmental monitoring, disaster response, and sustainable development efforts. Whileaspects such as ethical dimensions, mitigating unfair biases, and preserving (geo)privacy are largely explained inSections 2, 4, and 5, we focus on uncertainty modeling, adversarial defenses, and explainability in this section.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "ba4795ae-5edc-4156-89fe-abca4ce81a71",
                    "text": "Table 1 illustrates the primary concerns regarding AI security in GRS, encompassing adversarial attacks, the uncer-tainty of AI predictions, and the explainability of black-box models. These significant concerns will be addressed inthis section. While this paper aims to emphasize concepts, we employ basic equations in this section to define AIsecurity concerns.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "90213b5c-f041-4d71-b094-9ffb61acf2df",
                    "text": "The identification of adversarial examples in AI4EO tasks [32, 33] has forced research into adversarial attacks ondeep neural networks (DNNs). By introducing imperceptible perturbations to the data, these adversarial examples can6Responsible AI for Earth ObservationFigure 5: Illustration of adversarial attack: a minor perturbation can fool the classifier into wrong predictions. [31]deceive well-trained DNNs, leading to substantial performance degradation, even in state-of-the-art AI approacheslike image classification and semantic segmentation [34]. Since the deep learning-based AI4EO methods minimizeloss functions as an object to train the models, adversarial attacks employ the idea of maximizing the loss functions tofind a perturbation to contaminate the clean samples, which can consequently fool well-trained AI4EO algorithms. Asa result, this behavior severely threatens the responsibility of the current AI4EO approaches by making them predictincorrect results with high confidence.Adversarial attacks can appear virtually and physically when applying deep learning models. On the one hand, theattackers can directly modify the applied remote sensing data before they are forwarded to the application models. Forexample, Wu et al. [35] demonstrated that the current end-to-end autonomous driving system could be easily broken bytempering the data with mild perturbations generated by white-box attacks. On the other hand, physical attack methodsare more imperceptible since the adversarial perturbations are introduced during the remote sensing imaging process[36]. Zhang et al. [37] explored the physical attack in real-world AI4EO applications by applying an adversarial patchattack on the multi-scale object detection for UAV remote sensing images. As a corollary, responsible AI is required tomitigate the potential threats of these adversarial perturbations. Adversarial defense methods are desired that improvethe reliability and response of the model, which are introduced in a later paragraph in this section.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "3c04dae7-1128-4bf7-a020-d444ce28e6d7",
                    "text": "GRS data, often sourced from varied platforms, exhibit diverse domain distributions, introducing uncertainty in AImodel predictions, as described in Section I. The stochastic nature of machine learning models further amplifies thisuncertainty, posing a significant challenge to prediction reliability [38]. Recognizing and managing uncertainty is vitalfor the development of responsible AI4EO models. The application of AI in GRS data involves distinct training andtesting phases to optimize a mapping function f : X \u2192 Y, where \u03b8 represents the initialized parameters. To this end,uncertainties may arise in the samples of training and testing datasets, as well as in the model parameters, categorizedinto data and model uncertainty [39, 40].Lack of domain knowledge is a prominent factor contributing to data uncertainty, arising from the disparate domaindistribution observed between the training dataset X and the testing dataset X . In remote sensing imaging, spectralfeatures of observed data are closely tied to spatial and temporal conditions, such as solar illumination, season, andweather. Consequently, variations in these conditions can markedly influence imaging outcomes, resulting in hetero-geneous data with diverse domain distributions, a phenomenon known as domain invariance [41]. Regrettably, existingAI approaches struggle to make accurate predictions with a decision boundary that accounts for data exhibiting domaininvariance, often referred to as domain shift [42]. This poses a significant threat to model performance, particularlywhen faced with data uncertainty in testing samples during the model deployment stage. In attempts to mitigate do-main shift issues, numerous studies in geoscience and remote sensing have proposed methods to adapt and extend thedecision boundary of classification models to encompass unknown data in inference samples [43, 44]. However, thesemethods can only fine-tune models to a limited extent, and the data uncertainty associated with domain invariancecannot be eradicated entirely. In addition to data uncertainty, model uncertainty encompasses the errors and random-ness inherent in the model parameters \u03b8, initialized and optimized during the model design and training processes.As researchers strive to develop advanced AI algorithms with cutting-edge performance and efficient training, diversemodel architectures linked with optimization strategies have been devised for various GRS applications [45]. However,determining the optimal model architecture and training hyperparameters remains challenging, introducing uncertaintyinto the trained model that can subsequently impact predictions, commonly referred to as model uncertainty [46].",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "33a71920-9cd4-40f7-bdba-1e3232923652",
                    "text": "As high-performance computing technology advances, contemporary AI applications in geoscience and remote sens-ing studies explore models with deeper architectures and more complex parameters to enhance classification perfor-mance. However, the increased complexity raises challenges in understanding and interpreting the internal operational7Responsible AI for Earth Observationsteps of these architectures, rendering the models akin to black boxes [47]. Compared with traditional machine learn-ing approaches, the opacity of these black box models hinders the identification of hidden security hazards that cansignificantly impact classification, as well as the reasons for \u201dwrong\u201d predictions in real-world applications [48].Moreover, the growing emphasis on data privacy and high confidentiality in many GRS tasks underscores the need fortrustworthy AI models aligned with ethical and judicial requirements for both developers and users.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "bc7a56f6-0948-4b40-a810-fb53911ba696",
                    "text": "In response to the identified security hazards outlined in the preceding section, researchers are working towards thedevelopment of more robust AI models, emphasizing enhanced responsibility for AI security. Several strategies arebeing employed, including adversarial defense, uncertainty quantification and explainable AI, to address challengesrelated to adversarial examples, data and model uncertainty, and black-box models, respectively.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "463ea460-1499-4790-9b48-b34d55b42e78",
                    "text": "Common adversarial defense approaches include adversarial training, randomization, detection, and adversarial purifi-cation methods. Adversarial training methods aim to bolster the model\u2019s robustness against adversarial examples byincorporating them directly into the model training stage alongside normal training samples. For example, Goodfellow[49] proposed an empirical adversarial training scheme involving the inclusion of extra-generated Fast Gradient SignMethod (FGSM) attack adversarial samples. However, it is important to note that adversarial training methods mayenhance the model\u2019s robustness against specific attack methods, yet vulnerabilities against adversarial examples ofother types may persist.Given that adversarial perturbations can be perceived as additive noise on adversarial examples, randomness methodsintroduce random components to enhance the model\u2019s robustness. For example, Cohen developed a randomizedsmoothing technique for adversarially robust classification involving the randomization of the input to the DNN toeliminate potential adversarial perturbations in the input samples [50]. However, it is important to note that theperformance of randomness methods depends greatly on the quality of the randomization process and can be affectedsignificantly by both the lack of prior knowledge of attack types and theoretical explanations.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "bfebe154-f734-4a77-b5b3-ad0c3ea379dc",
                    "text": "To better comprehend and manage potential uncertainty, uncertainty quantification techniques have been proposed toassess the credibility of predictions.In many GRS applications, classification results manifest as distributions of possibilities associated with object classes.These distributions are typically transformed by a softmax function at the final stage of the network. Grounded in theprinciples of Bayesian inference, the data uncertainty of an input sample x can be described as a posterior distributionover the class label y given a set of model parameters \u03b8. Similarly, the model uncertainty can be formalized as aposterior distribution given the training dataset D = (X , Y), as expressed by the equation:P(y|x , D) = (cid:90) P(y|x , \u03b8)(cid:124) (cid:123)(cid:122) (cid:125) .P(\u03b8|D)d\u03b8(cid:125)(cid:123)(cid:122)(cid:124) (1)Building upon this equation, various uncertainty quantification approaches aim to obtain an uncertainty distribution \u03c3of model predictions y by marginalizing \u03b8. These methods fall into two broad categories: deterministic methods andBayesian inference methods, differing in subcomponent model structures and error characteristics. In deterministicmethods, the parameters of AI models are deterministic and remain fixed during the inference step, aligning withthe common behavior of most developed AI models. In contrast, Bayesian inference methods employ the Bayesianlearning strategy, leveraging the ability to seamlessly integrate scalability, expressiveness, and predictive performanceof neural networks for uncertainty quantification [51, 52].",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "8352f6de-c8d9-46f4-8dc8-2028ea8352a8",
                    "text": "In general, explainable AI (XAI) aims to provide an understanding of the pathways through which output decisionsare made based on the parameters and activations of the trained models. To this end, many XAI approaches havebeen proposed for a comprehensive understanding of the model behavior to justify the individual feature attributionsof a test sample x \u2208 X , which provides a powerful tool to examine incorrect predictions and inspect the potentialthreats of black-box models, especially in high-risk GRS applications [53]. For example, Ribeiro et al. [54] devel-oped a Local Interpretable Model-agnostic Explanation (LIME) method to provide understandable representationsfor the predictions of black-box classifiers by highlighting attentive contiguous superpixels of the source image with8Responsible AI for Earth Observationpositive weight towards a class, which benefits users in learning \u201dhow the model thinks\u201d when classifying a specificimage. Furthermore, the deep features of DNNs can also be processed into visualizations to observe how the trainingprocedure works on specific tasks, as shown in Fig. 3.2.3.Figure 6: Heatmaps of UNet decoder block with different CAM-based XAI algorithms for the class of \u201dbuildings\u201din the semantic segmentation of an RS image. (source from [55]). Pixels that have a brighter color indicate a higherprobability of being classified into the target class.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "696421ad-99d7-4b7a-b17a-052aa5676c0f",
                    "text": "In this section, we discuss the future perspectives of AI security for EO tasks from three perspectives:",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "41eaa85f-b959-4ee5-8dc4-49b8697ec796",
                    "text": "While adversarial examples have been identified widely in several EO applications, research on adversarial attacksand defense algorithms has been developed and raised the debate in game theory. On the one hand, the attackershave proposed more powerful attack algorithms to affect a wider range of DNNs applied in various EO missions,including scene classification and semantic segmentation. On the other hand, the defense group has also identifiedthe emerging adversarial threats and tended to secure AI algorithms from more complex adversarial examples withfrontier techniques, such as diffusion models [31]. In conclusion, the game between adversarial attack and defensealgorithms has been started and extended to more subtopics in the AI4EO research field [56].3.3.2 Practical Uncertainty Analysis for AI4EOThe uncertainty in both model and data towards AI4EO missions has been a critical obstacle for developing AI modelswith more reliability and responsibility. Although some uncertainty quantification methods have been seen in geo-science and RS studies, most of them were designed for dedicated models and are not adaptable to most of the existingAI4EO approaches. To this end, universal uncertainty analysis with greater capability to be applied in more typicalDNNs should be developed for the AI4EO missions.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "f96c503e-dfc9-411c-8cdb-0c1fa2410ab0",
                    "text": "Explainability is considered to be a significant attribute of modern AI4EO missions, which provides the opportunityfor humans to learn the intrinsic process of the AI algorithms, security risks can be largely alleviated. However, currentXAI methods designed for EO missions can only generate visualization interpretations, which are usually semanticallymisaligned and still too abstract to understand directly. Therefore, future XAI studies should focus on increasing thedepth and accuracy of XAI results by introducing more constraints and optimization problems to explanations.9Responsible AI for Earth Observation",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "3e4163a3-cef3-4cc8-ab85-75c51e459d73",
                    "text": "Remotely sensed images and geospatial data offer unprecedented opportunities to monitor our planet and supportspatial planning, management and decision-making. Nevertheless, as remote sensing technologies evolve, the need tobalance the advantages of data-driven decision-making with the protection of individual privacy becomes paramount.The increasing resolution and quality of remotely sensed data, with their ability to capture detailed information aboutthe Earth\u2019s surface and (directly or indirectly) the people living in those areas, raises concerns about the potentialmisuse of data and the accidental revelation of sensitive information [57]. AI technologies allow the scale-up ofgeospatial applications, reducing analysis costs and subjectivity and, at the same time, magnifying the risk of exposingsensitive personal information.Effectively addressing the above challenges requires thoughtful consideration of ethical principles, legal frameworks,and technological safeguards to ensure that the benefits of geospatial advances are realized without compromising in-dividual privacy rights. Earlier research endeavors aimed at constructing a conceptual privacy framework; for example,[58] identified seven different concepts of privacy by examining distinct emerging technologies.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "4d0aaaa3-929f-49d5-b04a-18a9b9b1f654",
                    "text": "In remote sensing, privacy concerns often arise when the resolution of an image is sufficiently fine to discern individu-als\u2019 identities and expose personal details. This scenario commonly occurs during the analysis of images captured byUnmanned Aerial Vehicles (UAVs), potentially encroaching upon: (1) Location privacy, as individuals can be iden-tified and tracked within UAV images. (2) Behavior privacy within private settings, free from external surveillance.(3) Space privacy, revealing details about secluded areas like backyards. (4) Association privacy, disclosing groupmemberships and affiliations. (5) Data and image privacy, involving individuals\u2019 control over images featuring theirpresence.Gevaert et al. [59] evaluated the societal impacts of using UAVs for informal settlement mapping through two casestudies in Eastern Africa. The study stresses the importance of notifying citizens of the purpose of data collection,the rights to access, data processing, and distribution, especially when flying over marginalized communities. Theabsence of a cohesive policy framework governing such activities shifts a significant portion of responsibility ontoindustry self-regulation, potentially falling short of adequately safeguarding marginalized communities. Nevertheless,the existence of a \u2018unified\u2019 framework is challenged by the specific application of UAV image acquisition and the factthat the concept of sensitive information or privacy may vary among people, groups, and cultures. The study illustratesthe importance of considering the local context regarding privacy issues. Another example application with potentiallyprivacy-related concerns is the use of UAV images in support of first responders in emergency situations (e.g., afteran earthquake or other types of disasters). The authors in [60] developed a deep-learning network to detect victimsin UAV images, which is trained to detect humans even in complex situations where body parts of the victims arepartially covered by dust or debris. However, collecting a dataset for training such a network is challenging because ofthe sensitive nature of the data. Therefore, the authors proposed a framework to generate a harmonious composite ofimages for training. The framework first pastes images of body parts onto a debris background to generate compositevictim images, then uses a deep harmonization network to make the composite images more harmonious.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "f0075bbc-a6da-474d-850d-7bfec57dd2ed",
                    "text": "Besides decimeter/centimetre-resolution UAV images, privacy concerns may also arise with satellite sensor imagesof various resolutions, especially when applied to map societal and socioeconomic features. Examples of sensitiveapplications include the mapping of informal settlements, slums, deprived urban areas [61\u201363] and refugee camps[64]. The peril associated with disclosing sensitive information is evident in the \u201dMass Atrocity Remote Sensing\u201dinitiative, which sought to furnish forensic evidence of crimes against humanity through the analysis of satellite sensorimagery and other datasets. Nevertheless, researchers soon recognized that these data could also be exploited toidentify and harm individuals and groups. Publishing sensitive maps should always be done with care to preventunintended consequences for local communities. In the case of informal settlements, being classified as living in sucha neighborhood may result in social exclusion or even exposure to eviction (e.g., see the reports of forced evictionsof 30,000 people from Nairobi\u2019s largest slum, Kibera, in 2018 for building a new road [65]). Measures for privacypreservation typically include downgrading map spatial resolution (e.g., to cells of 100\u00d7100 m), aggregating data atthe administrative unit level, and removing geo-location from datasets.Another strategy to preserve household privacy is to apply jitter to geospatial point data (adding random noise togeocoordinates). The authors of [66] developed a comprehensive and openly accessible collection of microestimatesdetailing the distribution of relative poverty and wealth across 135 low- and middle-income countries (LMICs). They10Responsible AI for Earth Observationgathered data from Demographic and Health Surveys (DHSs) through traditional face-to-face surveys conducted with1,457,315 distinct households residing in 66,819 villages spread across 56 different LMICs globally. The DHS datacontain approximate GPS coordinates indicating the centroid of each surveyed village among the 66,819. However, tosafeguard privacy, the precise geocoordinates are obfuscated, with a margin of error of up to 2 km in urban areas andup to 5 km in rural areas.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "378a0df2-4dfb-410c-9f23-612bb1cb1e6f",
                    "text": "A recent challenge is the increasing availability of satellite sensor images with sub-meter resolution up to 30 cm (e.g.,WorldView-3, Pleiades NEO, Skysat) able to capture potentially sensitive information. The potential of such data formapping and object detection should, therefore, be counterbalanced with privacy-preserving measures. Moreover, itis worth noting that privacy requirements may clash with open science principles, restricting the possibility of openlypublishing and benchmarking datasets.Future investigations should develop strategies to openly share data for scientific advancements while preservingsensitive personal or group information. Such strategies are much needed in applications such as mapping deprivedurban areas or victim detection, where further advances in the development of AI-based methods critically depend onthe availability of large benchmark datasets, which are currently missing.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "16fb4eee-4a3f-42af-b521-b8647dfa9ca2",
                    "text": "The real-world character of AI4EO applications with downstream political, societal, and environmental impacts offerstremendous opportunities for positive change while simultaneously bearing the risk of negative impacts when notcarefully designed and implemented [67\u201369]. Maintaining the highest scientific standards and making data and codeopenly accessible while complying with ethical principles are essential prerequisites to enhance the transparency ofknowledge production, ensuring high reproducibility of scientific work and maximizing the societal, economic orenvironmental benefits of AI4EO research, development and applications [67]. Compliance with fundamental ethicalprinciples is, thus, a major duty of the scientific community.A series of review papers laid out the relevant definitions and concepts of ethics in AI. For example, Jobin et al. (2019)[70] identified five broad categories from ethical guidelines in AI research: transparency, justice and fairness, non-maleficence, responsibility and privacy. With a focus on AI4EO, Kochupillai et al. (2022) [71], layed out six ethicaldimensions: privacy, honesty, integrity, fairness, responsibility, and sustainability (for an overview on recent AI ethicscategorizations, see Table 2 in [71]). We here revisit the six categories proposed by [71] focusing on aspects thattranscend the themes of maintaining scientific excellence, open data, and guiding AI usage in AI4EO, that is, directlyor indirectly relate to workflows, from defining objectives and research questions, data inputs and analysis outputs andtheir downstream impacts.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "ec195644-d264-4aa0-a253-d8654bae2f69",
                    "text": "In the context of responsible AI4EO, we situate privacy concerns arising under the open data paradigm and with regardto (territorial) stigmatization. Open access to datasets and code is a core element of scientific reproducibility. Scientificexcellence in AI4EO can, thus, only be achieved when the public dissemination of essential research data and codeare made available to enable full reproducibility and robustness checks [72]. The scientific community has pushedfor improving the Findability, Accessibility, Interoperability and Reuse of digital assets, which materialized in theFAIR-principles [73]. Increasingly, however, the publication of geospatial data, including UAV or fine to very-fine-resolution satellite sensor imagery, labeled datasets, or the outputs of AI4EO workflows, may raise privacy concerns[74], see 4). The FAIR-principles demand data to be free from use restrictions, fees or embargos, but legal, moral, orethical reasons, such as safeguarding individual privacy, or endangered species, may justify deviation from these bestpractices on data accessibility [75]. However, detailed metadata should be made available enabling the discovery ofsuch data and describe explicitly the conditions under which data access can be granted.Trade-offs between the benefits of open data and privacy concerns therefore need to be evaluated case-by-case toavoid privacy-violating or other ethically concerning impacts of public data dissemination. Besides the techniquesto obscur geolocation information described above, a selective disclosure of data upon requests may be justified toavoid risks of data misuse. Additionally, purpose-specific data licensing may provide opportunities to avoid unin-tended use of datasets. One example for purpose-oriented data releases is Norway\u2019s International Climate and ForestsInitiative (NICFI) Satellite Data Program, providing commercial satellite imagery covering the world\u00b4s tropics fornon-commercial sustainability-centered applications (https://www.planet.com/nicfi/).11Responsible AI for Earth ObservationStigmatization may occur in cases where urban neighborhoods are categorized as slums [76, 77], or categorizations ofcrime rates or poverty levels adversely affect public perceptions of a community [78]. The resulting territorial stigma-tization may influence decision-making and negatively affect the local population. Similarly, stigmatization applies toAI4EO applications failing to consider the realities on the ground, for example, designed to expose illegal activities,such as deforestation, fire use, or mining activities, without knowledge about potential concessions to conduct therespective activity. Similarly, stigmatization may occur when global or broad-scale analyses fail to sufficiently con-sider local realities. For example, small-scale farming has been found to be responsible for 97% of deforestation onthe African continent [79], but regional dynamics such as the rapid emergence of medium-scale commercial farmingoperations [80, 81], the increasing role of export-oriented commodity crops such as cocoa, cashew and oil palm indeforestation [82], and trade-offs between environmental degradation, food production, livelihoods and labor oppor-tunities in semi-subsistence agriculture [83, 84] allow for a more nuanced perspective of such dynamics. As such,avoiding stigmatization of people, communities, or localities requires a high level of awareness of researchers in thedesign of research questions and label heuristics, as well as embedding scientific findings and their limitations in therealities of the region under investigation [85]. We highlight that cooperation with regionally knowledgeable partnersin the early project design phase is an efficient way to avoid pitfalls of stigmatization in responsible AI4EO [86].",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "87a01a7e-970d-47fa-abae-ca126dd97b9e",
                    "text": "Truthfulness and trustworthiness are inherent to scientific inquiry, but the current replication crisis undermines thecredibility of science and, thus, hampers the societal uptake of scientific outputs. As a community producing knowl-edge with real-world impact, responsible AI4EO requires transparency, explainability, and data veracity [67].In AI4EO, transparency not only requires thorough documentation and referencing of the datasets and workflowsused but also that need to understand and clearly communicate ambiguities, potential biases, and errors, or conceptuallimitations. In light of the ongoing democratization of EO data and workflows through cloud platforms, and accessibleuser interfaces, empowerment through access to EO data requires a thorough understanding of the limitations of EOdata and workflows. Furthermore, transparency enables critical reflection of scientific findings. One example is arecent study linking the observed emerging abilities of LLMs (sudden, unexpected increases in performance) to studydesign choices made by researchers (e.g. evaluation metrics) [87].Assuring explainability in AI4EO continues to be challenging, but is a prerequisite for early diagnostics regardingunwanted bias and sources of error. We strongly encourage the community to conduct research on XAI, and incor-porate it, wherever possible. Focusing research on explainable AI4EO will contribute to the recognition of EO dataas a distinct modality in the AI community at large, by better understanding how AI models leverage the informationcontained in EO data [68]. Greater explainability may also profit the emerging sub-field of self-supervised learning[88], and help to ensure oversight of (geographic) biases in AI4EO foundation models which claim high degrees ofgeographic generalization, but which are trained in selected world regions [89]. Such biases are, for example, presentin LLMs [90], and may lead to error propagation to many downstream applications [91].Data veracity in EO concerns both the imagery and the EO data pre-processing workflows used, as well as referencedata / labels being in accordance with a (non-stigmatizing) heuristic defining the objects of interest. Using well-established and tested processing pipelines to assure the highest possible quality of EO data, for example, in terms ofatmospheric correction [92], but also regarding QA flags, including cloud masks for optical data [93], is recommendedto avoid downstream errors related to poor data quality. Labeling (the process of categorizing real-world phenomenainto distinct classes or groups) is highly context-specific. Therefore, it requires regional expertise and knowledge, isideally coordinated with local experts, and can be considered an iterative process in which exchange of knowledgefrom various disciplinary lenses is an effective means to reduce the risk of false categorizations or stigmatization [85].",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "31527371-fb30-4a54-bfd5-eb67ae3996e4",
                    "text": "Technical robustness and uncertainty reporting are key elements of integrity in AI4EO. Detailed documentation andreporting of the employed processing pipelines, including the respective processor version, enhances the reproducibil-ity of AI4EO analyses. Detailed reporting of biases, errors, and uncertainty adds credibility and is detrimental to avoiderroneous use of datasets in downstream applications. Given the increasingly close interactions between the ML andEO communities, the maintenance and use of well-established evaluation standards particular to the EO domain, suchas in reporting map accuracies and area estimates in categorical settings [94], or for detecting trends in spatiotemporaldata [95] is key.In AI4EO research, a focus on single aggregate evaluation metrics (e.g. overall accuracy), as discussed in Section 2,is insufficient, as often the detection of minority classes is most relevant in many AI4EO applications. Generally, andfor imbalanced problems in particular, reporting area-adjusted class-specific user\u00b4s and producer\u00b4s accuracies from12Responsible AI for Earth Observationan independent and randomized sample must be preferred over simplistic one-dimensional statistics. Similarly, classarea estimates are to be derived from reference samples rather than pixel counting in maps with non-random errordistribution. For continuous targets (e.g. reflectance, biomass, crop yields, canopy height), reporting multiple errormetrics and assessing them across the entire value range reveals heteroskedasticity and can inform downstream usersof potential pitfalls when using a product [96].",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "8fc2f312-846f-494e-a7cd-29fe7c84630b",
                    "text": "Fairness here refers to avoiding bias, and to respecting all aspects of diversity and sociocultural differences, ultimatelycreating standards based on principles related to fairness. Bias may relate to all stages of AI4EO analyses, beginningwith defining a research question, through the workflow itself, and ending with the manifold downstream impactsrelated to interpreting the results - as laid out in 2. We will not delve deeper into the broader ethical frameworkdiscussed elsewhere [71, 97]. Instead, we focus on core needs, before, during and after setting up an AI4EO workflow.Before setting up an AI4EO workflow, we may consider opportunities to invite potential stakeholders of anticipatedoutcomes to participate in co-creating the research agenda [98]. Stakeholders typically include different institutionsacross all levels, from village heads in an area of interest to the United Nations or the World Bank. Stakeholders mayalso be citizens who are acquainted with the topic or the area of interest, ideally (but not mandatory) being engagedin related citizen-science opportunities [99], such as data sampling or field-based mapping evaluation [100, 101].Citizens and institutions acquainted with the research questions will help to mitigate bias in the project design phase.During workflow creation, interpretable AI, as discussed in Section 3, should support paving the way towards optimiz-ing workflows and avoiding downstream misinterpretations [102, 103]. Interpretable AI also allows tuning algorithmsand workflows based on the scientists\u2019 and non-science stakeholders\u2019 expertise [104]. Non-scientists thereby provideconsultancy to EO scientists to overcome the different biases mentioned in Section 2 during the workflow implemen-tation and roll-out, including historical, representation, measurement, aggregation and evaluation bias (compare 2 A).Next to training data creation, providing sufficient and independent evaluation data for accuracy assessment is both ofcore importance and often one of the most challenging tasks (see also 5.3).Implicitly, avoiding workflow-related bias is also tightly connected to recognizing deployment bias. Deploymentbias relates to the inappropriate interpretation of results after the data analysis, which is often related to downstreamdecision-makers. This underlines the importance of co-creation and co-production to minimize and mitigate deploy-ment bias early on. In other words, confronting stakeholders with results created in the scientific \u201civory tower\u201d withoutengaging with them in the design and production phase regularly creates unsatisfactory results for those affected orsupposed to build their decision-making processes on AI4EO outcomes.Deployment bias also points directly to the need to respect diversity and sociocultural differences. For example,we need to recognize different ethnic or religious communities, as well as AI4EO experts compared to non-expertstakeholders to avoid mis-interpretations or disrespectful outcomes of AI4EO workflows. Typical examples are themapping of slums or of agricultural practices in the Global South [76, 105]. Many studies on related research questionsneed to integrate stakeholders early on in AI4EO to include the necessary expertise to create meaningful results,specifically across large areas of interest, where diverse and multicultural stakeholders are needed. AI4EO workflowsalso rarely consider resource restrictions in the Global South - regarding the \u201cdata desert\u201d, limited processing resources,or limited financial backup, e.g. for implementing cloud computing or using HPC or supercomputing facilities [106].In other words, cooperation at \u201ceye level\u201d is needed to embed AI4EO in the proper sociocultural context, specificallywhen solving SDG-related questions and global change issues in the Global South [86, 102]. In conclusion, wecreate win-win situations when joining the forces of stakeholders \u201don the ground\u201d (deliberately including stakeholdersacross scales from local dwellers to global institutions, depending on the research questions to be tackled) with AI4EOexperts.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "1d342340-3049-46a6-b3de-0693a61ca139",
                    "text": "The need for interdisciplinary and regional collaboration related to domain knowledge across the entire workflow ofAI4EO and unwanted outcomes or interpretations were tackled in the section on \u201cFairness\u201d. We, therefore, here focuson handling the distinctness of EO data well [68].It has been argued that the AI/ML community urgently needs to recognize the distinct features of satellite-based EOdata and to \u201cmove out of the comfort zone\u201d of applying AI methods established in other domains without improvedadaptation to EO or satellite data machine learning (SatML, [68]). The reasons are manifold as, for example:\u2022 Satellite data is rarely \u201canalysis-ready data\u201d (ARD) [107]), requiring state-of-the-art preprocessing to repre-sent the same values for the same features in different images, i.e., to avoid pseudo-variance.13Responsible AI for Earth Observation\u2022 Spatial autocorrelation according to Tobler\u2019s first law of geography [108] is rarely used to improve AI4EO,even less so newer concepts like Partitioned Autoregressive Time-Series (PARTS) that consider both spatialand temporal autocorrelation in a congruent fashion [95]\u2022 Satellite data provide a wide range of spatial resolutions (from very-fine-resolution to pixels of several km )and extend across all scales to be observed (from local, fine-scale objects in urban environments to globalphenomena). Both very diverse spatial detail and multi-scale research ask for adapted solutions.\u2022 Different temporal resolutions, i.e., data densities over time, represent the same land surface differently. Inother words, change analyses over years or decades are far from trivial, and need contextualization [96, 109].This is even more important when phenology is considered and specific temporal windows across the seasonsare crucial to identify the relevant phenomena, e.g. different crop types, land use intensities or tree speciesseparable only at certain phenological stages [110\u2013112].\u2022 Most AI algorithms for image analyses were developed on three-band red-green-blue (RGB) imagery, oftenbased on VHR drone data. Today\u2019s multi- to hyperspectral satellite data ask for more sophisticated approachesto detect not just the obvious, but the relevant. This is aggravated by more spectral detail in radiometric databeyond 8-bit and even more by using virtual constellations [113], ranging from optical over radar to LiDARdata.\u2022 Sparse labeling problems occur with increasing areas of interest enabled by AI4EO. More severe problemsare inherited by biased samples across larger regions or non-trivial features to be sampled from existing dataor the imagery itself [114, 115].While all these aspects are relevant for any EO analysis framework, opportunities have increased rapidly with AI4EO(e.g., regarding the size of areas of interest that can be handled). Along with these are the related risks and potentialawareness deficits. Many, if not the majority of, AI4EO studies focused on comparably small use cases in the past todevelop or apply algorithms. However, the roll-out to real-world problems, for example, at the national, continental oreven global scale, requires ubiquitous algorithms that can handle much more diverse feature spaces (over time). Thereis indeed a lack of upscaling studies demonstrating the suitability of most algorithms for tackling real-world problems[68, 116, 117].",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "797aa056-cc21-46b0-b33c-1a3d9cf1f153",
                    "text": "Since 2015, the United Nations has set an ambitious agenda related to the Sustainable Development Goals (SDG),aiming to improve social, economic and environmental conditions globally by 2030 [118, 119], with a wealth ofimplications for AI and also specifically for AI4EO [106]. Here, we take the broadest possible standpoint, includingthe intrinsic sustainability of AI4EO workflows and the external impacts for all three SDG dimensions. Given thatAI at large offers the potential for both positive and negative impacts across all SDGs, with a majority on socialsustainability [120], AI4EO has a similar potential for both positive and negative impacts.Focusing on environmental sustainability, EO is data- and energy-hungry, thereby creating immense CO emissions.This footprint relates to the entire lifecycle of EO engineering, science and outcomes, including but not being limited tobuilding, launching and running satellite missions, data creation and curation, data storage and data analysis workflows- the data-related aspects being core for AI4EO [121].Zooming in on economic and social SDG, the FAIR principles are highly relevant: We thematized avoiding \u201chelicopterresearch\u201d already in 5.4 [86]. Offering inclusiveness is non-trivial to deliver, as many EO scientists and companies,specifically in the Global South, do not have access to powerful cloud computing or HPC services or to GPU-equippedhigh-performance workstations. Even if access to online services may become available through dedicated programsor international cooperations, proper education and training for the next generation of AI4EO scientists is of utmostimportance [106].The outcomes from AI4EO research may greatly support improvements along the lines of the UN SDGs [119, 120]- but may also lead to ill-posed conclusions. Such conclusions can, in the worst case, create an existential economicthreat (e.g., if loans or insurance are not provided based on AI4EO-derived indicators [122]). Also, local stakeholdersincluding indigenous people, who may not even be aware of governments\u2019 decision making processes, are regularlyimpacted, though. Last but not least, AI4EO may be in the \u201cwrong hands\u201d; for example, used by autocrats, dictators,or criminals to suppress or threaten specifically environmental and social SDGs. The most disturbing examples of thisare ubiquitous AI4EO-supported surveillance and warfare.14Responsible AI for Earth Observation",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "f4297877-fc77-4ba0-af8d-d61891f99e30",
                    "text": "In this section, we build upon the discussions in Section 5 regarding the use of AI4EO for social good, which involvesapplying AI technologies to address and solve social, economic and environmental challenges such as healthcare,education, environmental sustainability, poverty alleviation and disaster response. By leveraging AI technologies andthe ethical considerations mentioned in the previous sections, these initiatives seek to improve outcomes and make apositive difference in people\u2019s lives, which is the ultimate goal of responsible AI.Although the applications of AI4EO for social good are extensive [123, 124], we focus here primarily on understandingdisaster risks and evaluating their impacts through several key examples crucial for effective disaster management andfinancing strategies [125\u2013127]. Robust frameworks are necessary for investing in projects that enhance resilience toclimate change and its associated disasters. AI, leveraging integrated EO data, can significantly contribute to buildingthese frameworks by analyzing such data through innovative computational methods [125, 126, 128]. However, it isalso crucial to implement policies and decisions at the local level, as the impacts of climate change vary over time andacross different regions [125, 129\u2013132].Understanding disaster risks and evaluating their impacts are crucial for effective disaster management and financingstrategies [125\u2013127]. Robust frameworks are necessary for investing in projects that enhance resilience to climatechange and its associated disasters. AI leveraging integrated EO data can contribute significantly to building theseframeworks by analyzing such data through innovative computational methods [125, 126, 128]. Moreover, it is alsocrucial to address the implementation of policies and decisions at the local level, as the impacts of climate change varyover time and across space [125, 129\u2013132]. For example, rising temperatures in Europe can intensify evapotranspi-ration, potentially causing more floods in Germany and water shortages in Italy. Meanwhile, Portugal, France, andGreece may face increased wildfires due to similar processes [125, 129, 130, 133]. Global warming also impacts thewater cycle, necessitating increased irrigation for vegetation health and food security. This can heighten the occur-rence of natural hazards, even across significant distances. For example, irrigation in North America\u2019s Central Valley,High Plains, and Mississippi River Valley affects precipitation in regions like the Colorado River, USA Midwest, andSoutheast, respectively [134\u2013137].The above instances emphasize the diverse impacts of climate change on different regions, even at relatively closerange. Thus, it is crucial to enable each region to be prepared and ready to implement specific contingency plansbased on local changes to environmental events characterized by changes in atmospheric and oceanographic variables[125, 127, 129, 130]. It is also worth noting that these changes might affect the socio-economic fabric of diversebiogeographic regions, thereby influencing the stability and security of human welfare and demographic fluxes [125\u2013127, 129\u2013131]. It is, therefore, paramount for AI to provide a deep understanding of EO data so that decision- andpolicymakers are enabled to implement climate change adaptation and mitigation strategies. These strategies affectinvestments and solutions that citizens, local authorities, and businesses can undertake. Additionally, it implies theability to engage stakeholders and public opinion to gain societal consensus [128\u2013130, 133].To provide concrete examples of the actual impact and possibilities unlocked by the appropriate use of AI in decision-making processes, we present two instances of operational scenarios that would benefit greatly from a deep under-standing of Earth processes to enhance human welfare and improve communities\u2019 wellbeing.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "52065a31-b99c-460b-bb8e-3831a94972a3",
                    "text": "One of the most severe effects of climate change is the increased occurrence of catastrophic events caused by massmovements. This affects human welfare and results in huge economic losses. As such, the development of earlywarning systems (EWSs) is crucial to the everyday decisions of local governments and communities, and exemplifyResponsible AI by prioritizing human safety and equity through timely alerts and accessible technology [125\u2013127,129, 130].Historically, the main objective of EWSs has been to quantify the degree of immediate danger that every humansettlement in each region of interest is exposed to (see, [138\u2013141]). In fact, state-of-the-art EWSs used in operationalpipelines are based on the analysis of data and measurements at the regional level, which are then aggregated to avoidissues related to the irregularity of the temporal and spatial scale at which each record can be acquired. In this way,state-of-the-art EWSs are prone to highly sensitive and coarse classifications and predictions, potentially leading to apoor perception of risk and inadequate local management plans. Moreover, the classic structure of operational EWSsdoes not account for cascading effects affecting local communities at social and economic levels. For example, state-of-the-art EWSs for mass movements do not consider a town that can be cut off from transportation and communicationsystems because of landslides blocking the roads around it as relevant for early warning [141].15Responsible AI for Earth ObservationAI4EO can be a game-changer in this arena [129, 133]. Recent studies [142] have shown the improvement to state-of-the-art EWSs\u2019 capacity provided by analysis of the environmental (hydrological and geological) characteristics ofmass movements derived from remote sensing platforms and the connectivity information of formal settlements androad network data in terms of graph structures. This architecture allows us to study the interaction of the probabilisticmass movement susceptibility (derived from the environmental properties by means of a supervised ensemble graphneural network) on the graph representing the communication network (that could involve roads, electricity grids,water pipelines) connecting the formal settlements. Investigating this structure by graph-based methods (e.g., spectralclustering) results in the derivation of indices to quantify the probability of each human settlement to be directly orindirectly affected by mass movements (i.e., the probability to be hit by a mass movement event - such as a landslide- or the probability to be isolated as a result of mass movements affecting their surroundings) [142]. This approachhas the capacity to pave the way for measures and policies for adaptation and mitigation through a new holisticgraphical perspective to assess various large-scale geospatial datasets of risk elements such as exposure, vulnerabilityand hazard. In particular, it has been shown that this strategy can deliver robust and reliable outcomes for hindcastingand forecasting of the impact of mass movements at different scales (local, regional, and national), able to explorelong temporal trends (e.g., over 68,000 incidents of reported mass movements since 1957) and accurately identify thecritical factors for climate change-induced mass movements [142].Figure 7: Estimation of mass movements susceptibility index across the Norwegian territory by means of the architec-ture proposed in [142].",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "498edca0-2c14-44f6-becb-2760f71d8817",
                    "text": "One of the most important open challenges in environmental monitoring and climate change studies is representedby the analysis of climate teleconnections, that is, the investigation of the interplay between weather phenomena atwidely separated locations on Earth. This implies the study of climate patterns that span thousands of kilometers.Examples of teleconnection phenomena include shifts of atmospheric mass and/or pressure back and forth betweentwo distant locations, where changes in atmospheric pressure in one location lead to corresponding changes in pressurein another location far away [143\u2013145]. Therefore, responsible AI-driven insights are crucial for informing globalclimate strategies and policies to ensure that all communities benefit equitably.The water cycle is one of the primary drivers of climate teleconnections worldwide. In this respect, the three worldpoles (i.e., Arctic, Antarctic, high mountain Asia (HMA)) play a key role in influencing global weather patternsthrough their impact on teleconnections [146\u2013149]. Specifically, the weather and climate of HMA and the surroundingregion have been demonstrated to be influenced by global modes of variability (MoV), which are fluctuations ofatmospheric and oceanographic variables (e.g., sea surface temperature, rainfall, surface pressure, wind speed) causedby the continuous exchange of mass, momentum and energy on all timescales between Earth\u2019s atmosphere, oceans,cryosphere and continental hydrology. For example, the planetary-scale Rossby-wave propagation that causes the16Responsible AI for Earth Observationintra-seasonal variability over central Asia and the northern part of India has been demonstrated to be affected by theEurasian teleconnection. Also well-known is the interplay and impact on HMA climate variations of the Indian OceanDipole (IOD) and El Ni\u02dcno Southern Oscillation (Nino34), North Atlantic Oscillation (NAO), the Central Indian Oceanmode, and the boreal summer intraseasonal oscillation. However, a thorough understanding of the historical behaviorof global MoV and its influence on weather patterns within the HMA has not yet been derived [143, 144, 146\u2013151].The use of advanced technologies for machine learning, relying on records acquired by EO platforms, plays a key role[143, 145]. In fact, as remote sensors can acquire properties of the previously mentioned environmental variables withprecision at a global scale, they enable the investigation of MoVs that drive climate teleconnections worldwide. Inparticular, when considering HMA, the integration of remote sensing data with hydrometeorological climate variables(e.g., geopotential height at 250 hPa (z250), 2-m air temperature (T2M), total precipitation (PRECTOT), and fractionalsnow cover area (fSCA)) leads to a comprehensive exploration of the interactions across several MoVs over HMA (e.g.,the Eurasian teleconnection, the IOD, the NAO and the Nino34). This results in the quantification of correlations andcause/effect scenarios across MoVs and climate variables, as well as the predictability of weather and climate patternsof the variables of interest, so that proper planning of resources and policies to address and adapt to climate changecan be put in place [125\u2013127, 129\u2013131, 133].",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "7c9308e0-52e6-4eb0-bae9-665c2979a71a",
                    "text": "The integration of AI with low-cost, high-performance computing is fostering business innovation, with organiza-tions spanning various sectors and industries, disrupting both commercial and sustainability-focused business models.These may include monitoring remote infrastructure, assessing climate risk, dynamically managing supply chains anddecision-making processes across various industries. For example, an oil and gas company could enhance the monitor-ing of its infrastructures, such as the pipelines used for the transportation of natural gas, crude and refined petroleumby incorporating geotagged data into the operational workflow of the status and health of its distributed systems. Theseinsights could facilitate timely and cost-effective decisions, guiding preventive maintenance efforts to reduce oil spillsand gas leakages. However, the use of AI-based solutions requires appropriate governance and safeguards to miti-gate potential negative impacts. Therefore, emphasizing governance, standards, open-source solutions, and innovativebusiness models is crucial to foster the fair adoption of AI for EO. For example, while satellite data proves invaluablein agricultural management, farmers may lack direct access to the collected data. In contrast, business competitorswith greater resources may access this information and potentially use it to acquire land at favorable terms. On theother hand, biases embedded within AI models may lead to outcomes that lack equity and inclusivity across differentregions and populations. For example, the use of farm imagery without agricultural knowledge could result in theimplementation of costly regulations or insurance premiums, adversely affecting farmers. The integration of AI-basedsolutions into climate action and sustainable growth provides consistent, objective measurements, fostering trust andtransparency in efforts toward achieving a net-zero economy. The convergence of AI and Environmental, Social, andGovernance (ESG) factors represents a significant milestone in today\u2019s dynamic business environment. In this context,sustainability refers to how a company\u2019s business model, including its products and services, contributes to sustainabledevelopment. By incorporating AI into ESG risk management and strategies, commercial data providers can achievetheir sustainability objectives and explore novel pathways for fostering sustainable development and innovation. ESGreports are generally published yearly by commercial satellite providers such as Maxar Technologies [152], Planet[153] and Airbus [154], and technology companies such as Microsoft [155], Google [156], and IBM [157]. Someexamples of ESG-oriented use-cases of AI in Earth observation include:\u2022 Carbon footprint of AI models: the growing carbon footprint of the AI industry inspired the Universityof Copenhagen to develop \u201ccarbontracker\u201d [158], a tool that estimates the carbon emissions associated withdifferent hardware and software configurations, and forecasts training runs\u2019 carbon footprint. It also supportspredicting the total duration, energy, and carbon footprint of training an AI model. Then, the carbon intensityis used to indicate the carbon footprint in ESG reports.\u2022 Distribution of open-source training data: SpaceNet [159] is a nonprofit consortium dedicated to acceler-ating the research and application of open-source AI technology for geospatial applications. Co-founded byCosmiQ Works and Maxar Technologies, SpaceNet includes partners such as Amazon Web Services (AWS),Capella Space, IEEE Geoscience and Remote Sensing Society (GRSS), National Geospatial-IntelligenceAgency (NGA), Oak Ridge National Lab, Open Geospatial Consortium (OGC), Planet, Topcoder, and Um-bra. SpaceNet offers open, precision-labeled, electro-optical and synthetic aperture radar satellite datasets andruns challenges to encourage the development of algorithms designed specifically for geospatial applications.\u2022 Integrating diverse datasets to facilitate comprehensive situation and monitoring changes over time:Microsoft\u2019s Planetary Computer [160] combines a multi-petabyte catalog of global environmental data that17Responsible AI for Earth Observationbe queried via APIs, a scientific environment that allows users to answer global questions about that data, andapplications that put those answers in the hands of conservation stakeholders.\u2022 Identifying sources of environmental issues, such as methane emissions from landfills, farms, orpipelines: Carbon Mapper [161] is a nonprofit organization co-founded by the State of California, NASA\u2019s JetPropulsion Laboratory (JPL), Planet, the University of Arizona, Arizona State University (ASU), High TideFoundation, and RMI that plans to deploy a hyperspectral satellite constellation with the ability to pinpoint,quantify and track point-source methane and CO2 emissions.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "1a2ff54d-3f1c-4e13-8cc8-ed5ee6befd5b",
                    "text": "This paper addresses the pressing need to navigate the ethical and societal complexities inherent in AI4EO applica-tions. By synthesizing insights from diverse sources, including the academic literature, business models and ethicalframeworks, it provides actionable guidance for researchers and practitioners in this rapidly evolving field. The paper\u2019ssignificance lies in its comprehensive examination of ethical dimensions such as privacy, security, fairness, respon-sibility and mitigating (unfair) biases, shedding light on key considerations that underpin responsible AI practices inEO missions. Through fostering collaboration and dialogue among stakeholders, the paper aims to advance ethicaldiscourse and promote a culture of responsible AI development and deployment. It serves as a foundational frame-work for navigating ethical dilemmas and guiding the responsible integration of AI technologies into EO missions.Ultimately, the paper underscores the transformative potential of AI4EO applications in addressing global challengeswhile emphasizing the importance of upholding ethical principles. However, it is crucial to acknowledge that in thecurrent context of significant global conflicts, some state actors may resort to non-peaceful means to achieve their owninterests over the collective good defined at the global level. Ensuring that these technologies contribute to a moreequitable and sustainable world requires navigating these complex realities.Given the current geopolitical state of the world, the role of AI in Earth observation (AI4EO) takes on heightenedsignificance in contributing to global stability and peace. Aligning with UNSDG 16, which emphasizes peace,justice, and strong institutions, this work underscores the critical importance of leveraging AI4EO to supportthese goals and mitigate conflict.Furthermore, the research paper explores the significant effects of AI4EO applications on political, societal and en-vironmental landscapes. It frames the United Nations Sustainable Development Goals as the guiding North Star forthe ethical use of AI4EO, emphasizing that these goals should define what \u2019good\u2019 looks like. By aligning AI4EOefforts with the SDGs, the paper sets a clear benchmark for transparency, reproducibility and trust in AI4EO research.The paper advocates for open access to data and code to facilitate evidence-based decision-making processes inpolicy-making, enabling stakeholders to make well-informed choices. Societally, it highlights the critical need toaddress biases and safeguard privacy rights to promote fair outcomes and reduce stigmatization in vulnerablecommunities. The emphasis on responsible AI practices prioritizes fairness and inclusivity, fostering societal trustand cohesion. Environmentally, the paper stresses AI\u2019s vital role in addressing pressing environmental challengesthrough sustainable resource management and conservation efforts. By aligning AI4EO workflows with sustain-ability principles, the paper offers a path for leveraging AI technologies to advance the SDGs, thereby contributing toa more resilient and sustainable future for humanity and the planet.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                },
                {
                    "id": "6183fd8b-a9a7-4cc3-9228-ff24971ebc22",
                    "text": "Future investigations, as indicated multiple times throughout this paper, should focus on developing strategies thatfacilitate the open sharing of data for scientific advancements while simultaneously preserving sensitive personalor group information. Ensuring the ethical use and protection of such data is crucial, particularly in applicationslike mapping deprived urban areas or victim detection, where the responsible use of AI can have significant societalimpacts. These applications critically depend on the availability of large benchmark datasets, which are currentlylacking. To promote responsible AI, it is essential to implement robust privacy-preserving techniques and dataanonymization methods that protect individuals\u2019 identities while maintaining the utility of the data for research pur-poses. This includes using techniques like differential privacy, federated learning, and secure multiparty computationto safeguard sensitive information. Additionally, it is vital to establish clear guidelines and ethical standards for datasharing and usage. These guidelines should ensure that data are used in a manner that respects the rights and dignity ofindividuals and communities. Collaboration with stakeholders, including ethicists, legal experts and representa-tives from affected communities, is necessary to create frameworks that balance the need for scientific progress withthe imperative to protect sensitive information. Moreover, transparency in data collection, processing and sharing18Responsible AI for Earth Observationpractices is essential. Researchers and organizations must be open about the sources of their data, the methods usedto process it, and the steps taken to protect privacy. This transparency builds trust and ensures that AI advancementsare grounded in ethical practices. By prioritizing the responsible handling of sensitive data, the research communitycan develop AI-based methods that contribute to societal well-being while respecting individual and group privacy.In the early 2020s, following decades of model-centric approaches and the deep learning revolution, there wasa notable shift from model-centric to data-centric learning. The concept of data-centric AI began gaining traction,emphasizing the quality of data over the complexity of models. Concurrently, a strong societal push emerged withthe advent of Responsible AI recommendations and legislation, as outlined in this paper. This necessitates research todevelop methods that can capture biases in compliance with such legislation while maintaining scientific robustnessand addressing the unique characteristics of GRS data. Responsible AI must consider the entire machine learningpipeline and involve multiple stakeholders. This includes identifying sensitive attributes and integrating specificdomain knowledge into bias mitigation techniques. Currently, there is a balanced focus on both data quality andmodel innovation (the era of data-model-centric approaches). Techniques such as foundation models, syntheticdata generation, uncertainty modeling and explainability, and robust data validation are employed to enhance data-centric approaches, emphasizing both model and data equally. Ethical considerations, including fairness, transparency,and bias mitigation, are also integral to these AI practices.",
                    "reference": "[1] Pedram Ghamisi, Wei Yu, Andrea Marinoni, Carmen Maria Gevaert, Marcela A. Guedes de Silva R. de Souza, Shih-Yuan Lin, Julian A. Tordillos, Devis Tuia, and Naoto Yokoya. 2024. Responsible AI for Earth Observation. arXiv:2405.20868. Retrieved from https://arxiv.org/pdf/2405.20868"
                }
            ]
        },
        {
            "paper_title": "A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations",
            "authors": "G Berman, N Goyal, M Madaio",
            "publication_info": "arXiv preprint arXiv:2401.17486 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2401.17486",
            "chunks": [
                {
                    "id": "e69e6fda-728e-476a-ac77-ba30bfe23b53",
                    "text": "",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "0c96f6e8-bd14-4573-b7fb-e72be2ba5d51",
                    "text": "Researchers concerned about issues of fairness, accountability, transparency, and equity in algorithmic systems areincreasingly focusing on arti\ufb01cial intelligence (AI) development practices as a site for intervention [e.g., 99, 133, 165,168]. In particular, the potential for AI systems to discriminate and exacerbate societal inequities [34, 119, 152, 182]has prompted the interdisciplinary responsible AI (RAI) community to develop frameworks, resources, and processesfor proactively identifying and mitigating potential societal impacts throughout system development and deployment[20, 99, 133, 138, 140, 218]. As part of this e\ufb00ort, a range of tools have been developed for use by AI practitioners [e.g.,127, 133, 167], stakeholders [e.g., 60, 123, 167, 171, 177, 180], and community members [e.g., 45, 47, 120, 183], to supportRAI at all stages of the AI development and deployment pipeline.Responsible AI tools are designed to in\ufb02uence the norms, decisions, and daily practices of AI development to mitigatethe potential for AI systems to introduce or exacerbate societal inequities\u2014RAI tools are designed not only to be widelyused, but to be widely impactful [138, 140]. As such, the question of what it means for an RAI tool to be e\ufb00ectiveis important for the developers of RAI tools, their intended users, and the broader \ufb01eld. Further, as RAI tools areincreasingly relied upon by technology \ufb01rms to support the translation of ethical AI principles into practices [e.g.,141], their e\ufb00ectiveness is also of relevance to policymakers developing policies or regulation for AI systems [e.g.,191] and to communities who may be impacted by the deployment of such systems. Yet, while RAI tools have beenstudied from the perspective of their coverage of the ML pipeline [140], their use by data scientists [e.g., 55, 127, 173],their suitability for speci\ufb01c tasks or contexts [127, 174, 176], and the normative commitments implicit in their design[157, 218], they have largely not been considered from the perspective of how they are (or might be) evaluated. As such,in this paper we ask: what are existing practices for evaluating RAI tools, as reported in publicly-available documentation?We address this research question through a thematic analysis of 37 publications, within which evaluations of 27 RAItools are discussed. We focus on publicly-available publications as only publicly-available documents (rather than, e.g.,internal, unpublished evaluations) are available to inform decisions about which RAI tools to adopt or recommend.We \ufb01nd that evaluations of RAI tools largely focus on questions of usability, primarily studied through small-scalequalitative evaluations conducted in in-lab settings. We demonstrate that such evaluations, while a valuable aspectof RAI tool development, are distinct from determining whether an RAI tool is e\ufb00ective in achieving its desired aims.That is, most evaluations do not consider whether the overarching objective informing the design of an RAI tool\u2014often expressed in terms of desired changes to AI development and deployment practices\u2014is achieved. In this context,e\ufb00ectiveness refers to the extent that a causal relationship can be established between a desired outcome and the useof an RAI tool in a given context [38]. We extend our analysis by considering best practices in evaluations from the\ufb01eld of education\u2014a \ufb01eld in which the evaluation of the e\ufb00ectiveness of interventions in educational processes hasreceived signi\ufb01cant research and public policy attention. Based on these best practices, we outline desiderata for thedevelopment of an e\ufb00ectiveness evaluation framework and suggest practical steps for both RAI tool developers and theRAI and HCI communities to take towards developing and implementing such a framework.This paper helps to bridge the gap between \ufb01eld-level overviews of RAI tools [e.g., 140, 218] and empirical studiesof RAI tool usage [e.g., 56, 113, 127, 132, 173]. The paper contributes a critical perspective on RAI tool evaluationpractices (and their gaps) through an analysis of RAI tool publications, and recommendations for the HCI and RAIcommunities to support e\ufb00ectiveness evaluations of RAI tools. Across these two contributions, we emphasize thesigni\ufb01cance of action at the level of the RAI and HCI communities, as RAI research and implementation in industrypractice faces substantial resource constraints [e.g., 99, 132, 168]. We call for community-level interventions to addressthese constraints and improve opportunities for more robust evaluation of RAI tools.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "51a9ca14-36f1-457a-8d44-71d34c77e4c3",
                    "text": "In this section we provide an overview of prior work developing and studying RAI tools. We then introduce keyconcepts in evaluation, to contextualize our discussion of evaluation practices of RAI tools.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "6ae8b9f8-0b77-4957-b8d3-1dafe64997c9",
                    "text": "We de\ufb01ne RAI tools as interventions that support AI practitioners and other stakeholders in addressing ethical issuesin AI system development and deployment. Re\ufb02ecting the interdisciplinary nature of AI research and development,researchers and practitioners developing RAI tools have drawn from, and contributed to, a wide range of \ufb01elds, includ-ing HCI [e.g., 47, 50, 204], design [e.g., 7, 31], software engineering [e.g., 10, 104], and public policy [e.g., 11, 44, 171].For reviews of RAI tools, see Morley et al. [140], Petterson et al. [157], and Wong et al. [218].Following Morley et al. [140] and Black et al. [28], the breadth of RAI tools developed is re\ufb02ected in their coverageacross all stages of the AI development and deployment pipeline. RAI tools include: frameworks for guiding problemformulation and decisions to procure an AI system for a given use case [e.g., 7, 47]; guides for considering ethical issueswhen designing an AI system [e.g., 31, 204], such as design playbooks [100], checklists [63, 133], and workshop guidesto support product teams to operationalize ethical guidelines in AI development [14, 16]; approaches for enablingcommunity participation in the development process [e.g., 27, 54, 184]; protocols and templates for curation, evaluation,and documentation of training datasets [e.g., 23, 52, 58, 62, 73, 78]; software tools for conducting fairness assessmentsof trained models [e.g., 10, 13, 20, 22, 26, 132, 193, 209]; protocols and templates for documentation of trained modelsand their evaluations [104, 137]; and, processes for auditing AI systems by third parties [e.g., 60, 120, 123, 136, 167, 171,177, 180, 183]. Also re\ufb02ecting the breadth of RAI tool development is the range of intended users. RAI tools have beendeveloped for use by AI practitioners [127, 133, 167], stakeholders [e.g., 60, 123, 167, 171, 177, 180], and communitymembers [e.g., 45, 47, 120, 183].We situate RAI tools within the space between high-level ethical AI principles [6, 110, 199] and low-level formal-izations of intrinsically social and unobservable concepts, such as fairness metrics [e.g., 46, 64, 75, 162]. High-levelprinciples help shape the broader environment in which practitioners work, but are generally too abstract to be di-rectly applied by practitioners [99]. Similarly, low-level mathematical formalisms of social phenomena like fairness[46] and algorithms that operationalize the measurement of these concepts are critical (and often contested [107]) in-puts into RAI tools, but without integration into a tool, they require a substantial level of technical expertise to putinto use during AI development.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "e36f4bee-8fff-4510-b402-425001fb9a4e",
                    "text": "The development of RAI tools is a signi\ufb01cant undertaking. As researchers who have developed RAI tools (see ourpositionality statement in Section 3.4), we are familiar with the resource constraints that must be overcome to developan RAI tool and convince practitioners to adopt it. Within this context, we recognize that a narrow focus on evaluationbrings risk, including suppressing innovation [85]. Our view, however, is that greater focus on evaluation may be apath towards greater impact of RAI tools. By robustly substantiating the e\ufb00ectiveness of RAI tools, we can make amore compelling argument for their adoption (as Rakova et al. [168] explore for RAI work more generally). Yet, whilethe translation of ethical AI principles into industry practices and tools has received attention in HCI and adjacent\ufb01elds [e.g., 70, 99, 127, 133, 168, 200, 204, 208], relatively little attention has been directed to evaluating RAI tools\u2019e\ufb00ectiveness. Recent surveys of RAI tools [42, 129, 130, 140], and studies of particular types of tools [42] highlight thechallenges of designing e\ufb00ective RAI tools, and of integrating RAI tools into the complexities of an AI developmentpipeline [28, 56, 99, 132, 133, 168, 218].Questions of tool usability and usefulness have been investigated in recent studies of RAI tools: six open sourcetools for fairness testing were compared in an interview and survey study [127], and the usability of two of these toolswas studied in a think-aloud evaluation with practitioners encountering the tools for the \ufb01rst time [56]. Similarly, twosoftware tools for fairness testing were assessed through a simulated scenario with practitioners [173], and RAI tools tosupport AI system auditing and impact assessments were mapped against standards for audits and impact assessmentsin other domains [15, 136, 144, 180]. These evaluations provide valuable insights on the usability challenges facing RAItool developers, which are critical to address in order to improve the likelihood of tool adoption.As RAI tool development continues to mature, enhancing and expanding usability evaluation practices will be im-portant. As usability studies unblock adoption, we argue that evaluations that aim to address the e\ufb00ectiveness of anintervention will be necessary and complementary extensions to usability evaluations. E\ufb00ectiveness is the extent towhich a causal relationship can be established between an intervention and a desired outcome in a given context [38].In contrast, usability in an evaluation context refers to the ease with which particular groups can implement the toolor intervention [125]. E\ufb00ectiveness and usability are thus contextually interdependent concepts, with their opera-tionalization varying from one intervention to another. As one example, a dataset documentation protocol may aim toincrease transparency in the ML pipeline, reduce the likelihood of data misuse, and improve accountability for datasetcreation and model training decisions [73, p.1] . The e\ufb00ectiveness of a dataset documentation tool against each of theseaims is a re\ufb02ection of the causal relationship between adoption and use of the protocol in a particular context (e.g., agiven technology \ufb01rm) and measurable changes in system development practices or outcomes (e.g., reduction in datamisuse); although, as we discuss in Section 5.2, such measures may not currently exist for many responsible AI goals,or may be di\ufb03cult to develop due to contestation in operationalizing constructs such as fairness [108].Prior work demonstrates the importance of questions of e\ufb00ectiveness, and highlights the risk of failing to attendto the evaluation of RAI tools. Studies have found that organizational cultures, structures, and incentives can actas barriers to the e\ufb00ective operationalization of ethical AI principles [133, 168, 178]. Chang and Custis [40] \ufb01nd thatorganizational structures and incentives can present implementation challenges for the e\ufb00ective use of documentationtemplates. Metcalf et al. [136], drawing from \ufb01elds adjacent to ML, found that algorithmic impact assessments, intendedto highlight risks of AI system deployment, can instead be co-opted by \ufb01rms developing such systems to further theirinterests. Meanwhile, recent work has also explored and critiqued evaluation practices of AI systems more broadly,highlighting the implications of decontextualization when evaluating AI systems [103, 135] and raising attention to therisks of corporate capture when private actors evaluate their own systems [219]. Our aim is to support improvementsin the design and development of RAI tools through an analysis of existing evaluation practices for RAI tools.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "0f677d9f-0ad6-4dad-a76d-5540486454c1",
                    "text": "While there have been advances in the \ufb01eld of RAI tool development, evaluation of RAI tools is still a nascent areaof research and practice. In contrast, tool evaluation more generally is a longstanding focus of research and practicewithin HCI [145]. In HCI research, evaluations have historically tended to focus on questions of tool usability, whichare frequently investigated through user studies [19, 85]. However, a focus solely on usability evaluations has beencritiqued across HCI research areas, notably by Olsen [155] and Greenberg and Buxton [85].Olsen [155], in his analysis of user interface systems research, highlights the \u2018usability trap\u2019. On the one hand, theassumptions and practical constraints of usability experiments are di\ufb03cult to reconcile with the complexity of userinterface systems. On the other, running a usability experiment, however \ufb02awed, is perceived as a requirement forpublishing. Olsen [155] argues that evaluation methods should be grounded in the claims researchers hope to make,and proposes a framework of \"Situations\", \"Tasks\", and \"Users\" for thinking about the kinds of claims one might make,and therefore the sorts of evaluation activities that may be necessary. Greenberg and Buxton [85] similarly argue thatevaluation design decisions must \ufb02ow from the research question(s) under consideration, and are also critical of thepredominance of usability evaluations in HCI research. They note that the kinds of claims one might make also di\ufb00eraccording to the design status of the system under evaluation: early design sketches intended to demonstrate a novelapproach need not be evaluated in the same way as working prototypes. Lastly, Greenberg and Buxton [85] also notethat usability and usefulness are distinct concepts: a tool can be usable but useless.Of particular relevance to our focus are evaluations of interactive design toolkits [125, 146], which, like many RAItools, are designed to be used by practitioners to support the development of computing artefacts (here, developingsoftware; for RAI tools, developing AI models [99] or AI-powered applications [208]). Ledo et al. [125] reviewed publi-cations from leading HCI venues in which interactive design toolkits are discussed, and identi\ufb01ed four types of evalua-tions: demonstrations, which show what the toolkit might enable; usage, which use qualitative evaluations to show theusability of the toolkit; technical, which use software engineering techniques to evaluate technical performance; andheuristics, which are informal assessments of toolkit usability through the application of design guidelines. Echoingothers [e.g., 85, 155], Ledo et al. [125] reiterate that evaluation methods for a speci\ufb01c toolkit should be determined inlight of the claims researchers wish to make about the toolkit.As we explore further in Sections 5.2 and 5.3, we agree with the call to align evaluation methods with research claims,and draw on HCI guidelines for evaluation design that have been developed in response to this call [e.g., 53, 146] toinform an RAI tool e\ufb00ectiveness evaluation framework. As RAI tools aim to address ethical issues in AI development,claims related to the consequences of their use or e\ufb00ectiveness are wide in their sociopolitical scope, implicating notonly RAI tool developers and users, but also AI system stakeholders and communities who may be a\ufb00ected by AIsystem deployment. This necessitates evaluation designs of commensurate scope, and prompts us to consider how\ufb01elds outside of HCI conceptualise and design evaluations.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "79b08142-3b6d-49f8-9a86-2e770b1ce05c",
                    "text": "The evaluation of trained models, and of automated systems incorporating these models, is a signi\ufb01cant area of researchin the ML community. In part, these research e\ufb00orts are responses to reproducibility concerns in ML, which haveprompted researchers to consider how claims about model performance can be better evaluated [105, 128, 159]. Thesee\ufb00orts also respond to critical analyses that demonstrate that evaluation of models, in isolation from the systemsthey are deployed within, is incomplete\u2014model performance on a benchmark dataset may not be predictive of systembehaviour [166, 167]. In natural language processing (NLP), for instance, researchers have surveyed existing NLP modelevaluation methods, \ufb01nding no standardised evaluation practices[79, 220]. Relatedly, e\ufb00orts are underway to developstandards for evaluation of ML applications [88] and models [103]. Our focus complements these e\ufb00orts, by attendingto the evaluation of interventions in ML production, rather than the evaluation of the outputs of ML production, suchas trained models or new AI systems.In doing so, we look to education [194] and medicine [49], two \ufb01elds outside of ML and HCI, which have also su\ufb00eredfrom reproducibility concerns [e.g., in education research, 179], and have developed robust norms for the evaluationof process interventions (e.g., implementing a new curriculum), despite the challenges of dynamic and heterogeneousintervention contexts like schools and hospitals. In education, for example, decades of research and public policy haveidenti\ufb01ed the need to develop standards and norms for evaluating the e\ufb00ectiveness of an intervention. Governmentagencies have sought evidence to inform investment in interventions such as new curricula or learning technologies[12]. In response, the U.S. Department of Education\u2019s Institute for Education Sciences (IES) created a \u201cWhat WorksClearinghouse\u201d to provide a standard process and repository for evaluating the evaluations of educational interven-tions, to inform teachers, curriculum designers, school leaders, and policymakers about what works in improvingeducational outcomes [5, 51].The \ufb01elds of education and medicine are imperfect analogues for RAI. We can, however, draw lessons from theirapproaches to evaluation. In doing so, we acknowledge that evaluations practices in these \ufb01elds are imperfect, andcontinue to be debated. Education and medicine interventions aim to be highly generalisable, but have to contendwith varied social contexts and the numerous confounding variables these contexts present (e.g., see discussion ofconfounding variables in education interventions in the What Works Clearinghouse procedure manual [212] and dis-cussion of di\ufb00erent social contexts for medical interventions in [37]). In both \ufb01elds, interventions are a site of policydebate and regulatory action [e.g., 81]. Similarly, many RAI tools aim to be generalisable to a wide range of contexts,such as di\ufb00erent types of technology companies, di\ufb00erent application domains for which AI systems are developed,di\ufb00erent cultures or geographies of deployment contexts of a given AI system, as well as relevance for various stagesof the AI development pipeline. And, as e\ufb00orts to enhance regulatory oversight of AI mature, one area of focus ison best practices for AI production pipelines [e.g., in a health context, 115], which may extend to use of RAI tools.Yet, unlike the education and medicine \ufb01elds, these RAI e\ufb00orts are occurring against a historical backdrop of minimalpublic oversight (despite some emerging AI risk frameworks from government agencies in some countries [191]), andscant formal accreditation for AI practitioners.It is important to note that di\ufb00erent types of evaluations have di\ufb00erent goals. For instance, Gertler et al. [80] divideintervention evaluations into several types: descriptive evaluations that seek to answer questions about the currentstate of the world (e.g., understanding processes, conditions, relationships), normative evaluations that compare whatis taking place with what should take place (e.g., with what \ufb01delity did teachers implement a particular curricular inter-vention), and causal evaluations, which explore the e\ufb00ect a particular intervention has on behaviors, attitudes, or otheroutcomes of interest. Di\ufb00erent evaluation goals entail di\ufb00erent types of studies, including observational (which mayinvolve case studies, surveys, etc.), design-based research [18, 147], quasi-experimental (e.g., di\ufb00erence-in-di\ufb00erence),and experimental studies (e.g., randomized controlled trials). In many \ufb01elds, experimental studies are held up as the\u201cgold standard\u201d for evaluation quality (although not without critiques [e.g., 43, 142, 190]), as they allow researchers tocompare the outcomes (e.g., test scores) of a group who received an intervention with a similar control group that didnot [5, 80]. When randomized assignment of participants to a treatment or control group makes experimental stud-ies impossible or infeasible, as in state-level health policy research [e.g., 9]), researchers may use quasi-experimentalmethods (e.g., regression discontinuity analysis [160], di\ufb00erence-in-di\ufb00erence [9], propensity score matching, and in-strumental variables [93, 96]) to approximate the controls used in an experimental study.A longer discussion of these evaluation methods is beyond the scope of this paper. However, we note that eachmethod presents its own set of compromises and trade-o\ufb00s, which need to be considered before adapting them toRAI contexts, as we discuss further in Section 5.3. Additionally, education and medicine have also su\ufb00ered from re-producibility crises [e.g., in education research, 179], with their evaluation methods continuing to develop in response.Of particular relevance are di\ufb00erences in the way RAI tools are developed and deployed compared to interventionsin the education or medicine \ufb01elds. As outlined in Section 2.1, many, but not all, RAI tools are software tools, oftendeveloped within an agile paradigm, or are open-source software where the functionality may change over time [e.g.,209]. In this context, implementing experimental studies, which assume that the intervention will remain static, maybe challenging. In addition, many experimental studies assume a controlled intervention environment, in which con-founding variables are well understood [37, 48]. Yet many RAI tools are not only designed within an agile paradigm,but are also intended to be used in an agile AI development paradigm, where work practices are continually changing.In our view, however, these challenges should not forestall consideration of how evaluation methods from other\ufb01elds can be adapted for use in RAI tool evaluations. Indeed, research in educational contexts like schools is alsosubject to changing policies, state standards, students moving from an intervention to control group when enrolling ina new class or school, as well as exogenous shocks like teacher strikes, political violence, and school closures, amongothers [117]. Conversely, as AI technologies and development paradigms are increasingly incorporated into medical oreducation interventions, these domains are also starting to consider how to apply their familiar evaluation approachesto AI technologies [e.g., in healthcare, 134, 170][e.g., in education, 36], and how to translate insights from RAI to theirevaluation practices [e.g., 185]. These e\ufb00orts demonstrate the potential of methods drawn from otherwise disparatedisciplines for guiding new evaluation practices, although doing so successfully will require a thoughtful approach tomanaging threats of evaluation validity.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "e14c68a3-1673-466c-8e5b-2f6f26c9d193",
                    "text": "Regardless of the goal of the evaluation, there are criteria for assessing the validity of the evaluation. For studies thatattempt to establish a causal relationship, internal validity is one way of understanding whether a given interventioncauses the intended e\ufb00ect, and refers to the aspects of a study design that \u201csupport causal claims between variables\u201d[76]. For experimental evaluations, this might involve exploring threats to internal validity, such as alternative causesother than the intervention, study attrition, or leakage between a treatment and control group (e.g., if a student in thecontrol group class switches into a class receiving the treatment) [5].In addition, external validity is the aspect of a given study that captures the extent to which the \ufb01ndings from thatstudy are able to be generalized to additional populations or contexts (e.g., new study sites, or other geographic orcultural contexts) [29, 80, 147]. In other words, how do you know whether your \ufb01ndings will be relevant for otherswho did not participate in the study? Some ways to avoid threats to external validity include conducting multi-sitestudies, random sampling, evaluating the representativeness of the sample to the population as a whole, and morebroadly, being transparent in documentation of the study site, population, recruitment and inclusion criteria, and othermethodological decisions [76].One key component of external validity is the ecological validity of the study, or the extent to which the studyis re\ufb02ective of a \u201creal world\u201d [98]. Originating in critiques of in-lab psychology studies that were not re\ufb02ective ofpeople\u2019s behavior or contexts outside of the lab, the term, despite its popularity, has su\ufb00ered from ambiguous usageand is too often used as a shorthand when discussing the speci\ufb01c contexts in which a study is conducted (and for whichthe results might be more broadly applicable) [98].An additional related concept is e\ufb03cacy, the extent to which an intervention produces a desired outcome in idealexperimental settings [38]. That desired outcome might be expressed in terms of usability, or e\ufb00ectiveness, or someother goal. But, crucially, the extent to which an e\ufb03cacy evaluation is valid outside of the in-lab settings under whichit is conducted\u2014i.e., the extent to which it has external validity\u2014depends on the desired outcome under consideration.Consider an evaluation design that consists of an observational study, in which participants will follow an interactive tu-torial to use a software toolkit to explore a dataset. Threats to the external validity of this evaluation vary depending onwhether the objective is to test the usability of the software toolkit or its e\ufb00ectiveness. As an evaluation focused on us-ability, one might say that the in-lab settings closely approximates the context in which the software toolkit is intendedto be used, e.g., because participants use a computer that is similar to their workplace computer, and a dataset that theyalso use in their daily work. As such, one might say that con\ufb01rming the e\ufb03cacy of the software toolkit through thisin-lab evaluation is likely to indicate that intended users will be able to successfully use the software toolkit. However,if the aim of the evaluation is to determine the toolkit\u2019s e\ufb00ectiveness, then the limitations of this e\ufb03cacy evaluationdesign are much more acute. The in-lab settings have only considered interactions between individual users and thesoftware toolkit, not the relationship between an individual user (using the software toolkit) and the broader collabora-tive process of developing an AI system. As such, there is a misalignment between this e\ufb03cacy evaluation design andthe objective of an e\ufb00ectiveness evaluation. In short, an e\ufb03cacy evaluation asks \u201ccan the RAI tool work?\u201d, whereas ane\ufb00ectiveness evaluation asks \u201cwhen used, is the RAI tool impactful?\u201d [24, 74]. It is the latter question which we argueis of most relevance to the evaluation of RAI tools. We return to this tension in more depth in Section 4.1.Finally, it is critical to acknowledge that there may be tradeo\ufb00s between the types of validity\u2014for instance, a studythat has high ecological validity (e.g., an in-situ classroom study) may su\ufb00er from issues with internal validity (i.e., dueto students changing classes or dropping out in the middle of the study); a laboratory study that provides compellingevidence of e\ufb03cacy may only have external validity for a very narrow range of contexts. Although other \ufb01elds haveextensive literature documenting these potential threats to validity and their tradeo\ufb00s for particular evaluation goalsand methods, the \ufb01eld of RAI has had little such investigation. In this paper, we intend to open this conversation.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "29ff6fea-a26c-49bd-bfd9-806ac9e695f2",
                    "text": "The research question informing this study is: what are existing practices for evaluating RAI tools, as reported in publicly-available documentation? To address this question, we analyze publicly available documentation of RAI tools. Inspiredby similar approaches in CHI [125, 157] and CSCW [218], we use publicly-available documentation produced by re-searchers and practitioners as a primary source through which to explore evaluation practices. While RAI tool eval-uations may be documented in other ways (e.g., internal reports or monitoring dashboards), we limit our study topublicly-available documentation because, from the perspective of potential RAI tool users, policymakers, or otherstakeholders, only publicly-available documentation can inform decisions about which RAI tools to deploy in a givencontext. We discuss the limitations of this approach in Section 6.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "0597d9f6-0dc8-4fba-a13f-cea46efe7699",
                    "text": "To the best of our knowledge, there is currently no central repository for evaluations of RAI tools, or indeed for RAItools themselves (see Section 2.1 for a description of existing tools and surveys of tools). Additionally, as existingsurveys of RAI tools demonstrate, publication practices regarding RAI tools vary widely: RAI tools have been publishedin HCI and ML academic venues [e.g., 70, 131, 137, 204], on organizations\u2019 websites [e.g., 26, 171], and on GitHub [e.g.,8]. As such, following [218] and in keeping with the exploratory nature of this study, we adopted a purposive samplingstrategy [72, 157, 218], whereby our goal was to sample a variety of tool types, creators, and dimensions of responsibleAI (e.g., fairness, transparency, accountability), rather than to develop an exhaustive or statistically representativesample.We conducted our search for RAI tools between October and December, 2022. We created an initial list of RAI toolsby mining reviews of RAI literature [15, 42, 127, 140, 218]. We used this list to design a keyword search of the ACMDigital Library (DL), which includes SIGCHI publications and the proceedings of the AIES and FAccT conferences.One author reviewed the title and abstracts of publications returned by the ACM DL search to identify further RAI tools.This search process resulted in an initial list of 243 unique RAI tools, with each tool linked to at least one publicationabout it. To select a subset for qualitative analysis we used a two-stage \ufb01ltering process. First, one author applied thefollowing exclusion criteria, designed to ensure alignment with our de\ufb01nition of RAI tools (see Section 2.1):\u2022 Unavailable: 5 tools were excluded because they are now obsolete or cannot be found online; 2 were excludedbecause they are only available through commercial licenses.\u2022 Category error: 6 potential tools were excluded as they were organizations that provide services, rather thantools; 14 were excluded because they were information provision services (e.g., websites that track resources forjournalists [1]), rather than tools; 15 were excluded because they were conceptual contributions (e.g., a proposalto use the concept of a windfall tax as a tool for redistributing technology \ufb01rm pro\ufb01ts [154]), rather than tools.\u2022 Low-level construct: 35 potential tools were excluded as they were low-level formalizations (e.g., the ShiftedDecision Boundary method for optimizing the trade-o\ufb00 between bias and accuracy in a classi\ufb01er [75]), whichfall outside our de\ufb01nition of an RAI tool, as they are not directly usable by practitioners or other stakeholders.\u2022 High-level framework: 17 potential tools were excluded because they were high-level ethical frameworks, prin-ciples, or guidelines (e.g., the Ontario Privacy Commissioner\u2019s Privacy by Design principles [39]), which donot meet our de\ufb01nition of an RAI tool, as they require operationalization before they can be directly used bypractitioners or other stakeholders.\u2022 No relevance to RAI issues: 4 tools were excluded because their creators did not describe them as addressingissues of responsible AI (e.g., fairness, accountability, transparency, or equity). For example, Flipper [201], a toolfor debugging training sets, was excluded.The application of these exclusion criteria left 145 RAI tools and their associated publications. To reduce this to a man-ageable number of tools for qualitative analysis, following Wong et al. [218] and Petterson et al. [157], we sampledfor breadth, using the following inclusion criteria: stages of the pipeline of AI design, development, and deployment;the types of RAI tools proposed; the sectors of tool creators (i.e., academic, industry, civil society); and di\ufb00erent formsof evidence for tool adoption (e.g, GitHub, paper citations, commercial platforms). To apply these inclusion criteria,one author reviewed the full text and metadata of all publications and classi\ufb01ed the publications accordingly. Twoauthors reviewed these classi\ufb01cations and together identi\ufb01ed a sample corpus of publications with breadth across allcriteria for thematic analysis. This corpus consisted of 27 RAI tools (T1 - T27, listed in Appendix A). These tools wereassociated with 37 publications (A1 - A37, listed in Appendix C). The corpus is described further below in Section 3.3.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "933da500-4512-4547-9764-98dfc2b045c2",
                    "text": "Then, we conducted an inductive thematic analysis [32, 150] on the corpus of 37 publications, following Wong et al.[218] and Petterson et al. [157]. One author initially excerpted quotes related to toolkit evaluation (based on ourresearch question) from each publication, which were discussed with another author. Then, through several rounds ofinductive thematic analysis, using an online whiteboard, two authors iteratively generated codes to capture patterns ofshared meaning across multiple quotes and clustered these codes into related themes, with all three authors discussingthe \ufb01nal set of themes. A summary of all themes and codes, with example excerpts, is provided in Appendix D.3.3 Corpus description",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "5cdc75a5-96a0-4c24-9e64-00658b588fc6",
                    "text": "As shown in Table 2, the majority of RAI tools in our corpus are createdwithin industry contexts (15), in addition to a mix of academic (6), civil society (3), and cross-sector creators (3). Theplurality of RAI tools are software toolkits (10). Others are transparency artifacts (6), workshop guides or playbooks(5), third-party review tools (3) and more (see Table 3). In Table 4, we categorise RAI tools in the corpus in terms ofthe stage of AI development for which they are designed to be used. We note that some tools can be used acrossmultiple stages of development (e.g., during training, testing, and deployment), and AI development does not alwaysmove linearly through these stages. A description of the RAI tools categorised in each stage of AI development can befound in Appendix B.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "cb73d4a0-8388-476b-885c-03428a6716d8",
                    "text": "Publications in the corpus position their contributions in di\ufb00erent ways:several publications describe their primary contribution as the evaluation of an RAI tool (e.g., [A9, A24, A25, A29]); inothers, evaluation practices are discussed, but the primary stated contribution is a description or demonstration of anRAI tool (e.g., [A2, A6, A8, A19]). In several publications, evaluation practices are not explicitly discussed, althoughtools\u2019 capabilities or e\ufb00ectiveness are still considered. In three publications the tool developers report on their ownexperience of using the tool, which includes discussions of tool impact [A4, A22, A32]. In four publications a case studyis provided to highlight how the tool has been used and to support claims related to e\ufb00ectiveness [A2, A3, A16, A19].In four publications, worked examples are used to demonstrate the tool [A1, A5, A6, A10, A14]. In a small number ofpublications only technical descriptions of an RAI tool are provided [A13, A17, A19, A26]. Finally, several publicationsdescribe iterative cycles of development and evaluation of an RAI tool [A7, A11, A18, A20, A34, A35], highlighting closerelationships between those developing RAI tools and those evaluating them. Similarly, several publications report the\ufb01ndings of evaluations conducted speci\ufb01cally to inform future tool development [A9, A15, A23, A25, A31].",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "722be54f-16c6-41a4-9bf1-81016e24586c",
                    "text": "This paper is the product of the authors\u2019 shared interest in the development of RAI tools and AI development practices.All authors are male, based in the Global North, and work for an industry research institution in various capacities.Several authors have conducted prior research contributing to the development of AI tools, guides, and other resources,as well as research studying responsible AI development and evaluation practices. As authors based in industry, we areattuned to the challenges of development and use of RAI tools in applied industry settings, and thus we have primarilyfocused on how RAI tools can be better evaluated for their e\ufb00ectiveness in such settings (although we discuss impli-cations for public policy). Future work should consider evaluation processes for the use of RAI tools in other settings,such as the public sector [cf. 203] or civil society.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "3ef29d63-0159-44b7-94da-5d8593ce756c",
                    "text": "In this section, we report the \ufb01ndings of our analysis of existing practices for evaluation of RAI tools. We structurethis section by themes about existing practices. For each theme, we describe how it manifests in the corpus, and wediscuss gaps between the existing practice and the theories of change for RAI tools. We emphasize that the gaps weidentify are at the level of collective practice. No gap is uniformly present across all publications in our corpus. In someinstances, the evaluation activity gaps we identify are also self-identi\ufb01ed by authors in their evaluation write-up\u2014ourcontribution is to demonstrate how these gaps manifest across a wide range of RAI tool evaluations. The themes wediscuss include the focus of existing evaluation practices on usability (rather than e\ufb00ectiveness); evaluating \ufb01t withexisting ways of working (rather than changing existing development practices); individual use (rather than use withinteams or organizations); and, evaluating \u201creal world\u201d use (without speci\ufb01city about what world(s) they are evaluatingRAI tools for). In Section 5 we open a conversation about how the \ufb01eld might support the development and adoptionof an e\ufb00ectiveness evaluation framework for RAI tools.Table 5 provides an overview of evaluations reported in the corpus. The majority of RAI tools in the corpus wereevaluated using qualitative methods drawn from HCI, including semi-structured interviews, think-aloud studies, andcontextual inquiries [156], as well as workshops or focus groups, and pilot or proof-of-concept case studies. For in-stance, contextual inquiry is used to explore whether users accurately interpret the visualizations produced by a ma-chine learning interpretability tool [A31]. Think-aloud studies are used to explore the internal states of users as theyinteract with a software toolkit [A20, A27, A30] or transparency artifact [A23]. Interviews are used to probe whetherEvaluation activity Count PublicationsContextual inquiryFocus groupIllustrative case studyInterviews (type unspeci\ufb01ed)Personal experienceSurvey (standalone)Survey (as follow up)Think-aloud interviewsWorked exampleWorkshop 2242337379 A28, A31A7, A29A2, A3, A16, A19A7, A31A4, A22, A32A27, A29, A31A7, A9, A20, A21, A24, A30, A34A20, A23, A30A1, A5, A6, A10, A14, A18, A32A9, A11, A20, A21, A24, A25, A27, A34, A35users interact with a software toolkit as intended [A15, A20], and to identify challenges in adoption of transparencyartifacts [A7, A8], a software toolkit [A29], and a workshop guide [A33]. Other mixed-methods approaches, such assurveys, are used in a smaller number of tool evaluations, for instance, as a data collection approach following a work-shop or pilot study of a transparency artifact [A7], workshop guide [A9, A21], or software toolkit [A20], or surveys areused as a way to understand the broader applicability of \ufb01ndings arising from small-scale interview studies of softwaretoolkits [A27, A29, A31].4.1 Usability as the primary evaluation method",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "7f4ae854-6eae-4ee7-874a-0a065e321ba1",
                    "text": "Choices about evaluation design, participant recruitment, and evalu-ation metrics are made\u2014often implicitly\u2014to understand the extent to which users of an RAI tool use it as the designersintended. Several publications in the corpus explicitly adopt a user-centered framework [A7, A15, A31, A35] or us-ability focus [A7, A15, A16, A20, A30] in their evaluation of an RAI tool. In others, evaluation design choices implicitlyre\ufb02ect a user-centered framework: data collection methods focus on capturing how individual users interact with atool [A15, A20, A23, A28, A31] and usability metrics are used for evaluations [A21, A29, A30]. Meanwhile, claims abouttool e\ufb00ectiveness are only directly discussed in four publications: in [A9] and [A21] the authors study the \u201cinitial ef-fectiveness\u201d of the tools they investigate through a case study of an educational workshop guide in a classroom [A9]and an empirical investigation of a workshop guide across a series of pilot workshops [A21]; and in [A34] the authorsposit that the \u201csocietal impact\u201d of the workshop guide they describe \u201cstems from its e\ufb00ectiveness as a tool to supportthe exploration of ethical considerations.\u201d Additionally, one publication in the corpus does distinguish usability frome\ufb00ectiveness, recognizing that \u201cthe metrics that \ufb02ow directly from\u201d user-focused evaluation techniques \u201conly partiallycapture what is instrumental to project success\u201d [A35]. And, several publications note in their future work or limitationssections the need for expanding evaluations to measure e\ufb00ectiveness \u201cin practice\u201d [A9, A21, A27, A31]. Outside ofthese four references, no other publication in the corpus identi\ufb01es e\ufb00ectiveness as the focus of their evaluation.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "84e067fb-6a11-4971-b6ef-55753b746847",
                    "text": "Usability is an important consideration in the design and evaluation ofRAI tools. However, RAI tools often aim to have impact in ways that may be in tension with, or outside the scope of,usability evaluations. In such cases, the results of an evaluation focused on usability may be misleading, and a focus one\ufb00ectiveness may be preferable: an RAI tool may be easy to use by practitioners, but ine\ufb00ective in terms of changingpractitioners\u2019 behavior or the resulting product they are developing. Relatedly, as [A28] discusses, a tool\u2019s ease of usemay result in over-reliance and over-trust in it. Re\ufb02ecting this, across several types of RAI tools, publications describethe tool\u2019s objective as being to prompt practitioners to re\ufb02ect on and disrupt their usual ways of working (e.g., via atransparency artifact [A4, A25], software toolkit [A27, A30], or workshop guide [A33, A34, A36]). In some cases, thepurpose of the RAI tool is to enable consideration or inclusion of the perspectives of stakeholders or communities whomay be a\ufb00ected by the algorithmic system being developed [A4, A33, A34]. Implicit in this purpose is the recognitionthat usual ways of working do not enable these perspectives to be included during AI system development. Giventhis, RAI tools may need to balance the needs of their users against the broader objectives of the tool in changing AIdevelopment processes towards more fair, transparent, or accountable systems. One publication [A30] recognizes thistrade-o\ufb00: the authors evaluate an automated tool in comparison to an interactive tool, noting that while automation\u201chas a bene\ufb01cial e\ufb00ect of improving e\ufb03ciency\u201d for users, \u201cit might come at the cost of reduced understanding\u201d on thepart of the user \u201cdue to lack of exploration.\u201d As we discuss in Section 2.4, by broadening the scope of evaluation fromusability to also include e\ufb00ectiveness, the \ufb01eld\u2019s evaluations of RAI tools can address both sides of this trade-o\ufb00.4.2 Fitting within work practices",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "fdde39bb-d616-42e4-931e-4c19f6a83faf",
                    "text": "Consistent with a focus on usability, evaluation practices focus ondetermining whether RAI tools are suitable for the existing work contexts of AI practitioners. This focus stems fromdesign requirements for RAI tools and assumptions made about how practitioners approach RAI-related work. Designrequirements, which, in an evaluation focused on usability, form the benchmark against which an RAI tool is evaluated,emphasize the need for RAI tools to integrate into practitioners\u2019 work\ufb02ows, across a range of RAI tool types (e.g., trans-parency artifacts [A4], software toolkits [A15, A16, A22, A27, A29], or workshop guides [A36]). For instance, sometool publications explicitly state that their users should be able to use the tool \u201cwithin their existing work\ufb02ow havingto write little or no code\u201d [A16], or modify the tool \u201cbased on their existing organizational infrastructure and work\ufb02ows\u201d[A4]. Design requirements also assume that use of an RAI tool will occur within a fast paced work environment. RAItools are thus framed as needing to be easy to use for busy practitioners [A2, A14, A16, A19, A27]. Tools need to enablerapid onboarding \u201cdue to workplace time constraints\u201d [A27]; need to be \u201capproachable\u201d [A14] and \u201ceasily adopted\u201d [A2];need to enable users to \u201ctry out\u201d features \u201cwithout reading documentation\u201d [A16]; and need to enable users to \u201cquicklytest hypotheses and build understanding\u201d [A19].",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "9f561785-1df7-471e-aa59-7e100545e244",
                    "text": "An underlying premise of RAI tools is that the default ways of developingAI systems are insu\ufb03cient to lead to the goals of responsible AI (e.g., fairness, transparency, accountability) on theirown. Re\ufb02ecting this, researchers have focused on the \u201cprinciples to practice\u201d gap in responsible AI, identifying variousobstacles to the translation of high-level responsible AI principles into changes in AI development practices [e.g.,67, 133, 178]. When evaluating an RAI tool, evaluators should consider whether framing the use of the RAI tool assubordinate to existing ways of working is appropriate, given the objectives of the tool under evaluation. It may be thecase that an RAI tool can only be e\ufb00ective when existing ways of working are changed. Re\ufb02ecting this, the authorsof [A29] note that it is \u201cimportant to consider whether toolkits with necessarily reductionist de\ufb01nitions of fairness areappropriate and bene\ufb01cial from a societal standpoint.\u201d Similarly, in their discussion of future challenges, the authorsof [A4] argue that, \u201corganizational infrastructure and work\ufb02ows\u2014not to mention incentives\u2014will need to be modi\ufb01ed toaccommodate\u201d meaningful adoption of an RAI tool.Emerging work on maturity models of organizational readiness for RAI [e.g., 21, 40, 94, 109, 121] suggests that orga-nizational culture, incentives, and processes may matter as much as (and are likely to impact) individual practitioners\u2019ability to use an RAI tool in their work. As such, existing ways of working should be accounted for in the evaluationof an RAI tool (and may act as a confounding factor for a given tool\u2019s e\ufb00ectiveness in a particular organizational con-text). For example, the size of a company in which an RAI tool is being used (and, similarly, the company\u2019s internalorganizational structure) may have a large e\ufb00ect on whether or to what extent that tool is e\ufb00ective\u2014e.g., smaller com-panies may lack the capacity to have dedicated teams or individuals focusing on RAI [cf. 168, 213, 217]. In fact, greaterprecision in investigating such questions (e.g., precisely how individual, team, or organizational factors may impactAI design processes or outcomes) is one potential result from more robust methodologies for evaluating e\ufb00ectivenessof RAI tools, as we explore further in Section 5.2.4.3 Replicating \u201creal world\u201d settings, tasks, or users",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "80c5d4cb-51ca-4bc4-9900-5b144d0209ee",
                    "text": "Publications describe their evaluation designs using the terms \u201creal-istic\u201d [A25, A26, A27, A31] or \u201creal world\u201d [A1, A9, A18, A20, A27, A28]. In particular, evaluations attempt to importthree aspects of the \u201creal world\u201d into the evaluation design: participants that are thought to be representative of \u201crealworld\u201d users [A7, A20, A27, A29] (but sometimes without an explicit de\ufb01nition of what representative means in thiscase [cf. 41]); activities for participants to undertake that are chosen to be re\ufb02ective of the sorts of activities practi-tioners will use the tool for in the \u201creal world\u201d [A1, A4, A9, A14, A27, A28, A30, A31]; and, \u201creal world\u201d environmentsin which such activities are thought to occur [A18, A31]. Re\ufb02ecting this, in [A27] the authors note they \u201cdesigned arealistic ML task in which we required practitioners to build an ML model based on a real-world dataset.\u201d The use ofscenarios designed to resemble \u201creal world\u201d tasks is repeated across the corpus.In many cases, this means relying on publicly available datasets for the tasks. In [A31] the authors note that \u201cwetried to put data scientists in a realistic setting,\u201d which consisted of a Jupyter Notebook that individual participants usedto explore a \u201cpublicly available ML dataset based on 1994 US census data.\u201d COMPAS data and recidivism predictionscenarios are used frequently [A1, A9, A14, A30], as are the German Credit Data dataset [A14, A16, A32] and the AdultCensus Income datasets [A14, A22, A28, A30, A31] in credit-worthiness and loans scenarios. Various other publiclyavailable datasets are also used [A2, A4, A6, A8, A27, A32]. Alternatively, participants may be asked to use the RAItool in \u201creal-world projects\u201d and report back \ufb01ndings [A18].Participants in evaluations are also often recruited for their verisimilitude to intended users of RAI tools. One pub-lication notes, \u201cWe recruited and explored the perspectives of people who we believe are representative of the future dataworkforce\u201d [A15]. Several publications in the corpus describe the \u201creal world\u201d users they recruit in terms of job title[A21, A23] or team membership [A24, A34], academic training [A20], or experience with certain technologies [A15,A27, A31]. Participants are often recruited through the networks of evaluators through direct outreach, snowball sam-pling, and outreach through online practitioner forums [A21, A8, A29, A27, A28, A15, A23, A31]. In some instances,recruitment of participants occurs exclusively in the large technology \ufb01rm which the evaluation authors work at [A7,A20, A31, A34]. In other instances, recruitment e\ufb00orts focus on computer science students [A9, A20, A25, A30].",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "9a71b1e3-16b5-4e84-9b01-ec3ed9ccaada",
                    "text": "Designers of RAI tools aspire for them to be used by practitioners in theirday-to-day work. As such, evaluating tools in settings that are as close as possible to those in which practitionerswork makes sense. However, evaluations of RAI tools capture only certain aspects of the contexts in which the tool isintended to be used. As such, current evaluations often fail to consider how decisions about which aspects of the \u201crealworld\u201d to capture in evaluations can impact the types of claims that can be made about how RAI tools are and will beused. Existing \u201creal world\u201d evaluations of RAI tools thus risk making claims to generalizability that are unsupported bytheir evaluation design. When participants in an evaluation, for instance, are recruited for their similarity to intendedusers of an RAI tool, evaluators must choose which aspects of intended users\u2019 identities and sociopolitical context fallwithin their de\ufb01nition of \u201creal world\u201d users. Are evaluation participants de\ufb01ned solely by their professional training?Doing so means that the potential for other aspects of their identity\u2014e.g., demographics, cultural background, ororganizational context\u2014to in\ufb02uence their use or impact of the RAI tool cannot be established by the evaluation. Onepublication [A9] reports the demographic background of evaluation participants; this publication also concludes witha short discussion of additional social contexts in which the e\ufb00ectiveness of the RAI tool should also be evaluated.An additional study, [A23], documents its decision not to collect or report demographic information, due to concernsabout framing and anchoring e\ufb00ects, and notes that future research should consider the relationship between useridentity and tool use.Although di\ufb00erent methods and research communities have di\ufb00erent standards for sample sizes [35] (and this shouldnot be used as a proxy for evaluation quality), the general reliance in evaluations of RAI tools on small sample sizesand evaluators\u2019 networks for participant recruitment, means claims that participants are representative of \u201creal world\u201dusers are particularly fraught\u2014or should at least be contextualized with greater detail about participants and theircontexts, as in rigorous qualitative research [e.g., 188]. Indeed, several publications in the corpus note their smallsample size as a limitation [e.g., A28, A29]]. Likewise, when tasks in an evaluation are designed to be similar to theways intended users are expected to interact with the tool, evaluators must determine which aspects of intended useto replicate in the evaluation design, as the authors of [A31]acknowledge when discussing the limitations of theirevaluation design. Will participants be allowed to use their usual computer setup, or will they need to log into anevaluation-speci\ufb01c environment? Will participants have a time limit to undertake the task? Will they be able to worktogether with teammates or other collaborators? These decisions are often described in terms of pragmatic constraints,but nonetheless need to be considered from the perspective of external validity of evaluations of RAI tools.Indeed, the importance placed on evaluating RAI tools in the \u201creal world\u201d demonstrates the implicit value placedgenerally on questions of external validity in evaluations of RAI tools, and in particular, on questions of ecologicalvalidity (as one sub-type of external validity). Yet, describing an evaluation design as a \u201creal world\u201d task, withoutdocumenting the ways the design inevitably di\ufb00ers from actual use, risks misrepresenting the extent to which an RAItool is ready for widespread RAI community adoption and makes determining the extent to which an evaluation hasecological validity challenging. We expand on limitations of the \u201creal world\u201d in Section 5.2, where we recommendreconsidering how external validity is re\ufb02ected in evaluation designs.4.4 Individual use of RAI tools",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "06fc0d02-a509-49de-934f-7648097e89b5",
                    "text": "Evaluations of RAI tools tend to study interactions between an in-dividual user and the tool (e.g., between a practitioner and a software toolkit [A15, A20, A28, A31], or a practitionerand transparency artifact [A23]), despite the collaborative nature of AI development (and software development morebroadly). Re\ufb02ecting this, evaluations in the corpus involve activities where individual participants interact in isolationwith an RAI tool, e.g. by using an interactive programming environment (e.g., Jupyter Notebook or Google Colab) toexplore a software toolkit [A20, A28, A31]. This individualistic conceptualization of tool use extends to the way evalu-ations envision how users might learn about a tool. For the most part, evaluation designs introduce participants to theRAI tool under evaluation by inviting them to read technical documentation [A29], follow a tutorial [A31], or interactwith a worked example [A16, A20, A28, A31]. A number of authors note that their evaluation designs were compro-mised by COVID-19 social distancing requirements [A9, A28, A30], which may have contributed to this tendency.In contrast, a smaller subset of evaluations introduce participants to the RAI tool under evaluation through moresocial modes of learning, such as by participating in a workshop [A7], or by using the tool in daily work for a number ofweeks [A9]. Where evaluations \ufb01nd that participants \u201cmisuse\u201d a tool, they are likely to interpret misuse as indicating\ufb02aws in the design of the tool, rather than in the onboarding or support provided to participants [A27, A28, A29].One evaluation reports identifying \u201cseveral gaps between the tools\u2019 capabilities and the practitioners\u2019 needs\u201d [A29], andanother reports that \u201calmost half of the participants were not able to provide the correct interpretation of what the modeldid using the [tool] alone\u201d [A15]. In some instances, such as [A15], while the evaluation activity only explores individualuse of an RAI tool, the broader evaluation write-up does consider how insights from individual user-tool interactionsmight translate into collaborative team environments. Re\ufb02ecting this, in [A15] the authors conclude their paper byobserving that, \u201cDecision-making with data and deep learning models is a collaborative and distributed process... For [theRAI tool] to be adopted and impact organizational processes, they must support knowledge sharing and negotiation acrossstakeholders.\u201d",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "809e7d28-21fa-45ba-be8f-ae80095f7250",
                    "text": "As the above extract from [A15] argues, development of AI systems isa complex social activity, often undertaken in collaborative team contexts, within a web of power relations [e.g.,157, 187, 214]. RAI tools aim to intervene in this social activity, and as such are themselves likely to be used in teamenvironments, where the intended user of an RAI tool may not be the team leader or primary decision maker. Addi-tionally, as many social theories of learning highlight [e.g., 205, 210], use of a tool is likely to occur alongside supportfrom more experienced peers. As such, evaluation designs in which individual participants who are thought to berepresentative of intended users interact independently with a tool are unable to consider how the team environmentmay a\ufb00ect tool adoption or e\ufb00ectiveness. While some publications noted that future work should include developingtraining workshops or mentor networks to support tool adoption [e.g., A2], most did not consider incorporating socialapproaches to learning into the evaluation design itself.An AI practitioner may \ufb01nd that following a protocol to produce a transparency artifact for a training dataset iseasy to do during an evaluation task, but then struggle to secure their manager\u2019s support for investing time in produc-ing such artifacts during their day-to-day work. Similarly, a practitioner may \ufb01nd using a software toolkit to evaluatemodel fairness straightforward during an evaluation task, but in practice \ufb01nd it di\ufb03cult to take action on insights fromtheir analysis within their broader team context [cf. 57, 132]. Re\ufb02ecting this, in one study, participants were placed ina scenario where they \u201cwere told that they would serve as the decision-maker for their team\u201d and were provided with anotebook containing \u201canalysis done by one of their team members\u201d which the study asked them to review [A28]. Such anevaluation recognizes that the RAI tool will be used in a team setting, but operationalizes that use in a way that placesthe interpersonal dynamics of a team outside the scope of the evaluation: communication between a team memberand the team leader is operationalized as review of a notebook; decision-making is operationalized as an individualre\ufb02ective task. While this may render the evaluation more tractable, it is likely to pose threats to external validity,particularly in terms of claims about the e\ufb00ectiveness of the tool in team settings that rely on di\ufb00erent communica-tion and collaboration approaches. More generally, it may also be the case that the aim of independent RAI tool useis unrealistic. The complexity of intervening to shift AI development processes towards more responsible practicesmay require both individual training in particular skills, knowledge, or tools, as well as collaboration from other teammembers or external stakeholders.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "56fad6a0-bbbd-4b79-a682-3797c103df47",
                    "text": "In Section 4 we highlighted patterns in existing evaluation practices. For each pattern we discussed gaps between thepractice and the aims of RAI tool developers and the RAI community. To address these gaps, in this section we applythe insights from evaluation practices outside the RAI community introduced in Section 2 to propose design desideratafor the development of an e\ufb00ectiveness evaluation framework, and suggest steps for RAI tool developers and the RAIand HCI community to take in support of more robust e\ufb00ectiveness evaluations of RAI tools.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "43033c1d-2131-4c06-8a28-a70055ed6e24",
                    "text": "The objective of an e\ufb00ectiveness evaluation framework is to support evaluators of RAI tools to design robust evalua-tions, commensurate to similar e\ufb00orts in the HCI and software engineering \ufb01elds to develop frameworks for usabilityevaluations [e.g., 53, 118], evaluations of ML models [103], and the design of empirical studies in software engineering[e.g., 116]. Below we provide initial design desiderata for the development of a framework that meets this objective.It is important to note that RAI tools are often evaluated within the context of an ongoing, resource-constrainedtool development process. As such, an e\ufb00ectiveness evaluation framework cannot exist in isolation from the processesby which RAI tools are developed. For instance, the resource constraints facing those developing RAI tools may needto be considered\u2014as it is likely those evaluating RAI tools will face similar constraints (indeed, they may often bethe same teams). Therefore, an e\ufb00ectiveness evaluation framework needs to enable evaluators to re\ufb02ect on potentialtrade-o\ufb00s or limitations they face in conducting evaluations of RAI tools, and to re\ufb02ect on their own relationship tothe developers and other stakeholders of the RAI tool under evaluation. Additionally, as discussed in Section 3.1, theintended users of RAI tools are often AI practitioners. Accessing practitioners and observing their work environmentsis a well-documented challenge [e.g., 200, 208], as re\ufb02ected in the tendency of existing evaluation practices to usemock scenarios and recruit participants from a single workplace (Section 4.3). An e\ufb00ectiveness evaluation framework,therefore, must provide evaluators with guidance even in situations where participant recruitment or engagement maybe challenging.An e\ufb00ectiveness evaluation framework must also o\ufb00er clear guidance on issues of internal and external validity,and trade-o\ufb00s between these, as the expanded scope of questions of e\ufb00ectiveness, compared to questions of usability,heightens the importance of careful justi\ufb01cation of evaluation claims. A claim that a tool is easy to use has far narrowerscope, and less signi\ufb01cant consequences if invalid, than a claim that a tool is e\ufb00ective at addressing a particular ethicalissue during AI development. In this regard, the discussion of external validity and best practices in education evalua-tions in Section 2.5 can provide guidance for the development of an e\ufb00ectiveness evaluation framework. In particular,the emphasis placed in other \ufb01elds on documentation requirements for evaluation is instructive (see the discussion inSection 2.4 on the IES\u2019s What Works Clearinghouse). Detailed documentation of evaluation design, participant recruit-ment and demographics, and evaluation limitations is needed, as all of these have implications for the broader set ofuse cases, contexts, and populations for which evaluation \ufb01ndings are likely to be relevant, as with external validity ofevaluations in other \ufb01elds [e.g., 80, 194]. Ideally, evaluations should be documented with enough detail to enable RAIcommunity members who are considering adopting an RAI tool to determine whether evaluation settings are likely tobe comparable to their own context.Further work is required to develop these desiderata into a practical framework to guide evaluations of RAI tools.This work will need to be collaborative, engaging RAI tool designers, users, and stakeholders, and drawing on theexpertise of the RAI community. Any framework will need to be maintained and updated over time. The What WorksClearinghouse\u2019s Procedures and Standards Handbook [212] provides one example of a highly formalised approachto framework development and maintenance. Northwestern University\u2019s Machine Learning Impact Initiative, whichproduced a framework for evaluation of ML applications [88], provides another example of framework developmentwithin a collaborative academic context. Finally, the development of a sociotechnical framework for explainable AIby Ehsan et al. [66] provides an additional example of framework development and evaluation through a long-termindustry-academic partnership. In the intervening period, however, RAI tool evaluations can be improved, in terms oftheir robustness and focus on e\ufb00ectiveness, by attending to the validity concerns outlined in Section 2.5.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "feff17de-a626-46ce-9edc-2cdfa2aaaf9d",
                    "text": "This section builds on the validity concerns described in Section 2.5, and o\ufb00ers practical guidance on addressing threatsto validity during RAI tool evaluation. We note that issues of internal and external validity are intertwined, and ad-dressing one can at times exacerbate the other. As such, we outline steps for improving internal and external validity,and then discuss how trade-o\ufb00s between them can be managed during evaluation design. Across this section, we usethe example of a dataset transparency protocol to illustrate our suggestions. In this context, we conceive of a datasettransparency protocol as an RAI tool designed to enable data scientists to document relevant aspects of a dataset in astandardized manner [e.g., 23, 78, 163].",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "aca0a2d2-d0ff-4215-8213-4bf91b3592f6",
                    "text": "refers to the aspects of evaluation designthat support claims of a causal relationship between variables (i.e., between use of an RAI tool and some desired changein AI development processes). As such, two prerequisites for internal validity in an evaluation are: (1) clarity on whatthe evaluation is attempting to measure, and (2) clarity on any potential confounding factors. Re\ufb02ecting this, softwareengineering and HCI guides for experimental or evaluation design begin by recommending that evaluators clearlyde\ufb01ne the evaluation or research question which their design will attempt to answer [e.g., 85, 116, 118, 125]. In thecontext of evaluations of RAI tools, internal validity is di\ufb03cult to establish when there is a lack of clarity on the goals ofthe tool itself, or on how e\ufb00ectiveness of the tool is de\ufb01ned. A dataset transparency protocol, for example, could havea goal of improving dataset curation practices, or of reducing misuse of datasets in ML training processes. For eachof these goals, e\ufb00ectiveness can be de\ufb01ned in multiple ways, and di\ufb00erent confounding factors may be relevant. Anadditional challenge to internal validity is the dynamic and highly interdependent nature of AI development processes,which can confound attempts at establishing a causal relationship between RAI tool use and desired outcomes. Giventhis, for the purposes of improving internal validity, what matters is that evaluators are speci\ufb01c about the particulargoals against which they are evaluating an RAI tool.RAI tool evaluators can begin to address internal validity in two ways. First, evaluators can increase documenta-tion of the confounding variables that may a\ufb00ect any claims they make as to tool e\ufb00ectiveness. Whilst this will notmitigate the impact of confounding variables, it will enable readers of an evaluation to better interpret evaluation re-sults. Second, RAI tool evaluators can articulate a theory of change for the e\ufb00ectiveness of the RAI tool, which can beused to guide identi\ufb01cation of confounding variables and prioritise their mitigation. A theory of change is a workinghypothesis for how adoption and (intended) use of a given tool will achieve the tool\u2019s goals. The theory of changeexplains, in a sequence of causal statements, precisely how use of the tool in a speci\ufb01c context may achieve the desiredgoals. Ideally, each causal statement in a theory of change will be evaluated. Where this is not possible, identifyingand documenting the assumptions underpinning a given tool will enhance the ability of third-parties to determine forthemselves whether the tool is likely to be applicable in their context.As an illustrative example, for a dataset transparency protocol, one goal may be to ensure downstream users of thedataset know how to use it for tasks or contexts for which the dataset is appropriate. A corresponding theory of changemay be: If documentation creators use the tool to record relevant characteristics of datasets, and if practitioners reviewa dataset\u2019s documentation before deciding to use it, then training datasets will only be used in training tasks for whichthey are appropriate. Pertinent confounding variables this theory of change reveals include: documentation creators\u2019expertise or lived experience, which may a\ufb00ect how they use the tool and what characteristics of the dataset theyidentify as relevant ; and, the various factors practitioners must balance when determining whether to use a givendataset (e.g., availability of alternative datasets).",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "4895163c-f99f-4ad6-8814-dcb73d386cff",
                    "text": ", in the context of an RAI tool evaluation,refers to the extent to which evaluation \ufb01ndings may be generalized to new contexts. The extent to which externalvalidity is a concern for RAI tool evaluators largely depends on the sorts of claims they wish to make about the tool.In a usability study, external validity can be enhanced through careful documentation of study participants, their de-mographics, their existing skills, and the training or support they received during evaluation activities\u2014each of whichmay constrain external validity. In an evaluation focused on determining RAI tool e\ufb00ectiveness, however, these stepsare insu\ufb03cient as the scope of the evaluation is inherently broader. Here, conceptualizing RAI tools as interventionsin AI system development and deployment, as we discuss in Section 2.1, is instructive: the e\ufb00ectiveness of an interven-tion is inherently tied to the social context in which it occurs. As such, to address external validity in an e\ufb00ectivenessevaluation requires RAI tool evaluators to consider the role of social context in shaping the outcomes of RAI tool use.In other words, external validity requires RAI tool evaluators to ask: to what extent is the apparent success of this RAItool a result of social factors (e.g., pressure from management, high buy-in from evaluation participants, a workplaceculture that emphasises ethics), rather than tool characteristics?As with internal validity, the theory of change behind an RAI tool can serve as a useful starting point for attendingto external validity in RAI tool evaluation. Returning to our illustrative example, for a dataset transparency protocol,we outlined two causal links in the theory of change (there are likely others too): documentation creators must use thetool correctly, and practitioners must review dataset documentation when making decisions about using the dataset.Both of these links present threats to external validity. Snowball or convenience recruitment methods, for example,might result in a sample of participants who are far more highly engaged in RAI issues and discourse than the targetuser group for the tool, leading to evaluators over-estimating the e\ufb00ectiveness of their tool. To mitigate this, thesocial context associated with each causal statement in the theory of change can be documented as a limitation toexternal validity, or can be used to expand the scope of evaluation practices\u2014i.e., from largely observational studieswith individual practitioners using the tool in in-lab studies or single-site case studies (see Table 5). For instance, eval-uations of e\ufb00ectiveness might instead adopt wider units of analysis: e.g., from individual users to team-level analyses;from single site to multi-site comparative studies.New evaluation approaches may be needed to support these team-level analyses as well as cross-context compara-tive studies. Approaches from the social sciences, such as educational intervention evaluations [e.g., 5, 194] and publichealth or economic policy evaluations [e.g., 80] may be constructively adopted by the RAI community. At a communitylevel, such approaches may o\ufb00er evidence of the positive impact of robust evaluation practices; at an individual eval-uator level, such approaches may o\ufb00er guidance on implementation of new evaluation methods. Additionally, recentwork has explored how existing algorithmic fairness paradigms originating in Western contexts may not be applicablein the global contexts of AI systems\u2019 use [e.g., 25, 82, 84, 161, 176]. Such work is likely to be fruitful in informingmore comparative, cross-context studies evaluating the e\ufb00ectiveness of RAI tools in di\ufb00erent cultural contexts, forinstance, by indicating aspects of RAI tool use that are likely to change from context to context, and highlightingculturally-speci\ufb01c assumptions encoded in tool designs.An important aspect of external validity is ecological validity, which is the extent to which an RAI tool evaluation isre\ufb02ective of a \u201creal world\u201d. Indeed, addressing ecological validity may prompt RAI tool evaluators to reconsider howevaluation settings relate to the \u201creal world\u201d. When RAI researchers discuss \u201creal world\u201d studies, it should be clearwhich aspects of the world (or even which world [cf., 71, 196]) their study is intended to emulate. The distinction be-tween implicitly synthetic or arti\ufb01cial laboratory settings and the messy reality of the everyday may be helpful, insofaras it invites re\ufb02ection on di\ufb00erences between the context in which an RAI tool is developed and tested [172], and thecontexts in which the tool will be deployed [83]. However, the contrast risks making the same positivist assumption asthat underpinning the distinction between so-called \u201craw\u201d and \u201cprocessed\u201d data: that there is a single, shared world (i.e.,a world in which data exists in its raw state, separate from its situatedness in sociocultural systems). This assumptionis ill-suited to the task of evaluating RAI tools for two reasons: \ufb01rst, in its focus on a singular world, the assumptiondownplays the role of culture in co-constructing social worlds, including in co-constructing the ways users interpretand interact with RAI tools and AI systems. Second, the con\ufb02ation of the \u201creal world\u201d with the world of intended RAItool users, who are overwhelmingly AI practitioners working in WEIRD (Western, educated, industrialized, rich, demo-cratic) contexts [see, for a discussion of this acronym, 95], obfuscates critical discussion about the ways that WEIRDnorms and logics may shape the adoption of RAI tools.Additionally, when participants in RAI evaluations are described as \u201creal world\u201d practitioners (see Section 4.3.1),the downplaying of cultural context and tacit centering of WEIRD norms can be seen in the aspects of participants\u2014such as, their cultural background and physical location\u2014that are (not) documented. Here, Haraway\u2019s god trick [89]is in full e\ufb00ect: evaluation participants representing only a narrow slice of a world are rendered as universal (andtherefore their interactions with an RAI tool as objective and generalizable) by representing them as unsituated andplaceless. Indeed, beyond greater re\ufb02ection on the choices implicit in operationalizing the \u201creal world\u201d in the design ofevaluations, development and adoption of an e\ufb00ectiveness evaluation framework may result in evaluators abandoningthis \ufb01ction in evaluation designs altogether. Instead, the external validity of evaluations may be improved by adoptingmore modest aims, such as evaluating the e\ufb00ectiveness of an RAI tool in a speci\ufb01c practice and context.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "c8dfeb72-784f-41ed-b949-558e1bd6bfe1",
                    "text": "As discussed in Section 5.1, building on existingusability evaluation practices to also consider evaluation of RAI tool e\ufb00ectiveness will necessarily amplify threats tointernal and external validity. For evaluators, responding to this is complicated by the fact that prioritizing one type ofvalidity often leads to concessions on the other type. In the example of a dataset transparency protocol, for example,prioritizing internal validity may lead to a highly controlled in-lab evaluation task (e.g., where participants interact withan example dataset and scenario, following structured prompts). Here, the impact of variations in participants\u2019 expertiseor lived experience can be managed through random assignment of participants to control and treatment groups (i.e., agroup who does not use the dataset transparency protocol and a group that does). Such an evaluation design, however,will necessarily have limited generalizability, due to the arti\ufb01ciality of the evaluation set up. Conversely, prioritizingexternal validity in the design of an evaluation of a dataset transparency protocol may lead to a longitudinal studyof day-to-day protocol use in multiple sites, across multiple teams. This design will necessarily have weak internalvalidity due to the number of confounding variables, but may have strong external validity if participant recruitmentand site selection are carefully managed.In general, narrowing the breadth of evaluation claims, particularly regarding generalizability, will also narrowthe breadth of external validity issues, thereby reducing pressure on the trade-o\ufb00s between internal and externalvalidity. In the dataset transparency protocol example, we might adopt a snowball sampling strategy, but then includein our evaluation write-up a summary of participants\u2019 professional training or experience (e.g., as in [30, 50]), anda discussion of the extent to which participants\u2019 backgrounds is likely to be representative of the backgrounds ofintended tool users. However, given the breadth of di\ufb00erent contexts in which RAI tools are used, the ongoing evolvingnature of AI development practices, and resource constraints facing evaluators, it may be impossible to meaningfullyaddress all confounding variables in an evaluation design. In such a situation, one productive approach may be torefocus evaluation e\ufb00orts away from determining the overarching e\ufb00ectiveness of an RAI tool and towards identifyingindicators for e\ufb00ective use of such a tool. Here, confounding factors, such as the overarching culture or risk approachin a workplace where a tool is trialed, are reconceptualized as potential indicators of e\ufb00ectiveness, enabling evaluationactivities to focus on surfacing all confounding factors (e.g., through participant observation of in-situ tool use), ratherthan on attempting to control for such factors. Future work will be required to rigorously identify such indicators anddevelop appropriate measures. E\ufb00orts in software engineering to identify capability measures that are predictive ofe\ufb00ective software development processes may be a useful resource to support this work [e.g., 68, 139], although, asthese e\ufb00orts demonstrate, a \ufb01eld-level approach will be required to ensure any capability measures are valid acrossthe wide range of organizational and culture settings in which RAI tools are intended to be used.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "b4289f53-455c-44dc-9feb-81f872d11abf",
                    "text": "RAI tool developers and evaluators face signi\ufb01cant resource constraints, and organizational settings that may not in-centivize or reward RAI work [168, 200] or may actively disincentivize it [133]. While working towards an e\ufb00ectivenessevaluation framework that responds to these constraints, we must also work towards improving the resources devotedto RAI tool evaluation. In this Section we identify three \ufb01eld-level challenges to more robust e\ufb00ectiveness evaluationsof RAI tools, and propose \ufb01eld-level responses to these.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "a2e24da7-5b84-4196-8b89-824fe2d7b7f2",
                    "text": "Operationalizing e\ufb00ectiveness through the designof evaluations targeted at each causal link in a theory of change will require the development of new measures fore\ufb00ectiveness. For instance, if researchers evaluating an RAI tool articulate a theory of change that involves the toolchanging development practices in particular ways, which are then intended to change the system outcomes in par-ticular ways, each of those steps might have their own measures that can be tracked as part of an evaluation\u2014as wellas their own confounding variables. In an experimental study (e.g., such as a randomized controlled trial), one mightinvestigate the causal e\ufb00ect of introducing an RAI tool by comparing such measures from teams who used those toolswith a relevant control group. However, the RAI community does not yet have a clearly de\ufb01ned set of what suchprocess- or outcome-level measures might be (nor how to think about what might constitute a control group).An additional challenge is developing process- and outcome-level measures that can be consistently operationalisedacross RAI tools and their intended user groups. Critics of the role that \u2018learning outcomes\u2019 play as the metric de rigueurin evaluating educational interventions, however, highlight three risks arising from an overly narrow focus on a singlemeasure [101]: learning outcomes can appear speci\ufb01c, but in reducing complex phenomena into discrete outcomes canbecome detached from the phenomena (i.e., learning outcomes might measure test-taking skills, not learning); learningoutcomes can be insensitive to the di\ufb00erences between disciplines; and, enforcing learning outcomes as the only eval-uation metric can have the unintended consequence of suppressing innovation [cf. 36]. To mitigate these risks, Husseyand Smith [102] have developed the concepts of \u2018predicted/unpredicted\u2019 and \u2018desirable/undesirable\u2019 learning outcomes,arguing that this enables a more \ufb02exible approach to evaluation. Analogously, for each stage of the AI pipeline, existingresearch [e.g., 28, 168] may be able to identify a set of desirable or undesirable RAI practices. Field-level alignment onappropriate proxies for these practices could be a useful \ufb01rst step towards developing a consistent approach to RAItool evaluation, with RAI tool developers identifying which RAI practices they predict will be improved, and RAI toolevaluators determining the predicted and un-predicted changes in RAI practices as a result of tool adoption.Alternatively, evaluation designers might look to prior research on \u201cprocess improvement\u201d in software development[e.g., 197] for potential indicators of changes in development processes\u2014however, some indicators from that line ofwork, such as product quality, might need to be adapted for responsible AI, such as by developing de\ufb01nitions of \u2018qual-ity\u2019 that re\ufb02ect impacts on marginalized communities and society [cf. 166]. Similarly, researchers might look to priorresearch on designing user experiments for software tools [e.g., 118] for inspiration for such process-level indicatorsof e\ufb00ectiveness. However, the question of which outcome measures may be most relevant for RAI tool evaluations (e.g.,aspects of the AI system that might be changed by the use of an RAI tool; or the resulting fairness-related harms toimpacted stakeholders) will require serious re\ufb02ection and substantial e\ufb00orts from the \ufb01elds of HCI and RAI.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "500a84ac-51a2-413d-8199-54449f38171b",
                    "text": "In widening the scope of RAI tool evaluations we shouldconsider the ways evaluation practices signal who and what is valued in the RAI \ufb01eld [cf. 124]. Who decides when anexternal RAI audit and review framework has led to improvements in an AI development process, and, by extension,to improvements to the AI systems developed? The goals of an RAI tool may help evaluators identify the relevantstakeholders in its evaluation. For an RAI tool whose goals are related to mitigating the impact of discriminatoryAI systems, for instance, evaluation may require involving stakeholders beyond AI practitioners (e.g., members ofcommunities impacted by discriminatory AI systems) in determining whether and in what ways the tool is e\ufb00ective,such as by contributing to designing and conducting the evaluation.Some RAI tools are targeted at broader audiences than just AI practitioners and (perhaps as a result of this broaderset of intended users) are also evaluated with a wider set of stakeholders [e.g., 164]. This, as well as recent work callingfor more, and deeper, participatory approaches to the design of AI systems [e.g., 9, 27, 54, 122, 186] and involving ev-eryday users in \u201ccrowd audits\u201d of AI systems [e.g., 55, 60, 123, 183] suggest opportunities for widening the lens of whomay be involved in evaluating the e\ufb00ectiveness of RAI tools. However, open questions remain about precisely howto involve such stakeholders in designing or conducting such evaluations\u2014although there are lessons to be learnedfrom participatory action research and community-based participatory research [e.g., 45, 69, 91, 92, 169, 195, 206].For instance, how might impacted community members contribute to determining what it means for an RAI tool toe\ufb00ectively lead to more responsible AI systems for their community?",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "90c4d5a6-1abe-4a81-80f4-fbda31071e8b",
                    "text": "Translating the desiderata in Section 5.1 into an e\ufb00ec-tiveness evaluation framework, along with evaluation guidelines and methods for researchers to apply in their eval-uation study designs, will require support from the RAI\u2014and broader HCI\u2014community. In part, support might helpexpand the evaluation toolbox available to researchers developing, evaluating, or otherwise studying RAI toolkits [e.g.,112, 157, 209, 218], allowing them to target their evaluations for particular goals and choose the methods appropriate tothose evaluation goals. For instance, descriptive, observational studies that explore how such tools or interventions arechanging practices, can be complemented by experimental or quasi-experimental studies that identify causal e\ufb00ects ofan intervention [cf. 80].More generally, since any individual researcher may not have the expertise, capacity, or incentives to conduct longi-tudinal, multi-site evaluation studies, the RAI community might provide opportunities to support or incentivize suchwork. This might include workshops, tutorials, or special tracks at conferences to share best practices for interven-tion evaluation methods. In HCI, for instance, the \u201cRepliCHI\u201d special interest group was convened in response to thereplicability crisis in psychology and related \ufb01elds [106], as a way to foster replication studies (and to focus on repli-cability as a desirable aspect of HCI research) [215, 216], while the 2023 CHI conference saw the \ufb01rst Special InterestGroup on \u201cHuman-Centered Responsible AI\u201d [192]. Additionally, HCI researchers have proposed addressing partici-pant recruitment and design replicability challenges through the development of shared evaluation platforms, whichmight include reusable evaluation design guides and participant recruitment pools [53].In addition, incentivizing such evaluations might require addressing the pro\ufb01t motive underlying technology \ufb01rms\u2019investments in responsible AI work [219] through support of new centers or clusters of research institutions, multi-stakeholder initiatives (e.g., Partnership on AI), or the involvement of government agencies (e.g., the U.S. NationalInstitute of Standards and Technology) or inter-governmental organizations. For instance, the World Bank has cre-ated the Development Impact Evaluation (DIME) [2] and the Strategic Impact Evaluation Fund (SIEF) [4] to fund andincentivize robust impact evaluations of development initiatives. As previously mentioned, the U.S. Department ofEducation\u2019s Institute for Education Sciences has developed the What Works Clearinghouse [5] to aggregate evalu-ations of particular educational interventions (e.g., curricula, programs, educational models) and provide consistentstandards for reporting and evaluating the robustness (including internal and external validity) of such evaluations.While not a clearinghouse, the International Organization for Standardization (ISO) and International ElectrotechnicalCommission (IEC) have similarly developed standards for software process evaluation [68]. An additional model theRAI community might look to is the Open Science Framework [3], which provides a consistent framework and publiclyaccessible platform for reporting details of research studies, and which is widely used to \u201cpre-register\u201d hypotheses be-fore conducting an experiment [149], to avoid potential threats to validity from hypothesizing after results are known[114]. What might such e\ufb00orts look like for the evaluation of RAI tools?",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "bf5f4cd6-c1d0-4e3f-be3c-bf1d9a51ae52",
                    "text": "In this paper, we conduct an inductive thematic analysis of a purposive sample of publications discussing RAI toolsand their evaluation. Several limitations are inherent in the purposive sampling strategy we adopted, which impactour ability to make representative claims about RAI tools and evaluation practices. Our sampling strategy did notconsider regional or cultural background of tool creators, as this information was rarely included in publications, andsimilarly our search for RAI tools was limited to English-language publications. As such, our study cannot shed light onpotential di\ufb00erences in evaluation practices across regions or cultures. We recognize that this is a signi\ufb01cant limitation,as existing work highlights the importance of regional contexts in the formal and informal evaluation and regulation ofRAI practices [176]. Our sampling strategy also focused on breadth of RAI tools, rather than on breadth of evaluationmethods. Evaluation activities contained within our corpus were largely qualitative (as shown in Table 5), and as suchqualitative methods are the primary focus of our \ufb01ndings and discussion. Additionally, although publications are asigni\ufb01cant component of HCI and ML research, and a primary way of sharing research within academia, they do notre\ufb02ect the entirety of RAI tool evaluation practices (e.g., if they are conducted and shared internally, but not published).It is possible that evaluations of the RAI tools we analyze have taken place, but they have not been publicly reported.Finally, our review of evaluation methods from the HCI and education \ufb01elds is not exhaustive. We draw from these\ufb01elds to highlight the range of evaluation methods and resources which could be adapted for RAI tool evaluations. Werecognise, however, that evaluation approaches in these \ufb01elds are also debated and continue to evolve [cf., regardingrandomized controlled trials for medical interventions 37].",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                },
                {
                    "id": "4cf16a8d-8c10-4712-9142-054830476909",
                    "text": "Responsible AI tools are envisioned to lead towards more responsible AI development practices and systems. We con-ducted a qualitative analysis of publications that discuss RAI tools to understand current practices for their evaluation.We \ufb01nd that the \ufb01eld has focused on evaluating the usability of RAI tools, rather than on evaluating the e\ufb00ective-ness of those tools in changing development practices and outcomes. We identify gaps between the scope of existingevaluation practices and the ambition of RAI tools, particularly in terms of evaluation design, tasks, and participantselection. To address these, we propose the development of an e\ufb00ectiveness evaluation framework for the RAI \ufb01eld,informed by best practices from intervention evaluations in education and medicine. We consider design desideratafor such a framework, and o\ufb00er initial guidance for RAI tool evaluators to improve evaluation validity. Finally, we o\ufb00erideas for how the HCI and RAI communities might support more robust evaluations of the e\ufb00ectiveness of RAI toolsin intervening in AI system development to lead towards more responsible AI.ACKNOWLEDGMENTSThank you to Ben Hutchinson, who was an early supporter of this research project and provided invaluable advice,and to colleagues at Google Research, and the anonymous reviewers for their feedback and deep engagement with thiswork.",
                    "reference": "[1] Gilad Berman, Navrati Goyal, and Michael Madaio. 2024. A Scoping Study of Evaluation Practices for Responsible AI Tools: Steps Towards Effectiveness Evaluations. arXiv:2401.17486. Retrieved from https://arxiv.org/pdf/2401.17486"
                }
            ]
        },
        {
            "paper_title": "Making Responsible AI the Norm rather than the Exception",
            "authors": "A Gupta",
            "publication_info": "arXiv preprint arXiv:2101.11832 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2101.11832",
            "chunks": [
                {
                    "id": "1b2310f6-d256-49d1-8b87-65c69744f0a0",
                    "text": "a. This can be achieved if we are cognizant of the friction points that we encounterin the existing engineering workflows and working with the engineering teams to. An example of this isunblock these will be criticalhighlighted in some early work that MAIEI presented at ICML 2020 Challenges inDeploying and Monitoring Machine Learning Systems titled Green Lighting ML:Integrity, and Availability of Machine Learning Systems inConfidentiality,Deployment .  increasing adoptionin",
                    "reference": "[1] A. Gupta. 2021. Making responsible AI the norm rather than the exception. arXiv:2101.11832. Retrieved from https://arxiv.org/pdf/2101.11832"
                },
                {
                    "id": "67474167-5bc2-47cb-8d02-1c8fe4526b5f",
                    "text": "a. This also requires buy-in from stakeholders (which can be achieved throughnot a barrier to) across the value-add chain and organizationalhighlighting the value that responsible AI brings and how it isachieving organizational goals 4                                                              tweaks thatmove them and their teams towards building more responsible AI systems. empower the stakeholders to take necessary action that can",
                    "reference": "[1] A. Gupta. 2021. Making responsible AI the norm rather than the exception. arXiv:2101.11832. Retrieved from https://arxiv.org/pdf/2101.11832"
                },
                {
                    "id": "516955e3-778a-4d94-82b0-35269703f3ab",
                    "text": "a. And finally a translation of the myriad standards, policies, principles, guidelines,and regulations into more measures that are tied to engineering\u200bworkflows will help in the integration of these notions into our everyday practices.  actionableIn addition to specific comments on the various points highlighted in the Document, which iswhat we begin this report with, we also make some proposals along the lines of: 1. Learning, knowledge, and information exchange",
                    "reference": "[1] A. Gupta. 2021. Making responsible AI the norm rather than the exception. arXiv:2101.11832. Retrieved from https://arxiv.org/pdf/2101.11832"
                },
                {
                    "id": "3214590f-5a54-4023-8fba-973d96ca5e7f",
                    "text": "that we believe will help improve the efficacy of the measures outlined in the Document. All ofour comments on the specific ideas in the Document are embedded in the above framework tohelp contextualize our recommendations.  Each of the areas follow the template of defining what we mean by that area, why it is needed,what it will look like, how to put it into practice, and how to measure the impact. This frameworkwe believe will be essential in bolstering the focus on operationalization of the measureshighlighted in the Document.  We expand on these in this report and provide more details on how it will aid in the principles,gaps, and future research directions as articulated in the Document.  Reiterating our strong endorsement of the ideas highlighted in the Document from the NSCAI,we hope that this report aids the iteration process of the Document and we are available andmake Responsible AIeager to work with the NSCAI and other US Government Agencies to. the Norm rather than the Exception\u200bFounder and Principal Researcher, Montreal AI Ethics Institute Machine Learning Engineer and CSE Responsible AI Board Member, Microsoft Abhishek Gupta  5                                                              Some general comments Organizational  1. Within each of the services that have been identified as the beachhead adopters for thethemeasures and recommendations made in the Document,individuals who will be in-charge is going to be important:  identification ofa. As it will help those individuals prepare and lay down the groundwork forb. integrating these ideas into their organization\u2019s work It will also provide for public accountability mechanisms that will help externalstakeholders get a sense of the concrete progress that is going to be made onthe implementation of the ideas in the Document. 2. Sharing the criteria for the selection of these organisations will also be importantfrom a public trust building perspective since this was a comment that emerged in theworkshops repeatedly as a concern.  3. There would need to be a balance between the flexibility offered to the different USGovernment agencies and the degree of prescription so that there isn\u2019t too much leeway  a. On the one hand too much prescription might hamper emergent efforts from theorganizations on the ground as they field AI systems.  b. But, having inconsistency in application of these standards will also limit thethe USusefulness in getting a strong Responsible AI posture across allGovernment agencies.  4. The earlier that there isGovernment agenciescan support and bolster each other.  shared vocabulary developed across the different US, the more likely it is that the efforts happening across the boardreliable,a. Building on the above point, when the following terms are used in the Document,robust, and understandable, having more accessibletrustworthy,definitions that take into particular consideration that audience from whoseperspective we are considering reliability, as an example, will help those chargedwith putting these ideas into practice with more clear guidance on which aspectsof these ideas to prioritize. b. This will also help the system operate more efficiently by\u200bjudging when there might be a violation of the different agreed upon definitions asthey relate to Responsible AI.   checks and balances5. While the Document does mention training of the workforce as a critical requirement, wewould like to reiterate the importance of training that accounts for human-computerinteraction in a way that makes the human operators and collaborators with the 6                                                              automated systems competentautomation bias  or algorithmic aversion .  to the point that they don\u2019t suffer from eithera. The token human problem is definitely something we would want to avoid in thiscase, as highlighted famously in the Air France 447 fatal crash where the crewwas ill-prepared for a handover when the automated system disengaged .   b. This also relates to the articulation of the performance standards and metrics ashighlighted in the Document. The staff needs to be trained adequately to be ableto interpret these metrics correctly and make informed decisions based on them. would be\u200bquite useful so that people develop a better understanding of probabilitiesbecause it has been demonstrated time and again that people struggle fromunderstanding those well.  c. When it comes to training, having things like Guess the Correlationd. Understanding of both the fundamental concepts and the high-level concepts interms of how the system components interact with each other is important for theefficient collaboration of human operators with their machine counterparts.   In addition, training to recognize human cognitive biases so that human operatorsworking with machines can recognize where they might be succumbing to themwill help to increase the efficacy of the other measures that are going to be put inplace. e.f. With these points in mind, an examination of the human-human interface asmediated by machines might also warrant investigation and research to makethem more amenable to achieving the goals of Responsible AI.   6. The Document already highlights the alignment that exists between the measures andideas proposed in the Document with the DoD principles in the Appendix, a moredetailed analysis of any other principles and approaches adopted by the other USGovernment agencies should also be considered and shared with the public to build.  trust and confidence in the ability of all the agencies to work as a  issurrounding defense and nondefense uses specifically in relation to the thoroughnesswith which the Responsible AI principles will be applied.  more explicit enunciation in the Document7. Something that could bear harmonized system8. Augmenting the already mentioned measures in the Document, we also recommend theinclusion of a \u201cbug bounty\u201d program that can centralize the communication of failuremodes and instances so that there is a shared understanding of how and where thesystems go wrong and tapping into network-wide expertise to address those failures.This can be bolstered through the use of the LKIE as elucidated in this report.  a. This is of great importance because AI systems smoothen out small errors whilemaking larger, more catastrophic errors much more likely and they can surface in 7                                                              unexpected ways making their prediction and preparation to counter their effectsmuch harder.  9. In working to incorporate value judgements in the selection of the objectives of thesystem, the Document mentions the involvement of diverse stakeholders, but we wouldfurther add that theorganization will make it an inevitable part of the everyday processes rather than havingto think about it in every instance when an AI system is being procured or developed.  codifying this as a requirement in the operations ofa. A related concern is the problem of who may either claim to\u200brepresent a community or have had past experiences in representing acommunity but don\u2019t any longer either because of a shift in their own livedexperiences and/or the evolution of the needs of that community in the first place.  faux gatekeepersb. Making sure that we are aware of these systemic failings will help to ensure thatrepresentation doesn\u2019t into a tokenization problem when considering theinclusion of diverse stakeholders in making determinations about the objectivesof the system.   fall10. The capabilities and limitations of AI systems are fast-evolving and unevenly distributedacross various ML subdomains and industry domains meaning that the trend predictionin the success ofcapabilities of the US Government agencies will be essentialkeeping the Responsible AI initiatives up-to-speed and ready to handle the emergentlandscape. This can be achieved quite effectively through the proposed LKIE.  a. Related to this is the point of keeping an eye out on adjacent fields that mighthave faced similar challenges in the past and can serve as good models to learnand adapt from to create solutions to challenges in the field of AI, for examplebioethics.  11. While diversity is a term that will constantly evolve to incorporate new social categoriesas they emerge in society, having a public rubric that shares how those considerationsare incorporated into the various organizations will be important to build trust but alsoempower public accountability.  12. From a budgetary standpoint, making mandatory line items that relate to investigationand implementation of Responsible AI will help to make them more of a norm rather thanthe exception as we move towards normalizing these ideas within the work of thedifferent US Government agencies.  Technical  1. When thinking about risk assessments as mentioned in the Document, there is also the potential to which will comprise different elements giving more holistic coverage. The work titled separate out those considerations into risk and impact assessments  8                                                              as summarized and Examining the Black Box: Tools for Assessing Algorithmic Systems \u200banalyzed here  provides more information on this approach.  2. For all of the technical measures mentioned in the Document, an emphasis on their viability and effectivenessreport will be a critical instrument.   is going to be for which the LKIE as mentioned later in this 3. For each of the metrics and technical measures mentioned in the Document, we use of measurable outcomes against which success can be advocate for the determined.  a. From a practical standpoint it will help in adherence and prevent from burn-out ordesensitization because it might be perceived as a huge, unsolvable, intractableproblem in trying to achieve Responsible AI. 4. In support of the technical measures proposed in the document, we would like to call outparticular attention to the responsibility that the US Government agencies can take on in. Acreating more public benchmarks against which AI systems can be evaluatedgreat example of this is the FRVT from NIST .  5.6. a. Documentation of the assumptions and limitations of the benchmarks so createdwill also be essential in helping those utilizing them to make sure that they will getthe intended intelligence from it rather than becoming falsely confident about thesystem.  In some of the techniques that have been proposed as active measures or future areashave case studiesof research, it would be useful for those utilizing the document towhere those techniques have worked and more importantly where they haven\u2019t sothat they can make appropriate determinations of which techniques to pick. In thesection on LKIE, we elaborate on a few mechanisms that will enable this knowledge tobe widely dispersed within the US Government agencies network.  In relation to the point of perhaps not using AI systems in certain scenarios, we alsopropose that clear mechanisms for disengaging and deactivating the. This means building failsafes and backup modes thatsystem when things go wrongdon\u2019t have to rely on continuous access to the \u201cintelligent\u201d elements and have gracefulfailures that minimize harm. This is already something that is practised in many of thewarfighting systems built by the US Government agencies and borrowing from thoseprinciples to put them into practice here will aid the process of building more robust,reliable, and safe AI systems.  there bea. The handovers between humans and machines, as mentioned in the Document,should be done in a way where we correctly optimize for their relevant strengthsand weaknesses .  7. As brought up in the discussions in the workshop, participants pointed out that there is aneed for having more refinement on what is meant by feedback when used in various 9                                                              the Document, perhaps disambiguating both the social and technicalparts ofconnotations in their use and integration into the AI lifecycle.  8. As some of the techniques have been mentioned by name in the Engineering Practicessection, the participants from the workshop also brought up the idea of having moredomain and department specific guidance that would further lower the barrier forimplementing these ideas in practice.  Community  1. When taking into account due process, in line with the overarching principles of theDocument to uphold American values and Rule of Law, a consideration to keep in mindis how the second- and third-order effects of utilizing AI within a system mightlimit the capability to uphold the rule of law, specifically due process, because of lackof comprehension on the part of the defendants and those who are charged with makingdecisions. a. This can happen often when there is a gap between the level of intelligibilityoffered by the mechanisms in the system vs. the capabilities that the individualshave in terms of their background and training to interpret the outputs from the AIsystems.  b. Some of these ideas are further elaborated in the work titled Different Intelligibilitythat recommends tailoring the process to meet those needs\u200bfor Different Folksas an explicit requirement.  2. A recurring concern in the workshop discussions surrounded the lack of transparencyin terms of how the documentation as mentioned in the Document would beshared publicly, especially in light of the recommendations of having external audits,this is something that will be crucial to building a high-degree of public trust.  a. While some information will have to be kept private for national security reasons,defaulting to keeping the information publicly accessible for higher scrutiny andevaluation will aid in more thorough evaluation of the systems underconsideration as people will have the opportunity to share more insights thanwould otherwise be possible through a single audit.  b. Making the classification system public that will be used to present and share theaudit reports and other documentation as outlined in the Document will also beessential in building public trust.  3. Acknowledging that this document is meant to serve the US Government agencies, weanalyzing the emphasis on American Values in conjunction withwould recommendthat focus\u2019 impact on how it shapes the interaction with the key allies of the US.  a. Building on this idea, what would be the means of engagement andthere is athe US Government agencies ifdisengagement on the part of 10                                                              difference between the values and principles adopted by these agencies vis a visthe counterparties who might come from different ideological and politicalcontexts.  4. As articulated in the Document, a paramount consideration should be whether AIshould be used at all. This emerges from discussions around how sometimes simpler,explainable models have  the same performance as the more complex models.  nearly \u200b5. An active consideration should be the degree of onus which some of these requirementsreduce marketwill place, especially in the procurement process which mightcompetitiveness. This is not to say that we must compromise on the safety of thesesystems but more so that in places where there is a high burden, perhaps it is anopportunity to create public commons with the tooling that will help others build onthe work that the US Government agencies do in ensuring Responsible AI.  6. Community norms around what is acceptable and what isn\u2019t change over time and giventhe especially long shelf-life of government run systems, we advocate for building inflexibility to adopt the disallowed outcomes as mentioned in the report and having a wayto keep them up-to-date over time.  In terms of communication of the ideas from this and other US Government agencies\u2019efforts, the framing and language should be collaborative rather than competitive (forexample, the use of the term \u201carms race\u201d is commonplace in the ecosystem) which willaid in mutually beneficial collaborations and development to move us towardsResponsible AI.  7.8. Working hand-in-hand with industry efforts will also accelerate the adoption andimplementation of Responsible AI and organizations like Microsoft are already doing soby way of sharing their expertise and insights publicly.  9. Communication with the general public in terms of how AI systems are being used in theUS Government agencies will also be essential to provide the right level of insights tothem and break away from the trope of \u201ckiller robots\u201d and other ill-informed narratives onhow AI is used within the government.  10. Another aspect of communication that we felt is important is addressing (and dispelling)unwarranted misrepresentations of superintelligence, its emergence, and the potentialthreats posed by that given how distant and perhaps unlikely it is. We include this hereas it was something that was brought up as a point in the workshop discussions whereparticipants were concerned that there wasn\u2019t a mention of how those will be addressedby the US Government agencies.  Learning, Knowledge, and Information Exchange (LKIE) 1. What is it?   11                                                              a. The LKIE is a way to accelerate organizational knowledge which is crucial forsome of the measures outlined in the Document; specifically, there is a dire needto ensure that we have a way to leverage collective insights that are gleanedthrough on-the-ground practice rather than letting them sit in silos across thevarious arms of the US Government where the measures will be put into practice.  b. Having something to the effect of who are \u201csocializers\u201d\u200bthis knowledge will help to transfer knowledge across the different USResponsible AI ChampsofGovernment agencies.  c. There are a lot of lessons that are learned from hands-on deployments and a lotd. Creating aof them are transferable across domainsrepository of use cases and associated guidance on how thosechallenges were addressed, say for example the benefits and shortcomings ofusing ART , SmartNoise , Aequitas , etc. in practice .  e. Also, surfacing and sharing best practices that are actionable through thislearning exchange will also help to move the idea of responsible AI from being afuzzy notion (which is how some technical stakeholders see it) into somethingthat is steeped in engineering fundamentals (at the same time not abandoningthe social dimensions of these challenges). 2. Why is this needed? a. Reducing redundancy i.ii.iii. As an example, we might have several groups across the various US Government agencies that are working on NLP applications, when it comes to mitigating bias, there are tried-and-tested techniques that one department might figure out which we can share with others so that we don\u2019t start from scratch every time we want to address bias mitigation in NLP applications. Specifically, we would want to avoid the need (as much as possible) in using techniques that are known to have failures that other teams have discovered because they are ahead in the deployment and testing of those techniques.  This is inline with the recommendations made in the Document on having robust testing and monitoring infrastructure in place to ensure Responsible AI.     12                                                              b. Accelerating the deployment of Responsible AI i.ii.iii.iv. Often the integration of responsible AI principles is seen as a  to hindrance \u200bthe rapid deployment of products and service because of the additional work that is required in researching and experimenting to find the techniques that might work well. Such an exchange can short-circuit the discovery process and make it easier for practitioners to include responsible AI as an integral part of their AI lifecycle. It can also help to boost the confidence that both technical and non-technical stakeholders have in the capabilities of the deployed techniques rather than having to guess the efficacy of the methods. A point that was brought up both in the workshop discussions that we hosted and in internal deliberations on the Document was being able to adequately demonstrate and evoke trust from various stakeholders which can be developed through such a process: by highlighting what techniques were considered and justifying why certain choices were made supplemented with empirical evidence, one has a higher chance of building trust.  3. What will it look like? a. RSS feed-like subscriptions  i.ii. Linking into the US Government agencies\u2019 project management systems, having templates (somewhat akin to the Datasheets for Datasets  and Model Cards for Model Reporting ) that will be utilized to track the efficacy of the techniques that have been tried is a way to make this step more realistic. The purpose for having a standardized template is to ease the tracking of information and make it more findable and accessible. This will have tagging for the industry domain, ML sub-domain, and other metadata that will be useful for surfacing relevant items.  13                                                              iii. Then, when we have engineering teams embarking on a new project, they can  to this exchange with particular tags and receive up-to-date subscribe \u200binformation from across the organization on the techniques that have been tried already, what worked, what didn\u2019t, and tips and tricks in effective deployment of those techniques.  b. Engineering focused discussion fora i.ii.iii. Led by practitioners, around each of the templates that have been filled  was whatout, we will have discussions diving into the  \u200bdone.   and why \u200b for how \u200b1. We believe that this Golden Circle  framework for contextualizing the work is critical in effective communication, especially in a system like the US Government agencies where there might be different styles of communication and working based on functional and regional differences.  These will be different from informal and abstract discussions that are had at the moment, since we would be highly use-case driven and particularly focused on the discussion of the gaps and challenges in the application of the techniques. Tagging staff who volunteer to share their expertise will also be associated with the metadata in the templates to have convenient contact points for those who are willing to guide colleagues in their own deployments.  c. Lived-experience community consultations  i.ii. In our work, we are no strangers to the importance of the sociotechnical considerations regarding the impacts of the work that we do. When we don\u2019t have internal staff with experiences on how the technology we develop can have consequences, we will utilize open fora to seek insights from those that have lived experiences and bring those back to assimilate into the templates and enhance the understanding of the issues over time for everyone.  This will also serve the purpose of methodically bridging the chasms in our knowledge and provide clear research directions backed by evidence organizations like the JAIC, Office of Science and Technology Policy, and  14                                                              iii. the National AI Initiative Office to embark on the research and development of tools and policies.  In addition, it will serve to build trust with external stakeholders that we indeed strive to keep their best interests at the core of our design, development, and deployment processes.  4. How will it work in practice? a. Integration into existing project management workflows  i.ii.iii. As a high-performing organization, team members have constant demands on their limited time and adding net new workflows to the processes can be onerous. To ease adoption, deep and seamless integration into their existing workflows will be crucial.  This can be done with the inclusion of progressive disclosures from a UX perspective for the technical and project management staff. That is, as the project progresses, we  new items within the template that need open \u200bto be filled out, amortizing the requirements and onus over the lifecycle of the project. In addition, this will serve the purpose of starting the whole process with micro-habits as a way to build exercising responsible AI practices on a regular basis, akin to how James Clear advocates building habits in his book  when it comes to muscle memory \u200bAtomic Habits . b. Part of the US Government Agencies\u2019 internal evaluations i.ii. Ultimately a lot boils down to creating the right incentive structures around these practices for their adoption. we propose an initial, informal inclusion of attention paid to the addressal of these concerns in the employee evaluations. This will be gradually bumped into inclusion in official criteria thus sending a clear message to all the staff that we take responsible AI as a core consideration in all the work that we do.  15                                                              5. How will we measure the impact? a. SRE-inspired metrics i.ii. , we can utilize metrics like Mean Time to Recovery (MTTR) Borrowing from the well-established field of Site Reliability Engineering (SRE)among others by adapting them into how quickly we are able to detect, address, and monitor the ethics implications of our products and services.  As an example, upon pointing out that there is bias in some internal language models, how quickly are we able to redeploy pretrained models, deliver updates to the downstream customers, and share new guidance for them so that they can incorporate this in their work can be utilized as a proxy for the success of this initiative. The Three Ways of Responsible AI 1. What is it? a. Borrowing from the idea of The Three Ways of DevOps , we\u2019d like to proposei.  centered on the following ideas:  The Three Ways of Responsible AI\u200bposture of the overall system by helping to hold\u200bImproving the responsibleeach other accountable  Providing feedback that is fast, visible, and accurate Continual learning and implementation of Responsible AI ii.iii.2. Why is this needed? a. Clarity on the importance of Responsible AI as a concrete endeavour  16                                                              i.ii.iii.iv. From anecdotal observations, there is rising concern among practitioners within the ecosystem on the subject of Responsible AI but a lot of those notions are fragmented with varying degrees of understanding on the core issues.  One of the frustrations expressed by practitioners is the lack of concrete translations of the principles into things that they can deploy in their work. This is also manifested in the hesitation to try different things when there isn\u2019t a well established framework to evaluate what is working and what isn\u2019t.  The first way (Improving the responsible\u200b) which can instill the helping to hold each other accountable\u200bmindset where we strive to continuously make incremental improvements as a way to achieving a better posture for our systems.  This ties in quite well with the notion of micro-habits since they are stepping stones towards exercising Responsible AI practices on a more routine basis.   posture of the overall system by \u200b kaizen b. Actionable advice  i.ii. Providing feedback that is fast, visible, and accurate The other two ways (and  ) serve to Continual learning and implementation of responsible AI\u200boperationalize the advice in a manner that produces artifacts that are reusable by others in the organization while also tying principles to concrete outcomes that are measurable in terms of product improvements. This is very much in line with the emphasis in the Document on producing tracing artifacts throughout the AI lifecycle that will help stakeholders work on the problems in a manner that is visible to others while also helping them better achieve their goals by providing them with explicit items that aid their development process.  3. What will it look like? a. Shared manifesto  i. Given that the high-level principles as we articulated them are quite simple, we can generate a shared manifesto that anyone can take a look  17                                                              ii.iii. .  wiki-styled at and provide comments on, likely through having it open in a platformThis is something that the NSCAI, JAIC, OSTP, or now the National AI Initiative Office can review periodically to allow for revisions and alignment such that it is efficacious and reflective of emerging concerns. As pointed out in the document and something was a point of discussion both at the workshop and the internal team discussions was how there are many disparate efforts and differences in how shared terms are used whereby groups communicating with each other think that they are following the same ideas but in reality they are not. The ideas highlighted in this section are thus critical in developing that shared vocabulary that will enable all these puzzle pieces to fit together nicely.  b. Feedback templates i.ii. Second\u200b  we need something that helps people Way\u200bTo achieve the provide feedback to each other in a manner that meets the requirements for it to be fast, visible, and accurate to extract more value from their efforts. The templates will not only lower the barriers in terms of what needs to be included in the feedback but it will ensure that the qualities that we seek to have in that feedback are captured every time.  4. How will it work in practice? a. Colloquiums i.ii.iii. in practice.  The Three Ways \u200bHaving regular colloquiums led by practitioners from different areas, especially those who are in management positions will be a way to  when it comes to rubber is meeting the roadsurface how well the  \u200bapplying Just as we have in academic labs, over time we could emphasize the need for practitioners to attend these colloquiums as a part of continuing education credits in responsible AI.  \u200bThis notion of  will also act as an unofficial continuing education credits \u200bincentive over time for people to engage in the material. This is quite in line with the recommendations on workforce training and recertification processes as mentioned in the Document.   unofficial  18                                                              b. Post-mortems i.ii. Having post-mortems on projects to see how would be a good way to not only test the efficacy of these principles but also find places for improvement. Such post-mortems will also be quite familiar to people across disciplines and thus lower the barrier for trying out an activity because of the familiarity with it.   performed The Three Ways \u200bc. Pre-mortems i.ii. They are an opportunity to foresee and plan for some of the worst-case scenarios that might pan out with the use of AI and see what are the that can help to be The Three Ways of Responsible AI practices from  \u200bproactive about that.  This is in line with existing practices in the IC and DoD with wargames that help to play out different scenarios and anticipate and practise various strategies to better prepare for the real-world. d. Growing dedicated staff i.ii. All of this requires time and effort and even though at the moment the team directly working on these ideas might be small, planning for  growth over time as we are able to demonstrate the effectiveness of investing efforts on this front.  Horizontal scaling on the part of teams on the ground will also help to alleviate the need for an expanded team that helps to run this along with the upskilling of the staff that is on the team that can take on these functions as a part of their own roles.  5. How will we measure the impact? a. Engineering feedback forms   19                                                              i.ii. Sending out engineering feedback forms to gauge to what extent fundamentals like CI/CD are being applied in different projects, we can integrate questions on Responsible AI being applied to projects within the same survey forms. This can be useful to gain an understanding of the degree of maturity of these practices across various departments and where educational and awareness efforts should be targeted.  These will help to capture both the pervasiveness of application and the depth and rigor of application of these ideas in practice and try to find gaps in places that they are not being applied as such. b. Pulse checks on internal fora  i.ii. are being actively discussed Three Ways \u200bBy applying sentiment analysis and other NLP techniques, we can also get a live pulse on whether the and imbibed across the organization (keeping in mind not to violate privacy of individuals who post and interact on the fora).  This can supplement the data from the less frequent surveys that are sent out, this is particularly useful in the case when we have a fast-moving field such as Responsible AI which can have meaningful developments taking place within a few months, something that the surveys might be a bit slow in capturing.  c. Potential improvements in the SRE-inspired metrics i.ii. would be The Three Ways \u200bThough a pure causal analysis of the impact of hard, we could draw some conclusions from the degree of adoption of these ideas with the change in the SRE-inspired metrics. This will help to provide empirical evidence on the success of the measures being undertaken and help to make a stronger case for dedicating more resources to the deployment of Responsible AI across the organization.  Empirically-driven risk prioritization matrix 1. What is it?    20                                                              a. One of the places where we have seen hesitation and impediments to theimplementation of Responsible AI is that it can be overwhelming in terms ofwhat needs to be done (if it wasn\u2019t already done by design) and that can placeadditional burdens on those who are tasked with ensuring that the product getsdelivered on time to customers.  c.b. Our proposal is to adopt an empirically-driven prioritization matrix that hastwo axes of the likelihood of occurrence of Responsible AI violations and theseverity of those violations (borrowing from the world of cybersecurity) In addition, having a CVE-NVD like database to which practitioners cansubscribe for updates from whence we can dispatch learnings that might applyto problems that you are working on and solutions that might have beenencountered in different parts of the organization to address those. This willevolve over time as more people add their experiences into the system and tyingthis to the previous point will help to make the entire process a lot moreactionable. 2. Why is this needed? a. Lifecycle view i.ii.iii. Addressing AI ethics concerns as we mentioned earlier can seem like a burden and potentially even orthogonal to delivering products and services on time. But, if we adopt a more pragmatic approach that looks at the entire lifecycle investment rather than just the upfront investment that needs to be made, we can arrive at a more sustainable pace for the inclusion of these practices in the everyday work of the engineers.  This is already something that is fundamental to the Document and we appreciate this focus because there are many efforts ongoing at the moment that don\u2019t take this aspect into account.  Ultimately, an important consideration is the consistency and persistence of application of these ideas rather than just one-off experimentation that can have small wins in the near-term but not affect the status quo too much in the long-run.  b. Roadmap i. The risk matrix can provide a convenient framework for technical and product managers to select those areas that pose the highest risks to the  21                                                              success of the product, at the same time helping to achieve the bang for the buck in terms of the efforts that are invested by the team \u200bwhen it comes to deploying Responsible AI practices.  They can also serve as an easy gateway into making possible the adoption of these ideas without having to overcome too much inertia.  biggest ii.3. What will it look like? a. Two-dimensional matrix i.ii.iii. The matrix will take the form of a two-dimensional grid with the axes of severity or risks and likelihood of risks (borrowing from cybersecurity risk matrices ) that can help the team make fast and effective determinations on which directions to pursue. The simplicity of the matrix is also an important consideration because it lowers the barriers to adoption and makes it accessible across multiple job roles rather than requiring very specialized expertise which can decrease the potential for cross-disciplinary collaborations.  Keeping in line with one of the opening remarks in this report, aiming for low friction to increase ubiquity of adoption should always be a consideration when looking to include new processes or modifying existing processes.  4. How will it work in practice? a. Gathering empirical data i.ii. Through the learning, knowledge, and information exchange (LKIE) that we mentioned in this report, we can associate empirical data over time with each of the use cases that are encountered by teams over time and aggregate them by industry domain and ML sub-domain.  This will also help to convince the stakeholders experimenting with and deploying these measures have a degree of confidence in the  22                                                              recommendations since they will have the trust and experience of their colleagues behind it.  b. Post-mortem on the effectiveness of the matrix recommendations i. While the generation of counterfactuals in this case is not easy, aggregating over many use-cases, we could arrive at some causal inferences on the efficaciousness of the recommendations that are utilized from the matrix evaluations, specifically, in picking which areas should be prioritized to be addressed first.  5. How will we measure the impact? a. Pulse feedback from employees i.ii. Just as we looked at the other places where integrating the ask for feedback on the usefulness of the measures proposed here, we can utilize engineering pulse surveys to get a sense for what works well and what doesn\u2019t.  In addition, this is also an opportunity to learn about the degree of adoption across the organization and other useful metrics that come from such an exercise. This will help tailor the approaches to different departments over time perhaps and surface best practices that can help organizational units get more mileage from these measures. b. SRE-inspired metrics i.ii.iii. Borrowing again from the other SRE-inspired metrics mentioned before, we can get a quantitative estimate of the efficacy of these measures which will be useful in justifying their adoption both internally within teams and across teams within the organization.  These will provide a much-needed quantitative aspect to supplement any qualitative data that is collected on the efficacy of these measures.  As one seeks to get more organizational support to implement these ideas in practice, we believe that such empirical evidence will be essential in convincing those with resource allocation powers to dedicate more firepower to the Responsible AI efforts.  23                                                              Achieving the right level of complexity 1. What is it? a. Finally, we believe that there is much to be taken from Tesler\u2019s Law that wecould apply to the proposed measures above: achieving the right level ofcomplexity will be essential, both from a process and backend perspective andthe front-end experience of the practitioners who will be tasked with implementingthe responsible AI standard in their everyday work. 2. Why is this needed? a. Exposing too much to the developers i.ii.iii.iv. The developers are the end-users in this scenario in a sense who, unless they are deeply interested in the area of Responsible AI themselves, have severe constraints on their time to have to figure out which techniques will work well and which won\u2019t.  In terms of complexity, if we leave open all the choices for different kinds of software that they can use to implement Responsible AI in their products and services, we add to the burden of the end-user in having to make that choice, often with limited information on their efficacy which can lead to misguided choices.  Yet, an argument can also be made that it might be premature to advocate too strongly for particular approaches when the developers on the team might have the greatest insight into how their use-case is unique compared to what has been done already at the organization.  In this case, it is important to consider the right level of complexity that is exposed to the user in terms of the granularity of control that is offered to the end-user, the developer, so that they are not overburdened but at the same time have adequate agency to make the required changes as they need to better meet the needs of their project.    24                                                              3. What will it look like? a. Progressive disclosures i.ii. This will involve principles of progressive disclosure as a way of assessing where we see the most effective deployments and uptake of the principles and practices as mentioned in the rest of this document.  This will be sourced from the data that is gathered from the initiatives under the other pillars mentioned in the document.  b. Feedback calibration  i.ii. Calibrating that with the feedback gathered from different instruments like the pulse surveys will further help to tune what level of granularity works best.  Keeping in mind that complexity can either be offloaded on to the user in terms of the number of choices that they have to make or abstracted away from them such that the interface appears simple to them but there is a lot of complexity hidden away which ultimately restricts the kind of control that they can exercise.  4. How will it work in practice? a. Working with design researchers  i.ii. Given the vast expertise that is available to the US Government agencies, we can utilize the insights that design researchers have on the front of understanding what will lead to achieving the right level of complexity for the end-users, the developers of the practices developed by the Document. Some of it can borrow from typical design methodologies, but we also suspect that there is potential here for new kinds of research to be done that will boost the effectiveness of the work that we are trying to do as well with this initiative.   25                                                              b. Community insights i.ii. Learning directly from the practitioners on the ground will also help to fine-tune this approach to better meet their needs.  Additionally, those who are using it on a regular basis in their work will also have invaluable feedback that we could utilize.  5. How will we measure the impact? a. SRE-inspired metrics and feedback sentiments i. Just as is the case with the other initiatives, the impact and efficacy of these measures can be evaluated by utilizing SRE-inspired metrics and assessing the sentiments as expressed in the feedback from the developers.  Conclusion Through some of the general comments made at the beginning of this report that were split along refinements and steps that need to be taken to address some of the organizational, technical, and community issues along with a framework including the learning, knowledge, and information exchange (LKIE), The Three Ways of Responsible AI, empirically-driven risk-prioritization matrix, and achieving the right level of complexity for the Norm rather than the Exception, we believe that the US Government agencies and \u200bCommissions like the NSCAI have a great role to play in making governments a role model in putting Responsible AI into practice.   Responsible AI Making\u200bGovernments have in the past been great leaders in undertaking massive investments and efforts to bring forth technological progress that has benefited society writ large and we believe that Responsible AI is another area where the US Government can play a crucial role, leveraging its massive influence and access to resources and allies across the globe in making this a reality.  Through the recommendations made in this report, we hope that we can further enhance the excellent work that the NSCAI has done in putting together the Document and we can collectively achieve the vision of .  Exception\u200b Making Responsible AI the Norm rather than the  26",
                    "reference": "[1] A. Gupta. 2021. Making responsible AI the norm rather than the exception. arXiv:2101.11832. Retrieved from https://arxiv.org/pdf/2101.11832"
                }
            ]
        },
        {
            "paper_title": "Responsible AI: Portraits with Intelligent Bibliometrics",
            "authors": "Y Zhang, M Wu, G Zhang, J Lu",
            "publication_info": "arXiv preprint arXiv:2405.02846 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2405.02846",
            "chunks": [
                {
                    "id": "28cc0572-43c8-450c-b228-f3a43ed39758",
                    "text": "",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "197fd5da-701a-4122-9cd3-c9e323f115b5",
                    "text": "",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "09913aa9-11d8-406b-913a-96c449a3450d",
                    "text": "",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "c2c1f1c3-3a20-41df-9077-91c6a201387c",
                    "text": "Before the advent of responsible AI, the AI community has shown heightened interest in addressing societal concerns and creating  positive  societal  impact  through  the  rise  of  several conceptual  trends.  This  section  is  to  analyze  three  prominent concepts: explainable AI, trustworthy AI, and ethical AI. Explainable  AI  highlights  an  AI  system\u2019s  capacity  to  be comprehensible to humans, not only in explaining its decisions, recommendations, and predictions, but also in delineating the process of generating these actions [22]. In the AI community, there has been a concerted effort to devise novel approaches to understand feature sensitivity and influence [23] and navigate the  trade-off  between  model  performance  and  explainability [24]. Often referred to by alternative names such as transparent and  accountable  AI  frequently intertwined with reliability and trust in the literature [26].   [25],  explainability  is Trustworthy AI emphasizes the safety, security, fairness, and privacy protection of an AI system, and underscores the crucial role  of  institutional,  software,  and  hardware  mechanisms  in constructing a reliable and accessible AI ecosystem [27]. The pursuit  of  trustworthy  AI  aligns  with  a  shared  interest  in detecting  and  preventing  attacks  and  protecting  data  privacy [8].  This  concept  has  far-reaching  influence  in  cybersecurity, internet of things, social media, etc. [28]  AI ethics (and ethical AI) transcends the boundaries of the AI community and marks AI\u2019s initiative societal interactions, with the intensive involvement of academia, industry, government, and  the  public.  Unlike  a  purely  technical  initiative,  AI  ethics encapsulates  the  collective  understanding  and  societal expectation regarding AI and its applications [29]. Some of its core ethical principles include: transparency, justice & fairness, non-maleficence, responsibility, privacy, beneficence, freedom &  autonomy,  trust,  sustainability,  dignity,  and  solidarity  [3]. The  comprehensive  set  of  ethical  is  in  line  with  fundamental human  values.  However,  some  ongoing  debates  within  the community argue that \u201cwe should not yet celebrate consensus around  high-level  principles  that  hide  deep  political  and normative disagreement\u201d [30].",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "c25dddfd-f652-4ed9-9459-94b17a35c0b4",
                    "text": "Responsible  AI  addresses  this  argument  by  transitioning from  principles  to  practices,  viewing  AI  as  a  socio-technical ecosystem  encompassing  a  comprehensive  interplay  of techniques, developers, and practitioners [4, 31]. Aligned with the  values  inherent  in  explainable  AI,  trustworthy  AI,  and ethical AI, this socio-technical ecosystem requires diversity and inclusiveness  to  foster  cross-boundary  innovation.  Thus, responsible AI goes beyond merely \u201cticking ethical boxes\u201d or incorporating add-on features in AI systems [31]. standards:  accountability, Conceptually,  the  practice  of  responsible  AI  follows  the management  architecture  of  an  AI  system,  traversing  ethical guidelines,  risk  controls,  data  governance,  and  training  and education [32]. Notably, Microsoft [6] defined six principles of responsible  AI  transparency, fairness,  reliability  &  safety,  privacy  &  security,  and inclusiveness.  These  principles  are  accompanied  by  detailed targets  specifying  the  stakeholders  to  whom  each  standard applies, executive requirements, and stepwise instructions with milestones. Similarly, addressing specific pain points faced by practitioners,  Accenture  proposed  four  pillars  for  responsible AI implementations: organizational, operational, technical, and reputational [33]. In  academia,  Cheng,  et  al.  [10]  outlined  the  root  causes  of AI\u2019s responsible issues, such as data bias (e.g., inconsistency, lack  of  transparency,  and  imbalance),  over-simplified  loss inappropriate  evaluation  metrics,  and  mis-functions, interpretation. Despite the absence of a unanimous consensus, the  AI  community  has  undertaken  substantial  efforts  to individually  address  some  of  these  issues,  e.g.,  adversarial learning  to  foster  fair  representation  by  mitigating  biases  in features  [34],  deep  uncertainty  quantification  [35],  and  self- 3 explainable graph neural networks [36].  In this study, we adhered to the definition and interpretation of  responsible  AI  put  forth  by  Dignum  [31].  However,  we augmented  this  perspective  by  integrating  the  Microsoft responsible  AI  standards  [6]  and  the  conceptualization  of socially  responsible  AI  [10].  This  approach  enables  us  to encompass  the  viewpoints  from  both  industry  and  academia. We, thus, defined: \u2022  Responsible AI is a socio-technical ecosystem with AI techniques,  developers,  and  practitioners.  The  system enables  AI  with  fundamental  functionalities  to  reason about  and  act  according  to  human  values,  and  fosters accountability and awareness among AI developers and practitioners regarding AI\u2019s societal impact.  \u2022  The  responsibility  principles  include  accountability, explainability, transparency, fairness, intelligibility, un-bias,  non-discrimination,  reliability,  safety,  privacy, security, inclusiveness, and accessibility.  III.  D M  This  study  is  to  investigate  the  AI  community\u2019s  efforts  on responsible  AI,  including  proposing  principles  and  standards, developing responsible models, and implementing applications to tackle real-world responsibility-related concerns. To achieve this,  we  applied  a  search  strategy  for  collecting  and  cross-referencing  relevant  research  articles  and  developed  an intelligent bibliometrics-based analytical framework to analyze the  data  and  generate  a  comprehensive  set  of  portraits  on responsible AI.  A.  Data Recognized as two primary sources of scientific databases, the Web of Science (WoS) Core Collection  and Scopus  offer a comprehensive  coverage  of  journal  articles  and  conference papers, each with distinct emphases. The WoS Core Collection highlights curating high-quality journals indexed by the Science Citation Index and Social Science Citation Index, while Scopus enables  a  broader  scope  by  encompassing  a  diverse  array  of conference  proceedings,  a  platform  where  the  AI  community actively disseminates research findings. Therefore, we decided to  leverage  both  data  sources  to  gain  their  complementary strengths. Referring to the definition of responsible AI and its principles, we built the skeleton of the search string as follows:  responsib* OR accountab* OR explainab* OR transparan* OR fair* OR intelligib* OR bias OR discriminat* OR reliab* OR safety OR privacy OR security OR inclusive* OR accessib* Despite the extensive history of AI, we chose to commence our search from January 1, 2015, for the following reasons:  \u2022  Deep learning triggered the AI boom around 2011, but societal concerns regarding AI ethics did not materialize until +at least 2015 [37]. \u2022  The  earliest  literature  on  responsible  AI  appears  to emerge around 2017 [38]. We conducted our search on October 24, 2023, and collected 12,952 articles from Scopus and 10,043 articles from the WoS Core Collection. Subsequently, we merged the two datasets and eliminated duplicates, resulting in a refined collection of 18,298 distinct articles. The detailed search strategy is in Table I, with the following notes: \u2022  In contrast to social sciences and humanities, the titles of research articles in computer science are typically direct and straightforward, devoid of rhetorical devices, artistic references, or embellishments. However, abstracts may introduce  extraneous  information  such  as  background details  and  practical  implications,  potentially  adding noise  to  the  search  results.  Thus,  our  search  focused exclusively on titles and keywords.    \u2022  Scopus  utilizes  the  All  Science  Journal  Classification (ASJC)  system  to  categories  journals  and  conference proceedings, linking them to related articles. The WoS Core  Collection  features  its  own  subject  category system.  While  both  systems  contain  an  AI-related category,  they  apply  distinct  classification  criteria  for journals and conferences. This motivated our decision to utilize  both  databases  to  ensure  a  comprehensive coverage. \u2022  We  concentrated  on  research  articles  published  in journals and conference proceedings and excluded other source types (e.g., books, book series, and reports) and document  types  (e.g.,  editorials,  review  articles,  notes, and letters).  \u2022  We only focused on English publications. Academic knowledge graphs, with structured bibliographical attributes  (e.g.,  titles,  abstracts,  authors,  and  references), disambiguated  name  entities  (e.g.,  country,  institution,  and author names), and predefined document-topic linkages, prove highly advantageous in the pre-processing of bibliometric data. OpenAlex  is  an  open  and  free-access  academic  knowledge  4 graph,  with  a  comprehensive  integration  of  the  Microsoft Academic  Graph  (MAG;  one  representative  academic  graph produced  by  Microsoft),  Crossref,  and  some  other  scientific data sources (e.g., PubMed). OpenAlex inherits the hierarchical topical system known as the Field of Study (FoS) from MAG. This  system  consists  of  a  series  of  topical  tags  created  by hierarchical topic models [39], with each article containing at least one FoS tag. In  this  study,  we  leveraged  OpenAlex  APIs  to  match  the 18,298  combined  articles  from  Scopus  and  WoS,  using  their Digital Object Identifiers (DOIs). This process returned 17,799 articles,  with  the  missing  articles  primarily  attributed  to  the absence of DOIs and the early publications of 2024. We, then, retrieved  the  complete  bibliographical  information  of  the 17,799  articles  and  extracted  key  entities,  including  49,792 authors, 6,068 affiliations, 137 countries and regions, and 8,523 FoS tags. B.  Intelligent Bibliometrics As an emerging cross-disciplinary research field, we defined: \u2022  Intelligent bibliometrics [14] refers to the development and applications of computational models that elaborate AI  and  data  science  techniques  with  bibliometric  data (e.g.,  scientific  articles  and  patent  documents)  and indicators (e.g., citations, keywords, and authorships).  Intelligent bibliometrics primarily targets real-world science, technology, and innovation (ST&I) challenges, and it includes diverse  methodologies,  such  as  embedding-based  ST&I  topic extraction [40], network analytics and graph learning for ST&I measurements [41, 42] and relationship discovery [16, 43], and prediction models for technological forecasting [17, 44].  This  study  employed  the  following  intelligent  bibliometric models,  in  conjunction  with  descriptive  statistics  and  co-occurrence  analyses,  which  identify  similar  items  (e.g., keywords  and  authors)  by  assessing  their  frequency  of occurrence together. i.  Hierarchical Topic Tree  their research  topics  and The  Hierarchical  Topic  Tree  (HTT)  model  is  a  network-based method of hierarchical community detection, designed to inherent  hierarchical unveil relationships  within  a  corpus  of  research  articles  [16].  This model operates on a weighted co-term network as its input and identifies  a  collection  of  community  anchors  based  on  two distinct  topological  characteristics:  1)  a  notably  high  density and  2)  a  relatively  greater  distance  with  other  high-density nodes.  Each  anchor  subsequently  forms  the  nucellus  of  a community, with the remaining nodes allocated to a community based on the shortest topological distance. This iterative process continues,  generating  sub-communities  until  no  discernible anchors  remain.  The  sub-communities  established  at  each iteration  contribute  to  the  construction  of  topical  layers,  with the  anchors  serving  as  representative  labels.  Ultimately,  the outcome  is  a  hierarchically  partitioned  co-term  network  that reflects  the  layered  intellectual  structure  within  a  field  of knowledge.  research  articles  on  responsible  AI,  aligning  with  the  Pareto principle.   5 Table  II  lists  the  top  15  most  productive  institutions  in responsible  AI  research.  In  conjunction  with  Fig.  2,  an observation is drawn regarding the prevalence of universities in contributing to the topic, but three national research institutions stand out: the French National Centre for Scientific Research (France),  the  Chinese  Academy  of  Sciences  (China),  and  the Industrial  Research Commonwealth Organization  (CSIRO,  Australia).  This  presence  suggests potential  national  interests  in  responsible  AI  research.  For example, as a possible response to China\u2019s Ministry of Science and Technology\u2019s release of the Governance Principles for a New Generation of Artificial Intelligence: Develop Responsible Artificial Intelligence in June 2019 , Chinese universities have notably surged ahead in Table II.  Scientific  and ii.  Scientific Evolutionary Pathways The innovation literature posits that scientific innovation is an  accumulative  process  driven  by  the  progression  of knowledge [45]. The Scientific Evolutionary Pathways (SEP) model  was  designed  to  track  this  accumulative  process  by identifying  a  predecessor-descendent  relationship  between  a newly  created  topic  and  previous  ones  [15].  The  SEP  model organizes articles with the same publication year into one time slice, treating the entire dataset as a bibliometric stream. Each topic  is  defined  as  a  collection  of  articles,  and  the  model analyses each article in the stream by classifying it into its most similar  topic  (i.e.,  predecessor).  It  determines  whether  the article  contributes  new  knowledge  by  assessing  its  semantic distance from the centroid of the topic, and then groups articles identified  as  contributing  new  knowledge  into  several  new topics, i.e., descendants. Utilizing Gephi [46], the SEP model visualizes predecessor-descendent relationships in a network: \u2022  A node represents a topic with its size proportional to the number of articles in the topic. represents \u2022  An  edge  the  predecessor-descendent relationship between its connected nodes. Its weight is determined by the similarity between the two topics. \u2022  Nodes in the same color signify their belonging to the same community. The assignment of colors is based on a  modularity-based  community  detection  algorithm integrated into Gephi. IV.  B P R AI We constructed the bibliometric portraits of responsible AI from  four  aspects:  Key  technological  players  and  their interactions  with  research  communities  and  the  responsibility principles, the topical hierarchy and dynamics of responsible AI and its historical dynamics, the interplay between primary AI techniques and the responsibility principles, and the profile of a core collection of the responsible AI research. Given  a  bird\u2019s  eye  view,  Fig.  1  illustrates  the  publication trends  of  the  17,799  articles  related  to  responsible  AI  and contributed by the AI community since 2015. As anticipated, a discernible  upward  trend  emerges  starting  in  2015  and accelerating after 2020. The incomplete data for 2023 would be an exclusion in this chart. A.  Key Technological Players and Interactions Defining  key  technological  players  as  the  most  productive countries/regions,  institutions,  and  research  communities  in  a specific  scientific  or  technological  area.  Fig.  2  paints  a geographical portrait of the key players in the responsible AI research. At the forefront, China and the USA lead the game as dominant players, collectively contributing to over 40% of the total  publications.  India,  the  UK,  and  Germany  follow,  each contributing more than 1000 publications to the field. The 10 most productive countries collectively account for 86.5% of the  6 Our investigation into research communities extends to the analysis  of  involved  journals  and  conference  proceedings  to responsible  AI.  Table  III  presents  the  top  15  journals  and conference  proceedings,  which  have  published  the  largest number  of  articles  on  responsible  AI.  Considering  AI\u2019s subareas, there is a notable representation of journals spanning various facets of AI: IEEE TNN and Artificial Intelligence for theoretical  developments  in  AI,  particularly  in  machine learning  studies;  Information  Sciences  and  Knowledge-based Systems  for  AI\u2019s  innovation  and  system applications;  and  IEEE  TKDE  and  Pattern  Recognition  for broad topics related to data mining and knowledge discovery. Despite our primary focus on the AI community, the inclusion of  the  journal  AI  &  Society  highlights  the  cross-disciplinary nature of responsible AI, showcasing active engagements with studies in humanities and social sciences.  technological In terms of conference proceedings, with their annual nature, the  number  of  publications  linked  to  one  proceeding  is comparable to as that of journals. However, a discernible surge in  responsible  AI  research  is  observed  after  2021,  with  the computer vision and communications societies emerging as the most active research communities for this new topic. This trend sheds  light  on  the  rising  prominence  of  journals  in  machine learning and intelligent systems. To portray the core research community of responsible AI, we linked journals (Fig. 3) and conference proceedings (Fig. 4) with  a  set  of  selected  responsibility  related  FoS  tags  in OpenAlex.  While  the  distribution  of  involved  countries  and publication venues mirrors that of the entire dataset, a distinct pattern emerges - most articles in this collection cluster around topics  \u201cprivacy\u201d  and  \u201csecurity\u201d,  with  only  a  few  articles scattered  across  topics  like  \u201cexplainability\u201d,  \u201ctransparency\u201d, and \u201ctrustworthiness\u201d. This is consistent with the involvement of the communications society in the realm of cybersecurity and privacy,  suggesting  that  these  issues  may  have  triggered  the initial  interaction  between  the  AI  community  and  real-world concerns related to responsibility. Simultaneously, this finding motivates us to delve deeper into the profiling of responsible AI through a series of topic analyses. B.  Topical Hierarchy and Dynamics of Responsible AI  Leveraging  OpenAlex\u2019s  FoS  tags  to  enable  a  pre-cleaned hierarchical  topical  system  for  detailed  topic  analyses,  we utilized an article\u2019s FoS tags as linked topics and constructed a FoS-based  co-occurrence  network  for  the  application  of  the HTT  and  SEP  models:  HTT  generates  a  data-driven  topical hierarchy,  offering  insights  into  the  knowledge  structure  of responsible AI research. SEP identifies predecessor-descendant relationships  between  research  topics,  and  these  evolutionary relationships provide understandings to the historical dynamics  of  responsible  AI  research,  e.g.,  the  origin  of  the  field,  the timeline  of  involved  concepts  and  technologies,  and  the potential future directions it might evolve.  7 Fig.  5  shows  the  hierarchical  topic  tree  of  responsible  AI research,  delineating  AI\u2019s  knowledge  structure  into  four fundamental components: machine learning, mathematics, data mining, and computer networks. Note that HTT highlights the discovery of a hierarchical relationship between two research topics within a specific dataset, and thus, this hierarchy does not necessarily  indicate  a  sense  of  a  belongingness  between  the components. We specifically annotated the following observations: e.g., applications, \u2022  Machine  Learning:  This  branch  draws  a  clear hierarchical structure of the machine learning research, consisting of neural networks , deep learning, and their downstream  human-computer interactions, bioinformatics, and recommender systems. Notably,  we  observed  several  strong  connections  with some  responsibility  issues,  e.g.,  accountability  when applying to knowledge management and specific tasks like  for systems, inference-related  studies,  and  trustworthiness  when performing human-computer interactions. interpretability recommender information \u2022  Data  Mining:  This  branch  is  application-focused, highlighting  the  broad  applications  of  AI  and  data science  techniques  in  real-world  cases,  e.g.,  database management,  retrieval,  cyber-physical systems, and governance. Information privacy is a key concern,  closely  internet  privacy, linked  with information sensitivity and confidentiality, information leakage,  privacy  software,  and  personal  identities.  In addition, interoperability is involved with database and data modelling.  8 \u2022  Computer  Networks:  This  branch  groups  topics  in and distributed telecommunications,  which  is  the  key  contributor  to privacy  and  security  related  studies  in  software engineering and electrical engineering.   computing, systems,  cloud \u2022  Mathematics: This branch highlights the theoretical base of computer science and contains topics such as statistics, parametric statistics, and Gaussian.  Fig. 6 illustrates the evolutionary pathways of responsible AI research spanning the years 2015 to 2023. We employed a K-means  clustering  algorithm  on  articles  published  in  2015. Utilizing the elbow method, we identified five topics \u2013 artificial intelligence,  computer  security,  internet  privacy,  encryption, and  computer  network  \u2013  as  the  starting  points  of  the evolutionary pathways. These topics serve as a technical profile of responsible AI before its formal terminology and concepts were  established.  These  initially  distinct  technologies  have gradually coalesced into their respective technological cohorts since 2015, forming the foundational clusters that construct the technological landscape of responsible AI.  We discussed the following observations: \u2022  Machine  Learning  and  Statistics  (pink  cluster):  This cluster reveals the evolution of machine learning-based algorithms,  techniques,  and  applications  from  2015  to 2023.  Key  sub-technologies  and  their  evolutionary pathways encompass neural networks, human-computer interaction, robots, and various AI algorithms. Notably, interpretability has emerged as one of the most critical concerns within the context of neural networks. \u2022 \u2022  Data  and  Information  Privacy  (blue  cluster):  This cluster highlights the evolution of privacy-related topics and  their  interactions,  e.g.,  the  mechanisms  of  privacy protection in blockchains, privacy safeguards in various data  and  information  sources  (e.g.,  the  web,  social media, and healthcare records), and some regulations of data  protection.  Interoperability,  trustworthiness,  and accessibility are intertwined throughout these pathways.  System Security (green cluster): This cluster progresses from  the  privacy  concerns  of  data  architecture  to  the security  design  and  implementation  of  analytical systems, interacting with data science, internet of things, telecommunications, and cloud computing techniques. It emphasizes the development of methods, software, and hardware  for  privacy  and  security  protection.  Some notably  learning, include adversarial system, private information retrieval, etc. \u2022  Encryption (yellow cluster): While relatively small, this cluster draws a clear pathway of cryptography, including both its fundamental techniques and applications. \u2022  Ethics  (orange  cluster):  This  cluster  predominantly addresses AI ethics from the perspective of humanities and  social  sciences,  spanning  areas  such  as  business, management science, politics, and law. techniques  federated  9 C.  Responsibility Principles vs. Primary AI Techniques Revisiting the responsibility principles defined in Section II-B  and  aligning  them  with  the  FoS  tags,  we  identified  12 principles  for  mapping.  We  added  \u201cresponsibility\u201d,  but excluded  \u201cintelligibility\u201d  due  to  its  extremely  low  frequency and \u201cinclusiveness\u201d since it mainly aligns with social science topics  when  existing  computer  science  literature  typically addresses similar concepts under the term \u201cfairness\u201d [47]. Then, we selected 16 primary AI techniques based on their frequency in  our  dataset.  To  visualize  the  co-occurrence  between  the responsibility principles and primary AI techniques, we utilized  the Circos approach [48], as illustrated in Fig. 7. We derived the following key observations: \u2022  As  depicted  in  Fig.  5  and  Fig.  6,  the  initial  studies  on responsible AI were primarily centered around privacy and  security.  Techniques  related  to  cloud  computing, machine  learning,  internet  of  things,  and  blockchains contribute more than 50% of related research. \u2022  Machine learning, including its sub-techniques such as neural  networks,  deep  learning,  and  reinforcement learning,  has  directly  engaged  with  all  principles  but demonstrated a tangible connection with explainability  (86%),  discrimination  (83%),  bias  (74%),  and  fairness (52%). Clearly, this is consistent with the community\u2019s continuous interests on explainable AI and ethical AI. > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  10 \u2022  Human-computer interaction occupies the accessibility principle (83%) and is also the second largest contributor to trustworthiness and transparency.  \u2022  Some  significant  matching  pairs  also  include  cloud computing  with  accountability,  distributed  computing with  fairness,  and  blockchain  with  accountability  and transparency.    Interestingly, the responsibility principle is linked with machine  learning  (31%),  robot  (31%),  and  blockchain (15%), indicating some of the AI backbones that initiate the topic of responsible AI.  \u2022 D.  Core Cohort of Responsible AI Research While our study primarily relied on the dataset of responsible AI-related articles contributed by the AI community since 2015, we maintained a keen interest in exploring the core concept of  responsible AI and its developmental progress. Therefore, with a specific focus on the WoS Core Collection to ensure the high standards of coverage, on November 23, 2023, we curated 380 articles  directly  employing  the  term  \u201cresponsible  artificial intelligence\u201d or \u201cresponsible AI\u201d in their titles, abstracts, and keywords.  Fig.  8  draws  a  series  of  portraits  for  the  core  cohort, illustrating  its  publication  trend  (Fig.  8A),  article  types  (Fig. 8B),  and  WoS  subject  categories  (Fig.  8C).  Intriguingly,  the term \u201cresponsible artificial intelligence\u201d was firstly mentioned in 2015 [49] , but as evidenced in Fig. 8A, academia did not commence investigations into responsible AI until 2019 and a significant  upswing  started  in  2021.  Compared  to  the  large volume  of  research  articles  on  AI,  responsible  AI  remains  a relatively recent and evolving topic. > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  11 the encompassing sustainability),  etc.  This,  again,  highlights  the  cross-disciplinary  nature  of  responsible  AI,  transitioning  from fundamental  AI  techniques  and  algorithms  to  a  high-level perspective  development, implementation, and regulation of AI in close coordination with academia,  industry,  and  society.  Particularly  intriguing  is  the relatively  scattered  knowledge  landscape  in  Fig.  9,  without clearly defined boundaries and communities. This aligns with the theory of complex systems and their transitions, indicating the chaos inherent in system disruption when new knowledge and technologies emerge [18, 52].   design, Certainly,  the  computer  science  community  is  the  primary contributor to the core cohort. The category \u201ccomputer science, artificial  intelligence\u201d  leads  the  ranking,  comprising  128 articles  (33.7%),  and  the  six  sub-categories  within  computer science  collectively  encompass  275  articles,  accounting  for 72.4%  of  the  cohort.  Remarkably,  the  inclusion  of  categories such  as  law,  ethics,  social  sciences,  and  information  sciences underscores  the  cross-disciplinary  nature  of  responsible  AI research. that  the  core  cohort  reflects Table IV presents the top 15 most productive institutions in responsible AI research within the core cohort. The USA takes the lead, followed by the UK, Australia, and the Netherlands. Interestingly,  China  is  conspicuously  absent  from  the  list. Given  the  most  recent conceptualization of responsible AI rather than its fundamental technologies and sub-technologies, the dominance of the West, supported  by  their  international-renowned  universities,  has demonstrated  their  capabilities  in  shaping  innovative  theories and  concepts  and  driving  global  technological  advancements. In addition, Microsoft emerges as one of the most productive institutions,  uniquely  representing  the  corporate  sector  and signaling its ambition in this cutting-edge domain. Aiming  to  understand  the  content  of  the  core  cohort,  we employed  the  KeyBERT  model  [50]  to  automatically  extract 1257 terms from the titles and abstracts of the 380 articles, and created a co-term network using VoSViewer [51], see Fig. 9. Slightly  different  from  previous  portraits  that  feature  a  large amount  of  AI-related  technical  terms,  Fig.  9  unviels  AI\u2019s extensive engagements across multiple disciplines and research areas, e.g., medicine and healthcare (with bioethics and CoVID-19),  governance  (with  industry,  economy,  law,  policy,  and  Notes.  A  node  represents  a  term  and  the  edge  between  two nodes indicates their co-occurrence relationships. The size of a node corresponds to the frequency of its related term, while its color refers to potential semantic similarities measured by the strength of co-occurrence.   V.  D C  To unveil the technological landscape of responsible AI and particularly  the  endeavors  undertaken  by  the  AI  community, this  study  developed  an  intelligent  bibliometrics-based analytical  framework  including  descriptive  statistics,  co-occurrence  analyses,  a  model  of  hierarchical  topic  tree,  and scientific  evolutionary  pathways.  This  study  recognized  key technological  players  and  their  interactions,  identified  the topical hierarchy and evolution of responsible AI, discovered the interplays between the responsibility principles and primary AI  techniques,  and  profiled  a  core  cohort  of  the  most  recent cross-disciplinary developments on responsible AI. A.  Intersection between Responsible AI and related Concepts Aligned  with  the  conceptual  discussion  on  responsible  AI and its interplay with related concepts such as trustworthy AI, explainable  AI,  and  ethical  AI,  the  bibliometric  portraits derived  from  the  primary  dataset  (e.g.,  Figs.  3-7)  reveal  a > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  12 significant emphasis on trustworthy AI, particularly regarding security and privacy considerations. Concurrently, explainable AI indicates profound engagements with machine learning and related  techniques,  whereas  ethical  AI  appears  somewhat detached from AI\u2019s broader societal aspects and applications. Intriguingly,  upon  closer  examination  of  the  core  cohort  of responsible  AI-related  studies,  Fig.  9  illustrates  a  cohesive synthesis of these AI concepts emerges within the framework of  responsible  AI,  and  this  synthesis  extends  to  encompass conceptual principles, AI techniques, and societal applications. B.  Technical and Practical Implications Traditional  bibliometric  studies,  relying  on  descriptive statistics  for  knowledge  domain  profiling,  have  been  widely applied.  However,  they  often  fall  short  in  uncovering  latent patterns and mechanisms. While topic models offer a semantic understanding  of  a  data  corpus,  they  come  with  challenges related  to  human  interventions  in  term  pre-processing  and parameter  settings.  Moreover,  high  model  accuracy  does  not necessarily  translate  into  promising  results  for  real-world applications. In this study, the proposed analytical framework with intelligent bibliometrics incorporates AI and data science techniques,  and  these  models  are  either  nonparametric  or possess  limited  parameters.  More  importantly,  these  models have been independently evaluated in various training datasets [15, 16] and empirical cases [53, 54], and their consistent results in  our  study  contribute  to  draw  a  comprehensive  narrative  of responsible AI. This  application  brings  some  fresh  ideas  to  traditional bibliometric studies: 1) To leverage AI\u2019s analytical capabilities into  bibliometrics  for  enhanced  knowledge  discovery,  e.g., extending co-occurrent relationships to intricate relationships, e.g.,  hierarchy  and  evolution.  2)  The  cross-validation  with R   experimental  comparisons  and  empirical  examinations  would ensure  the  practical  feasibility  of  a  computational  model  in addressing  real-world  issues  and  further  add  values  by uncovering  insights  behind  the  results  derived  from  data analytics. C.  Limitations and Future Directions Responsible AI is still a new topic, with the nature of high dynamics,  radical  development,  and  active  cross-disciplinary interactions. We acknowledged the limitations of this study and identified  the  following  future  directions.  1)  Future  studies should  focus  on  monitoring  the  ongoing  accumulation  of  the core cohort (Section IV-D) and profiling its inter-/multi-/cross-/trans-disciplinary  interactions  by  elaborating  extra  data sources (e.g., political documents and social media), along with multimodal  data  (e.g.,  images  from  full-text  research  articles and videos from social media). 2) The incredible capabilities of large  language  models  could  further  enhance  the  analytical framework, e.g., accurate data annotation, entity extraction, and knowledge summarization. A  The authors acknowledge the use of ChatGPT for polishing the language of this paper. The authors clarify that all aspects of  this  work  were  designed,  developed,  implemented,  and reported  by  the  authors,  without  any  contributions  from  AI-generated content. S M  Supplementary  Table  1  and  the  high-solution  versions  of Figs 2, and 5-9 can be retrieved through: https://github.com/IntelligentBibliometrics/Responsible-AI-Review > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  13 is  an  Associate Yi  Zhang  (SM\u201923) the  Australian  Artificial Professor  at Intelligence  Institute,  University  of Technology Sydney (UTS). He received a dual-PhD degree in Management Science and Engineering from Beijing Institute of Software Technology  (2017).  His Engineering research aligns with bibliometrics and technology management. He  has  published  more  than  100  research  articles  in  leading journals and conferences in related fields.  (2016) from  UTS and Dr  Zhang  was  the  receipt  of  the  2019  Australian  Research Council\u2019s  Discovery  Early  Career  Researcher  Award (DECRA). He serves as the Specialty Chief Editor of Frontiers in Research Metrics and Analytics, and an Associate Editor of Technological  Forecasting  and  Social  Change,  IEEE Transactions on Engineering Management, and Scientometrics. in  information  science Mengjia  Wu  received  the  B.Sc.  and  MA degrees  from Huazhong  University  of  Science  and Technology,  Wuhan,  China,  and  he completed his Ph.D. study at the Australian Artificial  Intelligence  Institute,  University of Technology Sydney. Dr  Wu  has  published  more  than  20 in  bibliometric  and  cross-conference/journal  papers disciplinary  venues.  His  research  leveraging bibliometrics,  text  analytics,  network  analytics,  and  graph neural networks to develop and optimize knowledge extraction and discovery models. In 2021, he was granted the ISSI student travel prize from the International Society for Informetrics and Scientometrics.  interest  is > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  14 at  the  University Guangquan  Zhang  is  an  Associate Professor  and  Director  of  the  Decision Systems and e-Service Intelligent Research Laboratory  of Technology  Sydney,  Australia.  He received  the  Ph.D.  degree  in  applied mathematics  from  Curtin  University  of in  2001.  His Technology,  Australia, research  interests  include  fuzzy  machine  learning,  fuzzy optimization,  and  machine  learning.  He  has  authored  five monographs,  five  textbooks,  and  460  papers  including  220 refereed international journal papers. Dr Zhang has won seven Australian Research Council (ARC) Discovery Projects grants and many other research grants. He was awarded an ARC QEII fellowship in 2005. He has served as a member of the editorial boards of several international journals, as a guest editor of eight special issues for IEEE transactions and other international journals and co-chaired  several  international  conferences  and  workshops  in fuzzy decision-making and knowledge engineering. Jie  Lu  (F\u201918)  is  an  Australian  Laureate Fellow,  IFSA  Fellow,  ACS  Fellow, Distinguished Professor, and the Director Intelligence of  Australian  Artificial Institute at the University of Technology Sydney,  Australia.  She  received  a  PhD degree  from  Curtin  University  in  2000. Her main research expertise is in transfer learning, concept drift, fuzzy systems, decision support systems and recommender systems. She has published over 500 papers in  IEEE  Transactions  and  other  journals  and conferences. She is the recipient of two IEEE Transactions on Fuzzy  Systems  Outstanding  Paper  Awards  (2019  and  2022), NeurIPS2022  Outstanding  Paper  Award,  Australia's  Most Innovative  Engineer  Award  (2019),  Australasian  Artificial Intelligence  Distinguished  Research  Contribution  Award (2022),  Australian  NSW  Premier's  Prize  on  Excellence  in Engineering  or  Information  &  Communication  Technology (2023), and the Officer of the Order of Australia (AO) 2023. leading",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "459c3fc8-e44c-47c5-8a6a-27f0b0972c8e",
                    "text": "",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "bbcb6cc8-4c32-4d90-b58f-955780d030b3",
                    "text": "Recognized as two primary sources of scientific databases, the Web of Science (WoS) Core Collection  and Scopus  offer a comprehensive  coverage  of  journal  articles  and  conference papers, each with distinct emphases. The WoS Core Collection highlights curating high-quality journals indexed by the Science Citation Index and Social Science Citation Index, while Scopus enables  a  broader  scope  by  encompassing  a  diverse  array  of conference  proceedings,  a  platform  where  the  AI  community actively disseminates research findings. Therefore, we decided to  leverage  both  data  sources  to  gain  their  complementary strengths. Referring to the definition of responsible AI and its principles, we built the skeleton of the search string as follows:  responsib* OR accountab* OR explainab* OR transparan* OR fair* OR intelligib* OR bias OR discriminat* OR reliab* OR safety OR privacy OR security OR inclusive* OR accessib* Despite the extensive history of AI, we chose to commence our search from January 1, 2015, for the following reasons:  \u2022  Deep learning triggered the AI boom around 2011, but societal concerns regarding AI ethics did not materialize until +at least 2015 [37]. \u2022  The  earliest  literature  on  responsible  AI  appears  to emerge around 2017 [38]. We conducted our search on October 24, 2023, and collected 12,952 articles from Scopus and 10,043 articles from the WoS Core Collection. Subsequently, we merged the two datasets and eliminated duplicates, resulting in a refined collection of 18,298 distinct articles. The detailed search strategy is in Table I, with the following notes: \u2022  In contrast to social sciences and humanities, the titles of research articles in computer science are typically direct and straightforward, devoid of rhetorical devices, artistic references, or embellishments. However, abstracts may introduce  extraneous  information  such  as  background details  and  practical  implications,  potentially  adding noise  to  the  search  results.  Thus,  our  search  focused exclusively on titles and keywords.    \u2022  Scopus  utilizes  the  All  Science  Journal  Classification (ASJC)  system  to  categories  journals  and  conference proceedings, linking them to related articles. The WoS Core  Collection  features  its  own  subject  category system.  While  both  systems  contain  an  AI-related category,  they  apply  distinct  classification  criteria  for journals and conferences. This motivated our decision to utilize  both  databases  to  ensure  a  comprehensive coverage. \u2022  We  concentrated  on  research  articles  published  in journals and conference proceedings and excluded other source types (e.g., books, book series, and reports) and document  types  (e.g.,  editorials,  review  articles,  notes, and letters).  \u2022  We only focused on English publications. Academic knowledge graphs, with structured bibliographical attributes  (e.g.,  titles,  abstracts,  authors,  and  references), disambiguated  name  entities  (e.g.,  country,  institution,  and author names), and predefined document-topic linkages, prove highly advantageous in the pre-processing of bibliometric data. OpenAlex  is  an  open  and  free-access  academic  knowledge  4 graph,  with  a  comprehensive  integration  of  the  Microsoft Academic  Graph  (MAG;  one  representative  academic  graph produced  by  Microsoft),  Crossref,  and  some  other  scientific data sources (e.g., PubMed). OpenAlex inherits the hierarchical topical system known as the Field of Study (FoS) from MAG. This  system  consists  of  a  series  of  topical  tags  created  by hierarchical topic models [39], with each article containing at least one FoS tag. In  this  study,  we  leveraged  OpenAlex  APIs  to  match  the 18,298  combined  articles  from  Scopus  and  WoS,  using  their Digital Object Identifiers (DOIs). This process returned 17,799 articles,  with  the  missing  articles  primarily  attributed  to  the absence of DOIs and the early publications of 2024. We, then, retrieved  the  complete  bibliographical  information  of  the 17,799  articles  and  extracted  key  entities,  including  49,792 authors, 6,068 affiliations, 137 countries and regions, and 8,523 FoS tags.",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "e15c04be-f888-4b01-a561-aa8e9f668eb5",
                    "text": "As an emerging cross-disciplinary research field, we defined: \u2022  Intelligent bibliometrics [14] refers to the development and applications of computational models that elaborate AI  and  data  science  techniques  with  bibliometric  data (e.g.,  scientific  articles  and  patent  documents)  and indicators (e.g., citations, keywords, and authorships).  Intelligent bibliometrics primarily targets real-world science, technology, and innovation (ST&I) challenges, and it includes diverse  methodologies,  such  as  embedding-based  ST&I  topic extraction [40], network analytics and graph learning for ST&I measurements [41, 42] and relationship discovery [16, 43], and prediction models for technological forecasting [17, 44].  This  study  employed  the  following  intelligent  bibliometric models,  in  conjunction  with  descriptive  statistics  and  co-occurrence  analyses,  which  identify  similar  items  (e.g., keywords  and  authors)  by  assessing  their  frequency  of occurrence together. i.  Hierarchical Topic Tree  their research  topics  and The  Hierarchical  Topic  Tree  (HTT)  model  is  a  network-based method of hierarchical community detection, designed to inherent  hierarchical unveil relationships  within  a  corpus  of  research  articles  [16].  This model operates on a weighted co-term network as its input and identifies  a  collection  of  community  anchors  based  on  two distinct  topological  characteristics:  1)  a  notably  high  density and  2)  a  relatively  greater  distance  with  other  high-density nodes.  Each  anchor  subsequently  forms  the  nucellus  of  a community, with the remaining nodes allocated to a community based on the shortest topological distance. This iterative process continues,  generating  sub-communities  until  no  discernible anchors  remain.  The  sub-communities  established  at  each iteration  contribute  to  the  construction  of  topical  layers,  with the  anchors  serving  as  representative  labels.  Ultimately,  the outcome  is  a  hierarchically  partitioned  co-term  network  that reflects  the  layered  intellectual  structure  within  a  field  of knowledge.  research  articles  on  responsible  AI,  aligning  with  the  Pareto principle.   5 Table  II  lists  the  top  15  most  productive  institutions  in responsible  AI  research.  In  conjunction  with  Fig.  2,  an observation is drawn regarding the prevalence of universities in contributing to the topic, but three national research institutions stand out: the French National Centre for Scientific Research (France),  the  Chinese  Academy  of  Sciences  (China),  and  the Industrial  Research Commonwealth Organization  (CSIRO,  Australia).  This  presence  suggests potential  national  interests  in  responsible  AI  research.  For example, as a possible response to China\u2019s Ministry of Science and Technology\u2019s release of the Governance Principles for a New Generation of Artificial Intelligence: Develop Responsible Artificial Intelligence in June 2019 , Chinese universities have notably surged ahead in Table II.  Scientific  and ii.  Scientific Evolutionary Pathways The innovation literature posits that scientific innovation is an  accumulative  process  driven  by  the  progression  of knowledge [45]. The Scientific Evolutionary Pathways (SEP) model  was  designed  to  track  this  accumulative  process  by identifying  a  predecessor-descendent  relationship  between  a newly  created  topic  and  previous  ones  [15].  The  SEP  model organizes articles with the same publication year into one time slice, treating the entire dataset as a bibliometric stream. Each topic  is  defined  as  a  collection  of  articles,  and  the  model analyses each article in the stream by classifying it into its most similar  topic  (i.e.,  predecessor).  It  determines  whether  the article  contributes  new  knowledge  by  assessing  its  semantic distance from the centroid of the topic, and then groups articles identified  as  contributing  new  knowledge  into  several  new topics, i.e., descendants. Utilizing Gephi [46], the SEP model visualizes predecessor-descendent relationships in a network: \u2022  A node represents a topic with its size proportional to the number of articles in the topic. represents \u2022  An  edge  the  predecessor-descendent relationship between its connected nodes. Its weight is determined by the similarity between the two topics. \u2022  Nodes in the same color signify their belonging to the same community. The assignment of colors is based on a  modularity-based  community  detection  algorithm integrated into Gephi. IV.  B P R AI We constructed the bibliometric portraits of responsible AI from  four  aspects:  Key  technological  players  and  their interactions  with  research  communities  and  the  responsibility principles, the topical hierarchy and dynamics of responsible AI and its historical dynamics, the interplay between primary AI techniques and the responsibility principles, and the profile of a core collection of the responsible AI research. Given  a  bird\u2019s  eye  view,  Fig.  1  illustrates  the  publication trends  of  the  17,799  articles  related  to  responsible  AI  and contributed by the AI community since 2015. As anticipated, a discernible  upward  trend  emerges  starting  in  2015  and accelerating after 2020. The incomplete data for 2023 would be an exclusion in this chart. A.  Key Technological Players and Interactions Defining  key  technological  players  as  the  most  productive countries/regions,  institutions,  and  research  communities  in  a specific  scientific  or  technological  area.  Fig.  2  paints  a geographical portrait of the key players in the responsible AI research. At the forefront, China and the USA lead the game as dominant players, collectively contributing to over 40% of the total  publications.  India,  the  UK,  and  Germany  follow,  each contributing more than 1000 publications to the field. The 10 most productive countries collectively account for 86.5% of the  6 Our investigation into research communities extends to the analysis  of  involved  journals  and  conference  proceedings  to responsible  AI.  Table  III  presents  the  top  15  journals  and conference  proceedings,  which  have  published  the  largest number  of  articles  on  responsible  AI.  Considering  AI\u2019s subareas, there is a notable representation of journals spanning various facets of AI: IEEE TNN and Artificial Intelligence for theoretical  developments  in  AI,  particularly  in  machine learning  studies;  Information  Sciences  and  Knowledge-based Systems  for  AI\u2019s  innovation  and  system applications;  and  IEEE  TKDE  and  Pattern  Recognition  for broad topics related to data mining and knowledge discovery. Despite our primary focus on the AI community, the inclusion of  the  journal  AI  &  Society  highlights  the  cross-disciplinary nature of responsible AI, showcasing active engagements with studies in humanities and social sciences.  technological In terms of conference proceedings, with their annual nature, the  number  of  publications  linked  to  one  proceeding  is comparable to as that of journals. However, a discernible surge in  responsible  AI  research  is  observed  after  2021,  with  the computer vision and communications societies emerging as the most active research communities for this new topic. This trend sheds  light  on  the  rising  prominence  of  journals  in  machine learning and intelligent systems. To portray the core research community of responsible AI, we linked journals (Fig. 3) and conference proceedings (Fig. 4) with  a  set  of  selected  responsibility  related  FoS  tags  in OpenAlex.  While  the  distribution  of  involved  countries  and publication venues mirrors that of the entire dataset, a distinct pattern emerges - most articles in this collection cluster around topics  \u201cprivacy\u201d  and  \u201csecurity\u201d,  with  only  a  few  articles scattered  across  topics  like  \u201cexplainability\u201d,  \u201ctransparency\u201d, and \u201ctrustworthiness\u201d. This is consistent with the involvement of the communications society in the realm of cybersecurity and privacy,  suggesting  that  these  issues  may  have  triggered  the initial  interaction  between  the  AI  community  and  real-world concerns related to responsibility. Simultaneously, this finding motivates us to delve deeper into the profiling of responsible AI through a series of topic analyses. B.  Topical Hierarchy and Dynamics of Responsible AI  Leveraging  OpenAlex\u2019s  FoS  tags  to  enable  a  pre-cleaned hierarchical  topical  system  for  detailed  topic  analyses,  we utilized an article\u2019s FoS tags as linked topics and constructed a FoS-based  co-occurrence  network  for  the  application  of  the HTT  and  SEP  models:  HTT  generates  a  data-driven  topical hierarchy,  offering  insights  into  the  knowledge  structure  of responsible AI research. SEP identifies predecessor-descendant relationships  between  research  topics,  and  these  evolutionary relationships provide understandings to the historical dynamics  of  responsible  AI  research,  e.g.,  the  origin  of  the  field,  the timeline  of  involved  concepts  and  technologies,  and  the potential future directions it might evolve.  7 Fig.  5  shows  the  hierarchical  topic  tree  of  responsible  AI research,  delineating  AI\u2019s  knowledge  structure  into  four fundamental components: machine learning, mathematics, data mining, and computer networks. Note that HTT highlights the discovery of a hierarchical relationship between two research topics within a specific dataset, and thus, this hierarchy does not necessarily  indicate  a  sense  of  a  belongingness  between  the components. We specifically annotated the following observations: e.g., applications, \u2022  Machine  Learning:  This  branch  draws  a  clear hierarchical structure of the machine learning research, consisting of neural networks , deep learning, and their downstream  human-computer interactions, bioinformatics, and recommender systems. Notably,  we  observed  several  strong  connections  with some  responsibility  issues,  e.g.,  accountability  when applying to knowledge management and specific tasks like  for systems, inference-related  studies,  and  trustworthiness  when performing human-computer interactions. interpretability recommender information \u2022  Data  Mining:  This  branch  is  application-focused, highlighting  the  broad  applications  of  AI  and  data science  techniques  in  real-world  cases,  e.g.,  database management,  retrieval,  cyber-physical systems, and governance. Information privacy is a key concern,  closely  internet  privacy, linked  with information sensitivity and confidentiality, information leakage,  privacy  software,  and  personal  identities.  In addition, interoperability is involved with database and data modelling.  8 \u2022  Computer  Networks:  This  branch  groups  topics  in and distributed telecommunications,  which  is  the  key  contributor  to privacy  and  security  related  studies  in  software engineering and electrical engineering.   computing, systems,  cloud \u2022  Mathematics: This branch highlights the theoretical base of computer science and contains topics such as statistics, parametric statistics, and Gaussian.  Fig. 6 illustrates the evolutionary pathways of responsible AI research spanning the years 2015 to 2023. We employed a K-means  clustering  algorithm  on  articles  published  in  2015. Utilizing the elbow method, we identified five topics \u2013 artificial intelligence,  computer  security,  internet  privacy,  encryption, and  computer  network  \u2013  as  the  starting  points  of  the evolutionary pathways. These topics serve as a technical profile of responsible AI before its formal terminology and concepts were  established.  These  initially  distinct  technologies  have gradually coalesced into their respective technological cohorts since 2015, forming the foundational clusters that construct the technological landscape of responsible AI.  We discussed the following observations: \u2022  Machine  Learning  and  Statistics  (pink  cluster):  This cluster reveals the evolution of machine learning-based algorithms,  techniques,  and  applications  from  2015  to 2023.  Key  sub-technologies  and  their  evolutionary pathways encompass neural networks, human-computer interaction, robots, and various AI algorithms. Notably, interpretability has emerged as one of the most critical concerns within the context of neural networks. \u2022 \u2022  Data  and  Information  Privacy  (blue  cluster):  This cluster highlights the evolution of privacy-related topics and  their  interactions,  e.g.,  the  mechanisms  of  privacy protection in blockchains, privacy safeguards in various data  and  information  sources  (e.g.,  the  web,  social media, and healthcare records), and some regulations of data  protection.  Interoperability,  trustworthiness,  and accessibility are intertwined throughout these pathways.  System Security (green cluster): This cluster progresses from  the  privacy  concerns  of  data  architecture  to  the security  design  and  implementation  of  analytical systems, interacting with data science, internet of things, telecommunications, and cloud computing techniques. It emphasizes the development of methods, software, and hardware  for  privacy  and  security  protection.  Some notably  learning, include adversarial system, private information retrieval, etc. \u2022  Encryption (yellow cluster): While relatively small, this cluster draws a clear pathway of cryptography, including both its fundamental techniques and applications. \u2022  Ethics  (orange  cluster):  This  cluster  predominantly addresses AI ethics from the perspective of humanities and  social  sciences,  spanning  areas  such  as  business, management science, politics, and law. techniques  federated  9 C.  Responsibility Principles vs. Primary AI Techniques Revisiting the responsibility principles defined in Section II-B  and  aligning  them  with  the  FoS  tags,  we  identified  12 principles  for  mapping.  We  added  \u201cresponsibility\u201d,  but excluded  \u201cintelligibility\u201d  due  to  its  extremely  low  frequency and \u201cinclusiveness\u201d since it mainly aligns with social science topics  when  existing  computer  science  literature  typically addresses similar concepts under the term \u201cfairness\u201d [47]. Then, we selected 16 primary AI techniques based on their frequency in  our  dataset.  To  visualize  the  co-occurrence  between  the responsibility principles and primary AI techniques, we utilized  the Circos approach [48], as illustrated in Fig. 7. We derived the following key observations: \u2022  As  depicted  in  Fig.  5  and  Fig.  6,  the  initial  studies  on responsible AI were primarily centered around privacy and  security.  Techniques  related  to  cloud  computing, machine  learning,  internet  of  things,  and  blockchains contribute more than 50% of related research. \u2022  Machine learning, including its sub-techniques such as neural  networks,  deep  learning,  and  reinforcement learning,  has  directly  engaged  with  all  principles  but demonstrated a tangible connection with explainability  (86%),  discrimination  (83%),  bias  (74%),  and  fairness (52%). Clearly, this is consistent with the community\u2019s continuous interests on explainable AI and ethical AI. > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  10 \u2022  Human-computer interaction occupies the accessibility principle (83%) and is also the second largest contributor to trustworthiness and transparency.  \u2022  Some  significant  matching  pairs  also  include  cloud computing  with  accountability,  distributed  computing with  fairness,  and  blockchain  with  accountability  and transparency.    Interestingly, the responsibility principle is linked with machine  learning  (31%),  robot  (31%),  and  blockchain (15%), indicating some of the AI backbones that initiate the topic of responsible AI.  \u2022 D.  Core Cohort of Responsible AI Research While our study primarily relied on the dataset of responsible AI-related articles contributed by the AI community since 2015, we maintained a keen interest in exploring the core concept of  responsible AI and its developmental progress. Therefore, with a specific focus on the WoS Core Collection to ensure the high standards of coverage, on November 23, 2023, we curated 380 articles  directly  employing  the  term  \u201cresponsible  artificial intelligence\u201d or \u201cresponsible AI\u201d in their titles, abstracts, and keywords.  Fig.  8  draws  a  series  of  portraits  for  the  core  cohort, illustrating  its  publication  trend  (Fig.  8A),  article  types  (Fig. 8B),  and  WoS  subject  categories  (Fig.  8C).  Intriguingly,  the term \u201cresponsible artificial intelligence\u201d was firstly mentioned in 2015 [49] , but as evidenced in Fig. 8A, academia did not commence investigations into responsible AI until 2019 and a significant  upswing  started  in  2021.  Compared  to  the  large volume  of  research  articles  on  AI,  responsible  AI  remains  a relatively recent and evolving topic. > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  11 the encompassing sustainability),  etc.  This,  again,  highlights  the  cross-disciplinary  nature  of  responsible  AI,  transitioning  from fundamental  AI  techniques  and  algorithms  to  a  high-level perspective  development, implementation, and regulation of AI in close coordination with academia,  industry,  and  society.  Particularly  intriguing  is  the relatively  scattered  knowledge  landscape  in  Fig.  9,  without clearly defined boundaries and communities. This aligns with the theory of complex systems and their transitions, indicating the chaos inherent in system disruption when new knowledge and technologies emerge [18, 52].   design, Certainly,  the  computer  science  community  is  the  primary contributor to the core cohort. The category \u201ccomputer science, artificial  intelligence\u201d  leads  the  ranking,  comprising  128 articles  (33.7%),  and  the  six  sub-categories  within  computer science  collectively  encompass  275  articles,  accounting  for 72.4%  of  the  cohort.  Remarkably,  the  inclusion  of  categories such  as  law,  ethics,  social  sciences,  and  information  sciences underscores  the  cross-disciplinary  nature  of  responsible  AI research. that  the  core  cohort  reflects Table IV presents the top 15 most productive institutions in responsible AI research within the core cohort. The USA takes the lead, followed by the UK, Australia, and the Netherlands. Interestingly,  China  is  conspicuously  absent  from  the  list. Given  the  most  recent conceptualization of responsible AI rather than its fundamental technologies and sub-technologies, the dominance of the West, supported  by  their  international-renowned  universities,  has demonstrated  their  capabilities  in  shaping  innovative  theories and  concepts  and  driving  global  technological  advancements. In addition, Microsoft emerges as one of the most productive institutions,  uniquely  representing  the  corporate  sector  and signaling its ambition in this cutting-edge domain. Aiming  to  understand  the  content  of  the  core  cohort,  we employed  the  KeyBERT  model  [50]  to  automatically  extract 1257 terms from the titles and abstracts of the 380 articles, and created a co-term network using VoSViewer [51], see Fig. 9. Slightly  different  from  previous  portraits  that  feature  a  large amount  of  AI-related  technical  terms,  Fig.  9  unviels  AI\u2019s extensive engagements across multiple disciplines and research areas, e.g., medicine and healthcare (with bioethics and CoVID-19),  governance  (with  industry,  economy,  law,  policy,  and  Notes.  A  node  represents  a  term  and  the  edge  between  two nodes indicates their co-occurrence relationships. The size of a node corresponds to the frequency of its related term, while its color refers to potential semantic similarities measured by the strength of co-occurrence.   V.  D C  To unveil the technological landscape of responsible AI and particularly  the  endeavors  undertaken  by  the  AI  community, this  study  developed  an  intelligent  bibliometrics-based analytical  framework  including  descriptive  statistics,  co-occurrence  analyses,  a  model  of  hierarchical  topic  tree,  and scientific  evolutionary  pathways.  This  study  recognized  key technological  players  and  their  interactions,  identified  the topical hierarchy and evolution of responsible AI, discovered the interplays between the responsibility principles and primary AI  techniques,  and  profiled  a  core  cohort  of  the  most  recent cross-disciplinary developments on responsible AI. A.  Intersection between Responsible AI and related Concepts Aligned  with  the  conceptual  discussion  on  responsible  AI and its interplay with related concepts such as trustworthy AI, explainable  AI,  and  ethical  AI,  the  bibliometric  portraits derived  from  the  primary  dataset  (e.g.,  Figs.  3-7)  reveal  a > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  12 significant emphasis on trustworthy AI, particularly regarding security and privacy considerations. Concurrently, explainable AI indicates profound engagements with machine learning and related  techniques,  whereas  ethical  AI  appears  somewhat detached from AI\u2019s broader societal aspects and applications. Intriguingly,  upon  closer  examination  of  the  core  cohort  of responsible  AI-related  studies,  Fig.  9  illustrates  a  cohesive synthesis of these AI concepts emerges within the framework of  responsible  AI,  and  this  synthesis  extends  to  encompass conceptual principles, AI techniques, and societal applications. B.  Technical and Practical Implications Traditional  bibliometric  studies,  relying  on  descriptive statistics  for  knowledge  domain  profiling,  have  been  widely applied.  However,  they  often  fall  short  in  uncovering  latent patterns and mechanisms. While topic models offer a semantic understanding  of  a  data  corpus,  they  come  with  challenges related  to  human  interventions  in  term  pre-processing  and parameter  settings.  Moreover,  high  model  accuracy  does  not necessarily  translate  into  promising  results  for  real-world applications. In this study, the proposed analytical framework with intelligent bibliometrics incorporates AI and data science techniques,  and  these  models  are  either  nonparametric  or possess  limited  parameters.  More  importantly,  these  models have been independently evaluated in various training datasets [15, 16] and empirical cases [53, 54], and their consistent results in  our  study  contribute  to  draw  a  comprehensive  narrative  of responsible AI. This  application  brings  some  fresh  ideas  to  traditional bibliometric studies: 1) To leverage AI\u2019s analytical capabilities into  bibliometrics  for  enhanced  knowledge  discovery,  e.g., extending co-occurrent relationships to intricate relationships, e.g.,  hierarchy  and  evolution.  2)  The  cross-validation  with R   experimental  comparisons  and  empirical  examinations  would ensure  the  practical  feasibility  of  a  computational  model  in addressing  real-world  issues  and  further  add  values  by uncovering  insights  behind  the  results  derived  from  data analytics. C.  Limitations and Future Directions Responsible AI is still a new topic, with the nature of high dynamics,  radical  development,  and  active  cross-disciplinary interactions. We acknowledged the limitations of this study and identified  the  following  future  directions.  1)  Future  studies should  focus  on  monitoring  the  ongoing  accumulation  of  the core cohort (Section IV-D) and profiling its inter-/multi-/cross-/trans-disciplinary  interactions  by  elaborating  extra  data sources (e.g., political documents and social media), along with multimodal  data  (e.g.,  images  from  full-text  research  articles and videos from social media). 2) The incredible capabilities of large  language  models  could  further  enhance  the  analytical framework, e.g., accurate data annotation, entity extraction, and knowledge summarization. A  The authors acknowledge the use of ChatGPT for polishing the language of this paper. The authors clarify that all aspects of  this  work  were  designed,  developed,  implemented,  and reported  by  the  authors,  without  any  contributions  from  AI-generated content. S M  Supplementary  Table  1  and  the  high-solution  versions  of Figs 2, and 5-9 can be retrieved through: https://github.com/IntelligentBibliometrics/Responsible-AI-Review > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  13 is  an  Associate Yi  Zhang  (SM\u201923) the  Australian  Artificial Professor  at Intelligence  Institute,  University  of Technology Sydney (UTS). He received a dual-PhD degree in Management Science and Engineering from Beijing Institute of Software Technology  (2017).  His Engineering research aligns with bibliometrics and technology management. He  has  published  more  than  100  research  articles  in  leading journals and conferences in related fields.  (2016) from  UTS and Dr  Zhang  was  the  receipt  of  the  2019  Australian  Research Council\u2019s  Discovery  Early  Career  Researcher  Award (DECRA). He serves as the Specialty Chief Editor of Frontiers in Research Metrics and Analytics, and an Associate Editor of Technological  Forecasting  and  Social  Change,  IEEE Transactions on Engineering Management, and Scientometrics. in  information  science Mengjia  Wu  received  the  B.Sc.  and  MA degrees  from Huazhong  University  of  Science  and Technology,  Wuhan,  China,  and  he completed his Ph.D. study at the Australian Artificial  Intelligence  Institute,  University of Technology Sydney. Dr  Wu  has  published  more  than  20 in  bibliometric  and  cross-conference/journal  papers disciplinary  venues.  His  research  leveraging bibliometrics,  text  analytics,  network  analytics,  and  graph neural networks to develop and optimize knowledge extraction and discovery models. In 2021, he was granted the ISSI student travel prize from the International Society for Informetrics and Scientometrics.  interest  is > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  14 at  the  University Guangquan  Zhang  is  an  Associate Professor  and  Director  of  the  Decision Systems and e-Service Intelligent Research Laboratory  of Technology  Sydney,  Australia.  He received  the  Ph.D.  degree  in  applied mathematics  from  Curtin  University  of in  2001.  His Technology,  Australia, research  interests  include  fuzzy  machine  learning,  fuzzy optimization,  and  machine  learning.  He  has  authored  five monographs,  five  textbooks,  and  460  papers  including  220 refereed international journal papers. Dr Zhang has won seven Australian Research Council (ARC) Discovery Projects grants and many other research grants. He was awarded an ARC QEII fellowship in 2005. He has served as a member of the editorial boards of several international journals, as a guest editor of eight special issues for IEEE transactions and other international journals and co-chaired  several  international  conferences  and  workshops  in fuzzy decision-making and knowledge engineering. Jie  Lu  (F\u201918)  is  an  Australian  Laureate Fellow,  IFSA  Fellow,  ACS  Fellow, Distinguished Professor, and the Director Intelligence of  Australian  Artificial Institute at the University of Technology Sydney,  Australia.  She  received  a  PhD degree  from  Curtin  University  in  2000. Her main research expertise is in transfer learning, concept drift, fuzzy systems, decision support systems and recommender systems. She has published over 500 papers in  IEEE  Transactions  and  other  journals  and conferences. She is the recipient of two IEEE Transactions on Fuzzy  Systems  Outstanding  Paper  Awards  (2019  and  2022), NeurIPS2022  Outstanding  Paper  Award,  Australia's  Most Innovative  Engineer  Award  (2019),  Australasian  Artificial Intelligence  Distinguished  Research  Contribution  Award (2022),  Australian  NSW  Premier's  Prize  on  Excellence  in Engineering  or  Information  &  Communication  Technology (2023), and the Officer of the Order of Australia (AO) 2023. leading",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "1653822c-adb2-44d6-a452-b550361268b9",
                    "text": "",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "8d063d3e-3caf-4783-92d9-23fb5bffec4b",
                    "text": "Defining  key  technological  players  as  the  most  productive countries/regions,  institutions,  and  research  communities  in  a specific  scientific  or  technological  area.  Fig.  2  paints  a geographical portrait of the key players in the responsible AI research. At the forefront, China and the USA lead the game as dominant players, collectively contributing to over 40% of the total  publications.  India,  the  UK,  and  Germany  follow,  each contributing more than 1000 publications to the field. The 10 most productive countries collectively account for 86.5% of the  6 Our investigation into research communities extends to the analysis  of  involved  journals  and  conference  proceedings  to responsible  AI.  Table  III  presents  the  top  15  journals  and conference  proceedings,  which  have  published  the  largest number  of  articles  on  responsible  AI.  Considering  AI\u2019s subareas, there is a notable representation of journals spanning various facets of AI: IEEE TNN and Artificial Intelligence for theoretical  developments  in  AI,  particularly  in  machine learning  studies;  Information  Sciences  and  Knowledge-based Systems  for  AI\u2019s  innovation  and  system applications;  and  IEEE  TKDE  and  Pattern  Recognition  for broad topics related to data mining and knowledge discovery. Despite our primary focus on the AI community, the inclusion of  the  journal  AI  &  Society  highlights  the  cross-disciplinary nature of responsible AI, showcasing active engagements with studies in humanities and social sciences.  technological In terms of conference proceedings, with their annual nature, the  number  of  publications  linked  to  one  proceeding  is comparable to as that of journals. However, a discernible surge in  responsible  AI  research  is  observed  after  2021,  with  the computer vision and communications societies emerging as the most active research communities for this new topic. This trend sheds  light  on  the  rising  prominence  of  journals  in  machine learning and intelligent systems. To portray the core research community of responsible AI, we linked journals (Fig. 3) and conference proceedings (Fig. 4) with  a  set  of  selected  responsibility  related  FoS  tags  in OpenAlex.  While  the  distribution  of  involved  countries  and publication venues mirrors that of the entire dataset, a distinct pattern emerges - most articles in this collection cluster around topics  \u201cprivacy\u201d  and  \u201csecurity\u201d,  with  only  a  few  articles scattered  across  topics  like  \u201cexplainability\u201d,  \u201ctransparency\u201d, and \u201ctrustworthiness\u201d. This is consistent with the involvement of the communications society in the realm of cybersecurity and privacy,  suggesting  that  these  issues  may  have  triggered  the initial  interaction  between  the  AI  community  and  real-world concerns related to responsibility. Simultaneously, this finding motivates us to delve deeper into the profiling of responsible AI through a series of topic analyses.",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "03917489-d213-4c8a-a7f2-80d562b25c89",
                    "text": "Leveraging  OpenAlex\u2019s  FoS  tags  to  enable  a  pre-cleaned hierarchical  topical  system  for  detailed  topic  analyses,  we utilized an article\u2019s FoS tags as linked topics and constructed a FoS-based  co-occurrence  network  for  the  application  of  the HTT  and  SEP  models:  HTT  generates  a  data-driven  topical hierarchy,  offering  insights  into  the  knowledge  structure  of responsible AI research. SEP identifies predecessor-descendant relationships  between  research  topics,  and  these  evolutionary relationships provide understandings to the historical dynamics  of  responsible  AI  research,  e.g.,  the  origin  of  the  field,  the timeline  of  involved  concepts  and  technologies,  and  the potential future directions it might evolve.  7 Fig.  5  shows  the  hierarchical  topic  tree  of  responsible  AI research,  delineating  AI\u2019s  knowledge  structure  into  four fundamental components: machine learning, mathematics, data mining, and computer networks. Note that HTT highlights the discovery of a hierarchical relationship between two research topics within a specific dataset, and thus, this hierarchy does not necessarily  indicate  a  sense  of  a  belongingness  between  the components. We specifically annotated the following observations: e.g., applications, \u2022  Machine  Learning:  This  branch  draws  a  clear hierarchical structure of the machine learning research, consisting of neural networks , deep learning, and their downstream  human-computer interactions, bioinformatics, and recommender systems. Notably,  we  observed  several  strong  connections  with some  responsibility  issues,  e.g.,  accountability  when applying to knowledge management and specific tasks like  for systems, inference-related  studies,  and  trustworthiness  when performing human-computer interactions. interpretability recommender information \u2022  Data  Mining:  This  branch  is  application-focused, highlighting  the  broad  applications  of  AI  and  data science  techniques  in  real-world  cases,  e.g.,  database management,  retrieval,  cyber-physical systems, and governance. Information privacy is a key concern,  closely  internet  privacy, linked  with information sensitivity and confidentiality, information leakage,  privacy  software,  and  personal  identities.  In addition, interoperability is involved with database and data modelling.  8 \u2022  Computer  Networks:  This  branch  groups  topics  in and distributed telecommunications,  which  is  the  key  contributor  to privacy  and  security  related  studies  in  software engineering and electrical engineering.   computing, systems,  cloud \u2022  Mathematics: This branch highlights the theoretical base of computer science and contains topics such as statistics, parametric statistics, and Gaussian.  Fig. 6 illustrates the evolutionary pathways of responsible AI research spanning the years 2015 to 2023. We employed a K-means  clustering  algorithm  on  articles  published  in  2015. Utilizing the elbow method, we identified five topics \u2013 artificial intelligence,  computer  security,  internet  privacy,  encryption, and  computer  network  \u2013  as  the  starting  points  of  the evolutionary pathways. These topics serve as a technical profile of responsible AI before its formal terminology and concepts were  established.  These  initially  distinct  technologies  have gradually coalesced into their respective technological cohorts since 2015, forming the foundational clusters that construct the technological landscape of responsible AI.  We discussed the following observations: \u2022  Machine  Learning  and  Statistics  (pink  cluster):  This cluster reveals the evolution of machine learning-based algorithms,  techniques,  and  applications  from  2015  to 2023.  Key  sub-technologies  and  their  evolutionary pathways encompass neural networks, human-computer interaction, robots, and various AI algorithms. Notably, interpretability has emerged as one of the most critical concerns within the context of neural networks. \u2022 \u2022  Data  and  Information  Privacy  (blue  cluster):  This cluster highlights the evolution of privacy-related topics and  their  interactions,  e.g.,  the  mechanisms  of  privacy protection in blockchains, privacy safeguards in various data  and  information  sources  (e.g.,  the  web,  social media, and healthcare records), and some regulations of data  protection.  Interoperability,  trustworthiness,  and accessibility are intertwined throughout these pathways.  System Security (green cluster): This cluster progresses from  the  privacy  concerns  of  data  architecture  to  the security  design  and  implementation  of  analytical systems, interacting with data science, internet of things, telecommunications, and cloud computing techniques. It emphasizes the development of methods, software, and hardware  for  privacy  and  security  protection.  Some notably  learning, include adversarial system, private information retrieval, etc. \u2022  Encryption (yellow cluster): While relatively small, this cluster draws a clear pathway of cryptography, including both its fundamental techniques and applications. \u2022  Ethics  (orange  cluster):  This  cluster  predominantly addresses AI ethics from the perspective of humanities and  social  sciences,  spanning  areas  such  as  business, management science, politics, and law. techniques  federated  9",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "ce9127da-6a28-453e-80eb-47520e0bcc7a",
                    "text": "Revisiting the responsibility principles defined in Section II-B  and  aligning  them  with  the  FoS  tags,  we  identified  12 principles  for  mapping.  We  added  \u201cresponsibility\u201d,  but excluded  \u201cintelligibility\u201d  due  to  its  extremely  low  frequency and \u201cinclusiveness\u201d since it mainly aligns with social science topics  when  existing  computer  science  literature  typically addresses similar concepts under the term \u201cfairness\u201d [47]. Then, we selected 16 primary AI techniques based on their frequency in  our  dataset.  To  visualize  the  co-occurrence  between  the responsibility principles and primary AI techniques, we utilized  the Circos approach [48], as illustrated in Fig. 7. We derived the following key observations: \u2022  As  depicted  in  Fig.  5  and  Fig.  6,  the  initial  studies  on responsible AI were primarily centered around privacy and  security.  Techniques  related  to  cloud  computing, machine  learning,  internet  of  things,  and  blockchains contribute more than 50% of related research. \u2022  Machine learning, including its sub-techniques such as neural  networks,  deep  learning,  and  reinforcement learning,  has  directly  engaged  with  all  principles  but demonstrated a tangible connection with explainability  (86%),  discrimination  (83%),  bias  (74%),  and  fairness (52%). Clearly, this is consistent with the community\u2019s continuous interests on explainable AI and ethical AI. > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  10 \u2022  Human-computer interaction occupies the accessibility principle (83%) and is also the second largest contributor to trustworthiness and transparency.  \u2022  Some  significant  matching  pairs  also  include  cloud computing  with  accountability,  distributed  computing with  fairness,  and  blockchain  with  accountability  and transparency.    Interestingly, the responsibility principle is linked with machine  learning  (31%),  robot  (31%),  and  blockchain (15%), indicating some of the AI backbones that initiate the topic of responsible AI.  \u2022",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "d5fe1fe5-be6f-49b3-aafc-d1ef87fdb098",
                    "text": "While our study primarily relied on the dataset of responsible AI-related articles contributed by the AI community since 2015, we maintained a keen interest in exploring the core concept of  responsible AI and its developmental progress. Therefore, with a specific focus on the WoS Core Collection to ensure the high standards of coverage, on November 23, 2023, we curated 380 articles  directly  employing  the  term  \u201cresponsible  artificial intelligence\u201d or \u201cresponsible AI\u201d in their titles, abstracts, and keywords.  Fig.  8  draws  a  series  of  portraits  for  the  core  cohort, illustrating  its  publication  trend  (Fig.  8A),  article  types  (Fig. 8B),  and  WoS  subject  categories  (Fig.  8C).  Intriguingly,  the term \u201cresponsible artificial intelligence\u201d was firstly mentioned in 2015 [49] , but as evidenced in Fig. 8A, academia did not commence investigations into responsible AI until 2019 and a significant  upswing  started  in  2021.  Compared  to  the  large volume  of  research  articles  on  AI,  responsible  AI  remains  a relatively recent and evolving topic. > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  11 the encompassing sustainability),  etc.  This,  again,  highlights  the  cross-disciplinary  nature  of  responsible  AI,  transitioning  from fundamental  AI  techniques  and  algorithms  to  a  high-level perspective  development, implementation, and regulation of AI in close coordination with academia,  industry,  and  society.  Particularly  intriguing  is  the relatively  scattered  knowledge  landscape  in  Fig.  9,  without clearly defined boundaries and communities. This aligns with the theory of complex systems and their transitions, indicating the chaos inherent in system disruption when new knowledge and technologies emerge [18, 52].   design, Certainly,  the  computer  science  community  is  the  primary contributor to the core cohort. The category \u201ccomputer science, artificial  intelligence\u201d  leads  the  ranking,  comprising  128 articles  (33.7%),  and  the  six  sub-categories  within  computer science  collectively  encompass  275  articles,  accounting  for 72.4%  of  the  cohort.  Remarkably,  the  inclusion  of  categories such  as  law,  ethics,  social  sciences,  and  information  sciences underscores  the  cross-disciplinary  nature  of  responsible  AI research. that  the  core  cohort  reflects Table IV presents the top 15 most productive institutions in responsible AI research within the core cohort. The USA takes the lead, followed by the UK, Australia, and the Netherlands. Interestingly,  China  is  conspicuously  absent  from  the  list. Given  the  most  recent conceptualization of responsible AI rather than its fundamental technologies and sub-technologies, the dominance of the West, supported  by  their  international-renowned  universities,  has demonstrated  their  capabilities  in  shaping  innovative  theories and  concepts  and  driving  global  technological  advancements. In addition, Microsoft emerges as one of the most productive institutions,  uniquely  representing  the  corporate  sector  and signaling its ambition in this cutting-edge domain. Aiming  to  understand  the  content  of  the  core  cohort,  we employed  the  KeyBERT  model  [50]  to  automatically  extract 1257 terms from the titles and abstracts of the 380 articles, and created a co-term network using VoSViewer [51], see Fig. 9. Slightly  different  from  previous  portraits  that  feature  a  large amount  of  AI-related  technical  terms,  Fig.  9  unviels  AI\u2019s extensive engagements across multiple disciplines and research areas, e.g., medicine and healthcare (with bioethics and CoVID-19),  governance  (with  industry,  economy,  law,  policy,  and  Notes.  A  node  represents  a  term  and  the  edge  between  two nodes indicates their co-occurrence relationships. The size of a node corresponds to the frequency of its related term, while its color refers to potential semantic similarities measured by the strength of co-occurrence.   V.  D C  To unveil the technological landscape of responsible AI and particularly  the  endeavors  undertaken  by  the  AI  community, this  study  developed  an  intelligent  bibliometrics-based analytical  framework  including  descriptive  statistics,  co-occurrence  analyses,  a  model  of  hierarchical  topic  tree,  and scientific  evolutionary  pathways.  This  study  recognized  key technological  players  and  their  interactions,  identified  the topical hierarchy and evolution of responsible AI, discovered the interplays between the responsibility principles and primary AI  techniques,  and  profiled  a  core  cohort  of  the  most  recent cross-disciplinary developments on responsible AI. A.  Intersection between Responsible AI and related Concepts Aligned  with  the  conceptual  discussion  on  responsible  AI and its interplay with related concepts such as trustworthy AI, explainable  AI,  and  ethical  AI,  the  bibliometric  portraits derived  from  the  primary  dataset  (e.g.,  Figs.  3-7)  reveal  a > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  12 significant emphasis on trustworthy AI, particularly regarding security and privacy considerations. Concurrently, explainable AI indicates profound engagements with machine learning and related  techniques,  whereas  ethical  AI  appears  somewhat detached from AI\u2019s broader societal aspects and applications. Intriguingly,  upon  closer  examination  of  the  core  cohort  of responsible  AI-related  studies,  Fig.  9  illustrates  a  cohesive synthesis of these AI concepts emerges within the framework of  responsible  AI,  and  this  synthesis  extends  to  encompass conceptual principles, AI techniques, and societal applications. B.  Technical and Practical Implications Traditional  bibliometric  studies,  relying  on  descriptive statistics  for  knowledge  domain  profiling,  have  been  widely applied.  However,  they  often  fall  short  in  uncovering  latent patterns and mechanisms. While topic models offer a semantic understanding  of  a  data  corpus,  they  come  with  challenges related  to  human  interventions  in  term  pre-processing  and parameter  settings.  Moreover,  high  model  accuracy  does  not necessarily  translate  into  promising  results  for  real-world applications. In this study, the proposed analytical framework with intelligent bibliometrics incorporates AI and data science techniques,  and  these  models  are  either  nonparametric  or possess  limited  parameters.  More  importantly,  these  models have been independently evaluated in various training datasets [15, 16] and empirical cases [53, 54], and their consistent results in  our  study  contribute  to  draw  a  comprehensive  narrative  of responsible AI. This  application  brings  some  fresh  ideas  to  traditional bibliometric studies: 1) To leverage AI\u2019s analytical capabilities into  bibliometrics  for  enhanced  knowledge  discovery,  e.g., extending co-occurrent relationships to intricate relationships, e.g.,  hierarchy  and  evolution.  2)  The  cross-validation  with R   experimental  comparisons  and  empirical  examinations  would ensure  the  practical  feasibility  of  a  computational  model  in addressing  real-world  issues  and  further  add  values  by uncovering  insights  behind  the  results  derived  from  data analytics. C.  Limitations and Future Directions Responsible AI is still a new topic, with the nature of high dynamics,  radical  development,  and  active  cross-disciplinary interactions. We acknowledged the limitations of this study and identified  the  following  future  directions.  1)  Future  studies should  focus  on  monitoring  the  ongoing  accumulation  of  the core cohort (Section IV-D) and profiling its inter-/multi-/cross-/trans-disciplinary  interactions  by  elaborating  extra  data sources (e.g., political documents and social media), along with multimodal  data  (e.g.,  images  from  full-text  research  articles and videos from social media). 2) The incredible capabilities of large  language  models  could  further  enhance  the  analytical framework, e.g., accurate data annotation, entity extraction, and knowledge summarization. A  The authors acknowledge the use of ChatGPT for polishing the language of this paper. The authors clarify that all aspects of  this  work  were  designed,  developed,  implemented,  and reported  by  the  authors,  without  any  contributions  from  AI-generated content. S M  Supplementary  Table  1  and  the  high-solution  versions  of Figs 2, and 5-9 can be retrieved through: https://github.com/IntelligentBibliometrics/Responsible-AI-Review > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  13 is  an  Associate Yi  Zhang  (SM\u201923) the  Australian  Artificial Professor  at Intelligence  Institute,  University  of Technology Sydney (UTS). He received a dual-PhD degree in Management Science and Engineering from Beijing Institute of Software Technology  (2017).  His Engineering research aligns with bibliometrics and technology management. He  has  published  more  than  100  research  articles  in  leading journals and conferences in related fields.  (2016) from  UTS and Dr  Zhang  was  the  receipt  of  the  2019  Australian  Research Council\u2019s  Discovery  Early  Career  Researcher  Award (DECRA). He serves as the Specialty Chief Editor of Frontiers in Research Metrics and Analytics, and an Associate Editor of Technological  Forecasting  and  Social  Change,  IEEE Transactions on Engineering Management, and Scientometrics. in  information  science Mengjia  Wu  received  the  B.Sc.  and  MA degrees  from Huazhong  University  of  Science  and Technology,  Wuhan,  China,  and  he completed his Ph.D. study at the Australian Artificial  Intelligence  Institute,  University of Technology Sydney. Dr  Wu  has  published  more  than  20 in  bibliometric  and  cross-conference/journal  papers disciplinary  venues.  His  research  leveraging bibliometrics,  text  analytics,  network  analytics,  and  graph neural networks to develop and optimize knowledge extraction and discovery models. In 2021, he was granted the ISSI student travel prize from the International Society for Informetrics and Scientometrics.  interest  is > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  14 at  the  University Guangquan  Zhang  is  an  Associate Professor  and  Director  of  the  Decision Systems and e-Service Intelligent Research Laboratory  of Technology  Sydney,  Australia.  He received  the  Ph.D.  degree  in  applied mathematics  from  Curtin  University  of in  2001.  His Technology,  Australia, research  interests  include  fuzzy  machine  learning,  fuzzy optimization,  and  machine  learning.  He  has  authored  five monographs,  five  textbooks,  and  460  papers  including  220 refereed international journal papers. Dr Zhang has won seven Australian Research Council (ARC) Discovery Projects grants and many other research grants. He was awarded an ARC QEII fellowship in 2005. He has served as a member of the editorial boards of several international journals, as a guest editor of eight special issues for IEEE transactions and other international journals and co-chaired  several  international  conferences  and  workshops  in fuzzy decision-making and knowledge engineering. Jie  Lu  (F\u201918)  is  an  Australian  Laureate Fellow,  IFSA  Fellow,  ACS  Fellow, Distinguished Professor, and the Director Intelligence of  Australian  Artificial Institute at the University of Technology Sydney,  Australia.  She  received  a  PhD degree  from  Curtin  University  in  2000. Her main research expertise is in transfer learning, concept drift, fuzzy systems, decision support systems and recommender systems. She has published over 500 papers in  IEEE  Transactions  and  other  journals  and conferences. She is the recipient of two IEEE Transactions on Fuzzy  Systems  Outstanding  Paper  Awards  (2019  and  2022), NeurIPS2022  Outstanding  Paper  Award,  Australia's  Most Innovative  Engineer  Award  (2019),  Australasian  Artificial Intelligence  Distinguished  Research  Contribution  Award (2022),  Australian  NSW  Premier's  Prize  on  Excellence  in Engineering  or  Information  &  Communication  Technology (2023), and the Officer of the Order of Australia (AO) 2023. leading",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "e58788c6-c54b-4660-a60a-48757caba354",
                    "text": "",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "a30f9a0f-72c4-46d3-af96-7cddd78a7ea5",
                    "text": "Aligned  with  the  conceptual  discussion  on  responsible  AI and its interplay with related concepts such as trustworthy AI, explainable  AI,  and  ethical  AI,  the  bibliometric  portraits derived  from  the  primary  dataset  (e.g.,  Figs.  3-7)  reveal  a > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  12 significant emphasis on trustworthy AI, particularly regarding security and privacy considerations. Concurrently, explainable AI indicates profound engagements with machine learning and related  techniques,  whereas  ethical  AI  appears  somewhat detached from AI\u2019s broader societal aspects and applications. Intriguingly,  upon  closer  examination  of  the  core  cohort  of responsible  AI-related  studies,  Fig.  9  illustrates  a  cohesive synthesis of these AI concepts emerges within the framework of  responsible  AI,  and  this  synthesis  extends  to  encompass conceptual principles, AI techniques, and societal applications.",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                },
                {
                    "id": "cfd55d32-9cd7-42ad-8e29-ec1f4bc73940",
                    "text": "Traditional  bibliometric  studies,  relying  on  descriptive statistics  for  knowledge  domain  profiling,  have  been  widely applied.  However,  they  often  fall  short  in  uncovering  latent patterns and mechanisms. While topic models offer a semantic understanding  of  a  data  corpus,  they  come  with  challenges related  to  human  interventions  in  term  pre-processing  and parameter  settings.  Moreover,  high  model  accuracy  does  not necessarily  translate  into  promising  results  for  real-world applications. In this study, the proposed analytical framework with intelligent bibliometrics incorporates AI and data science techniques,  and  these  models  are  either  nonparametric  or possess  limited  parameters.  More  importantly,  these  models have been independently evaluated in various training datasets [15, 16] and empirical cases [53, 54], and their consistent results in  our  study  contribute  to  draw  a  comprehensive  narrative  of responsible AI. This  application  brings  some  fresh  ideas  to  traditional bibliometric studies: 1) To leverage AI\u2019s analytical capabilities into  bibliometrics  for  enhanced  knowledge  discovery,  e.g., extending co-occurrent relationships to intricate relationships, e.g.,  hierarchy  and  evolution.  2)  The  cross-validation  with R   experimental  comparisons  and  empirical  examinations  would ensure  the  practical  feasibility  of  a  computational  model  in addressing  real-world  issues  and  further  add  values  by uncovering  insights  behind  the  results  derived  from  data analytics. C.  Limitations and Future Directions Responsible AI is still a new topic, with the nature of high dynamics,  radical  development,  and  active  cross-disciplinary interactions. We acknowledged the limitations of this study and identified  the  following  future  directions.  1)  Future  studies should  focus  on  monitoring  the  ongoing  accumulation  of  the core cohort (Section IV-D) and profiling its inter-/multi-/cross-/trans-disciplinary  interactions  by  elaborating  extra  data sources (e.g., political documents and social media), along with multimodal  data  (e.g.,  images  from  full-text  research  articles and videos from social media). 2) The incredible capabilities of large  language  models  could  further  enhance  the  analytical framework, e.g., accurate data annotation, entity extraction, and knowledge summarization. A  The authors acknowledge the use of ChatGPT for polishing the language of this paper. The authors clarify that all aspects of  this  work  were  designed,  developed,  implemented,  and reported  by  the  authors,  without  any  contributions  from  AI-generated content. S M  Supplementary  Table  1  and  the  high-solution  versions  of Figs 2, and 5-9 can be retrieved through: https://github.com/IntelligentBibliometrics/Responsible-AI-Review > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  13 is  an  Associate Yi  Zhang  (SM\u201923) the  Australian  Artificial Professor  at Intelligence  Institute,  University  of Technology Sydney (UTS). He received a dual-PhD degree in Management Science and Engineering from Beijing Institute of Software Technology  (2017).  His Engineering research aligns with bibliometrics and technology management. He  has  published  more  than  100  research  articles  in  leading journals and conferences in related fields.  (2016) from  UTS and Dr  Zhang  was  the  receipt  of  the  2019  Australian  Research Council\u2019s  Discovery  Early  Career  Researcher  Award (DECRA). He serves as the Specialty Chief Editor of Frontiers in Research Metrics and Analytics, and an Associate Editor of Technological  Forecasting  and  Social  Change,  IEEE Transactions on Engineering Management, and Scientometrics. in  information  science Mengjia  Wu  received  the  B.Sc.  and  MA degrees  from Huazhong  University  of  Science  and Technology,  Wuhan,  China,  and  he completed his Ph.D. study at the Australian Artificial  Intelligence  Institute,  University of Technology Sydney. Dr  Wu  has  published  more  than  20 in  bibliometric  and  cross-conference/journal  papers disciplinary  venues.  His  research  leveraging bibliometrics,  text  analytics,  network  analytics,  and  graph neural networks to develop and optimize knowledge extraction and discovery models. In 2021, he was granted the ISSI student travel prize from the International Society for Informetrics and Scientometrics.  interest  is > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <  14 at  the  University Guangquan  Zhang  is  an  Associate Professor  and  Director  of  the  Decision Systems and e-Service Intelligent Research Laboratory  of Technology  Sydney,  Australia.  He received  the  Ph.D.  degree  in  applied mathematics  from  Curtin  University  of in  2001.  His Technology,  Australia, research  interests  include  fuzzy  machine  learning,  fuzzy optimization,  and  machine  learning.  He  has  authored  five monographs,  five  textbooks,  and  460  papers  including  220 refereed international journal papers. Dr Zhang has won seven Australian Research Council (ARC) Discovery Projects grants and many other research grants. He was awarded an ARC QEII fellowship in 2005. He has served as a member of the editorial boards of several international journals, as a guest editor of eight special issues for IEEE transactions and other international journals and co-chaired  several  international  conferences  and  workshops  in fuzzy decision-making and knowledge engineering. Jie  Lu  (F\u201918)  is  an  Australian  Laureate Fellow,  IFSA  Fellow,  ACS  Fellow, Distinguished Professor, and the Director Intelligence of  Australian  Artificial Institute at the University of Technology Sydney,  Australia.  She  received  a  PhD degree  from  Curtin  University  in  2000. Her main research expertise is in transfer learning, concept drift, fuzzy systems, decision support systems and recommender systems. She has published over 500 papers in  IEEE  Transactions  and  other  journals  and conferences. She is the recipient of two IEEE Transactions on Fuzzy  Systems  Outstanding  Paper  Awards  (2019  and  2022), NeurIPS2022  Outstanding  Paper  Award,  Australia's  Most Innovative  Engineer  Award  (2019),  Australasian  Artificial Intelligence  Distinguished  Research  Contribution  Award (2022),  Australian  NSW  Premier's  Prize  on  Excellence  in Engineering  or  Information  &  Communication  Technology (2023), and the Officer of the Order of Australia (AO) 2023. leading",
                    "reference": "[1] Yuxin Zhang, Mu Wu, Guang-Zhong Zhang, and Jing Luan. 2024. Responsible AI: Portraits with Intelligent Bibliometrics. arXiv:2405.02846. Retrieved from https://arxiv.org/pdf/2405.02846"
                }
            ]
        },
        {
            "paper_title": "Towards a responsible AI development lifecycle: Lessons from information security",
            "authors": "E Galinkin",
            "publication_info": "arXiv preprint arXiv:2203.02958 - arxiv.org",
            "paper_url": "https://arxiv.org/pdf/2203.02958",
            "chunks": [
                {
                    "id": "5426bc89-2f82-49ec-834f-3220f9c0ca8e",
                    "text": "Legislation and public sentiment throughout the world have pro-moted fairness metrics, explainability, and interpretability as pre-scriptions for the responsible development of ethical artificial intel-ligence systems. Despite the importance of these three pillars in thefoundation of the field, they can be challenging to operationalizeand attempts to solve the problems in production environmentsoften feel Sisyphean. This difficulty stems from a number of fac-tors: fairness metrics are computationally difficult to incorporateinto training and rarely alleviate all of the harms perpetrated bythese systems. Interpretability and explainability can be gamed toappear fair, may inadvertently reduce the privacy of personal in-formation contained in training data, and increase user confidencein predictions \u2013 even when the explanations are wrong. In thiswork, we propose a framework for responsibly developing artifi-cial intelligence systems by incorporating lessons from the fieldof information security and the secure development lifecycle toovercome challenges associated with protecting users in adversar-ial settings. In particular, we propose leveraging the concepts ofthreat modeling, design review, penetration testing, and incidentresponse in the context of developing AI systems as ways to resolveshortcomings in the aforementioned methods.CCS CONCEPTS\u2022 Security and privacy \u2192 Human and societal aspects of se-curity and privacy; \u2022 Social and professional topics \u2192 Usercharacteristics; Computing / technology policy.KEYWORDSExplainable Artificial Intelligence, Interpretable Machine Learning,Fairness, AI Ethics",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "f1255be4-72ad-4907-9521-c71eb243d09d",
                    "text": "Increasing adoption of artificial intelligence in public life has sparkedtremendous interest in the fields of AI ethics, algorithmic fairnessand bias, and model explainability and interpretability. These ideasdid not spring out of thin air, but rather are a response to difficultquestions about when, where, and how it is appropriate to useartificial intelligence to perform tasks. Much of the extant literaturehas aimed to provide constructive movement in the direction ofensuring the principles of fairness, accountability, and transparencyare upheld by machine learning algorithms. For practitioners, thereare three dominant approaches in AI ethics:(1) Fairness metrics(2) Interpretability(3) ExplainabilityThroughout this work, we use the term \u201cAI system\u201d to meanproducts and services that leverages artificial intelligence as a deci-sion making component. The term \u201cResponsible AI\u201d then is an AIsystem that is built with a notion of minimizing the potential harm.The term \u201charm\u201d, used throughout the paper, we use in accordancewith Crawford\u2019s [20] use of the term to mean both allocative andrepresentational harms. Allocative harms are harms which resultin an improper distribution of resources on the basis of group mem-bership. Representational harms are more difficult to quantify thanallocative harms and result in the reinforcement of subordinationof some group on the basis of identify, e.g. race, gender, class, etc.The topics of fairness, interpretability, and explainability are notmerely of interest to the academic world. The European Union hasbegun work on their \u201cAI Act\u201d [19], a law that seeks to legislate andharmonize regulations of technologies and products that leverageartificial intelligence. In the United States, the National Instituteof Standards and Technology has begun work on a risk manage-ment framework for artificial intelligence [46], and a number ofstates have passed legislation regulating uses of artificial intelli-gence [47]. Consequently, all at once, we are developing methodsfor determining and achieving fairness and explainability, imple-menting these methods in industry, and seeing regulation of thetechnologies that encourage or require those same methods. Unfor-tunately, standardization of these topics is ongoing, there are noone-size-fits-all solutions, and there are significant methodologicaland computational hurdles to overcome.We look to the field of information security as one potentialmodel for success due to similarities between the two fields. Inparticular, information security deals with a number of competingtheories [45] and standards [56, 65, 68] that make it challenging toharmonize controls. Moreover, information security, like ethical AI,aims to find heuristics, stopgaps, and proxies for computationallyintractable problems [14, 17] with important human impacts. Ininformation security, it is widely accepted that even in the bestcase for mitigation, vulnerabilities and compromises cannot beavoided entirely. To this end, information security seeks to optimizemitigation, detection, and response. In this work, we demonstratehow practitioners in ethical AI can use the framework of mitigation,detection, and response to operationalize fairness, interpretability,and explainabiltiy frameworks.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "eb828105-2daa-4a3d-bc50-c75144fdec83",
                    "text": "This work builds on research in economics, AI ethics, softwareengineering, and information security, drawing inspiration fromHoward and Lipner\u2019s Security Development Lifecycle [30] and fromthe many ways that their work has been refined, implemented,and evolved in the software industry. In this section, we providebackground on fairness, interpretability, and explainability withan eye towards the insufficiency of existing methods. Crucially,in the realm of interpretability and explainability, the presence ofexplanations increases user confidence in the predictions of themodel, even when the explanations are incorrect [7].",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "1c09cc8d-1097-4960-bd28-52e3dec85b9e",
                    "text": "In the economics literature, there are a variety of fairness met-rics that have been established. For many fairness metrics in thecontinuous case, the problems are rarely able to be solved effi-ciently [55] and for indivisible goods, envy-free allocation \u2013 allo-cation where nobody would rather have someone else\u2019s good \u2013 isNP-hard [39]. Kleinberg et al. [33] explore the COMPAS risk tooland show that for integer values, risk assignment is NP-Complete;although for non-integer values, the problem of fair risk assignmentin polynomial-time remains open. Fairness in classification wasexamined by Dwork et al. [24], who developed fairness constraintsin a classification context, identifying statistical parity as one wayto determine fairness in classifiers. Yona and Rothblum [70] tacklethe issue of generalization from training to test sets in machinelearning by relaxing the notion of fairness to an approximation anddemonstrating generalization of metric-fairness. Like the aforemen-tioned works, much of the literature focuses on the fairness of asingle algorithm making classifications on groups of agents.In the multi-agent setting, where motivations of individual agentsmay worsen the outcomes of other agents, the problem becomeseven more difficult. The work of Zhang and Shah [71] attempts toresolve fairness in this multi-agent setting via linear programmingand game theoretic approaches. The game theoretic approach usedtries to find a Nash equilibrium for the two-player zero-sum setting,a problem which is known to be PPAD-Complete [13] and conjec-tured not to be in P unless P = NP. This suggests that in general,any attempt at algorithmic fairness is a substantial computationalproblem on top of whatever problem we are aiming to solve.In addition to computational difficulties, the work of Fazelpourand Lipton [25] addresses shortcomings in the ideological foun-dations of formalizing fairness metrics by connecting the existingwork to the political philosophy of ideal and non-ideal approaches.As in much of the fair machine learning literature, ideal modelsin political theory imagine a world that is perfectly just. By us-ing this as a target, we aim to measure \u2013 and correct for \u2013 thedeviation from this ideal. However, developing this fairness ideal in algorithmic settings necessitates comparison to other groupsand consequently, a \u201cfair\u201d approach may actually be worse for allgroups and yield new groups that need to be protected. Furtherwork by Dai et al. [21] shows that fairness allocations with narrowdesiderata can lead to worse outcomes overall when issues like theintrinsic value of diversity [64] are not accounted for. This suggeststhat because the term \u201cfairness\u201d is not well-defined, collaborationbetween developers of AI systems and social scientists or ethicistsis important to ensure any metric for measuring fairness capturesa problem-specific definition of the term.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "f5af9ab9-78b8-445c-bea8-c3cd314eb62f",
                    "text": "Recent work on model interpretability has indicated that users findsimpler models more trustworthy [57]. This is built on the definitionof Lipton [40] that presumes users are able to comprehend the entiremodel at once. However, the ability to interpret high-dimensionalmodels is limited, even when those models are linear [44]. Thisinitially suggests that model interpretability limits the availablemodels to low-dimensional linear models and short, individualdecision trees.Spurred by these notions, Generalized Linear Models and Gen-eralized Additive Models have been developed and seek to be suf-ficiently robust to be useful in practice while retaining strong no-tions of human-interpretability. These methods allow for linear andnon-linear models that are inherently interpretable. However, asMolnar notes [44], high-dimensional models are inherently lessinterpretable even when those models are linear. Moreover, eventhe most interpretable models rely on assumptions about the sta-bility of the data generation process and any violation of thoseassumptions renders interpretation of those weights invalid.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "a01dc417-ae5a-4ae0-98e9-6d4b6eac6ccf",
                    "text": "Post-hoc explanations have proven very popular due to their intelli-gibility and their ability to be used with complex machine learningmodels, particularly neural networks. Explainablity methods tendto be model agnostic and are more flexible than model-specificinterpretation methods. We refer readers interested in the techni-cal details of these methods to other resources, such as the bookby Molnar [44] or appropriate survey literature [12]. In practice,explainability methods manifest in a variety of ways:(1) Partial Dependence Plots(2) Individual Conditional Explanations(3) Accumulated Local Effects(4) Feature Interaction(5) Feature Importance(6) Global Surrogates(7) Local Surrogates(8) Shapley values(9) Counterfactual Explanations(10) Adversarial Examples(11) Attention Layer VisualizationThe above methods can be broadly grouped into two buckets:global explanations and local explanations. Global explanationsseek to provide overall model interpretability for models that areotherwise difficult to understand. These methods will demonstrate,for example, how certain features are weighted more heavily thanothers or show how correlation between variables can cause a par-ticular prediction. Local methods, on the other hand, purport toprovide explanations for individual predictions. The most popularamong these are LIME [53], GradCAM [58], and SHAP [41], whichleverage local surrogate models, gradient-based localization, andShapley values respectively to foster explanations. In response totheir popularity, the robustness of these methods have been inves-tigated. Slack et al. [63] demonstrated that these methods do notwork well in an adversarial setting \u2013 that is, they can be fooledby a modeler who wishes to provide convincing explanations thatappear innocuous while maintaining a biased classifier. Furtherwork by Agarwal et al. [1] attempts to establish foundations forrobustness in explanation methods, finding that there are somerobustness guarantees for some methods, but those guarantees aresubject to variance in the perturbations and gradients. Beyond theseissues, substantial critiques have been leveraged against the use ofShapley values for feature importance [36] based on their incon-sistency across distributions and the lack of a normative humanevaluation for the values [32].Counterfactual explanations offer a particularly useful line of ex-planation, effectively answering the question: \u201cwhat would need tobe different to get a different outcome?\u201d Humans desire counterfac-tual explanations, since they provide a direction to create a differentoutcome in the future [50]. As an example, when a person appliesfor a bank loan and is denied on the basis of their credit score,they expect a counterfactual explanation that says what factors,specifically, contributed to the denial and would need to improvein order to approve the loan. Though metacognition \u2013 thinkingabout thinking \u2013 has been studied in computer science, and partic-ularly in cognitive architectures [42], recent attempts have beenmade [6, 38, 67] toward a metacognition for explaining difficult tointerpret models, largely in the mold of providing counterfactualexplanations. However, to date, counterfactal explanations and ar-tificial metacognition have not developed sufficiently to allow fortheir use.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "1118144e-1b8c-46f5-8ab0-8238adc1de4b",
                    "text": "There is a deep connection between security and fairness in ma-chine learning systems. Aside from clear connections like the linkbetween differential privacy and fairness in classification [24], tech-niques like adversarial examples [66] \u2013 inputs to models that aresimilar to humans but are perturbed to cause misclassification \u2013 canbe used to evaluate the robustness of model fairness, interpretabil-ity, and explainability. Adjacent to our taxonomy of allocative andrepresentational harms, we also have a taxonomy of harms thatour model can perpetrate against users and third parties: one, theharms caused by the system itself, including the aforementioned al-locative and representational harms; two, the harms caused to usersby other users of the system. The first case is well-studied, thoughstrategies for renumerating and redressing uncovered harms out-side of calibration primarily prescribe putting human-in-the-loopor mandating explanations for a human gatekeeper. The harmscaused to users by other users of the system tend to align moreclosely with attacks on AI systems, which we provide a high-leveloverview of below and refer readers to surveys on attacks in ma-chine learning [51] and threats to privacy in machine learning [3] for additional details. These user-on-user harms largely align withfour overarching categories:(1) Classification-level attacks(2) Model-level attacks(3) System-level attacks(4) Privacy attacksClassification-level attacks are those attacks which seek to causemisclassification. These attacks include adversarial examples inimages, but also distinct techniques like \u201cBad Characters\u201d [10]that use imperceptible characters to bypass text content filters.Essentially, these attacks allow one user to harm another by causingan input to be misclassified without altering the model, data, oranything else. These attacks would also include attacks like theone used against Tesla\u2019s Traffic Aware Cruise Control [52] where amalicious individual could easily modify a 35 mph speed limit signwith a small piece of black tape and cause the model to incorrectlyclassify the sign as an 85 mph speed limit sign.Model-level attacks differ from classification-level attacks in thatthey alter the model itself. The most common example of this isa poisoning attack \u2013 an attack in which the training data of themodel are altered to cause consistent misclassification. This oftenrequires access to the model or the data itself, making the attackchallenging. However, in the online setting, a number of onlinedata poisoning attacks [34, 72] have been demonstrated to greateffect. A malicious user then, could poison the model and causeproblems for all users.System-level attacks are intend not to simply affect the predic-tions of the model, but rather damage the system itself. An examplehere is that of sponge examples [62], model inputs that are gen-erated to maximize energy consumption and inference time todegrade the functionality of the system. This can also include ex-ploitation of conventional vulnerabilities which could allow fortampering with model inputs or outputs to harm users.Privacy attacks include membership inference [15, 60] and modelinversion [11, 27]. Membership inference attacks seek to identifywhether or not individuals are present in the training data of amodel, potentially damaging user privacy. Model inversion then,is a step further. Rather than ask whether or not a user\u2019s data ispresent in the training data of the model, model inversion seeks toextract training data directly from the model \u2013 a phenomenon thathas been observed in generative models. Both of these attacks canfacilitate harms to users and are within the purview of responsibleAI to limit.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "1a483fb2-be1c-49f0-a8b3-21b3d6e7330f",
                    "text": "As discussed, attempts to satisfy fairness criteria can be limitingfrom a computational perspective. Within information security,there is a notion of formal verification [54], a computationallyintensive process of ensuring that under any input, the programbehaves as expected. This leads to more reliable software that isless prone to exploitable bugs. Note however, that the missionstatement of formal verification \u2013 designing a program that haltswhen a bug is detected \u2013 is undecidable because it is exactly thehalting problem. This has led to extensive work in both automatedand interactive verification to overcome this theoretical barrier bysolving subproblems, approximations of the problem, or writingdomain-specific automation. In many cases, formal verification forsoftware is a larger engineering effort than the software projectitself and as a result, most software is not formally verified. Howthen, do we ensure that software is not riddled with exploitablebugs? In general, the presence of exploitable bugs in software [23,69] is reduced through a number of steps in the secure softwaredevelopment lifecycle. For our purposes, we identify analogiesbetween ethical AI development and the following:(1) Design Review(2) Threat Modeling(3) Penetration TestingThese principles reduce risk that may be introduced in softwaredevelopment and produce more robust code without the overhead offormal verification methods. In ethical artificial intelligence, we alsoseek to reduce the risk of negative outcomes and discrimination.As such, we adapt these secure software development lifecycleprinciples to ethical AI. One point of disagreement in the securitycommunity that may be reflected here is whether to perform threatmodeling ahead of design review. The idea of performing threatmodeling first is to provide a thorough view of the threats so that therisks uncovered in design review are threat-centric. We follow theconvention of performing design review ahead of threat modelingbased on the rationale that defining the threats for a system thathas not yet been designed makes the scope too broad to be useful.We note that both approaches are valid and can be tailored to fitthe maturity and preferences of the organization.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "4c97fed9-44f5-4203-85ec-bea3e8abd6ac",
                    "text": "In information security, a design review looks at the system underdevelopment and assesses the architecture, design, operations, andtheir associated risks [23] allowing for implementation of systems-level security controls such as authentication, encryption, logging,and validation. When developing AI systems, a similar sort of designreview should be conducted, with a view toward AI risks. Thismeans that during the design review process, we should explorequestions like:\u2022 How can we check for distribution drift?\u2022 Are we logging model queries in a way that allows us to findreported bad behaviors?\u2022 What features do we input to the model, and do they intro-duce potential issues?\u2022 Are there other data sources we should be incorporating intothis model?\u2022 What actions, if any, are taken automatically as a result ofmodel predictions?This step provides a system-level view of how data goes intoand predictions come out of the system and is ideally conductedbefore the system is deployed. The idea, at the design review step,is to identify data flows and consider how the system could berefactored or rearchitected to avoid potential risks. Things likedata pre-processing or calibration [8] should be discussed at thisstep, and if they are not needed or not sufficient, there should bedocumentation as to why they are omitted. This goes beyond theactual model and training pipeline to include where data is derived from, what additional data is collected, where predictions and logsare stored, and other system-level issues.An important part of the design review process is a discussion ofhow data related to the system is generated, processed, and stored.This is an important part of the system that is often viewed througha lens of privacy and policy, but not always with a view of how toresponsibly manage data. While data management and mismanage-ment can cause one to run afoul of data privacy legislation, thereare a variety of personal data misuses [35] that can cause harm.This means that the privacy of data per se is not the entirety of thediscussion, but how the data moves through the system to becomea classification needs to be uncovered. An investigation into thisrequires analysis of all data used in predictions, whether these areraw data, proxy features that stand in for data that is not directlyavailable, or transformed features like those yielded from principlecomponent analysis.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "4c29469d-60c5-4034-81c6-e05fd53e79f0",
                    "text": "Threat modeling is the phase of the development process that aimsto predict the threats that a system may face [61]. Akin to howone might imagine ways to secure a home by evaluating the locks,windows, and entrances to their home, threat modeling seeks toevaluate how attackers may gain entry to a system. Since AI sys-tems are software, the security threat modeling conducted shouldincorporate those systems. By analogy, we want to think not only ofthreats to our system, but how our system could pose a risk to users.This comes in two forms: malicious users of our system harmingother users, and harms that our system could hypothetically cause.When it comes to harming other users, we look to AI security anddata privacy for potential harms [35]. Essentially, we must assess ifusers are fully independent and if not, the ways in which one usercould potentially harm another. As an example, malicious userscould extract training data from trained models or infer individualsmembership in the training data [11, 15] which could then be usedto harm those individuals privacy. Another example is malicioususers conducting data poisoning attacks [2], particularly for onlinemachine learning systems [72] that might lead to bad outcomes forother users. This is one way that AI security directly influences AIethics.On the other hand, enumerating ways in which a system usingAI could harm users is also critical. Some harms may be expected: aself-driving car that does not recognize a pedestrian [59], a discrim-inatory bail-setting algorithm [5], an image cropping algorithmsuffering from the \u201cmale gaze\u201d [9]. However, other harms couldrear their head. For example, the EMBER malware (malicious soft-ware) dataset [4] includes a large number of features for WindowsPortable Executable files, including the language of the system themalware was compiled on. One could conclude, based on the com-mand and control infrastructure and the compilation language ofthe malware, that the presence of Chinese language is indicativeof maliciousness and correspondingly restrict access to Chineselanguage websites. One harm this could introduce, however, is in-advertent discrimination against Chinese-speaking users who maywish to visit legitimate webpages or run legitimate software. Ulti-mately, we may conclude that the benefit of deploying the systemoutweighs the risk \u2013 but identifying this possible harm is still animportant part of the threat modeling process that we will revisitin our section on Incident Response.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "1135ef53-95f0-45bf-9365-fd310ec6f4ac",
                    "text": "The concept of a penetration test is simple \u2013 a trusted individual orteam with adversarial skills seeks to find weaknesses in a systemin accordance with the same techniques an attacker would use. Inthe context of developing ethical AI systems, a \u201cpenetration test\u201dthen approaches our AI system with the same tools and intent asa malicious actor. This test should evaluate an attacker\u2019s abilityto harm the system, harm the users of a system, and also uncoverharms latent in the system. Much like the Twitter AlgorithmicBias Bug Bounty [16], we can and should directly evaluate ouralgorithms from an adversarial perspective, even if only internally.Though the term penetration testing has a particular meaning inthe information security context, we use it here to refer to the useof adversarial techniques to uncover potential harms in AI systems.Additionally, we eschew the phrase \u201calgorithmic bias assessment\u201dsince bias is only one potential cause for harm and we seek to usea more task-oriented term.Conducting these sort of assessments require both AI securityskills and sociotechnical knowledge. As of 2021, only 3 out of 28organizations surveyed conducted security assessments on theirmachine learning systems [37], suggesting that many organiza-tions are not currently well-equipped to evaluate these vulnera-bilities and would need to cultivate teams capable of performingalgorithmic harm assessments. Utilities like Counterfit [49] andPrivacyRaven [31] have lowered the barrier to entry for securityprofessionals to use adversarial examples and membership infer-ence attacks on machine learning models, but many organizationsstill do not assess their machine learning security. These same util-ities are critical to conducting these assessments against models.Additionally, simple tactics like using so-called beauty filters canalso demonstrate bias in machine learning systems [26]. In order todevise new tactics to target these algorithms, AI assessors need tounderstand both the technical and social factors included in thesesystems. Importantly, the act of testing these systems assists usnot only in identifying potential harms but also in assessing therobustness of our system.Another key to penetration testing is the need to test the full sys-tem as deployed. Since the algorithm is not deployed in a vacuum,there may be feature engineering, allow and block-listing, prepro-cessing, post-processing, and other steps that could allow problemsto creep into the system. Many so-called \u201cAI systems\u201d are not sin-gle algorithms deployed behind an API, but are instead a tapestryof data engineering, multiple algorithms, and post-processing sys-tems. In some cases, an algorithm may be biased against a particulargroup, but some calibration [8] in a post-processing system cor-rects for the identified issue. In other cases, the added complexityof the overall system may actually amplify small changes to inputsand cause a larger effect that one might observe on the individualalgorithm.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "495e5bbb-a0cd-4535-946a-60e35e2e1229",
                    "text": "An often overlooked discussion is how to deal with a harm per-petrated by an AI system once it is identified. In the field of in-formation security, there is the concept of a breach \u2013 a successfulintrusion by an attacker into our system \u2013 and when this occurs, webegin the incident response process. Typically an incident responseprocess occurs alongside execution of a business continuity plan,a predefined plan for how to continue execution when there is asecurity event or natural disaster. The incident response processinvolves eliminating the attacker\u2019s access to systems, patching vul-nerabilities that were exploited, and taking steps to ensure that theattacker does not get back in. Similarly, we should be prepared inthe field of AI to respond to events where our system creates orperpetuates harm.There are a number of ways harms can be identified even afterdesign review, threat modeling, and penetration testing such asthrough a bias bounty, a news report, or a user reporting that theyhave been harmed. Once the existence of a harm is identified, thework of incident response begins with identifying what the actualharm is. This can be an acute damage or harm to an individual, asystemic bias problem, or the potential for a third-party to harmother users of the system. A self-driving car that strikes a pedestrianis a commonly-used example because the harm is clear: there existsa configuration of vehicles, pedestrians, and other distractions suchthat the vehicle does not stop before a pedestrian is struck. Otherharms, such as bias against racial and gender minorities as observedin the cases of COMPAS [5] and Amazon\u2019s hiring algorithm [22]are less obvious until we conduct research into exactly what harmsoccurred. Whether the harm identified is an acute damage to anindividual or an ongoing systemic harm, we must take immediateaction to:(1) Continue operations if possible(2) Perform root cause analysis(3) Remediate the harms caused",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "6bfccc31-d04c-4012-a864-8b5aecf7bff6",
                    "text": "Once a harm is established, all reasonable efforts to prevent anotherincident should be taken. In many cases, this means removing asystem from production for a period of time while the remainderof the incident response process is executed. Some sort of proce-dure should be established to allow for continuity of operationsduring this period that is contingent on the severity of the harm.For example, a self driving car that strikes a pedestrian may requiretemporarily suspending self-driving across a fleet or limiting whereit can be used. In the case of something like a discriminatory sen-tencing algorithm, we may simply allow judges to operate as theydid before the tool was available, suspending its use. Other cases,such as Twitter\u2019s image cropping algorithm\u2019s \u201cmale gaze\u201d bias or itsaversion to non-Latin text may not rise to the need for continuityresponse and can remain in production.In many cases, the scale of these harms \u2013 bad user experience,emotional pain and suffering, loss of life \u2013 can be anticipated, evenif the specific harm cannot. This provides the ability to set uprisk-based continuity planning. Essentially, we seek to answer thequestion: \u201cif we have to remove this system from production, whatwill we do instead?\u201d to ensure that those who depend in some wayon these systems are still able to leverage them, even with limitedfunctionality.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "c335e7e5-cd50-4e5e-8619-9c37dce3fe36",
                    "text": "In security, root cause analysis is used to ask and answer questionsabout the series of events which led to a security incident, oftenwith a particular focus on the vulnerabilities exploited and whythey were not patched. Even in so-called blameless post-mortems,the root cause analysis seeks to determine what was the weak linkin the chain and how said weak link could have been avoided. Inthe case of algorithmic harms, a root cause analysis is likely to bemuch more involved, due to the large number of pieces at play.The first place to look when a harm occurs is what, if anything,has changed in the system since it most recently functioned atan acceptable level. If there was an update the morning before anincident, it is prudent to investigate whether or not the previousversion of the system would have caused the harm. If not, an abla-tion study should be conducted across the pipeline to identify whatcomponents, if any, could be changed to mitigate the harm. Thisroot cause analysis then informs future penetration tests and threatmodels to ensure that another incident is not caused by the samecause.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "02727b90-9202-425e-b74e-fbbb5b2f7185",
                    "text": "After a root cause is identified, it is prudent to remediate both theharms themselves and the causes of said harms. Remediating aharm depends a lot on the particulars of the harms caused and iscurrently an issue being openly discussed. For the teenagers harmedby the promotion of eating disorders on social media [48], it isunlikely that they will be directly compensated by the organizationperpetuating the harm. Remediating the harm itself is a difficulttask that asks much larger questions about who is responsible forthese incidents, how the costs are handled, and who, if anyone,owes harmed parties reparations for said harms. For harms at thescale of COMPAS, the questions grow even larger. However, asgovernments like the EU consider revising their product liabilityregimes to incorporate AI [18], developers and purveyors of thesesystems should develop a plan for how to address potential claimsagainst their systems.Remediating the cause of the harm then, is the more straightfor-ward task \u2013 though by no means is the task simple. Remediating theharm extends the work of root cause analysis and opens the ques-tion of how to fix the root cause. In the case of bias, this could be amatter of finding a new dataset, calibrating according to sensitive at-tributes, leveraging multicalibration [29], decision calibration [73],or some other method. In other cases, the cause of the harm maynecessitate pre- or post-processing of data and decisions to createguardrails. Yet other cases may require a fundamental reconsid-eration of the system in use and whether or not it is feasible tohave a safe, fair system. These harms and remediations must bedocumented to ensure that future projects do not fall into the sametrap and can be evaluated using similar methods.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "75e46676-5cf3-463d-b1c4-2d0566607e69",
                    "text": "Given the analogies between security and ethical artificial intelli-gence in Sections 3 and 4, we propose a framework for a responsi-ble AI development lifecycle, illustrated in Figure 1. Organizationsworking with artificial intelligence and machine learning haveexisting processes for design, development, training, testing, anddeployment of AI systems. Though we do not detail those processeshere, the proposed framework aims not to replace any part of thatprocess, but rather to augment existing processes with ethical prin-ciples. The responsible AI development lifecycle consists of fivesteps:(1) Planning and Review(2) Design Review(3) Harm Modeling(4) Penetration Testing(5) Incident ResponsePlanning and review, as both the first and last step in the process,is intended to revise existing systems and inform the developmentof new systems. This step should occur before any new code iswritten, whether that is in the process of remediating harms ordesigning a new system. The planning and review process shouldlook back at prior findings in this system and other systems and setforth the steps that need to be taken in developing this new system.Additionally, this step is where business continuity planning asoutlined in Section 4.1 should occur. One critical part of planningand review is the process of documentation \u2013 this involves docu-menting plans and findings, capturing learnings from incidents, andidentifying structures that should be in place ahead of development.This process leads into the design review step where the overalldesign of the system is set out. The intent of the system should beclear, the algorithmic components should be well understood, anddata sources should be documented. In a more mature organization,a design review should include datasheets [28] or model cards [43]to nail down specific places where there are known issues. Asdiscussed in Section 3.1, the design review should consider not onlythe artificial intelligence component, but the scaffolding aroundit and the system as a whole to include logging and auditing, pre-processing, post-processing, and in-processing.Coming out of the design review step, training and tuning mod-els, deployment, and testing can occur in parallel with harm model-ing. As discussed in Section 3.2, this is a good place for counterfac-tual reasoning \u2013 asking all of the \u201cwhat if\u201d scenarios to understandwhat can go wrong. This process should involve stakeholders fromthroughout the organization to identify potential harms to usersof the system and to external parties. Where possible, this processshould also identify mitigations that can be put in place beforethe system goes live. The reason to put mitigations in place aheadof time is twofold: first, having mitigations in place ahead of de-ployment reduces the likelihood that an individual experiences anidentified harm; second, it reduces the cost of putting the mitigationin place since there is no downtime needed to implement it.When the system is deployed or ready for deployment, we canconduct penetration testing of the system, as in Section 3.3. Thispenetration test differs from a traditional penetration test in anumber of respects, but is not entirely divorced from the originalFigure 1: An Illustration of the Responsible AI Development Lifecyclenotion. Specifically, we are still concerned with discovering securitybugs since many of those can be leveraged to cause harm. Wherethis approach differs is in taking a broader view of what is in-scope for a penetration test, since we are concerned not only withthe possibility of attacks on our models, but also with potentialrepresentational harms that are difficult to uncover in securitytesting.Assuming that we have established a continuity plan and doneour job in the design review phase, there should be good feedbackmechanisms for uncovering harms in our systems. These can bemonitored and spot-corrected over time to ensure that the systemis functioning as intended and is not perpetrating harms. Whensomething goes out of wack or a user or third-party reports aproblem, we should initiate our incident response. As mentionedin Section 4, we should begin by identifying the scope and scale ofthe harm, then proceed to initiate our business continuity plan ifnecessary. Once we have determined the root cause of the harm,we develop a plan to alleviate the problem and proceed back to thetop of the process \u2013 reviewing our findings and planning our fixes.As teams cycle through this process for a particular system, eachtrip through the cycle should be shorter and easier. In some cases,the cycle may terminate entirely if the outcome of the review andplanning step is a decision not to use an AI system. When evaluatingthe risks and benefits of deploying an AI system, it is important toalways consider the reference point of simply not using artificialintelligence",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                },
                {
                    "id": "7c339c3a-7348-4cd4-be8a-b484e64d76ff",
                    "text": "This work establishes a framework for developing AI systems re-sponsibly. We identify two parallel taxonomies of harm: allocativeharm versus representational harm and system-on-user harm ver-sus user-on-user harm. These two taxonomies allow us to developmethods of uncovering, identifying, and classifying harms to users.Since AI development occurs not in a vacuum but rather as partof a broader development cycle, we view the proposed frameworkas something that can easily work alongside existing AI systemproduction methods.The proposed system consists of 5 steps that mirror the stan-dard lifecycle of a software system \u2013 design, development, training,testing, and deployment. As development proceeds, our frameworkhelps AI developers and stakeholders evaluate their system forpotential harms and address them. This framework differs fromexisting prescriptions of fairness metrics, explainability methods,and interpretable models due to the shortcomings of those methodscomputationally, epistemologically, and practically. Since nearly allsystems inevitably change, fail, or prove insufficient, this frameworkoffers an opportunity for iterative ethical improvements withoutsacrificing practicality. We achieve this by analogy with informa-tion security, and our framework built on the foundations of thesecure software development lifecycle.This framework makes a number of strong assumptions by ne-cessity. Specifically, we assume that a \u201charm\u201d can be identified andthat given sufficient care, can be resolved. Crucially, we also as-sume that organizations actually want to identify and remediateharms to users and are willing to expend effort to improve theirsystems to that end. Finally, we assume that the skills to evaluatethese models is present in organizations that wish to adopt theseprinciples \u2013 something that we know is untrue for the majority oforganizations.In future work, we aim to address the feasibility of automatingparts of this process to reduce the need for specially-skilled individ-uals. Today, the creation of documentation around bias, robustness,and other important features of responsible AI is cumbersome andmanual. By offering opportunities to automate and operationalizesome of this work, adoption of these processes is eased.",
                    "reference": "[1] Ece Galinki. 2022. Towards a responsible AI development lifecycle: Lessons from information security. arXiv:2203.02958. Retrieved from https://arxiv.org/pdf/2203.02958"
                }
            ]
        },
        {
            "paper_title": "Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries",
            "authors": "AM Piskopani, A Chamberlain\u2026",
            "publication_info": "Proceedings of the First \u2026 - dl.acm.org",
            "paper_url": "https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015",
            "chunks": [
                {
                    "id": "316d89da-76f4-4415-96d0-997fb859ad67",
                    "text": "This position piece starts to examine the ways in which AI-basedautonomous technologies have begun to influence a range of humanactivities in the arts and creative industries.The rise of AI-generatedart could potentially transform the act of creation and impact ourunderstandings of creativity \u2013 from painting, writing, and musiccomposition, to video animation. At the same time, there is increas-ing debate about the social, ethical, and legal implications of usingthese tools (eg copyright, biased data sets, devaluing artistic pro-cesses). Responsible Innovation (RI) could have a crucial role toplay in understanding and responding to the complexity of debates.We will explore and unpack how artists, AI developers and asso-ciated audiences/consumers of art have started to approach someof these issues. We will use these ideas as a starting point to ex-plicate and further develop discourses surrounding the challengesassociated with these technologies in the context of the creativeindustries. Finally, we will investigate how and if these challengesmight be addressed.CCS CONCEPTS\u2022 Social and professional topics; \u2022 Applied computing \u2192 Law;KEYWORDSdatasets, neural networks, creative industries",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "51a9ec44-2c06-4689-acad-9c677a306566",
                    "text": "In recent years, advances in AI research have produced a plethoraof algorithms applying Generative AI techniques. Companies haveused these algorithms to create generative AI art-based platforms.The most popular of these are Stable Diffusion, DALL-E 2, Midjour-ney and Stability AI. Users enter text prompts into a text-basedinterface, and the model then produces content (video, image, text)based on its training dataset. At the end of 2022 ChatGPT arguablybecame a genre of tool in its own right, it is able to replicate ordinarylanguage and can create a sense of conversation.Since these generative tools can also produce high quality art-work from text prompts using artistic styles, keywords, subjectsand concepts, creators have started using them. In February 2022,the winning photo in an Australian photography competition wascreated by AI generative tools.[12] A few months later, The Crow,an \u201cAI-made\u201d film won the Jury Award at the Cannes Short FilmFestival[23]. In September 2022, an artist used text-to-image soft-ware to take the top prize in the Colorado Art Fair .[28] AI-generatedart has been used for the covers of magazines, such as the Economistand Cosmopolitan, while stock image libraries, including Adobeand Dreamstime, have also started to accept AI-generated images.Contemporary artists are still discussing if using AI-generatedwork represents a massive cultural shift \u2013 this is linked to debatescentring on how these developments will impact upon human cre-ativity. Will such tools democratise the creative processes, as someAI supporters suggest, and will tools be developed that encapsulatehumanity\u2019s artistic flair ?Debates are also ongoing among artists, AI developers, policy-makers, and art audiences about the legality of the outputs fromgenerative AI systems and their responsible development and use.How will these tools affect creative industries? Will they cost artistsand illustrators their jobs? Are they fair? Were they designed withsufficient respect for people\u2019s rights? Do they infringe copyright?Are they capable of other forms of harm? Are they trustworthyenough to be used by anyone who wishes to? Is the existing regu-latory framework sufficient to confront these challenges? What istheir future impact on art and upcoming artists? Is there somethingsocially important at stake?",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "e7e97885-ed4c-4072-b984-b92f42de2e0b",
                    "text": "Although the founders of AI platforms, such as Midjourney, claimthat these tools will augment people\u2019s imagination, the artisticcommunity seems divided about using AI as a creative tool.[16]Some visual artists can clearly see the potential benefits. For exam-ple, Refic Anadol, has been exploring the use of computer-basedtechniques developed as an artistic tool for years. In 2022 Anadolreceived a commission by American museum MoMa to create anartwork based on 138,000 artworks from the museum\u2019s collection.His work Unsupervised casts shapeshifting images of museum art-works curated in real time. Some critics characterised it as a costlyand shallow screensaver.[35] Other artists are just beginning toexperiment with these tools. Many artists enjoy the rapidity of theresults. Danielle Baskin, who creates \"alternate realities\", used towork with Photoshop but now uses a system such as DALL-E 2as a quicker way to realise her artistic vision.[18] The routine andrepetitive tasks of creators such as video editing could potentiallybe performed more quickly and efficiently by algorithms, whilstartists who explore dystopian or sci-fi visions in their work findinspiration in AI aesthetics.[16]Professor Ted Underwood believes these tools could be a power-ful educational tool for artists as they give them the opportunity toexplore the entire corpus of art in an active and creative way[34].Some have characterised it as \u2019collective unconsciousness\u2019 or be-lieve that it could be a fertile ground for a new aesthetic movement,similar to early 20th century surrealism.[20]. Others believe that itis a form of conceptual art.[10] Creative process has always beenbased on previous art for technical knowledge and inspiration. Us-ing these systems can also become a new artistic process and changethe focus from the mental and technical skills acquired throughendless practice to creative ideation, skilful use of language, andcuratorial taste. \u2018Prompting\u2019 could become an new artistic skill.For example, Jason Allen, who won the Colorado Art Fair with hiswork \u201cTh\u00e9\u00e2tre D\u2019op\u00e9ra Spatial\u201d by experimenting with differenttext prompts, had no artistic background. Instead, he devoted timeand energy to testing different scenarios, settings, and effects untilhe reached a result that he thought was a form of supernaturalreality. His win was considered controversial, and to be a threatto human art and creativity. Despite this pushback, contest judgeMcKinley said that she would not change her judgement becauseshe valued the concept and the vision of the artist as well as theartwork itself. She concluded that the AI was a tool with which toadvance what an artist envisions.[17]",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "91fcbedd-d096-40f2-ae1f-b0dfc3671e3d",
                    "text": "While some artists explore the artistic potential of using these toolsas inspiration or enablers of their work, others considered thattheir rights have been violated, feel powerless to confront the techcompanies and express concerns and fears about the impact of theiruse in art and human creativity.",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "430f488c-56ad-4f25-82f7-f58bf601ba1f",
                    "text": "Many artists and illustrators say that AI generators are often trainednot only by public domain images but also by copyrighted imagesscraped from their portfolios on sites like Pinterest and Artstationwithout their knowledge or license. For example Greg Rutkowski,an established Poland-based digital creator, uses classical paintingstyles to create dreamy fantasy landscapes. Rutkowski\u2019s name hasbeen used as a prompt around 93,000 times and when he discoveredthis, he realised the risks these systems posed.[3]On January 2023 three artists (Sarah Andersen, Kelley McKernan,and Karla Ortiz) filed a class action against three companies offering AI image generators (Stability AI, Midjourney, DeviantArt). In theircomplaint, the artists argue that the companies obtained access totheir copyrighted works through web scraping. The outputs arederivative works of the images it draws from as they permit usersto create works \u201cin the style of...\u201d . They also claim that the AI-created works could compete against their own existing work inthe marketplace. In the following weeks, Getty Images initiatedcopyright infringement legal proceedings against Stability AI bothin a US and an UK court, with similar claims .",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "e4b3570a-e071-4f96-bf98-275e4154d7a8",
                    "text": "In 2022 a California-based AI artist with the pseudonym, Lapine,searched \u201cHave I Been Trained\u201d, a website created by artists to helpother artists to find out if their work has been used to train algo-rithms. Instead of her artwork, Lapine uploaded a medical photo andfound out that her photo, which depicted her rare medical condition,was included in the image set. The photo had been circulating onthe internet without her knowledge and consent. LAION-5B doesnot host the images but simply points to other websites from whichusers can upload them. Lapine will need to discover the hostingwebsite and ask to be removed. She is distressed by the fact thather photos have been used to train algorithms and possibly createAI-generated images without her consent, and her experience haschanged how she will engage with these tools in the future. Shefeels reluctant to use AI art and hesitates to use generated photore-alistic images of people for fear of unknowingly violating others\u2019rights. [2]",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "db3993a2-fd58-4ede-ab0b-4cf4693277c3",
                    "text": "Fears are also expressed that as AI generators are trained by datasetsthat are not diverse enough, any work based on them could beamplifying or perpetuating racist and sexist prejudice and racialstereotypes. According to Tracey Spicer, a journalist and socialjustice advocate, if someone asks AI-generated art platforms forimages of a CEO, it\u2019s generally an older white male. If the usersearch \u201cnurses\u201d, the images are almost all females and if the userdoes not specify skin colour, the default image will be white people.[32] Also, these AI models shown bias against certain groups, likeMuslims, whom it disproportionately associates with violence andterrorism[29].",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "75c5ec96-375e-40de-9029-e8070eaf1be4",
                    "text": "An AI-generated children\u2019s book that was created in a weekendcaused strong reactions. [27] Children\u2019s book author and illustra-tor, Rob Biddulph, said that AI-generated art \u201cis the exact oppositeof what I believe art to be. Fundamentally, I have always felt thatart is all about translating something that you feel internally intosomething that exists externally. And simply pressing a button togenerate an image is not a creative process.\u201d Artists are concernedthat these tools undermine artistic work and reinforce opinionsthat artists\u2019 work is easy, and they do not need to be well-paid.[30]In January 2023 a Nick Cave fan asked ChatGPT to create a songin the style of his favourite artist \u2013 he then sent it to Nick Cave.Cave responded, calling it a \u201cgrotesque mockery of what it is to bea human\u201d. He explained his artistic process as following: \u201c it is thebreathless confrontation with one\u2019s vulnerability, one\u2019s perilousness,one\u2019s smallness, pitted against a sense of sudden shocking discovery; itis the redemptive artistic act that stirs the heart of the listener, wherethe listener recognizes in the inner workings of the song their ownblood, their own struggle, their own suffering.\u201d [7] AI could convertart \u2013 an inner process of self-revelation, of asking deep questionsabout life, of creating a desire for truth and authenticity, of deephuman communication \u2013 into a fast, fun, and shallow substitute.",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "c5283b88-db08-4fb3-8373-ad2048ccb74e",
                    "text": "Another ethical issue is whether the artist has the obligation toreveal if he used generative AI and under what circumstance. TheColorado Fair Contest\u2019s winner explained that he decided to keepit a secret in an attempt to prove the artistic potential of AI. Hisdecision was perceived as an attempt to cheat. People argued thathe should return the award and post a public apology .[17] Thissuggests an unsettling feeling when one discovers that a work hasbeen created by algorithm \u2013 as though something inauthentic hasentered the relationship between the artist and the audience. It is afeeling of betrayal, cheating, or fraud. The experience instils thefear of being deceived in the future. The audience cannot easilydistinguish AI art from human art and this confusion is expectedto become even deeper.[37]",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "489a6ad1-6a4c-4490-b3c5-6c067870f691",
                    "text": "AI platforms\u2019 supporters claim that these platforms can \u2019democra-tise\u2019 the creative process, as such tools put the power of creativ-ity into the hands of more people, enabling them to participatein high-value content creation.[14] Artists question whether this\u2019democratisation\u2019 will benefit society as whole or just a few privatecompanies who can monetise the \"essence\" of living artists\u2019 workand appropriate humanity\u2019s collective imagination and knowledge.[9]As artists are not compensated, they fear that these technologiesmay have a detrimental impact on their income. Furthermore, up-and-coming artists could lose assignments for small projects, whichthey often rely on to build up their portfolio[30]. There is also arisk of artistic skills and forms of art atrophying.",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "8f31359d-63a6-4bbc-9b86-e2c8d705c380",
                    "text": "Generative AI tools rely on having access to and analysing largenumbers of artworks in order to produce results. Although thistext- and data-mining infringes copyright, there is wide disparityin the scope of exceptions in national copyright laws as regards fairuse.[13] , [6] These exceptions are not clear and have not adequatelytaken into consideration the recent AI generative art paradigm.In June 2022 UK government announced a plan to introduce anew copyright and database exception which allows data miningfor any purpose to foster AI innovation \u2013 with no opt-out forrights holders. After discussion with representatives of creativeindustries, the government decided to rethink this. There is also legal uncertainty about the copyright status of AI-created works.[8], [4]There are debates about who will be considered as the authorof these works (prompter? investor? developer?) and under whichconditions/ requirements.[5] [36], [25] [15]New legal provisionsthat would balance these conflicting interests and rights could resultin more trustworthy and socially acceptable innovations.",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "b065c58b-ec8a-45ec-bcd0-4cc0f9c7b350",
                    "text": "Many experts agree that algorithms and AI should be \u201cethical bydesign\u201d with no in-built bias, created in a way that guaranteesthe respect of citizens\u2019 fundamental rights, and the avoidance ofpotential liability. [22]The main principles are: explainability, fair-ness, accountability, transparency. Of key importance is also theresponsibility principle, to ensure the availability of measures toredress adverse individual or social effects of an algorithmic systemand designate a person responsible for the timely remedy of theseissues. [11]Some generative AI platforms attempt to address these principles.They may publish terms of use, have AI ethics research teams, orcooperate to formulate best practice. A few companies, such asStability AI, accept that it is unfair to use the work of millionsof artists without any credit or compensation and are willing tocorrect the injustice[24] Some platforms inform users about whichdatasets they use and point out that other platforms, such as OPENAI, do not.[2] It seems more difficult to comply with the principlesof fairness and explainability. These datasets have in-built bias [29]and it is still difficult to explain how they work.[31] Some techcompanies attempt to confront the legal and ethical challenges butthese efforts vary and additional measures are needed[19].",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "239b81a3-fdc0-4057-9501-0c53526cf76d",
                    "text": "In recent years, agencies have sought to regulate AI \u2013 in the UK,policymakers are not currently considering regulation beyond theOnline Safety Bill, but have not excluded the possibility. The EU,USA and China have all proposed new AI regulation. Artists\u2019 associ-ations are suggesting a specific section in the EU AI Act dedicated tothe creative arts, including safeguards requiring that rights-holdersgive explicit, informed consent before their work is used by AItools.[21]Some governments and international organizations propose AIrights, principles and measures for the design and development ofAI applications. Oversight and public consultation are consideredimportant measures to protect human rights.",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "1b03ccac-9a33-43e5-ad74-5c832c403d04",
                    "text": "RI\u2019s purpose is to create processes that enable the exploration ofinnovation and its potential consequences in an open, inclusive, andtimely way. Developers of new technologies should be trained toengage with stakeholders to anticipate pathways, reflect on impact,and respond accordingly. [33]The researchers who developed Dramatron, an interactive co-writing tool, have followed such processes. They assessed possiblerisks, invited artists to write screenplays and theatre scripts usingthis tool and finally asked them to reflect on the process and theirresponse to it.[26]RI can provide an opportunity for people that are often excludedfrom these discussions to be heard and to influence legislation andpolicies. AI-generated art can challenge people\u2019s values and theirperception of the world, but using RI principles to engage artists,critics and audience in the debate could contribute to identify socialimplications and potential risks of generative AI-based art. Theycould contribute to development of technologies responsible bydesign[1] This would provide an opportunity to the artistic commu-nity to be prepared for future development which may affect themdirectly. For example art schools could teach students about thesenew tools, and legal and ethical ways to use them . Art contestscould have a category purely devoted to AI art. Additionally, thepublic could learn how to identify content that is created by AI andwhat is best practice for using these tools.",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                },
                {
                    "id": "fc6c93fd-c651-4a9e-b350-cc3b973ee4e5",
                    "text": "The multiple legal and ethical implications discussed here threatenthe trustworthiness and acceptability of these new technologies.Deployed in responsible ways, however, generative AI tools couldnot only enhance the work of artists, but be used to enable thecreation of entirely new forms of art and expression. Artists acceptthe potential of this growing genre of tool and do not ask for themto be banned, but they want to control their level of involvement,and for their rights to be protected. Mostly they want to safeguardwhat they see as important aspects of human creativity, and bereassured that these will not be undermined or made redundantby potential AI technological storms. Initiating such discussions,discourses and debate is part of our future research goal, as part ofour Responsible AI approach.",
                    "reference": "[1] Amanda Piskopani and Alan Chamberlain. 2023. Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries. In Proceedings of the First. Retrieved from https://ora.ox.ac.uk/objects/uuid:a803ecc0-c8cc-42a0-be17-848334dc3cf5/files/r5d86p1015"
                }
            ]
        }
    ]
}